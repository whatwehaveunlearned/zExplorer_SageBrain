index,pdf_file,text,topics,year,author,clusterID,citations,type,abstract,globalID,tags,user,words,pages,citationsList,versions,url,notes,title,parent_item,organization,citationArticles
HDQVL9W2,UYMW6IC5,"Semantic Interaction for Visual Text Analytics 
Chris North 
Alex Endert 
Virginia Tech 
Virginia Tech 

Patrick Fiaux 
Virginia Tech 

Blacksburg, VA USA 

aendert@vt.edu 

Blacksburg, VA USA 

pfiaux@vt.edu 

 

Blacksburg, VA USA 

north@vt.edu 

by 

For 

through 

ABSTRACT 
Visual analytics emphasizes sensemaking of large, complex 
datasets 
interactively  exploring  visualizations 
generated 
example, 
statistical  models. 
dimensionality  reduction  methods  use  various  similarity 
metrics to visualize textual document collections in a spatial 
metaphor,  where  similarities  between  documents  are 
approximately  represented  through  their  relative  spatial 
distances  to  each  other  in  a  2D  layout.  This  metaphor  is 
designed to mimic analysts’ mental models of the document 
collection  and  support  their  analytic  processes,  such  as 
clustering similar documents together. However, in current 
methods, users must interact with such visualizations using 
controls  external  to  the  visual  metaphor,  such  as  sliders, 
menus, or text fields, to directly control underlying model 
parameters  that  they  do  not  understand  and  that  do  not 
relate  to  their  analytic  process  occurring  within  the  visual 
metaphor.  In  this  paper,  we  present  the  opportunity  for  a 
new  design  space  for  visual  analytic  interaction,  called 
semantic  interaction,  which  seeks  to  enable  analysts  to 
spatially interact with such models directly within the visual 
metaphor using interactions that derive from their analytic 
process,  such  as  searching,  highlighting,  annotating,  and 
repositioning  documents.  Further,  we  demonstrate  how 
semantic  interactions  can  be  implemented  using  machine 
learning 
tool,  called 
ForceSPIRE, for interactive analysis of textual data within 
a  spatial  visualization.    Analysts  can  express  their  expert 
domain knowledge about the documents by simply moving 
them,  which  guides  the  underlying  model  to  improve  the 
overall layout, taking the user’s feedback into account. 
Author Keywords 
Visualization; visual analytics; interaction 
ACM Classification Keywords 
H5.m.  Information  interfaces  and  presentation  (e.g.,  HCI): 
Miscellaneous.  
General Terms 
Design; Human Factors; Theory 

in  a  visual  analytic 

techniques 

 
Permission to  make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CHI’12, May 5–10, 2012, Austin, Texas, USA. 
Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00. 
 

 

INTRODUCTION 
Visual analytics bases its success on combining the abilities 
of statistical models, visualization, and human intuition for 
users to gain insight into large, complex datasets [23]. This 
success often hinges on the ability for users to interact with 
the  information,  manipulating  the  visualization  based  on 
their  domain  expertise,  interactively  exploring  possible 
connections, and investigating hypotheses. It is through this 
interactive exploration that users are able to make sense of 
complex  datasets,  a  process  referred  to  as  sensemaking 
[19].  
The  two  primary  parts  of  sensemaking  are  foraging  and 
synthesis. Foraging refers to the stages of the process where 
users filter and gather collections of interesting or relevant 
information.  Then,  using  that  information,  users  advance 
through  the  synthesis  stages  of  the  process,  where  they 
construct  and  test  hypotheses  about  how  the  foraged 
information  may  relate  to  the  larger  plot.  Tools  exist  that 
support users for either foraging or synthesis – but not both. 
In  this  paper  we  present  semantic  interaction,  combining 
the  foraging  abilities  of  statistical  models  with  the  spatial 
synthesis abilities of analysts. Semantic interaction is based 
on the following principles: 
1. Visual  “near=similar”  metaphor  supports  analysts’ 
spatial  cognition,  and  is  generated  by  statistical  models 
and similarity metrics. [22] 

2. Use  semantic  interactions  within  the  visual  metaphor, 
based  on  common  interactions  occurring  in  spatial 
analytic  processes  [4]  such  as  searching,  highlighting, 
annotating, and repositioning documents.  

3. Interpret  and  map  the  semantic  interactions  to  the 
underlying parameters of the model, by updating weights 
and adding information. 

4. Shield  the  users  from  the  complexity  of  the  underlying 

mathematical models and parameters. 

5. Models  learn  incrementally  by  taking  into  account 
interaction during the entire analytic process, supporting 
analysts’ process of incremental formalism [10]. 

6. Provide  visual  feedback  of  the  updated  model  and 

learned parameters within the visual metaphor. 

7. Reuse  learned  model  parameters  in  future  or  streaming 

data within the visual metaphor. 

To  demonstrate  the  concept  of  semantic  interaction,  we 
present  a  prototype  visual  analytics  tool,  ForceSPIRE,  for 
spatial analysis of textual information. In ForceSPIRE, the 
user  interaction  takes on  a deeper,  more  integrated role in 

the  exploratory  spatial  analytic  process.  This  is  done 
through capturing the semantic interaction, interpreting the 
analytical  reasoning  associated  with  the  interaction,  and 
updating the statistical model, and ultimately updating the 
spatialization.  Hence,  users  are  able  to  leverage  semantic 
interaction  to  explore  and  analyze  the  data  interactively, 
while  the  system  is  responsible  for  properly  updating  the 
underlying statistical model.  
RELATED WORK 
Foraging Tools 

 

Figure  1.  A  model  of  interaction  with  foraging  tools.  Users 
interact  directly  with  the  statistical  model  (red),  then  gain 
insight  through  observing  the  change  in  the  visualization 
(blue). 
We  categorize  foraging  tools  by  their  ability  to  pass  data 
through  complex  statistical  models  and  visualize  the 
computed structure of the dataset for the user to gain insight 
(Figure  1).  Thus,  users  interact  with  these  tools  primarily 
through directly manipulating the parameters of the model 
used  for  computing  the  structure.  As  such,  users  are 
required  to  translate  their  domain  expertise  and  semantics 
about  the  information  to  determine  which  (and  by  how 
much) to adjust these parameters. The following examples 
further describe this category of tools. 
Visualizations such as IN-SPIRE’s “Galaxy View” (shown 
in  Figure  3)  present  users  with  a  spatial  layout  of  textual 
information where similar documents are proximally close 
to  one  another  [25].  An  algorithm  creates  the  layout  by 
mapping the high-dimensional collection of text documents 
down  to  a  two-dimensional  view.  In  these  spatializations, 
the  spatial  metaphor  is  one  from  which  users  can  infer 
meaning  of  the  documents  based  on  their  location.  The 
notion  of  distance  between  documents  represents  how 
similar the two documents are (i.e., more similar documents 
are  placed  closer  together).  For  instance,  a  cluster  of 
documents  represents  a  group  of  similar  documents,  and 
documents  placed  between  two  clusters  implies  those 
documents are connected to both clusters. These views are 
beneficial  as  they  allow  users  to  visually  gain  a  quick 
overview  of  the  information,  such  as  what  key  themes  or 
groups  exist  within  the  dataset.  The  complex  statistical 
models  that  compute  similarity  between  documents  are 
based on the structure within the data, such as term or entity 
frequency. In order to interactively change the view, users 
are  required  to  directly  adjust  keyword  weights,  add  or 
remove documents/keywords, or provide more information 
on how to parse the documents for keywords/entities upon 
import. 

 

to  a 

to  understand 

the 

[15].  Through  adjusting 

Similarly, an interactive visualization tool called iPCA uses 
Principal  Component  Analysis  (PCA)  to  reduce  high-
dimensional  data  down 
two-dimensional  plot, 
providing  users  with  sliders  and  other  visual  controls  for 
directly  adjusting  numerous  parameters  of  the  algorithm, 
such  as  individual  eigenvalues,  eigenvectors,  and  other 
components  of  PCA 
the 
parameters,  the  user  can  observe  how  the  visualization 
changes.  This  allows  users  to  gain  insight  into  a  dataset, 
given  they  have  a  thorough  understanding  of  PCA, 
necessary 
the 
changes they are making to the model parameters. 
Alsakran  et  al.  presented  a  visualization 
system, 
STREAMIT,  capable  of  spatially  arranging  text  streams 
based  on  keyword  similarity  [3].  Again,  users  can 
interactively  explore  and  adjust  the  spatial  layout  through 
directly  changing  the  weight  of  keywords  that  they  find 
important.  In  addition,  STREAMIT  allows  for  users  to 
conduct  a  temporal  investigation  of  how  clusters  change 
over time. 
Synthesis Tools 

implications  behind 

 

Figure  2.  A  model  of  interaction  with  synthesis  tools.  Users 
manually  create  a  spatial  layout  of  the  information  to 
maintain and organize their insights about the data. 
Synthesis  tools  focus  on  allowing  users  to  organize  and 
maintain their hypotheses and insight regarding the data in 
a  spatial  medium.  In  large  part,  this  is  done  through 
presenting users with a flexible spatial workspace in which 
they  can  organize  information  through  creating  spatial 
structures,  such  as  clusters,  timelines,  stories,  etc.  (Figure 
2). In doing so, users externalize their thought processes (as 
well  as  their  insights)  into  a  spatial  layout  of  the 
information. 
For example, Analyst’s Notebook [2] provides users with a 
spatial workspace where information can be organized, and 
connections  between  specific  pieces  of  information  (e.g., 
entities, documents, events, etc.) can be created. Similarly, 
The Sandbox [26] enables users to create a series of cases 
(collections  of 
information)  which  can  be  organized 
spatially within the workspace.  
From  previous  studies,  we  found  cognitive  advantages 
associated  with  the  manual  creation  of  a  spatial  layout  of 
the  information  [4].  By  providing  users  a  workspace  in 
which  to  manually  create  spatial  representations  of  the 
information, users were able to externalize their semantics 
of the information into the workspace. That is, they created 
spatial  structures  (e.g.,  clusters,  timelines,  etc.),  and  both 
the structures as well as the locations relative to remaining 
layout  carried  meaning  to  the  users  with  regards  to  their 
sensemaking process. Marshall et al. have pointed out that 

this 

interaction  (and 

From  the  sensemaking  loop  presented  by  Pirolli  and  Card 
[19],  we  learn  that  in  intelligence  analysis,  that  analytic 
process  consists  not  only  of  the  information  that  is 
explicitly  within  the  dataset  being  analyzed,  but  also  the 
domain knowledge of the analyst performing the analysis. It 
is through this domain knowledge that analysts interact and 
explore  the  dataset  to  “make  sense”  of  the  information. 
Thus,  we  believe 
the  domain 
knowledge  associated  with  it)  is  equally  important  as  the 
raw data, and must be incorporated into the visualization by 
tightly coupling the model with the interaction. 
From this body of work, we most notably come away with 
an understanding that 1) analysts fundamentally understand 
the spatial metaphor used in many spatial visualizations, 2) 
many  of  these  systems  are  constructed  using  complex 
mathematical  algorithms  to  transform  high-dimensional 
data  to  two  dimensions,  and  3)  in  most  cases  these 
algorithms  can  be  controlled  by  analysts  largely  through 
visual  controls  (e.g.,  sliders,  knobs,  etc.)  to  directly  adjust 
parameters of the algorithms, updating the spatial layout. 
SEMANTIC INTERACTION 

 

Figure 4. A model of semantic interaction. Users are able to 
interact directly in the spatial metaphor. The system updates 
the corresponding parameters of the statistical model based on 
the analytic reasoning of the users. Finally, the model updates 

the visualization based on the changes, thus unifying the 
synthesis and foraging stages of the sensemaking loop. 

In the purest sense, semantic interaction refers to interaction 
occurring  within  a  spatial  visualization,  with  the  added 
benefit that it is tightly coupled to the model calculating the 
spatial layout (Figure 4). Given the previous work of what 
interaction  in  visual  analytic  tools  is,  semantic  interaction 
occupies a new design space for interaction. It merges the 
ability to change the statistical model while maintaining the 
flexibility  and  familiar  methods  for  interacting  within  the 
metaphor  of  spatial  visualizations.  Users  can  benefit  from 
semantic  interactions  in  that  they  can  interact  within  a 
metaphor  which 
they  are  familiar  with,  performing 
interactions  which  are  part  of  the  spatial  analytic  process 
[4], without having to focus on formal updates to the model.  
Semantic  interaction  leverages  the  cognitive  connection 
formed  between  the  user  and  the  spatial  layout.  The 
following intelligence analysis scenario is representative of 
the strategies and interactions of analysts when performing 
an  intelligence  analysis  task  of  textual  documents  in  a 
spatial visualization, as previously found by Andrews et al. 
[4],  and  further  motivates  and  explains  the  concept  of 
semantic interaction: 

 
Figure  3.  The  IN-SPIRE  Galaxy  View  showing  a 
spatializtiation  of  documents  represented  as  dots.  Each 
cluster of dots represents a group of similar documents.  
 
allowing users to create such informal relationships within 
information  is  beneficial,  as  it  does  not  require  users  to 
formalize these relationships [17].  
From this related work, we believe a trend is emerging in 
how interaction is currently handled in many visual analytic 
systems where complex statistical models are used – users 
are  required  to  go  outside  of  the  metaphor.  That  is,  while 
the  visual  representation  given  to  users  is  spatial,  the 
methods of interaction require users to step outside of that 
metaphor  and  interact  directly  with  the  parameters  of  the 
statistical model using visual controls, toolbars, etc.  
There  has  been  some  work  in  providing  more  easy  to  use 
interactions  for  updating  statistical  models.  For  example, 
relevance feedback has been used for content-based image 
retrieval, where users are able to move images towards or 
away  from  a  single  image  in  order  to  portray  pair-wise 
similarity  or  dissimilarity  [24].  From  there,  an  image 
retrieval algorithm determines the features and dimensions 
shared between the images that the user has determined as 
being  similar.  We  view  this  as  one  example  where  the 
interaction stays in the spatial metaphor of the visualization.  
Also, spatializations of document sets exist that allow users 
to place “points of interest” into the spatial layout. In VIBE, 
users are allowed to define multiple points of interest in the 
spatial  layout  that  correspond  to  a  series  of  keywords 
describing  a  subject  matter  of  interest  to  the  user  [18]. 
Similarly,  Dust  &  Magnet  [27]  allows  users  to  place  a 
series  of  “magnets”  representing  keywords  into  the  space 
and observe how documents are attracted or repelled from 
the  locations  of  these  magnets.  Through  both  of  these 
systems, users can interact in the spatial metaphor through 
these  placements  of  “nodes”  representing  keywords. 
However, the focus of semantic interaction is on interacting 
with  data  (i.e.,  documents),  an 
important  distinction 
discussed in the following section. 

 

 

 
Figure  5.  (top)  The  basic  version  of  the  “visualization 
pipeline”.  Interaction  can  be  performed  on  directly  the 
Algorithm  (blue  arrow)  or  the  data  (red  arrow).  (bottom) 
Our  modified  version  of 
for  semantic 
interaction,  where  the  user  interacts  within  the  spatial 
metaphor (purple arrow). 

the  pipeline 

During her analysis, an intelligence analyst finds a 
suspicious  and 
interesting  phrase  within  a 
document. While reading through the document, she 
highlights  the  phrase  “suspicious  individuals  were 
spotted  at  the  airport”,  in  order  to  more  easily 
recall  this  information  later.  After  she  finishes 
reading the document, she moves the document into 
the  bottom  right  corner  of  her  workspace,  in  the 
proximity of other documents related to an event at 
an airport. To remind herself of her hypothesis, she 
annotates  the  document  with  “might  be  related  to 
Revolution  Now  terrorist  group”.  Now,  with  the 
goal  of 
the 
“airport”, she searches for the term, continuing her 
investigation. 

further  examining 

the  events  at 

investigating 

that  each  of 

instead  point  out 

the  analytic  process  of 

In addition to the three forms of semantic interaction in the 
scenario,  Table  1  provides  a  list  of  various  forms  of 
semantic  interaction,  including  how  each  can  be  used 
within 
textual 
information  spatially.  We  do  not  claim  that  this  list  is 
complete,  but 
these 
interactions  can  relate  to  a  user’s  reasoning  within  the 
analytic process.  
Designing for Semantic Interaction 
In order for analysts to interact with information in a spatial 
metaphor, it must first be created. Following the model of 
the visualization pipeline [13], this creation calls for a series 
of  mathematical  transformations,  turning  raw  data  into  a 
spatial  layout  –  much  the  way  many  of  the  visualizations 
mentioned  previously  are  constructed.  However,  these 
visualizations  fit  this  model,  as  their  user  interactions  are 
primarily  focused  on  directly  modifying  the  statistical 
model  (as  well  as  other  attributes  of  the  visualization  or 
data  transformation).  Designing  for  semantic  interaction 
requires  a  fundamentally  different  model  for  how  tools 
integrate  user  interaction  –  one  that  can  capture  the 
interaction,  interpret  the  associated  analytical  reasoning, 
and update the appropriate mathematical parameters.  
Figure  5  illustrates  this  model,  where  the  spatialization  is 
treated  a  medium  through  which  the  user  can  perceive 

 

Figure 6. Overview of how nodes and edges in ForceSPIRE’s 
force-directed layout are created from documents (Doc) and 
entities (Ent), respectively.  

 

 

it 

interaction, 

information  and  gain  insight,  as  well  as  interact  and 
perform  his  analysis.  Through  expanding  the  pipeline  to 
accommodate  for  semantic 
is  a  more 
appropriate match to the user’s sensemaking process. 
Capturing the Semantic Interaction 
A  non-trivial  first  step  in  the  model  is  capturing  the  user 
interaction.  Much  research  has  been  done  in  this  area, 
primarily  for  the  purpose  of  maintaining  process  history 
(e.g., [5], [21], [12], etc.). When considering how to capture 
interaction,  one  decision  to  be  made  is  at  what  “level”  to 
capture  it.  For  example,  GlassBox  [6]  captures  interaction 
at a rudimentary level (i.e. mouse clicks and key strokes), 
while  Graphical  History  [14]  keeps  track  of  a  series  of 
previous  visualizations  as  a  user  changes  the  visualization 
during the exploration of the data.  
Semantic  interaction  is  captured  at  a  data  level,  as  the 
interactions  occur  on  the  data,  and  within  the  spatial 
metaphor.  Using 
the 
interaction being captured would be: 

the  earlier  analytic  scenario, 

•  The highlighted phrase 
•  When the highlighting occurs (timestamp) 
•  The color chosen for the highlight 
•  The document in which the highlight occurs 
•  The new document location 
•  The text of the annotation 

By  capturing  (and  storing)  the  interaction  history,  we  can 
interpret the analytical reasoning of the user. Thus, we not 
only capture the interaction, but also use it. 
Interpreting the Associated Analytical Reasoning 
In interpreting the interaction, the goal is for the system to 
determine  the  analytical  reasoning  associated  with  the 
interactions  and  update  the  model  accordingly.  From 
previous findings [4], we can associate analytical reasoning 
with  forms  of  semantic  interaction  (see  Table  1).  It  is 
essentially the model’s task to determine  why, in terms of 
the data, the interaction occurred. To answer this question, 
we do not propose that this model can accurately gauge user 
intent.  Instead,  the  goal  is  to  calculate,  based  on  the  data, 

Figure 7. Using ForceSPIRE on a 32 megapixel large, 
high-resolution display. 

 

 
what information is consistent with the captured interaction. 
For  instance,  we  associate  text  highlighting  with  adding 
importance to the text being highlighted. We do not claim 
that we can associate the interaction of highlighting to the 
intuition that spurred the analyst to highlight the text, which 
is far more challenging, and arguably impossible. 
We refer to the captured and interpreted interactions as soft 
data, in comparison to the hard data that is extracted from 
the raw textual information (e.g., term or entity frequency, 
titles,  document  length,  etc.).  We  define  soft  data  as  the 
stored result of user interaction as interpreted by the system. 
In  representing  interaction  as  soft  data,  the  algorithm  can 
calculate  and  reconfigure  the  spatial  layout  accordingly. 
Figure  5  illustrates  how  our  approach  differs  from  the 
traditional visualization pipeline. 
There has been previous work in capturing and interpreting 
reasoning from user interaction. For instance, Dou et al. [7] 
performed  a  study  where  financial  analysts  were  asked 
analyze  a  dataset  using  WireVis,  an  interactive  financial 
transaction visualization. The tool developers then analyzed 
the captured interaction, and assumptions were made about 
the  reasoning  of  the  analysts  at  specific  points  in  the 
investigation. These results were compared to the analysts’ 
self-recorded  reasoning,  and  found  to  be  accurate  up  to 
82%. While our work has similar goals (i.e., interpreting the 
analytical reasoning associated with the analysts through an 
evaluation  of  the  interaction)  our  model  does  so  through 
tightly  integrating  the  interaction  with  the  underlying 
mathematical model. In doing so, the interpretation can be 
done algorithmically. 
Updating the Underlying Model 
Through  metric  learning  of  distance  weights,  the  layout 
uses  the  soft  data  to  update  the  underlying  model. 
Depending  on  the  algorithm  used  to  compute  the  spatial 
layout,  the  precise  parameters  being  updated  will  vary.  In 
general,  this  will  refer  to  weighting  of  a  combination  of 
dimensions  that  will  help  guide  the  model  as  to  which 
dimensions the user finds important.  
FORCESPIRE: SYSTEM OVERVIEW 
ForceSPIRE  is  a  visual  analytics  prototype  designed  for 
specific 
(document 
movement,  text  highlighting,  search,  and  annotation)  for 

forms  of 

interaction 

semantic 

 

Figure  8.  Moving  the  document  shown  by  the  arrow, 
ForceSPIRE  adapts  the  layout  accordingly.  Documents 
sharing entities with the document being moved follow. 

 

interactively exploring textual data. The system has a single 
spatial  view  (shown  in  Figure  12),  where  a  collection  of 
documents is represented spatially based on similarity (i.e., 
documents closer together are more similar).  
ForceSPIRE is designed for large, high-resolution displays 
(such  as  the  one  shown  in  Figure  7).  As  semantic 
interaction emphasizes the importance of context in which 
the  interaction  takes  place  (e.g.,  highlighting  text  in  the 
context  of  the  document),  having  the  full  detail  text 
available  in  the  context  of  the  spatial  layout  is  beneficial 
over having a single document viewer. Further, the physical 

Table  1.  Forms  of  semantic  interaction.  Each  interaction 
corresponds  to  reasoning  of  users  within  the  analytic 
process. 

Form of Semantic 

Interaction 

Document Movement 

Text Highlighting 

Pinning  Document 
Location 
Annotation, “Sticky Note” 

to 

Document Coloring 

Level of Visual Detail 

Query Terms 
 

Associated Analytic Reasoning 

• Similarity/Dissimilarity 
• Create 

spatial  construct 

timeline, list, story, etc) 

• Test 

hypothesis, 

see 
document “fits” in region 

(.e.g 

how 

• Mark 

importance  of  phrase 

(collection of entities) 

• Augment  visual  appearance  of 

document for reference 

to 

in 

• Give 

semantic  meaning 

space/layout 

• Put 

semantic 

information 

workspace, within context 
• Create visual group/cluster 
• Mark group membership 
• Change 

ease 

of 

visually 
referencing  information  (e.g.  full 
detail = more important = easy to 
reference) 

• Expressive search for entity 

(and 

to  match 

is  positioning 

Semantic Interaction in ForceSPIRE 
The  semantic  interactions  in  ForceSPIRE  are:  placing 
information  at  specific  locations,  highlighting,  searching, 
and annotating in order to incrementally change the spatial 
layout 
their  mental  model.  The  primary 
parameters  of  the  force-directed  model  that  are  being 
updated  through  this  learning  model  are  the  importance 
values of the entities.  
Document  Movement.  The  predominant  interaction  in  a 
spatial  workspace 
repositioning) 
documents.  In  previous  work,  we  have  demonstrated  how 
users can perform both exploratory and expressive forms of 
this type of interaction [9]. In ForceSPIRE, we allow for the 
following  exploratory  interaction  (i.e.,  interaction  that 
allows users to explore the structure of the current model, 
but  does  not  change  it).  Users  are  able  to  interactively 
explore the information by dragging a document within the 
workspace, pinning a document to a particular location (see 
Figure  8),  as  well  as  linking  two  documents.  When 
dragging a document, the force-directed system responds by 
finding the lowest energy state of the remaining documents 
given  the  current  location  of  the  dragged  document. 
Mathematically, this adds a constraint to the stress function 
being  optimized  (in  this  case  the  force-directed  model). 
This  allows  users  to  explore  the  relationship  of  that 
document in comparison to the remaining documents.  
In addition to the exploratory dragging of a document, users 
have the ability to pin a document. By pinning a document, 
users  are  able  to  incrementally  add  semantic  meaning  to 
locations in their workspace. By specifying key documents 
to  user-defined  locations,  the  layout  of  the  remaining 
documents will adapt to these constraints. Thus, users can 
explore  how  documents  are  positioned  based  on  their 
similarity  (or  dissimilarity)  to  the  pinned  documents.  For 
instance,  if  the  layout  places  a  document  between  two 
pinned  documents, 
the  particular 
document holds a link between the two pinned documents, 
sharing entities that occur in both. 
Finally,  users  can  perform  an  expressive  form  of  this 
interaction  by  linking  two  documents,  performed  by 
dragging  one  document  onto  another  pinned  document.  In 
doing so, ForceSPIRE calculates the similarity between the 
documents,  and  increases  the  importance  value  of  the 
entities  shared  between  both  documents.  As  a  result,  the 
layout will place more emphasis on the characteristics that 
make those two documents similar. 
Highlighting.  When  highlighting  a  term,  ForceSPIRE 
creates an entity from the term (if not already one), and the 
importance  value  of  that  term  is  increased.  Similarly, 
highlighting  a  phrase  results  in  the  phrase  being  first 
parsed for entities, then increasing the importance value of 
each  of  those  entities.  For  example,  Figure  11  shows  the 
effect of highlighting the terms “Colorado” and “missiles” 
in the document pointed to with the arrow. As a result, the 

it  may 

imply 

that 

 
Figure  9.  The  Effect  of  adding  an  annotation  (“these 
individuals  may  be  related  to  Revolution  Now”)  to  the 
document shown with an arrow. As  a result,  the document 
becomes 
linked  with  other  documents  mentioning  the 
terrorist organization “Revolution Now”.  

presence of these displays creates an environment in which 
the  virtual  information  (in  this  case  the  documents)  can 
occupy  persistent  physical  space.  As  a  result,  users  are 
further  immersed  into  the  spatial  metaphor,  as  they  can 
point and quickly refer to information based on the physical 
locations.  
Constructing the Spatial Metaphor 
The spatial layout of the text documents is determined by a 
modified  version  a  force-directed  graph  model  [11].  This 
model  functions  on  the  principle  of  nodes  with  a  mass 
connected  by  springs  with  varying  strengths.  Thus,  each 
node has attributes of attraction and repulsion: nodes repel 
other  nodes,  and  two  nodes  attract  each  other  only  when 
connected  by  a  spring  (edge).  The  optimal  layout  is  then 
computed  by  iteratively  calculating  these  forces  until  the 
lowest energy state of all the nodes is reached. A complete 
description of this algorithm can be found in [11].  
We  apply  this  model  to  textual  information  by  treating 
documents  as  nodes  (an  overview  is  shown  in  Figure  6). 
The entire textual content of each document is parsed into a 
collection  of  entities  (i.e.,  keywords).  The  number  of 
entities corresponds to the mass of each document (heavier 
nodes  do  not  move  as  fast  as  lighter  nodes).  A spring  (or 
edge) represents one or more matching entities between two 
nodes.  Therefore,  the  initial  distance  metric  is  a  based  on 
co-occurrence  of  terms  between  documents.  For  example, 
two  documents  containing  the  term  “airport”  will  be 
connected  by  a  spring.  The  strength  of  a  spring  (i.e.  how 
close together it tries to place two nodes) is based on two 
factors:  the  number  of  entities  two  documents  have  in 
common,  and  the  importance  value  associated  with  each 
shared entity (initially, importance values are created using 
a  standard  tfidf  method  [16]).  The  sum  of  all  importance 
values add up to 1. 
The resulting spatial layout is one where similarity between 
documents  is  represented  by  distance  relative  to  other 
documents.  Similarity  in  this  system  is  defined  by  the 
strength of the spring between two documents. A stronger 
spring  (and  therefore  a  larger  amount  of  shared  entities) 
will pull two documents closer together, and thus represent 
two similar documents. 

 

 
Figure  10.  Searching  for  the  term  ”Atlanta”,  documents 
containing the term highlight green within the context of the 
spatial  layout.  Additionally,  the  importance  value  of  entity 
“Atlanta” is increased. 

other  documents  containing  that  term  are  clustered  more 
tightly. 
Searching.  When  coming  across  a  term  of  particular 
interest, analysts usually search on that term in order to find 
other  occurrences.  In  a  spatial  workspace,  this  is  of 
particular  importance,  because  the  answer  to  “where  the 
term  is  also  found”  is  not  only  given  in  terms  of  what 
documents,  but  also  where  in  the  layout  those  documents 
occur. The positions of documents containing the term are 
shown in context of the entire dataset, from which users can 
infer the importance of that term (as shown in Figure 10).  
ForceSPIRE  first  creates  an  entity  from  the  search  term 
(unless  it  is  already  one),  then  increases  the  importance 
value  of  the  search  term.  Figure  10  gives  an  example  of 
how a search result appears in ForceSPIRE. Searching for 
the  term  “Atlanta”,  documents  that  contain  the  term  are 
highlighted  green,  and  links  are  drawn  to  show  where  the 
resulting documents are in relation to the current document.  
Annotation.  Annotations  (i.e.,  “sticky  notes”)  are  also 
viewed as a form of semantic interaction, occurring within 
the analytic process, from which analytic reasoning can be 
inferred. When a user creates a note regarding a document, 
that semantic information should be added to the document. 
For example, if Document A refers to “Revolution Now” (a 
suspicious  terrorist  group),  and  Document  B  refers  to  “a 
group of suspicious individuals”, and the user has reason to 
believe  these  individuals  are  related  to  Revolution  Now, 
adding a note to Document B stating “these individuals may 
be  related  to  Revolution  Now”  is  one  way  for  the  user  to 
add semantic meaning to the document.  
ForceSPIRE  handles  the  addition  of  the  note  (shown  in 
Figure 9) by 1) parsing the note for any currently existing 
entities,  then  2)  increasing  the  importance  value  of  each, 
and 3) creating any new springs between other documents 
sharing these entities. In the example in Figure 9, edges are 
created between Document B and Document A (as well as 
any  other  documents  that  mention  “Revolution  Now”). 
Additionally,  if  the  note  contains  any  new  entities  not 
currently in the model, they are created, with the intent that 

 

 
Figure 11. The effect of highlighting a phrase containing the 
entites  “Colorado”  and  “missiles”.  Documents  containing 
these  entities  move  closer,  as  the  increase  in  importance 
value increases the edge strength.  

the 

importance  values  of 

any future entities that may match to that note can be linked 
at that time. ForceSPIRE also handles cases where notes are 
edited,  with  text  added  or  removed  from  the  note,  by 
updating  the  entities  associated  with  the  document,  and 
adjusting 
these  entities 
accordingly. 
Model Updates 
Each  of  the  semantic  interactions  in  ForceSPIRE  impacts 
the  model  by  updating  the  importance  values  of  entities, 
and  the  mass  of  each  document.  The  calculation  for 
updating the importance value of an entity is the same for 
each interaction. If an entity was “hit” (i.e., it was included 
in  a  highlight,  it  was  searched,  it  was  in  a  note,  etc.), 
ForceSPIRE increases its importance value by 10%. As the 
sum  of  all  importance  values  of  entities  adds  up  to  1, 
ForceSPIRE  subtracts  an  equal  amount  from  all  other 
entities’ importance values. As a result, importance values 
decay over time, and entities that are rarely used during the 
analysis  have  less  impact  on  the  layout.  The  mass  of  a 
document  uses  a  similar  calculation,  in  that  each  time  a 
document  is  “hit”  (i.e.,  text  was  highlighted,  it  was  the 
result of a search hit, etc.), it increases by 10%.  
When  undoing  an 
standard 
the 
“Control+Z”  keyboard  shortcut,  a  linear  history  of  the 
interactions will be reversed, and the importance values of 
affected  entities  will  be  returned  to  their  prior  values  (as 
well  as  document  masses).  As  for  the  locations  of  the 
documents,  the  reverted  importance  values  and  document 
masses  will  be  responsible  for  updating 
layout. 
However, this does not guarantee that the layout will return 
to  the  exact  previous  view,  and  the  user  may  find  it 
necessary to perform small adjustments. 
The model updates used in ForceSPIRE serve as an initial 
approach at how to couple semantic interactions with model 
updates. Other, more complex methods may exist, and we 
encourage  further  research  in  this  area.  Sensemaking  is  a 
complex exploratory process. As such, semantic interaction 

interaction  using 

the 

through 

more  central  documents.  While  reading 
the 
documents, he highlighted phrases of interest. For example, 
he highlighted the phrase “Nizar A. is now known to have 
spent six months in Afghanistan”. In doing so, ForceSPIRE 
increased  the  importance  value  of  the  entities  within  the 
phrase,  particularly  “Afghanistan”  and  “Nizar  A”.  As  a 
result, the layout forms more tightly around those entities. 
Each change incrementally changes the layout. 
Continuing  with  his  investigation,  he  began  searching  for 
words  of  interest  (e.g.,  “weapons”,  “Colorado”,  “Atlanta”, 
etc.). ForceSPIRE provided him with quick visual feedback 
on where in the dataset each terms showed up (the search 
result  for  “Atlanta”  is  shown  in Figure  10).  In  addition  to 
gaining an overview of the distribution of the term within 
the  dataset  (by  highlighting  each  document  containing  the 
term  green),  ForceSPIRE  treats  performing  a  search  as 
either  creating  a  new  entity  from  the  search  term,  or 
increasing the importance value if an entity corresponding 
to the search term already exists. As a result of the multiple 
search terms and highlights corresponding to locations (e.g., 
“Atlanta”,  “Los  Angeles”,  “Missouri”,  etc.),  ForceSPIRE 
adapts  the  spatialization  by  creating  a  more  geographic-
oriented layout (shown in the “Mid Stage” layout in Figure 
12).  
During  further  investigation,  he  began  opening  more 
documents and adding annotations to documents where he 
found  information  missing  that  he  knew.  For  example, 
Figure  9  shows  how  he  opened  one  document  where 
“suspicious individuals” were mentioned. Earlier, he read a 
document  containing 
terrorist 
organization  named  “Revolution  Now”.  While  reading 
about  the  suspicious  individuals,  the  other  information  in 
the document triggered him to make a connection between 
these  individuals  and  Revolution  Now.  He  made  added  a 
note  to  the  document  about  the  suspicious  individuals 
stating  “these  individuals  may  be  related  to  Revolution 
Now”. As a result, ForceSPIRE parsed the note for entities, 
added  them  to  the  document,  and  pulled  the  document 
closer to other documents containing the entity “Revolution 
Now”.  
After  continuing  his  investigation  in  this  manner,  he 
ultimately  made  the  connections  within  the  dataset  to 
uncover  the  terrorist  plot.  The  progression  of  the  spatial 
layout,  shown  in Figure 12, shows the final layout, where 
he  was  able  to  pinpoint  regions  of  the  layout  as  being 
important  in  his  finding.  Some  of  the  spatial  locations  of 
clusters  are  a  result  of  him  pinning  documents  to  that 
region (e.g., “Atlanta”, “Los Angeles”, etc.). These pinned 
documents are shown in red. Perhaps more interestingly is 
not the regions that were created as a result of him pinning 
documents  to  that  location,  but  rather  how  the  remaining 
documents respond in the layout. For example, in the final 
state  shown  in  Figure  12,  a  group  of  documents  began  to 
emerge  in  the  middle  of  all  the  pinned  locations.  Upon 
examining  these  documents,  he  discovered  that  these 

information  about  a 

the 

layout 

 

interaction, 

instances  during 

 
Figure 12. The incremental change of the spatial layout (main 
view  of  ForceSPIRE)  from  the  initial  to  the  final  state. 
Through  semantic 
incrementally 
changed  based  on the  semantic  input of the user. We labeled 
the regions based on what the user told us the regions meant to 
him at each stage. 
can  enable  analysts  to  explore  their  hypothesis  in-situ, 
while  the  provenance  of  their  insights  is  captured  and 
stored. An open area of research is what analyzing the soft 
data might reveal about the analytic process. For instance, if 
the  importance  values  of  entities  converge  on  a  small 
number  of  entities,  specific  biases  might  be  revealed. 
Similarly, 
the  analysis  when  new 
hypotheses  are  being  explored  may  be  indicated  by 
diverging importance values. 
Use Case 
We  demonstrate  the  functionality  of  ForceSPIRE  through 
the  following  use  case.  In  this  scenario,  we  simulate  an 
intelligence  analysis  scenario  where  the  task  is  to  find  a 
hidden terrorist plot in a pre-constructed, ficticious textual 
dataset.  The  dataset  consists  of  50 
text  documents, 
containing  a  complex  terrorist  plot  (explosives  are  being 
transported to various cities in the U.S. using trucks). The 
combination of the task of finding the hidden terrorist plot 
and  the  textual  dataset  is  representative  of  daily  work 
performed  by  professional  intelligence  analysts  [8].  The 
analysis  described  below  lasted  70  minutes,  and  was 
performed  by  an  individual  computer  science  graduate 
student.  
The user began the investigation by loading the collection 
of  documents  into  ForceSPIRE.  The  documents  were 
automatically  parsed  for  entities  using 
the  LingPipe 
keyword  extraction  library  [1].  From  these  entities,  an 
initial layout was generated, shown in Figure 12(top). From 
this  layout,  he  began  investigation  by  reading  through  the 

 

interpreting 

leverage 

interactions 

DISCUSSION 
Unifying the Sensemaking Loop 
With the fundamentally different role occupied by semantic 
interaction, we explore a new design space for interaction in 
visual analytic tools. With the addition of soft data, and a 
model  capable  of 
the  user’s  analytical 
reasoning,  we 
that  are  already 
occurring in the spatial analytic process to further aid users 
in their sensemaking process.  
With  semantic  interaction,  the  amount  of  formalization 
between foraging and sensemaking (Figure 13) on the part 
of the user is reduced. For instance, in moving a document, 
users  can  formulate  a  hypothesis  based  on  that  document, 
expecting  similar  documents 
to  follow.  ForceSPIRE 
attempts to update the layout based on the interaction, and 
gives the user feedback. Thus, the foraging stage occurs as 
a  result  of  the  hypothesis  being  formed  through  semantic 
interaction.  By  not  forcing  users  to  over-formalize  their 
analytic  reasoning  too  early  in  order  to  forage  for  the 
relevant  information,  semantic  interaction  creates  a  more 
seamless 
transition  between 
foraging  and  synthesis, 
unifying the sensemaking loop.  
Future Work 
Semantic 
interaction,  as  a  concept,  opens  up  many 
possibilities for further research, such as: what interactions 
to  capture  and  store,  which  parameters  of  the  model  to 
update,  how  to  store  the  soft  data,  and  which  models 
present a metaphor that can be extended upon.  
In  order  to  make  more  concrete  claims  regarding  the 
usability  and  effectiveness  of  ForceSPIRE  (and  thus,  of 
semantic  interaction),  a  formal  user  study  is  needed.  Our 
plan is to introduce ForceSPIRE to professional intelligence 
analysts  and  have  them  solve  scenarios  that  model  their 
daily  task,  such  as  one  of  the  VAST  datasets  [2020].  The 
observations  and  feedback  from  these  users  will  provide 
ecological validity for semantic interaction. 
CONCLUSION 
In  this  paper  we  have  discussed  how  the  concept  of 
semantic  interaction  leads  to  a  new  design  space  for 
interaction 
information. 
Semantic  interactions  occur  directly  within  the  spatial 
metaphor,  support  spatial  cognition,  and  exploit  spatial 
analytic  interactions.  We  describe  semantic  interaction, 
discussing  the  three  components  required  –  capturing  the 
interaction, 
the  analytical  reasoning,  and 
updating  the  mathematical  model.  Further,  we  present 
ForceSPIRE, designed for semantic interaction with textual 
information, discussing its functionality and demonstrating 
how it can be used through a use case. Lastly, we discuss 
how  semantic  interaction  has  the  opportunity  to  unify  the 
sensemaking  loop,  creating  a  more  seamless  analytic 
process.  In  allowing  users  to  interact  within  the  spatial 
metaphor, they can remain more focused on their analysis 
of  the  data,  without  having  to  become  experts  in  the 
underlying mathematical models of the system.  

in  spatializations  of 

interpreting 

textual 

 

Figure  13.  The  sensemaking  loop,  illustrating  the  complex 
sequence  of  steps  used  by  intelligence  analysts  in  order  to 
gain insight into data.  
 
documents  are  about  the  terrorist  organization  using  “U-
Haul”  or  “Ryder”  trucks  for  transportation  between  these 
locations. ForceSPIRE placing these documents in between 
these  cities  in  the  layout  was  helpful,  as  these  documents 
contain  information  “connecting”  the  events  in  these 
locations.  Immediately  after  noticing  this  event,  he  also 
made use of the expressive form of interaction, performed 
by dragging two of these documents together to determine 
what  made  them  similar.  After  seeing  that  it  was  indeed 
terms  such  as  “Ryder”  and  “U-Haul”,  the  layout  formed 
more tightly around these terms. 
ForceSPIRE interpreted the analytical reasoning of the user 
through the creation of new entities that were not found by 
the  initial  keyword  extraction,  as  well  as  the  increase  of 
importance values of existing entities. This is evidenced by 
the  creation  of  39  new  entities  during  the  course  of  the 
analysis.  LingPipe  extracted  89  initial  entities  from  this 
dataset,  and  at  the  time  of  completing  our  investigation 
ForceSPIRE  included  128.  Examples  of  newly  created 
entities  are  “big  event”,  “grenades”,  “Fisher  Island”, 
“weapons”,  and  others.  The  ability  for  new  entities  to  be 
created  via  semantic  interaction  did  not  interfere  with  the 
fluid sensemaking process of the user. Instead, it aided the 
process  by  creating  new  entities,  which  in  turn  created 
semantically relevant connections within the dataset. 
In  addition  to  creating  new  entities,  existing  entities 
dynamically  changed  their  importance  value  based  on  the 
semantic 
interpreted 
reasoning 
interactions.  Examples  of  entities 
their 
importance  values  are  “Atlanta”,  “Revolution  Now”, 
“Colorado”,  “L.A.”,  and  others.  As  a 
the 
ForceSPIRE incrementally adapted the layout based on the 
user  input.  This  shows  that  adjusting  importance  values, 
creating entities, and changing locations of key documents 
helped  the  user  discover  the  structure  of  the  dataset,  and 
ultimately make out the hidden terrorist plot.  

of 
that  changed 

analytical 

result, 

the 

 

ACKNOWLEDGEMENTS 
This research was funded by the NSF grant CCF-0937071 
and the DHS center of excellence. 
REFERENCES 
1.  Alias-i. 2008. LingPipe 4.0.1. City, 2008. 
2.  i2 Analyst's Notebook. City. 
3.  Alsakran, J., Chen, Y., Zhao, Y., Yang, J. and Luo, D. 

STREAMIT: Dynamic visualization and interactive 
exploration of text streams. In Proceedings of the IEEE 
Pacific Visualization Symposium, 2011.  

4.  Andrews, C., Endert, A. and North, C. Space to Think: 
Large, High-Resolution Displays for Sensemaking. In 
Proceedings of the CHI '10, 2010.  

5.  Callahan, S. P., Freire, J., Santos, E., Scheidegger, C. E., 

C, Silva, u. T. and Vo, H. T. VisTrails: visualization 
meets data management. In Proceedings of the 
SIGMOD international conference on Management of 
data (Chicago, IL, USA, 2006). ACM.  

6.  Cowley, P., Haack, J., Littlefield, R. and Hampson, E. 

Glass box: capturing, archiving, and retrieving 
workstation activities. In Proceedings of the workshop 
on Continuous archival and retrival of personal 
experences (Santa Barbara, California, USA, 2006). 
ACM.  

7.  Dou, W., Jeong, D. H., Stukes, F., Ribarsky, W., 

Lipford, H. R. and Chang, R. Recovering Reasoning 
Processes from User Interactions. IEEE Computer 
Graphics and Applications, 2009. 

8.  Endert, A., Andrews, C., Fink, G. A. and North, C. 

Professional Analysts using a Large, High-Resolution 
Display. In Proceedings of the IEEE VAST Extended 
Abstract (2009).  

9.  Endert, A., Han, C., Maiti, D., House, L., Leman, S. C. 

and North, C. Observation-level Interaction with 
Statistical Models for Visual Analytics. IEEE VAST, 
2011. 

10. Frank M. Shipman, I. and Marshall, C. C. Formality 

Considered Harmful: Experiences, Emerging Themes, 
and Directions on the Use of Formal Representations 
inInteractive Systems. ACM CSCW, 8, 4, 1999, 333-352. 

11. Fruchterman, T. M. J. and Reingold, E. M. Graph 

drawing by force-directed placement. Software: Practice 
and Experience, 21, 11 1991, 1129-1164. 

12. Gotz, D. Interactive Visual Synthesis of Analytic 

Knowledge. IEEE VAST, 2006. 
13. Heer, J. prefuse manual, 2006. 
14. Heer, J., Mackinlay, J., Stolte, C. and Agrawala, M. 

Graphical Histories for Visualization: Supporting 
Analysis, Communication, and Evaluation. IEEE 
Transactions on Visualization and Computer Graphics, 
14, 6 , 2008, 1189-1196. 

 

15. Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. 

and Chang, R. iPCA: An Interactive System for PCA-
based Visual Analytics. Computer Graphics Forum, 28, 
2009, 767-774. 

16. Karen A Statistical Interpretation of Term Specificity 

and its Application in Retrieval. Journal of 
Documentation, 28, 1972, 11-21. 

17. Marshall, C. C., Frank M. Shipman, I. and Coombs, J. 

H. VIKI: spatial hypertext supporting emergent 
structure. In Proceedings of the European conference on 
Hypermedia technology (Edinburgh, Scotland, 1994). 
ACM.  

18. Olsen, K. A., Korfhage, R. R., Sochats, K. M., Spring, 
M. B. and Williams, J. G. Visualization of a document 
collection: the vibe system. Information Process 
Management, 29, 1 1993, 69-81. 

19. Pirolli, P. and Card, S. Sensemaking Processes of 

Intelligence Analysts and Possible Leverage Points as 
Identified Though Cognitive Task Analysis Proceedings 
of the International Conference on Intelligence 
Analysis,2005, 6. 

20. Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., 

O'Connell, T., Laskowski, S., Chien, L., Tat, A., Wright, 
W., Gorg, C., Zhicheng, L., Parekh, N., Singhal, K. and 
Stasko, J. Evaluating Visual Analytics at the 2007 
VAST Symposium Contest. Computer Graphics and 
Applications, IEEE, 28, 2 2008, 12-21. 

21. Shrinivasan, Y. B. and Wijk, J. J. v. Supporting the 

analytical reasoning process in information 
visualization. In Proceedings of the CHI '08 (Florence, 
Italy, 2008). ACM.  

22. Skupin, A. A Cartographic Approach to Visualizing 
Conference Abstracts. IEEE Computer Graphics and 
Applications, pp. 50-58, January/February, 2002. 

23. Thomas, J. J., Cook, K. A., National, V. and Analytics, 
C. Illuminating the path. IEEE Computer Society, 2005. 
24. Torres, R. S., Silva, C. G., Medeiros, C. B. and Rocha, 

H. V. Visual structures for image browsing. In 
Proceedings of the conference on Information and 
knowledge management (New Orleans, LA, USA, 
2003). ACM.  

25. Wise, J. A., Thomas, J. J., Pennock, K., Lantrip, D., 

Pottier, M., Schur, A. and Crow, V. Visualizing the non-
visual: spatial analysis and interaction with information 
for text documents. Morgan Kaufmann Publishers, 1999. 

26. Wright, W., Schroh, D., Proulx, P., Skaburskis, A. and 

Cort, B. The Sandbox for analysis: concepts and 
methods. In Proceedings of the CHI '06 (New York, 
NY, 2006). ACM.  

27. Yi, J. S., Melton, R., Stasko, J. and Jacko, J. A. Dust & 
magnet: multivariate information visualization using a 
magnet metaphor. Information Visualization, 4, 4, 2005, 
239-256. 

",False,2012.0,{},False,False,conferencePaper,False,HDQVL9W2,[],self.user,False,False,False,False,,"<p>The paper claims that current analytical methods make users interact with the statistical model by using sliders menus and text requiring them to go outside of the visual metaphor.</p>
<p>They define <strong>semantic interaction</strong> as an interaction that seeks to enable analysts to spatially interact with the models directly inside the statistical metaphor, using interactions that derive from their analytic process, such as searching, highlighting, annotating and repositioning documents. The focus of semantic interaction is on interacting with data directly not with the dimensions for example as in dust and magnet. <strong>I dont know whats the problem for doing both</strong></p>
<p>Semantic Interaction is based on the following principles:</p>
<p>1. Visual: Near= similar [citation 22]</p>
<p>2. Use the interactions within the visual metaphor [citation 4]</p>
<p>3. Interpret and map the interactions to the parameters of the model by updating weights and information</p>
<p>4. Shield users from the complexity</p>
<p>5. Model learns incrementally based on the interaction</p>
<p>6. Provide instant feedback of the updated model within the visual metaphor</p>
<p>7. Reuse learned model parameters in future,</p>
<p>Space to think found cognitive advantages associated with the manual creation of layout information.</p>
<p>[Cite 17] found that allowing users to create informal relationships within information is beneficial, as it does not require users to formalize these relationships.</p>
<p>Relevance Feedback has been used for content-based image retrieval by moving images closer or separate from each other to portray pair-wise similarity [cite 24]</p>
<p>Semantic interaction leverages the cognitive connection formed between the user and the spatial layout.</p>
<p>The captured interactions define a new type of data, the important part of this interactions is discovering intent from the specific interaction. For example we can give more importance to a piece of text that has been highlighted. This new data stored in the interaction is named as <strong>soft data</strong> in comparison with <strong>hard data</strong> extracted from the actual information.</p>
<p> </p>
<p><strong>Table of interactions and associated analytic reasoning in page 5, table 1.</strong></p>
<p> </p>
<p>The system is called <strong>ForceSpire </strong> documents are visualized as nodes that have more or less mass depending on the size of the document. Values are calculated using TFIDF</p>
<p> </p>
<p>Interactions:</p>
<p>Document Movement: Dragging, pinning or linking.</p>
<p>    Dragging changes similarity of documents</p>
<p>    Pinning gives meaning to the space surrounding a document</p>
<p>    Linking changes the relationships among documents</p>
<p> Highlighting: Gives importance to the highlighted text.</p>
<p> Annotations: Adds semantic meaning to a document or a link or the space.</p>
<p>Search: If the search is a new term it creates the term if not it gives more importance to the already created term.</p>
<p><strong>FUTURE WORK</strong><br />What interactions to capture and store, which parameters in the model to update, how to store the soft data. Which models have a good metaphor that can be extended upon.   </p>",Semantic interaction for visual text analytics,HDQVL9W2,False,False
ENF2YTJG,W7TVPE7L,"Towards a Systematic Combination of Dimension Reduction

and Clustering in Visual Analytics

John Wenskovitch, Student Member, IEEE, Ian Crandell, Naren Ramakrishnan, Member, IEEE,

Leanna House, Scotland Leman, Chris North

Abstract— Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both
families of algorithms assist analysts in performing related tasks regarding the similarity of observations and ﬁnding groups in datasets.
Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems.
However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating
some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and
clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are
processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension
reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes
use of both families of algorithms.
Index Terms—Dimension reduction, clustering, algorithms, visual analytics.

1 INTRODUCTION
Visual metaphors for exploring high-dimensional datasets come in a
variety of forms, each with their own strengths and weaknesses in both
visualization and interaction [37, 69]. In particular, datasets with high
dimensionality present tractability challenges for computation, design,
and interaction [29]. One frequently used method of visual abstraction
is to reduce a high-dimensional dataset into a low-dimensional space
while preserving properties of the high-dimensional structure (e.g., re-
tain or respect pairwise relationships from the higher dimensions in
the lower dimensional projection). Such dimension reduction algo-
rithms are useful abstractions because some of the dimensions in the
dataset may not be essential to understanding the underlying patterns
in the dataset [38]. Instead, a subset of the dimensions can be selected
or learned (or new dimensions introduced) to deﬁne the important
characteristics of the dataset. The visualization tasks associated with
dimension reduction algorithms have been well studied [14, 15].
Many dimension reduction algorithms employ a “proximity ≈ simi-
larity” metaphor, in which a distance function measures the similarity
of pairs of observations1 at the high-dimensional level and attempts to
preserve those distance relationships in the low-dimensional projection
by minimizing a stress function. Due to this “proximity ≈ similarity”
relationship, observations with high similarity or an underlying rela-
tionship can form implicit clusters in the low-dimensional projection.
Indeed, clustering can even be thought of as extremely low-resolution
dimension reduction, where knowledge about the various attributes
of the observations leads to a one-dimensional bin assignment (or a
set of probabilities for bin assignments). This relationship between
dimension reduction and clustering is also supported mathematically
in speciﬁc instances. For example, Ding and He [27] proved that prin-
cipal components are the continuous solutions to the discrete cluster
membership indicators for k-means clustering, indicating that Principal

• John Wenskovitch, Naren Ramakrishnan, and Chris North are with the

Virginia Tech Department of Computer Science. E-mails: {jw87 | naren |
north}@cs.vt.edu.
Department of Statistics. E-mails: {ian85 | lhouse | leman}@vt.edu.

• Ian Crandell, Leanna House, and Scotland Leman are with the Virginia Tech

Component Analysis (PCA) dimension reduction implicitly performs
data clustering as well.

Indications from previous studies [8, 33] have shown that analysts
use a complex combination of both developing clusters and organizing
observations in space in the sensemaking process [76] as they explore
a dataset. These explorations generate clusters created by the analyst
during exploratory interactions to spatially organize information on
the display, as well as clusters that naturally develop due to expressive
interactions updating the underlying layout (these interaction types are
deﬁned by Endert et al [34]). Other studies have also linked dimension
reduction algorithms to clustered data; for example, Choo et al. dis-
cusses dimension reduction methods for two-dimensional visualization
of high-dimensional clustered data, proposing a two-stage framework
for visualizing such data based on dimension reduction methods [21].
While dimension reduction algorithms and clustering algorithms
have been implemented together in a number of visualization systems,
these algorithms often operate independently and in parallel. In other
words, each algorithm supports some analysis component in the system
without the inﬂuence of the other algorithm: perhaps a collection
of observations are clustered, but the output of that clustering has
limited or no effect on the dimension-reduced layout of the observations.
Alternatively, a change to the spatialization may perceptually imply
the need for a change to the cluster assignment, but no update to
the cluster assignment may occur. The second case can be seen in
the iVisClustering system [57]. This tool clusters documents into a
collection of topics and uses a force-directed layout in the Cluster
Relation View to present the documents spatially. However, making a
change to the layout of the projection (see Fig. 1) has no effect on the
clustering assignments of the documents.

Exploring the connections between dimension reduction and clus-

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx

1In this work, we employ the convention of referring to the features of a
dataset as dimensions, individual data items as observations, and the features of
those observations as attributes.

Fig. 1. The iVisClustering system [57] incorporates dimension reduction
and clustering algorithms in the same system; however, making a change
to the layout has no effect on the clustering assignment.

Table 1. A selection of dimension reduction algorithms, organized by
the complexity of manifold each can learn: linear manifolds, nonlinear
manifolds, and algorithms that have implementations of both types.

Linear

Both

Nonlinear

Selected Dimension Reduction Algorithms
Factor Analysis [43]
Principal Component Analysis (PCA) [74]
Probabilistic PCA (PPCA) [84]
Projection Pursuit [40]
Feature Selection [42]
Independent Component Analysis (ICA) [49]
Multidimensional Scaling (MDS) [85]
Weighted MDS (WMDS) [18]
Glimmer [50]
Isomap [82]
Latent Dirichlet Allocation (LDA) [11]
t-Distributed Stochastic Neighbor Embedding
(t-SNE) [65]

tering algorithms leads to several natural research questions. If the
data separates into implicit clusters, and the analyst sees advantages
in the creation of these implicit clusters, can we appropriately sup-
port explicit cluster deﬁnitions so that the dimension reduction and
clustering algorithms support each other rather than conﬂict with each
other (or simply do not interact with each other)? If so, how should we
deﬁne, visualize, and interact with both observations and clusters in
a dimension-reduced projection? And ﬁnally, is there a difference be-
tween how analysts interpret and interact with low-dimensional clusters
as opposed to high-dimensional clusters?

Our research explores initial steps to address these questions. In

particular, this work includes the following contributions:

1. An overview of combining dimension reduction and clustering
techniques into a visualization system, including a discussion of
algorithms, tasks, visualizations, and interactions.

2. A discussion of the design decisions that must be addressed when
creating a visualization system that combines dimension reduction
and clustering algorithms.

The remainder of this paper discusses these contributions through the
exploratory data analysis process. We begin by providing an overview
of existing dimension reduction and clustering algorithms in Sect. 2.
From there, we discuss common high-dimensional data analysis tasks
is Sect. 3, visualizations to support those tasks in Sect. 4, and interac-
tions on those visualizations in Sect. 5. We close with a discussion of
further challenges and lessons learned in Sect. 6 and conclude in Sect. 7
with a summary of design questions that should be considered when
developing a tool combining these algorithm families.

2 ALGORITHMS
In this section, we summarize the variety of algorithms that address
dimension reduction and clustering tasks in visualization systems.

2.1 Dimension Reduction Algorithms
The goal of dimension reduction algorithms is to represent high-
dimensional data in a low-dimensional space while preserving high-
dimensional structures, including outliers and clusters [58]. Dimension
reduction has a scalability advantage over other methods for visualizing
high-dimension data such as parallel coordinate plots and heatmaps, but
with the disadvantage of information loss when transforming the data
into the low-dimensional projection [37, 61, 69]. Here, we summarize
many of the common dimension reduction algorithms in the Visualiza-
tion ﬁeld; more detailed surveys of dimension reduction algorithms can
be found in the literature [38, 39, 58, 91]. In addition, several tools have
been implemented that allow analysts to switch between and compare
dimension reduction algorithms [62, 77].

Dimension reduction algorithms can be divided into linear and non-
linear classes, referring to the structure of the underlying manifolds or
topological spaces that each class can learn. Linear dimension reduction
algorithms are limited to learning linear manifolds, while nonlinear di-
mension reduction algorithms can learn more complex manifolds. Still
other dimension reduction algorithms have been implemented in both
linear and nonlinear variants. Table 1 provides a set of commonly used
dimension reduction algorithms in the visualization literature, divided
into whether they are linear, nonlinear, or have implementations of both
levels of manifold complexity. Principal Component Analysis (PCA) is
perhaps the most frequently-used linear dimension reduction algorithm,
which works by determining the axes of maximum variance in the col-
lection of observations [74]. Another often-used dimension reduction
technique is Multidimensional Scaling (MDS), which computes pair-
wise distances between observations in the high-dimensional space and
attempts to preserve those distances in a low-dimensional projection.
MDS implementations exist in both linear and nonlinear forms.

Many of these dimension reduction algorithms require a distance
function as input, which provides the method for calculating the simi-
larity of each pair of observations. Much like the breadth of algorithms
discussed, a number of distance functions are used in Visualization
systems. The most popular metrics are those derived from p-norms,
which give distance functions of the form

(cid:12)(cid:12)xi,k − x j,k

(cid:12)(cid:12)p(cid:19)1/p

.

(cid:18)

∑

k

dp(xi,x j) =

Such a distance is deﬁned for any positive p. The most familiar ex-
amples are p = 1, which is known as Manhattan distance (due to the
city’s regular grid structure), and p = 2, which is Euclidean distance.
Aggarwal et al. [2] showed that Manhattan distances are preferable
to Euclidean distances for high-dimensional data, as Euclidean dis-
tance (and p-norms of p > 1 in general) tends to compress the space
as more dimensions are added, resulting in high-dimensional distances
that are less distinguishable. In determining the appropriate distance
function, it is also worth considering that some distance functions are
computationally more difﬁcult when optimizing a stress function.

Large datasets present performance difﬁculties with some dimen-
sion reduction algorithms. For example, MDS requires a distance to
be computed between each pair of observations, resulting in ∼ n2/2
distances computed for n observations. However, tools do exist to
visualize such large datasets. ASK-GraphView supports the interactive
visualization of graphs with up to 200,000 nodes and 16,000,000 edges
by using clustering algorithms to construct a hierarchical graph, thus
visualizing only internal subsections of the graph at any time [1]. A
related solution that combines these algorithm families is to initially
cluster the data and then apply a dimension reduction algorithm such
as MDS on the cluster centroids, followed by subsequent dimension
reduction executions on each individual cluster. This minimizes the
amount of memory required to store pairwise distances at the expense
of no longer having a single global distance measure.

2.2 Clustering Algorithms
Hundreds of clustering algorithms have been implemented, each with
inherent strengths and weaknesses. The broad collection of approaches
in this class of algorithms stems from the notion that a “cluster” is in-
herently a subjective structure, and as such cannot be precisely deﬁned.
Therefore, new algorithms or improvements on existing algorithms are
often created to solve a single problem, though these new solutions
may be applied to future problems where appropriate. As a result, there
is no globally optimal clustering algorithm; the best clustering algo-
rithm is problem-speciﬁc and often determined experimentally [36].
Surveys of clustering algorithms exist in the literature, which include
clustering from the perspectives of machine learning, human-computer
interaction, visualization, and statistics [23, 92].

Clustering algorithms come in two primary forms: hierarchical and
partitioning. Hierarchical algorithms in turn can be divisive (top-down)
or agglomerative (bottom-up). The divisive strategy approaches the
identiﬁcation of clusters through iterative partitioning, beginning with a

Table 2. Sample exploratory data analysis tasks, organized by stage in the data analysis process (rows) and algorithm family (columns).

See the Result

Dimension Reduction
See distribution of observations

Both
See relative positions of observations

Understand the Result Measure distances between observations

Identify attribute values of observations

Affect the Result

Change distance metric
Select different dimensions

Reposition observations in the full space
Enhance an existing pattern in the projection

Clustering
Identify clusters of observations
Label clusters
Determine cluster structure
Change cluster membership of observations
Create/remove clusters

single group and breaking it down into smaller portions according to an
algorithm-speciﬁc differencing measure [4]. In contrast, agglomerative
algorithms approach clustering through iterating aggregation, beginning
with every item in its own group and joining groups together through
an algorithm-speciﬁc similarity measure [78].

Perhaps the most common clustering algorithm is k-means [64],
which partitions a dataset into k clusters according to a distance between
each observation and the nearest cluster centroid. Finding an optimal k-
means solution is an NP-Hard problem; therefore, heuristic algorithms
exist to converge quickly to a local solution. The k-means algorithm
has been extended to support a variety of tasks, including weighted
clustering [48], hierarchical clustering [75], textual data [26], and
constrained clustering [89]. A number of k-means variants are discussed
in detail by Cordeiro de Amorim and Mirkin [25]. A major limitation
of k-means is that it can only ﬁnd clusters with convex shapes. The
algorithm also requires input parameter k for the number of clusters to
create, presenting an additional complication in generating the best set
of clusters with its heuristic approach. Several solutions to determine
the most appropriate k value are used, such as the elbow method [83].
Many of the distance functions that are applied to dimension reduc-
tion algorithms are also useful when considering cluster computations.
Cosine distance, for example, can be used to measure cohesion within
clusters [81]. The Jaccard similarity coefﬁcient is used for measuring
diversity and dissimilarity between clusters or sets of observations [60].
In selecting a clustering algorithm, an additional consideration
should be whether an observation can be assigned to only one cluster
(“hard” clustering) or can belong to multiple clusters (“fuzzy” or “soft”
clustering). The Fuzzy C-means Clustering algorithm [9, 30] is a fuzzy
extension of the k-means algorithm, in which the centroid of a cluster
is now computed as the mean of all observations weighted by their
probability of belonging to the cluster. Fuzzy C-means has found use
in the ﬁelds of bioinformatics [87] and image analysis [3].

A common clustering tool used in statistics is the Dirichlet process
mixture model (DPMM) [10]. This is a probabilistic method, and rather
than return a hard clustering assignment, it gives each observation a
probability of belonging to any given cluster. Additionally, and unlike
k-means, DPMMs learn the number of clusters dynamically, creating
new clusters and closing old ones as the algorithm proceeds. It is not
without drawbacks, however. The DPMM requires speciﬁcation of a
probability model for the observations in each cluster, which in turn
introduces its own difﬁculties. The algorithm also scales more poorly
than k-means with additional data, especially if the model parameters
are estimated with Markov chain Monte Carlo.

Much like with dimension reduction algorithms, large datasets can
present performance issues with clustering algorithms. Consider again
the k-means algorithm, for which the common Lloyd’s algorithm heuris-
tic implementation has a running time of O(nkdi) for a dataset with n
observations of d dimensions each, k clusters, and i iterations before
convergence [64]. The runtime of this algorithm is thus linear in terms
of both the number of observations and the number of dimensions;
however, performance can be greatly improved by reducing the number
of dimensions. Assuming that n and k are ﬁxed, the execution time
of the k-means algorithm can hence be improved substantially with
dimension reduction, potentially dropping the value of d from hundreds
to two (which may simultaneously reduce i as well). Several clustering
algorithms are designed to use on large datasets. For one example,
the Bradley-Fayyad-Reina (B-F-R) clustering algorithm is a variant
of k-means that works by internally maintaining summaries of large
collections of observations [13].

3 TASKS
This section presents an overview of tasks commonly seen in visu-
alization systems that implement dimension reduction and clustering
algorithms for exploratory data analysis. We discuss the implications
of the order in which these algorithms are executed, along with related
design decisions and considerations.

3.1 Dimension Reduction and Clustering Tasks
Our discussion of tasks for dimension reduction and clustering algo-
rithms focuses on exploratory data analysis tasks. When exploring a
high-dimensional dataset with dimension-reduced projections, there
are an immense number of possible 2D- or 3D-projections that can be
generated from the dataset. An analyst should be afforded the ability to
explore these alternate projections, as well as the related clusterings in
those projections, in order to gain insight from the data.

One method for enabling this exploration is by applying weights to
the dimensions in the dataset. Biasing the algorithms towards combina-
tions of dimensions in the dataset enables the creation of projections
that are similarly biased towards those dimension combinations. Thus,
an analyst can explore clusters and patterns in a projection that is biased
towards dimensions X, Y , and Z, and contrast that result with clusters
and patterns in a projection biased towards only dimensions U and V ,
both from the same initial high-dimensional dataset.

When interactively exploring a dataset, dimension reduction tasks
(the left columns of Table 2) typically relate to position, while cluster-
ing tasks (the right columns of Table 2) typically relate to grouping. For
example, identifying a similarity relationship between two observations
based on their separation distance in a projection is a dimension reduc-
tion task, while positioning two similar observations close together is
a clustering task. However, there exists obvious ambiguity even with
such basic interactions. When positioning two objects close together to
form a cluster, the analyst is also communicating a distance relationship
between those observations. Thus, space is overloaded for both group-
ing and layout interactions, further suggesting a relationship between
the dimension reduction and clustering algorithm families. As seen in
the selected tasks breakdown in Table 2, tasks can often be addressed by
only using a dimension reduction algorithm or a clustering algorithm,
but there do exist many cases where the interplay between algorithms
affects both when a task is performed.

This relationship can be further seen in Brehmer et al. [15], in which
ten analysts from six application domains were interviewed with the
goal of understanding how analysts explore dimension-reduced data.
The end result of this study was a set of ﬁve task sequences. Although
the authors were focused on analyst interpretations of dimension-
reduced data, three of the ﬁve resulting task sequences were related
to clusters of items revealed in the low-dimensional data projection.
Indeed, the “Verify Clusters” task sequence was performed by all ten
of their analysts and the “Name Clusters” sequence was performed by
eight of the ten analysts. In contrast, the tasks sequences that were
not cluster-based were only performed by two (“Name Synthesized
Dimensions”) and four (“Map Synthesized to Original Dimensions”)
of the ten analysts. These ﬁndings suggest that analysts are discretizing
these clusters of observations in dimension-reduced projections. In
other words, the dimension reduction algorithm is creating a continuous
visual distribution that analysts interpret in discrete segments. More-
over, investigating these clusters within the projection are common
goals of user exploration and interaction with datasets.

In addition to investigating clusters in an existing projection, studies
have shown that analysts create their own clusters of observations. For

example, the “Space to Think” study by Andrews et al. [8] investigated
how analysts use large displays to navigate and lay out documents in
the sensemaking process [76], and that these clusters occasionally have
spatial relationships, both to develop a timeline and to keep similar clus-
ters of documents near to each other spatially. When interviewed about
their sensemaking process later, analysts spoke of their documents and
clusters both in terms of proximity and in terms of groups, implying
that these are similar cognitive processes. The ForceSPIRE [33] and
StarSPIRE [12] systems were designed in part from these ﬁndings.

Similar behavior was seen in the “Be the Data” system reported by
Chen et al. [20], which allows participants to explore a dataset by taking
on the role of the observations in a deﬁned physical space. By moving
about the space, participants update a dimension-reduced projection.
The system is thereby able to learn which dimensions of the dataset are
most important to the current “projection” of people. Presented with
a collection of animals and their attributes, a group of seventh grade
students were posed the question “What makes some animals good to
eat?” The students began their exploration of the data by clustering
animals into discrete Edible and Inedible clusters. However, the student
who embodied the Rat observation did not consider herself a part of
either group, noting that rats are normally not edible but are consumed
in some cultures. She then positioned herself between the two clusters.
This caused the rest of the students to reconsider their distribution,
turning the discrete clusters into a continuous distribution of Edibility.
Cluster investigation tasks (the right columns of Table 2) come in a
number of forms, each of which have some meaning in a dimension-
reduced projection. For example, analysts may wish to understand the
overall layout of clusters in a projection, explore the proximity of one
cluster to another, investigate clusters of clusters and similar structures,
the shape of a cluster, and describe outlying clusters versus central clus-
ters. In addition, analysts may be interested in the relationship between
clusters and the individual observations in the projection, exploring to
which cluster(s) an observation belongs, understanding the properties
of observations that are outliers to all clusters, and investigating the
properties of a set of observations that form a cluster. There is a mix
of distribution and group questions that can be addressed through the
combination of both dimension reduction and clustering algorithms.

Adding clusters and clustering interactions to dimension-reduced
data can also improve scalability as datasets continue to grow in
size [32]. Having the ability to abstract collections of observations
into a single cluster that acts as an interaction target enables the abil-
ity to place more objects into virtual spaces, useful both for standard
monitors and for large display systems.

While the outputs of dimension reduction and clustering algorithms
are useful to locate patterns in a dataset, we also beneﬁt from enabling
these algorithms to learn from user interactions [34, 46, 59]. By in-
terpreting the semantic meaning of user interactions, each of these
algorithms can better enable exploratory data analysis. For example,
an analyst may wish to know what model parameters are necessary to
create a cluster from observations A, B, and C. By manipulating the pro-
jection to form such a cluster and initiating a semi-supervised machine
learning routine, the dimension reduction and clustering algorithms
can be trained to learn such model parameters and to update the entire
projection in response to those new parameters. The new projection
may create a new cluster from observations D, E, and F in addition to
the analyst-created cluster, a new insight into the dataset. Therefore,
the dimension reduction and clustering algorithms can help both at
the beginning of the exploration process by providing a na¨ıve starting
point, as well as throughout the exploration process by responding to
the interactions of an analyst.

3.2 Coordinating the Algorithms
Another consideration in selecting dimension reduction and clustering
algorithms is determining what parameters should be learned and used
by each algorithm, as well as what information should be learned by the
analyst. Beginning with the analyst, we discussed in the previous sub-
section that dimension reduction algorithms and clustering algorithms
serve similar purposes. However, dimension reduction algorithms are
more suited to tasks for pairwise comparisons and similarities between

observations, while clustering algorithms are better suited for compar-
isons involving the recognition and description of groups.

For the algorithms, one obvious design decision is to determine
whether or not the dimension reduction algorithm and clustering algo-
rithm should be using the same distance function, or even if they should
be using the same set of weights on the dimensions. It is possible for
the dimension reduction algorithm and the clustering algorithm to store
separate sets of weights, or to use different distance functions entirely.
When considering the semantics of the order of dimension reduction
and clustering algorithms, using clustering in high-dimensional space as
the ﬁrst operation makes uncovering clusters the primary semantic role
of the system, and hence results in a system designed to support locating
and understanding groups in the input data. In contrast, clustering as
the second operation in the low-dimensional space after executing a
dimensional reduction algorithm results in a clustering algorithm that
is merely a secondary aid to the dimension reduction algorithm.

An open question is determining whether analysts are cognitively
clustering in high-dimensional or low-dimensional space. Given that
analysts typically form clusters of text documents directly from the text
instead of ﬁrst converting those documents into another form [8], it
appears that clustering is performed in the high-dimensional space, at
least for textual data. Understanding the clustering process of analysts
will lead to better semantic interactions in this dimension reduction and
clustering design space, leading to further system interactions such as
enabling humans to provide corrections to clustering assignments and
hence update dimension reduction algorithm weights and projections.
Naturally, it is not possible to coordinate all pairs of dimension
reduction and clustering algorithms. For example, some dimension
reduction algorithms such as PCA do not rely on distances between
observations. Therefore, using the same distance measure between
PCA and a clustering algorithm is not possible.

3.3 Dimension Reduction and Clustering Combinations
When developing a system that includes both dimension reduction and
clustering algorithms, it is important to consider the order in which
these algorithms are performed on the data, as the order of these al-
gorithms will generate projections with different semantic meanings.
Fig. 2 includes six different pipelines that display execution orders
and data ﬂows between these algorithms. Each of these pipelines is
discussed in the following paragraphs. As the analyst progressively
explores the dataset, they may select a different pipeline for each round
of exploration, continuing to explore new projections (Fig. 3).

Independent Algorithms: As discussed previously, many visual-
ization systems incorporate both dimension reduction and clustering
algorithms, but these algorithms often execute independently and in
parallel so that the output of one algorithm has no effect on the other.
This pipeline is highlighted ﬁrst in Fig. 2 and was discussed in the
iVisClustering [57] example in the Introduction. In this system, topics
are computed and assigned as clusters, and a force-directed compu-
tation performs the node layout in the spatialization. However, an
update to the layout has no effect on the clustering assignments. In
addition, computing both algorithms on the high-dimensional data
will be more computationally expensive than performing only a single
high-dimensional computation.

Dimension Reduction Preprocessing for Clustering: Another
possibility is to execute a dimension reduction algorithm on the high-
dimensional data, and then pass the low-dimensional projection to the
clustering algorithm to determine groups, clustering on the reduced
data rather than the source data. This decision may be advantageous
because the clustering algorithm can execute faster on a dataset with
fewer dimensions, but the outcome may be misleading because the
low-dimensional positions of each observation are an approximation of
the high-dimensional relationships. Rather than generating clusters of
the input data, we generate clusters using data with less information,
resulting in potentially misleading cluster assignments. This risk is dis-
cussed by Joia et al. [52], noting that distances in the low-dimensional
space may be misleading due to projection errors. As a result, what
appear to be distinct clusters must be conﬁrmed, as there is no guaran-
tee that these clusters do contain unique content. An example of this

Fig. 2. Six different options for pipelines depicting combinations of dimension reduction algorithms and clustering algorithms. In each of these
pipelines examples, it is implied that each algorithm could use an independent distance function, resulting in more than just these six pipelines.
Further, these pipelines represent a single analysis iteration.

pipeline can be seen in Zha et al. [94], in which a technique similar
to PCA is performed ﬁrst and followed by k-means on that output.
Likewise, Ng et al. [71] propose an algorithm in which the observations
are embedded in low-dimensional space such as the eigenspace of the
graph Laplacian, and then k-means is applied to that low-dimensional
projection. Be The Data also creates clusters dynamically based on the
current projection [20].

Clustering Preprocessing for Dimension Reduction: The reverse
of the previous behavior occurs when the clustering algorithm is the
ﬁrst to execute, and then some information from the clustering output
(the cluster assignments, or the locations of the centroids) is used by the
dimension reduction algorithm for layout. Now, the clusters represent
relationships that exist in the initial data in high-dimensional space.
However, the clustering algorithm will take longer to execute due to
the additional number of dimensions processed. While less common,
some systems do operate in this way. For example, Ding and Li ﬁrst
use k-means clustering to initially generate class labels, followed by
LDA dimension reduction for subspace selection [28]. Fuzzy clustering
introduces a new complexity to this pipeline, as cluster assignments
are now a probability distribution rather than a ﬁxed bin assignment.
A pipeline of this form can also improve scalability, as the time and
space complexity of many dimension reduction algorithms make them
infeasible to execute on very large datasets. Clustering observations
and then performing a dimension reduction algorithm on those clusters
is one solution to this challenge.

One Algorithm Implicitly Includes the Other: Another alterna-
tive is to only execute one of the algorithms, either dimension reduction
or clustering, and then convert or interpret the output of the executed
algorithm as the output for the other algorithm as well. In these cases,
the results from one algorithm are structured to ﬁt the objective of the
other algorithm, exploiting the mathematical equivalence between these
algorithm families discussed brieﬂy in the Introduction. For example,
we can codify soft k-means clustering as assigning n observations to
k features with some associated weight or probability. Likewise, we
can formulate dimension reduction as reducing m features to p features

Fig. 3.
Interactions from the analyst will drive additional executions
through the pipeline during the data exploration process. The analyst
does not need to select the same pipeline on every iteration of the
analysis.

with some associated weight or probability. Therefore, the outcome of
soft k-means clustering can be interpreted in terms of dimension reduc-
tion by making the k clustering features also represent the p dimension
reduction features. A similar argument exists to map the outcome
of a dimension reduction algorithm directly to a cluster encoding by
executing a dimension reduction algorithm like PCA and binning the
output along one of the axes. Perhaps a more straightforward example
of this pipeline is the self-organizing map [55], a dimension reduction
technique which can be directly interpreted as a set of clusters without
any feature transformation. Kriegal et al. [56] present a survey of clus-
tering techniques for high-dimensional data, and include a discussion
on subspace clustering algorithms. Such algorithms simultaneously
reduce both the number of observations and the number of dimensions
in a dataset, in contrast with having a dimension reduction algorithm
that reduces the number of dimensions computing separately from a
clustering algorithm that reduces the number of observations.

Global and Local Algorithm Combinations: Because dimension
reduction algorithms typically take a global view of the overall space
while clustering algorithms take a local view [26], another option is
to implement a pipeline in which the overall structure of the space
is informed by the dimension reduction algorithm while local struc-
tures are governed by the clustering algorithm. These algorithms can
communicate with each other to converge towards an optimal layout,
but each is responsible for its own aspect of the structure. To further
clarify the difference between this pipeline and some of those discussed
previously, consider organizing a large collection of documents in a
display. One possibility is to place related documents into folders, and
then organize the folders in the space. This example reﬂects the “Clus-
tering Preprocessing” pipeline, as we organize the clusters rather than
individual documents. In contrast, the analyst could organize groups of
documents in the space, and then select and move those groups with
respect to one another. This example affords some additional fuzzy
clustering capabilities, as a document that may belong to two or more
clusters can be placed between those clusters. Here, the overall layout
of the documents can be handled by dimension reduction, while some
local structures of similar documents are supported by clustering.

Iterative, Alternating Algorithms: The ﬁnal pipeline represents a
structure where both dimension reduction and clustering are working
together in the same overarching algorithm. As k-means is an algo-
rithm that alternates between updating cluster assignments and centroid
positions, a third stage can be added for dimension reduction. Ideally,
this iterative alternating process will enable dimension reduction and
clustering to work in harmony to converge towards a best layout, trying
to ﬁnd the right set of dimensions and a good set of clusters simultane-
ously while also communicating between the algorithms. This pipeline
differs from “One Algorithm Implicitly Includes the Other” in that both
algorithms process the data cooperatively, rather than only executing
one of the algorithms and using its outcome to present both a projection

Fig. 4. Three options for encoding group membership as studied by
Saket et al [79]. In (a), nodes are free-ﬂoating and colored based on
cluster membership. In (b), the cluster coloring remains, and links are
drawn as necessary between some of the nodes. In (c), the nodes are
replaced by colored space-ﬁlling regions to indicate cluster membership.

and a clustering. Since both the dimension reduction algorithm and
the clustering algorithm will begin on the high-dimensional data, this
pipeline will be among the slowest to converge. Niu et al. [72] provides
an example of this pipeline.

This collection of pipelines and examples demonstrates methods
for combining dimension reduction and clustering algorithms, but are
not without limitations. Even extending these pipelines with a looping
structure to iterate through the dimension reduction and clustering
stages is insufﬁcient. To better model this and other similar cognitive
processes, we must extend this discussion of algorithms into the realm
of visualization and interaction; algorithms alone are insufﬁcient for
complex cognition [35].

4 VISUAL REPRESENTATION
After the algorithms have been selected, the next step is determining
how to present the results of the computations to the analyst. In this
section, we ﬁrst discuss common visual representations for dimension-
reduced data and clustered data. This is followed by a discussion of
potential visual outcomes of the pipelines introduced in Sect. 3.3.

4.1 Known Visualization Issues
As the sample interfaces in the bottom row of Fig. 6 show, most dimen-
sion reduction algorithm outputs are shown in scatterplots or node-link
diagrams. These scatterplots come with inherent issues in some cases,
such as difﬁculties in displaying and interpreting the dimensions that
result from an MDS projection. When dealing with large datasets,
the scatterplot or node-link representation of the dimension reduction
output runs a high risk of overplotting, especially if the spatialization
exhibits clear clustering in the layout. One solution for overplotting is
to abstract a cluster of observations into a single glyph to represent a
collection of observations, such as suggested by the Splatterplots im-
plementation [67]. An alternative is to ﬁlter the number of observations
visible in an overdrawn region, keeping a representative ratio of each
cluster in the overdrawn region [19].

While the natural representation of the dimension reduction output
uses a spatial projection like a scatterplot or node-link diagram, the
possibilities for representing cluster membership are much more diverse.
In addition to demonstrating clusters using a collection of nodes in close
spatial proximity, cluster membership can be encoded with colors or
glyphs. Even then, a number of design decisions can be made for how
best to express these memberships by color and shape.

Saket et al. [79] evaluate several encodings of cluster information
(see Fig. 4 for a visual representation of each of these encodings),
relating each to node-based tasks (for example, “Given node X, what
is its background color?”) and group-based tasks (“Given nodes X
and Y, determine if they belong to the same group”). They found that
the addition of group encodings does not negatively impact time and
accuracy on node-based tasks. As would be expected, group-based
tasks were best solved by node-link-group encodings. This outcome
suggests that the visual representation used to encode the clusters in
the projection depends on the tasks that the system addresses.

Fig. 5. Four options for displaying cluster membership as studied by
Jianu et al [51]. In addition to a node-link representation similar to that
included by Saket et al., this study included Linesets [5], GMap [41], and
BubbleSets [24].

Jianu et al. [51] perform a similar evaluation on four visual represen-
tations, including a node-link diagram similar to that studied by Saket
et al. as well as three other visual representations that are shown in
Fig. 5. Linesets [5] include link colors that match the node colors rep-
resenting cluster membership, highlighting connections between nodes
that are in the same cluster or group (top-right of Fig. 5). GMap [41]
is a space-ﬁlling representation that renders a geographic-like map for
clusters, containing all of the nodes in a colored region similar to the
node-link-graph representation studied by Saket et al. (bottom-left
of Fig. 5). Finally, BubbleSets [24] draws isocontours around clus-
ters, effectively balancing the Linesets and GMap representations by
using the isocontours to highlight links connecting members of the
same cluster but becoming space-ﬁlling in regions with high node den-
sity (bottom-right of Fig. 5). This study found that BubbleSets was
the superior representation for group-based tasks, but that encoding
group information onto node-link diagrams adds a 25% time penalty
onto network-based tasks, a conﬂict with the conclusion of Saket et al.
Clearly, more research is needed in this area to resolve such conﬂicts.
In addition to the above, another method for visualizing clusters
in a scatterplot or node-link diagram is to enclose nodes from indi-
vidual clusters in a convex hull [90]. Because k-means solves for
convex clusters based on a distance from an observation to the nearest
cluster centroid, a convex hull visualization may be the most natural
visualization representation for a k-means clustering output.

Moving away from scatterplot and node-link representations, an
alternative representation for clusters is to encode topics into a stream-
graph. For example, Liu et al. use streamgraphs to encode related text
keywords into topical collections, using the streamgraph to show how
the importance of those topics and keywords changes over time [63].

4.2 Algorithm Order Visualizations
Designers have an additional choice regarding which features are em-
phasized in the visual representation. For example, should the spatial
layout of the dimension reduction be emphasized over the cluster as-
signments? Alternatively, should the cluster assignments inform the
layout of the observations? Should we attempt to balance the two
outputs? How much of an impact should the algorithm order play in the
ﬁnal layout? The order in which we execute the dimension reduction
and clustering algorithms should have some impact on the outcome
of the visualization, but the degree to which this execution order is

emphasized can vary by system goals. Here, we describe potential
visualization properties for each of the pipelines described in Sect. 3.3.
Independent Algorithms: Consider the ﬁrst pipeline from Fig. 2,
in which both algorithms execute independently and in parallel. One
potential outcome of this pipeline is to represent clusters using convex
hulls. Here, the dimension reduction algorithm operates to ﬁnd an ideal
layout, while the clustering algorithm separately ﬁnds an ideal cluster
set. When combining the outputs, a potential result is a cluttered visual-
ization that is somewhat ambiguous in the cluster assignments of some
observations due to intersections between the clusters. A potentially
better solution, used by iVisClustering [57], is to use nodes colored by
class in cases of cluster occlusion such as these. Another solution that
allows the convex hulls to remain is to implement layout constraints
(such as those in IPSep-CoLa [31]) so that objects that clearly belong
to different clusters are visibly separated in the spatialization. However,
this requires prior knowledge of key cluster-deﬁning objects, or an
initial clustering computation that precedes the main clustering process.
This also defeats the goal of the pipeline by removing the separation
between dimension reduction and clustering algorithm execution.

Dimension Reduction Preprocessing for Clustering:

In this
pipeline, the output of the dimension reduction algorithm is fed into
the clustering algorithm, enabling clustering on the low-dimensional
reduced data rather than on the initial high-dimensional data. Because
clusters are drawn based on the proximity of observations in the projec-
tion, it is unlikely that these clusters will intersect. As noted previously,
executing the clustering algorithm on the dimension-reduced data may
not produce an optimal clustering on the high-dimensional data, which
could affect the analyst’s comprehension of the projection.

Clustering Preprocessing for Dimension Reduction: In the re-
verse of this process, we now cluster in the initial high-dimensional
data, and use some of that information such as the cluster assignments
to inform the dimension reduction. Such a visualization will likely re-
sult in visibly separated clusters as in the previous case, though perhaps
even more separated because space can be artiﬁcially added between
the clusters. Again, because we execute the dimension reduction algo-
rithm on the cluster assignment information (or other cluster algorithm
output) rather than on the initial high-dimensional data, the dimension
reduction projection may not be optimal and could also affect the ana-
lyst’s comprehension of the projection. More clearly stated, two points
that the dimension reduction algorithm judges to be somewhat similar
(but not similar enough to belong to the same cluster) may have an
artiﬁcially large distance applied between them in this projection.

One Algorithm Implicitly Includes the Other: A pipeline in
which only one algorithm is executed to perform both the dimension
reduction and clustering functions has inherent limitations depending
on which algorithm is performed. For example, if the dimension reduc-
tion algorithm is executed and clustering is applied only on the result
of the dimension-reduced spatialization, the clustering will likely be
far from optimal but the dimension reduction will be ideal. This could
result in a visualization in which, for example, the clusters are simply
assigned based on x-position in the projection.

Global and Local Algorithm Combinations: The global and local
pipeline describes the dimension reduction algorithm as responsible for
the global layout, while the clustering algorithm is responsible for local
reﬁnements and layout. These algorithms work together to create an
overall layout in which the dimension reduction algorithm effectively
lays out the clusters in a meaningful manner while the internal structure
of each cluster is maintained by the clustering algorithm. As such,
the ﬁne details of the projection will not be as accurate spatially as
the dimension reduction outcomes in the Independent Algorithms and
Dimension Reduction Preprocessing for Clustering pipelines, and the
clustering is still executing in part on the low-dimensional projection.
However, the layout should be relatively clean and understandable, and
the overall structure of the projection (e.g., the relative positions of the
clusters) will be meaningful.

Iterative, Alternating Algorithms: The ﬁnal pipeline in Fig. 2
includes both the dimension reduction algorithm and the clustering
algorithm working simultaneously and collaboratively to structure a
projection that is near-optimal for both representations. As such, this

Fig. 6. A selection of interfaces and tools that support Parametric In-
teraction or Observation-Level Interaction. The upper row shows PI
interfaces that include slider bars from Andromeda (PI view) [80], Star
Coordinates [53], and SpinBox widgets from STREAMIT [6]. The lower
row shows OLI interfaces from StarSPIRE [12], Paulovich et al. [73], and
Mamani et al. [66].

structure may produce the best visualizations with respect to the mean-
ing of the data, albeit at the cost of runtime.

A number of further design decisions can be incorporated into the
visualization. We have the option to emphasize the relative distance
between clusters more than the relative distance between pairs of ob-
servations. The visualization space is thus clusters of observations that
are obviously separated from each other in the space, possibly with
another iteration of the dimension reduction algorithm performed on
each individual cluster to generate a local layout. As yet another alter-
native, if the analyst is most interested in the clusters in the projection,
the emphasis could also be placed on the distance between each obser-
vation and the centroid of the cluster that it belongs to. Clusters could
also be artiﬁcially separated by a secondary execution of the dimension
reduction algorithm, but the superior layout determination is dependent
on the distance between each observation and a centroid.

We noted in Sect. 3.2 that it is not possible to combine all pairs
of dimension reduction and clustering algorithms. Likewise, it is not
possible to include all visual representations of dimension reduction
and clustering in the same visualization. For example, dendrograms are
often used to show hierarchical clustering; however, dendrograms are
not a useful visual encoding for dimension reduction algorithms.

5 INTERACTING WITH PROJECTIONS AND CLUSTERS
After displaying a visualization of dimension-reduced and clustered
data, the next step is to provide interactions to afford user exploration
through the dataset. Many studies have been performed and taxonomies
generated for interacting with high-dimensional data in a data analytics
context [7, 17, 88, 93].

In the context of exploring dimension-reduced data projections, two
primary methods exist for modifying an underlying distance function:
Parametric Interaction and Observation-Level Interaction. Surface-
level interactions are also often incorporated into visualization systems,
though these do not modify the underlying model. We begin this sec-
tion by discussing these interaction techniques and some representative
tools, as well as discussing interaction techniques that address cluster-
ing challenges. We follow this with a discussion of potential interaction
techniques that can support interaction with both dimension reduction
and clustering algorithms simultaneously.

5.1 Current Interaction Techniques
Parametric Interaction (PI) refers to manipulating parameters directly
in order to create a new projection and/or clustering assignment. This
presents a difﬁculty to novice or non-mathematically-inclined ana-
lysts, who may not understand how to update a set of weights to cre-
ate the dimension-reduced projection that they desire.
In contrast,
Observation-Level Interaction (OLI) refers to direct manipulation of
the observations, which in turn triggers a backsolving routine to learn
new parameters [34, 46, 59]. In this way, OLI hides the manipulation

Table 3. Sample interactions, organized by type of interaction (rows) and by the type of algorithm affected by the interaction (columns).

PI

OLI

Dimension Reduction

Rotate the projection

Reposition an observation external to clusters
or within a single cluster

Both
Modify the weight on a dimension
Select a different distance function
Reposition an observation into a
different cluster

Surface Measure a distance between observations

Details-on-demand to obtain attribute values

Clustering
Modify the max/min radius of a cluster
Change the number of clusters sought
Change cluster membership
Merge several clusters or split a cluster
Count the size of a cluster
Annotate a cluster

of the model from the analyst, allowing the analyst to perform more
natural direct manipulation interactions with the observations them-
selves. In Andromeda [80], PI allows analysts to modify weights on the
dimensions to modify the distance function directly by interacting with
sliders, while OLI uses an inverse MDS computation to interpret the
semantic meaning of the interaction in order to solve for those weights.
The upper row of Fig. 6 shows sample examples of PI from recent
visualization systems, complemented by some representative interac-
tions in the upper row of Table 3. Horizontal and vertical slider bars are
frequently utilized to enable analysts to interact with model parameters,
despite the fact that these model parameters have a variety of contexts.
Some of these sliders, such as those in Andromeda [80], include addi-
tional glyphs on the sliders to show the values of selected observations
on each dimension. In addition to slider bars, other techniques have
been utilized to support the manipulation of model parameters, such as
the SpinBox widgets of STREAMIT [6] and the transforming axes of
Star Coordinates [53]. PI techniques can also be extended to interact
with dimensions as well as observations, as shown by Turkay et al [86].
As seen in the lower row of Fig. 6 and discussed in Sect. 4, scatter
plots and node-link diagrams are the overwhelming favorite for display-
ing dimension-reduced projections, including those that support OLI.
Despite the ubiquity of these visual representations, individual OLI sys-
tems do display unique features and properties, such as supplementing
the scatterplot with additional views for context [16], supporting PI
in addition to OLI on the scatterplot [80], including local transforma-
tions [66], and focusing exclusively on textual data [12].

An additional consideration for OLI is the “With Respect to What”
problem detailed by Self et al. [80], which is the fundamental challenge
of using rigid algorithms to interpret the ambiguous meaning of an
interaction that involves dragging a node from one part of the display
to another. Andromeda solves this challenge by deﬁning a radius at
both the starting and ending point of the interaction, implying that
the analyst is moving an observation away from all other observations
within x pixels of the source and towards all other observations within
x pixels of the destination of the interaction, though the analyst is
afforded the ability to deselect observations that do not apply to the
interaction [80]. Points contained within this radius are highlighted in
the visual representation, allowing analysts to clearly see the interaction
targets that they are expressing within the projection [47].

In addition to Parametric and Observation-Level Interactions, the
introduction of clusters affords a variety of cluster-based interactions
that can support sensemaking. To begin, OLI can be applied to clusters,
including such interactions as moving clusters together and further
apart to reﬂect similarities and differences between clusters, as well as
transferring that information either to the weights on the clusters or the
weights on the nodes. We can also apply parameter tuning to clusters
at a global level, changing the number of clusters or the radius of all
clusters, or we can tune the parameters of individual clusters, creating
a collection of clusters with a variety of radii. The Vizster system, for
example, includes a PI-style slider bar to change the number of clusters
displayed in the X-ray view [44].

Clusters also introduce new cluster-speciﬁc interactions, such as clus-
ter merging, splitting, and creation [22,45], cluster annotation [54], and
hierarchies of clusters [70]. Performing any of these interactions can
communicate semantic information back to the system, re-executing the
pipeline that may or may not also include re-executing the dimension
reduction algorithm as a result of this user interaction.

5.2 Combined Interaction Techniques
The pipelines discussed in Sect. 3.3 naturally support the Parametric,
Observation-Level, surface-level, and clustering interactions discussed
in the previous subsection. Interactions in general can be designed for
each of these pipelines individually, but it is also useful to consider
interactions that can have meaning to both the dimension reduction
algorithm and the clustering algorithm simultaneously. To do so means
facing similar ambiguity that is addressed by the “With Respect to
What” problem and the issue of overloaded space.

For example, consider an analyst who is interacting with the clus-
tering assignment in a projection. Regardless of whether the analyst is
interacting with high-dimensional or low-dimensional clusters, drag-
ging an observation from one cluster to another is a natural interaction
to correct a misclassiﬁcation. However, the cause of that misclassiﬁca-
tion may be unknown to the analyst. Perhaps the analyst is interacting
with a system that implements the dimension reduction preprocess-
ing pipeline. If that is the case, then the analyst may be correcting
a misclassiﬁcation that results from the clustering operating on the
projected low-dimensional data. Thus, the goal of the system should
be to learn from that interaction, with the goal of getting closer to the
ideal high-dimensional clustering.

Alternatively, if the analyst is interacting with a system that imple-
ments clustering on the high-dimensional data, then performing the
same interaction is correcting for a case where the heuristic clustering
algorithm did not ﬁnd the optimal solution. The system can still learn
from this interaction to correct future clusterings, but the different cause
of the misclassiﬁcation should result in a different model update. These
two misclassiﬁcation corrections may be semantically identical to the
analyst who seeks to correct an error, but the underlying mechanics that
caused and must correct the misclassiﬁcation are different.

The same is true of an analyst interacting with observations in a
dimension-reduced projection. If an analyst drags an observation, it
may simply be that the analyst wishes to adjust the strength of the
relationship between two observations. However, adjusting the strength
of a relationship calculated on the high-dimensional data is inherently
different than adjusting the strength of a relationship calculated on clus-
ter algorithm output. And does the semantic meaning of the interaction
change if that drag interaction crosses a cluster boundary?

The introduction of explicitly-deﬁned clusters allows for a formal
target against which to judge interactions. When explicit clusters are
deﬁned, the analyst has four clearly deﬁned “with respect to what”
operations: (1) moving an observation into a cluster, (2) moving an
observation out of a cluster, (3) moving an observation from one cluster
into another, and (4) moving an observation without changing cluster
membership [90]. Each of these interactions can be designed to have
an effect on both the dimension reduction algorithm and the clustering
algorithm. Keeping an observation within a cluster, or dragging it from
one cluster into another, provides information to the clustering algo-
rithm that the classiﬁcation is either correct or incorrect. At the same
time, relocating an observation to a different position communicates
suggested distance information between the moved observation and
one or more additional observations in the projection. Each of these
algorithms can thus work to update the weight vector that then leads to
a projection and clustering update with this new information.

When mapping interactions to the pipelines summarized in Fig. 2,
choosing the primary target of the interaction is important even when
an interaction affects both algorithms. In the previous example, the

pipeline is implemented with the interaction primarily occurring on the
clusters, changing the cluster assignment of observations in order to up-
date the dimension reduction projection [90]. In contrast, “Be the Data”
also implements the same pipeline but with an interaction primarily
on the observation layout, using the dimension reduction algorithm to
update the clusters [20]. These two systems are both implementations
of the same pipeline, but place the interaction on different algorithms
to answer different questions about the high-dimensional data. Thus,
interactions can be considered independent of the pipelines.

A further open question to be addressed regards interactions on the
clusters themselves. If an analyst drags a cluster or interacts with it in
another manner, what adjustments should be made to the observations
and relationships within that cluster, as well as the relationships that
cross that cluster boundary?

6 DISCUSSION
Combining dimension reduction and clustering algorithms into the
same visualization system provides a number of opportunities for visu-
alization and interaction design. A system in which the two algorithm
classes cooperate for exploratory data analysis results in a relationship
in which the projection space (the outcome of the dimension reduction
algorithm) helps to explain the meaning of the clusters in the space,
while the clusters themselves help to explain the meaning of the space.
Including a machine learning aspect into a visualization system to
permit the dimension reduction and clustering algorithms to learn from
the actions of the analyst presents a number of additional challenges
for interaction design. In particular, the overloaded space metaphor
discussed in Sect. 3.1 causes challenges, as interactions within the
system must be mapped to at least one algorithm and may ambiguously
be mapped to both. For example, if an analyst drags and drops a
datapoint to reposition it in space, but the new coordinates did not
result in a cluster reassignment, should the clustering algorithm learn
nothing, or did the analyst provide some “fuzzy” clustering feedback
to the algorithm? A notion of iterative reﬁnement, in which the analyst
gradually trains the algorithms and offers corrections to mistakes at
each iteration is necessary in these cases. Such an iterative reﬁnement
process mimics Pirolli and Card’s Sensemaking Process [76].

Maintaining an analyst’s mental map during layout adjustments is a
well-studied problem [68], and is another factor that should be consid-
ered in visualization and interaction design for dimension reduction and
clustering systems. ForceSPIRE and Andromeda approach this mental
map challenge in different ways. ForceSPIRE, using a force-directed
layout, maintains the positions of nearly all observations during an
interaction, only altering the positions of observations near the interac-
tion [33]. In Andromeda, on the other hand, it is possible that all ob-
servations could move the entire distance across the space. The system
cognitively aids the analyst to understand such broad changes with an
animation slider, affording the analyst with the ability to incrementally
follow the post-interaction transition, as well as a layout stabilization
module to suppress the rotation invariant property of the Weighted
Multidimensional Scaling dimension reduction algorithm [80].

This work focuses on exploring the breadth of design options avail-
able to visualization researchers when combining dimension reduction
and clustering algorithms. Our goal with this work is to highlight
many of the decisions that exist in this design space, spurring further
exploration of this space with new tools. While we present a number of
design questions that must be addressed in creating such a visualiza-
tion system, we do not claim to answer any of these questions, as the
answers to many of them depend on the tasks and goals of the system.

7 CONCLUSION
The combination of dimension reduction and clustering algorithms rep-
resents an immense design space, including considerations of algorithm
selection and order, tasks, visualization, and interaction. In this paper,
we have provided a survey of each of these considerations, describing
existing research and discussing relevant design decisions applicable to
current and future systems (summarized in Table 4).

Returning to our discussion of the “Be the Data” interaction ﬁrst
addressed in Sect 3, we saw a smooth transition from discrete to con-

Table 4. A summary of the design challenges and questions discussed
throughout the paper regarding the combination of dimension reduction
and clustering algorithms.

Section

2.1 & 2.2

3.2

3.3

4.1

4.2

5.2

Design Decision
In general, clustering places an emphasis on re-
lationships within and between clusters.
In con-
trast, dimension reduction emphasizes observation-
to-observation relationships. Which of these tasks is
the primary goal of the analyst?
What properties of the data is the visualization seek-
ing to highlight? Which properties of the data are
the system and analyst trying to discover? Should
the primary goal of the visualization system be em-
phasizing observation relationships, clusters of ob-
servations, or both? Should the dimension reduction
and clustering algorithms use the same distance func-
tion (if possible), or should each algorithm use an
independent method of measuring similarity?
Which order and interaction of dimension reduction
and clustering algorithms best models the task that
the visualization system is addressing?
How can we encode distances and cluster member-
ship information when both algorithms are present?
As the dimension reduction and clustering algo-
rithms are competing in the same visualization, what
features should be emphasized in the visualization
to best address the problem?
Should interactions be designed independently for
the dimension reduction and clustering algorithms,
or should a given interaction affect both algorithms?

tinuous thinking. The students initially formed the clusters of Edible
and Inedible animals and then positioned those clusters in space, ini-
tially mimicking the cluster preprocessing pipeline. The transition from
this projection into a spectrum of Edibility amounts to iterative and
interactive reﬁnement of those initial clusters into a broader projection.
Without the interaction component, the pipelines could not successfully
model this student behavior.

A useful future direction for research would be a cognitive study,
further attempting to understand how analysts cognitively combine
the ideas of dimension reduction and clustering in both virtual and
non-virtual spaces. Such a study can further inform the pipelines,
visualizations, and interactions presented in this work.

ACKNOWLEDGMENTS
This research was supported by NSF Grants IIS-1447416, IIS-1633363,
and DGE-1545362, as well as by a grant from General Dynamics
Mission Systems. The authors would like to recognize the role of
comments from reviewers and discussions with InfoVis Lab @ VT
research group members in improving this work.

REFERENCES
[1] J. Abello, F. V. Ham, and N. Krishnan. Ask-graphview: A large scale graph
visualization system. IEEE Transactions on Visualization and Computer
Graphics, 12(5):669–676, Sept 2006. doi: 10.1109/TVCG.2006.120

[2] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the Surprising
Behavior of Distance Metrics in High Dimensional Space, pp. 420–434.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2001. doi: 10.1007/3-540
-44503-X 27

[3] M. N. Ahmed, S. M. Yamany, N. Mohamed, A. A. Farag, and T. Moriarty.
A modiﬁed fuzzy c-means algorithm for bias ﬁeld estimation and segmen-
tation of mri data. IEEE Transactions on Medical Imaging, 21(3):193–199,
March 2002. doi: 10.1109/42.996338

[4] M. S. Aldenderfer and R. K. Blashﬁeld. Cluster analysis. SAGE publica-

tions, Beverly Hills, USA, 1984.

[5] B. Alper, N. Riche, G. Ramos, and M. Czerwinski. Design study of linesets,
a novel set visualization technique. IEEE Transactions on Visualization

and Computer Graphics, 17(12):2259–2267, Dec 2011. doi: 10.1109/
TVCG.2011.186

[6] J. Alsakran, Y. Chen, Y. Zhao, J. Yang, and D. Luo. Streamit: Dynamic
visualization and interactive exploration of text streams. In 2011 IEEE
Paciﬁc Visualization Symposium, pp. 131–138, March 2011. doi: 10.1109/
PACIFICVIS.2011.5742382

[7] R. Amar, J. Eagan, and J. Stasko. Low-level components of analytic
activity in information visualization. In IEEE Symposium on Information
Visualization, 2005. INFOVIS 2005., pp. 111–117, Oct 2005. doi: 10.
1109/INFVIS.2005.1532136

[8] C. Andrews, A. Endert, and C. North. Space to think: Large high-
resolution displays for sensemaking. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI ’10, pp. 55–64.
ACM, New York, NY, USA, 2010. doi: 10.1145/1753326.1753336

[9] J. C. Bezdek. Pattern Recognition with Fuzzy Objective Function Algo-

rithms. Kluwer Academic Publishers, Norwell, MA, USA, 1981.

[10] D. M. Blei and M. I. Jordan. Variational inference for dirichlet process

mixtures. Bayesian Analysis, 1:121–144, 2005.

[11] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal

of machine Learning research, 3(Jan):993–1022, 2003.

[12] L. Bradel, C. North, L. House, and S. Leman. Multi-model semantic
interaction for text analytics. In 2014 IEEE Conference on Visual Analytics
Science and Technology (VAST), pp. 163–172, Oct 2014. doi: 10.1109/
VAST.2014.7042492

[13] P. S. Bradley, U. Fayyad, and C. Reina. Scaling clustering algorithms to
large databases. In Proceedings of the Fourth International Conference on
Knowledge Discovery and Data Mining, KDD’98, pp. 9–15. AAAI Press,
1998.

[14] M. Brehmer and T. Munzner. A multi-level typology of abstract visualiza-
tion tasks. IEEE Transactions on Visualization and Computer Graphics,
19(12):2376–2385, Dec 2013. doi: 10.1109/TVCG.2013.124

[15] M. Brehmer, M. Sedlmair, S. Ingram, and T. Munzner. Visualizing
dimensionally-reduced data: Interviews with analysts and a character-
ization of task sequences. In Proceedings of the Fifth Workshop on Beyond
Time and Errors: Novel Evaluation Methods for Visualization, BELIV
’14, pp. 1–8. ACM, New York, NY, USA, 2014. doi: 10.1145/2669557.
2669559

[16] E. T. Brown, J. Liu, C. E. Brodley, and R. Chang. Dis-function: Learning
In 2012 IEEE Conference on Visual
distance functions interactively.
Analytics Science and Technology (VAST), pp. 83–92, Oct 2012. doi: 10.
1109/VAST.2012.6400486

[17] A. Buja, D. Cook, and D. F. Swayne. Interactive high-dimensional data
visualization. Journal of Computational and Graphical Statistics, 5(1):78–
99, 1996. doi: 10.1080/10618600.1996.10474696

[18] J. D. Carroll and J.-J. Chang. Analysis of individual differences in mul-
tidimensional scaling via an n-way generalization of “eckart-young” de-
composition. Psychometrika, 35(3):283–319, 1970.

[19] H. Chen, W. Chen, H. Mei, Z. Liu, K. Zhou, W. Chen, W. Gu, and K. L.
Ma. Visual abstraction and exploration of multi-class scatterplots. IEEE
Transactions on Visualization and Computer Graphics, 20(12):1683–1692,
Dec 2014. doi: 10.1109/TVCG.2014.2346594

[20] X. Chen, J. Z. Self, L. House, and C. North. Be the data: A new approach
In IEEE Virtual Reality 2016 Workshop on

for immersive analytics.
Immersive Analytics, 03/2016.

[21] J. Choo, S. Bohn, and H. Park. Two-stage framework for visualization
of clustered high dimensional data. In 2009 IEEE Symposium on Visual
Analytics Science and Technology, pp. 67–74, Oct 2009. doi: 10.1109/
VAST.2009.5332629

[22] J. Choo, C. Lee, C. K. Reddy, and H. Park. Utopian: User-driven topic
modeling based on interactive nonnegative matrix factorization. IEEE
Transactions on Visualization and Computer Graphics, 19(12):1992–2001,
Dec 2013. doi: 10.1109/TVCG.2013.212

[23] J. Chuang and D. J. Hsu. Human-centered interactive clustering for data
analysis. Conference on Neural Information Processing Systems (NIPS).
Workshop on Human-Propelled Machine Learning, 2014.

[24] C. Collins, G. Penn, and S. Carpendale. Bubble sets: Revealing set
relations with isocontours over existing visualizations. IEEE Transactions
on Visualization and Computer Graphics, 15(6):1009–1016, Nov 2009.
doi: 10.1109/TVCG.2009.122

[25] R. Cordeiro de Amorim and P. Komisarczuk. On Initializations for the
Minkowski Weighted K-Means, pp. 45–55. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2012. doi: 10.1007/978-3-642-34156-4 6

[26] I. S. Dhillon and D. S. Modha. Concept decompositions for large sparse

text data using clustering. Machine Learning, 42(1):143–175, 2001. doi:
10.1023/A:1007612920971

[27] C. Ding and X. He. K-means clustering via principal component analysis.
In Proceedings of the Twenty-ﬁrst International Conference on Machine
Learning, ICML ’04, pp. 29–. ACM, New York, NY, USA, 2004. doi: 10.
1145/1015330.1015408

[28] C. Ding and T. Li. Adaptive dimension reduction using discriminant
analysis and k-means clustering. In Proceedings of the 24th International
Conference on Machine Learning, ICML ’07, pp. 521–528. ACM, New
York, NY, USA, 2007. doi: 10.1145/1273496.1273562

[29] D. L. Donoho. High-dimensional data analysis: The curses and blessings
of dimensionality. In AMS Conference on Math Challenges of the 21st
Century, 2000.

[30] J. C. Dunn. A fuzzy relative of the isodata process and its use in detecting
compact well-separated clusters. Journal of Cybernetics, 3(3):32–57, 1973.
doi: 10.1080/01969727308546046

[31] T. Dwyer, Y. Koren, and K. Marriott. Ipsep-cola: An incremental pro-
cedure for separation constraint layout of graphs. IEEE Transactions on
Visualization and Computer Graphics, 12(5):821–828, Sept 2006. doi: 10.
1109/TVCG.2006.156

[32] C. F. Eick, N. Zeidat, and Z. Zhao. Supervised clustering - algorithms and
beneﬁts. In 16th IEEE International Conference on Tools with Artiﬁcial
Intelligence, pp. 774–776, Nov 2004. doi: 10.1109/ICTAI.2004.111

[33] A. Endert, S. Fox, D. Maiti, S. Leman, and C. North. The semantics of
clustering: Analysis of user-generated spatializations of text documents.
In Proceedings of the International Working Conference on Advanced
Visual Interfaces, AVI ’12, pp. 555–562. ACM, New York, NY, USA,
2012. doi: 10.1145/2254556.2254660

[34] A. Endert, C. Han, D. Maiti, L. House, S. Leman, and C. North.
Observation-level interaction with statistical models for visual analyt-
ics. In 2011 IEEE Conference on Visual Analytics Science and Technology
(VAST), pp. 121–130, Oct 2011. doi: 10.1109/VAST.2011.6102449

[35] A. Endert, M. S. Hossain, N. Ramakrishnan, C. North, P. Fiaux, and
C. Andrews. The human is the loop: new directions for visual analytics.
Journal of Intelligent Information Systems, 43(3):411–435, 2014. doi: 10.
1007/s10844-014-0304-9

[36] V. Estivill-Castro. Why so many clustering algorithms: A position paper.
SIGKDD Explor. Newsl., 4(1):65–75, June 2002. doi: 10.1145/568574.
568575

[37] U. M. Fayyad, A. Wierse, and G. G. Grinstein. Information visualization

in data mining and knowledge discovery. Morgan Kaufmann, 2002.

[38] I. K. Fodor. A Survey of Dimension Reduction Techniques. May 2002. doi:

10.2172/15002155

[39] S. L. France and J. D. Carroll. Two-way multidimensional scaling: A
review. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), 41(5):644–661, Sept 2011. doi: 10.1109/
TSMCC.2010.2078502

[40] J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for
exploratory data analysis. IEEE Transactions on Computers, C-23(9):881–
890, Sept 1974. doi: 10.1109/T-C.1974.224051

[41] E. R. Gansner, Y. Hu, and S. Kobourov. Gmap: Visualizing graphs
and clusters as maps. In 2010 IEEE Paciﬁc Visualization Symposium
(PaciﬁcVis), pp. 201–208, March 2010. doi: 10.1109/PACIFICVIS.2010.
5429590

[42] I. Guyon and A. Elisseeff. An introduction to variable and feature selection.

J. Mach. Learn. Res., 3:1157–1182, Mar. 2003.
[43] H. H. Harman. Modern factor analysis. 1960.
[44] J. Heer and D. Boyd. Vizster: visualizing online social networks. In
IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., pp.
32–39, Oct 2005. doi: 10.1109/INFVIS.2005.1532126

[45] C. Heine and G. Scheuermann. Manual clustering reﬁnement using inter-
action with blobs. In Proceedings of the 9th Joint Eurographics / IEEE
VGTC Conference on Visualization, EUROVIS’07, pp. 59–66. Eurograph-
ics Association, Aire-la-Ville, Switzerland, Switzerland, 2007. doi: 10.
2312/VisSym/EuroVis07/059-066

[46] L. House, S. Leman, and C. Han. Bayesian visual analytics: Bava. Sta-
tistical Analysis and Data Mining, 8(1):1–13, 2015. doi: 10.1002/sam.
11253

[47] X. Hu, L. Bradel, D. Maiti, L. House, C. North, and S. Leman. Semantics
of directly manipulating spatializations. IEEE Transactions on Visual-
ization and Computer Graphics, 19(12):2052–2059, Dec 2013. doi: 10.
1109/TVCG.2013.188

[48] J. Z. Huang, M. K. Ng, H. Rong, and Z. Li. Automated variable weighting

in k-means type clustering. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27(5):657–668, May 2005. doi: 10.1109/TPAMI.
2005.95

[49] A. Hyv¨arinen, J. Karhunen, and E. Oja. Independent component analysis,

vol. 46. John Wiley & Sons, 2004.

[50] S. Ingram, T. Munzner, and M. Olano. Glimmer: Multilevel mds on
the gpu. IEEE Transactions on Visualization and Computer Graphics,
15(2):249–261, March 2009. doi: 10.1109/TVCG.2008.85

[51] R. Jianu, A. Rusu, Y. Hu, and D. Taggart. How to display group infor-
mation on node-link diagrams: An evaluation. IEEE Transactions on
Visualization and Computer Graphics, 20(11):1530–1541, Nov 2014. doi:
10.1109/TVCG.2014.2315995

[52] P. Joia, F. Petronetto, and L. G. Nonato. Uncovering representative groups
in multidimensional projections. In Proceedings of the 2015 Eurographics
Conference on Visualization, EuroVis ’15, pp. 281–290. Eurographics
Association, Aire-la-Ville, Switzerland, Switzerland, 2015. doi: 10.1111/
cgf.12640

[53] E. Kandogan. Star coordinate: A multi-dimensional visualization tech-
nique with uniform treatment of dimensions. In Proceedings of the IEEE
Information Visualization Symposium, vol. 650, p. 22.

[54] E. Kandogan. Just-in-time annotation of clusters, outliers, and trends
in point-based data visualizations. In 2012 IEEE Conference on Visual
Analytics Science and Technology (VAST), pp. 73–82, Oct 2012. doi: 10.
1109/VAST.2012.6400487

[55] T. Kohonen. The self-organizing map. Proceedings of the IEEE,

78(9):1464–1480, Sep 1990. doi: 10.1109/5.58325

[56] H.-P. Kriegel, P. Kr¨oger, and A. Zimek. Clustering high-dimensional data:
A survey on subspace clustering, pattern-based clustering, and correlation
clustering. ACM Trans. Knowl. Discov. Data, 3(1):1:1–1:58, Mar. 2009.
doi: 10.1145/1497577.1497578

[57] H. Lee, J. Kihm, J. Choo, J. Stasko, and H. Park.

ivisclustering: An
interactive visual document clustering via topic modeling. Computer
Graphics Forum, 31(3pt3):1155–1164, 2012. doi: 10.1111/j.1467-8659.
2012.03108.x

[58] J. A. Lee and M. Verleysen. Nonlinear dimensionality reduction. Springer

Science & Business Media, 2007.

[59] S. C. Leman, L. House, D. Maiti, A. Endert, and C. North. Visual to

276, 1953. doi: 10.1007/BF02289263

[72] D. Niu, J. G. Dy, and M. I. Jordan. Dimensionality reduction for spectral
clustering. In Proceedings of the 14th International Conference Artiﬁcial
Intelligence and Statistics, AISTATS ’11, pp. 552–560. ACM, New York,
NY, USA, 2011.

[73] F. Paulovich, D. Eler, J. Poco, C. Botha, R. Minghim, and L. Nonato.
Piecewise laplacian-based projection for interactive data exploration and
organization. Computer Graphics Forum, 30(3):1091–1100, 2011. doi: 10
.1111/j.1467-8659.2011.01958.x

[74] K. Pearson. Principal components analysis. The London, Edinburgh and

Dublin Philosophical Magazine and Journal, 6(2):566, 1901.

[75] D. Pelleg, A. W. Moore, et al. X-means: Extending k-means with efﬁcient
estimation of the number of clusters. In ICML, vol. 1, pp. 727–734, 2000.
[76] P. Pirolli and S. Card. The sensemaking process and leverage points for an-
alyst technology as identiﬁed through cognitive task analysis. Proceedings
of International Conference on Intelligence Analysis, pp. 2–4, 2005.

[77] B. Rieck and H. Leitte. Persistent homology for the evaluation of dimen-
sionality reduction schemes. Computer Graphics Forum, 34(3):431–440,
2015. doi: 10.1111/cgf.12655

[78] D. Ro and H. Pe. Pattern classiﬁcation and scene analysis. John Wiley &

Sons, New York, USA, 1973.

[79] B. Saket, P. Simonetto, S. Kobourov, and K. Brner. Node, node-link,
and node-link-group diagrams: An evaluation. IEEE Transactions on
Visualization and Computer Graphics, 20(12):2231–2240, Dec 2014. doi:
10.1109/TVCG.2014.2346422

[80] J. Z. Self, R. K. Vinayagam, J. T. Fry, and C. North. Bridging the gap
between user intention and model parameters for human-in-the-loop data
analytics. In Proceedings of the Workshop on Human-In-the-Loop Data
Analytics, HILDA ’16, pp. 3:1–3:6. ACM, New York, NY, USA, 2016.
doi: 10.1145/2939502.2939505

[81] P.-N. Tan, M. Steinbach, and V. Kumar. Data mining cluster analysis:
basic concepts and algorithms. In Introduction to data mining, chap. 8.
Pearson Education India, 2013.

[82] J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric frame-
work for nonlinear dimensionality reduction. Science, 290(5500):2319–
2323, 2000. doi: 10.1126/science.290.5500.2319

[83] R. L. Thorndike. Who belongs in the family? Psychometrika, 18(4):267–

[84] M. E. Tipping and C. M. Bishop. Probabilistic principal component
analysis. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 61(3):611–622, 1999.

[85] W. S. Torgerson. Theory and methods of scaling. 1958.
[86] C. Turkay, P. Filzmoser, and H. Hauser. Brushing dimensions - a dual
visual analysis model for high-dimensional data. IEEE Transactions on
Visualization and Computer Graphics, 17(12):2591–2599, Dec 2011. doi:
10.1109/TVCG.2011.178

[87] F. Valafar. Pattern recognition techniques in microarray data analysis.
Annals of the New York Academy of Sciences, 980(1):41–64, 2002. doi: 10
.1111/j.1749-6632.2002.tb04888.x

[88] T. von Landesberger, S. Fiebig, S. Bremm, A. Kuijper, and D. W. Fellner.
Interaction Taxonomy for Tracking of User Actions in Visual Analytics
Applications, pp. 653–670. Springer New York, New York, NY, 2014. doi:
10.1007/978-1-4614-7485-2 26

[89] K. Wagstaff, C. Cardie, S. Rogers, S. Schr¨odl, et al. Constrained k-means
clustering with background knowledge. In Proceedings of the Eighteenth
International Conference on Machine Learning, vol. 1, pp. 577–584, 2001.
[90] J. Wenskovitch and C. North. Observation-level interaction with clustering
and dimension reduction algorithms. In Proceedings of the 2nd Workshop
on Human-In-the-Loop Data Analytics, HILDA’17, pp. 14:1–14:6. ACM,
New York, NY, USA, 2017. doi: 10.1145/3077257.3077259

[91] A. Wism¨uller, M. Verleysen, M. Aupetit, and J. A. Lee. Recent advances
in nonlinear dimensionality reduction, manifold and topological learning.
In ESANN, 2010.

[92] R. Xu and D. Wunsch. Survey of clustering algorithms. IEEE Transactions
on Neural Networks, 16(3):645–678, May 2005. doi: 10.1109/TNN.2005.
845141

[93] J. S. Yi, Y. a. Kang, and J. Stasko. Toward a deeper understanding of the
role of interaction in information visualization. IEEE Transactions on
Visualization and Computer Graphics, 13(6):1224–1231, Nov 2007. doi:
10.1109/TVCG.2007.70515

[94] H. Zha, X. He, C. Ding, M. Gu, and H. D. Simon. Spectral relaxation
for k-means clustering. In Advances in Neural Information Processing
Systems, pp. 1057–1064, 2002.

parametric interaction (v2pi). PloS one, 8(3), 2013.

[60] M. Levandowsky and D. Winter. Distance between sets. Nature,

234(5323):34–35, 1971.

[61] S. Liu, D. Maljovec, B. Wang, P. T. Bremer, and V. Pascucci. Visualizing
high-dimensional data: Advances in the past decade. IEEE Transactions
on Visualization and Computer Graphics, 23(3):1249–1268, March 2017.
doi: 10.1109/TVCG.2016.2640960

[62] S. Liu, B. Wang, P.-T. Bremer, and V. Pascucci. Distortion-guided
structure-driven interactive exploration of high-dimensional data. Com-
puter Graphics Forum, 33(3):101–110, 2014. doi: 10.1111/cgf.12366

[63] S. Liu, M. X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian. Interactive,
topic-based visual text summarization and analysis. In Proceedings of
the 18th ACM Conference on Information and Knowledge Management,
CIKM ’09, pp. 543–552. ACM, New York, NY, USA, 2009. doi: 10.
1145/1645953.1646023

[64] S. Lloyd. Least squares quantization in pcm.

IEEE Transactions on
Information Theory, 28(2):129–137, Mar 1982. doi: 10.1109/TIT.1982.
1056489

[65] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. J. Mach.

Learn. Res., 9:2579–2605, Sept. 2008.

[66] G. M. H. Mamani, F. M. Fatore, L. G. Nonato, and F. V. Paulovich.
User-driven feature space transformation. Computer Graphics Forum,
32(3pt3):291–299, 2013. doi: 10.1111/cgf.12116

[67] A. Mayorga and M. Gleicher. Splatterplots: Overcoming overdraw in
scatter plots. IEEE Transactions on Visualization and Computer Graphics,
19(9):1526–1538, Sept 2013. doi: 10.1109/TVCG.2013.65

[68] K. Misue, P. Eades, W. Lai, and K. Sugiyama. Layout adjustment and the
mental map. Journal of Visual Languages & Computing, 6(2):183 – 210,
1995. doi: 10.1006/jvlc.1995.1010

[69] T. Munzner. Visualization Analysis and Design. CRC Press, 2014.
[70] E. J. Nam, Y. Han, K. Mueller, A. Zelenyuk, and D. Imre. Clustersculptor:
A visual analytics tool for high-dimensional data. In 2007 IEEE Sympo-
sium on Visual Analytics Science and Technology, pp. 75–82, Oct 2007.
doi: 10.1109/VAST.2007.4388999

[71] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis

and an algorithm. In NIPS, vol. 14, pp. 849–856, 2001.

",False,2018.0,{},False,False,journalArticle,False,ENF2YTJG,"[{u'tag': u'analytics'}, {u'tag': u'clustering'}, {u'tag': u'dimension reduction'}, {u'tag': u'projection'}, {u'tag': u'visual'}]",self.user,False,False,False,False,,"<p>The paper explores the decisions and design factors that have to be taken into account in the case of combining dimension reduction and clustering into the same visualization system.</p>
<p>Interesting Sentence:</p>
<p><strong>To better model the pipelines and models we need to extend the discussion into the realm of the visualization and interaction; algorithms alone are insufficient for complex cognition.</strong></p>
<p>They list different Dimension reduction algorithms:</p>
<p>    Linear:</p>
<p>        Factor Analysis</p>
<p>        PCA</p>
<p>        Probabilistic PCA</p>
<p>        Projection Pursuit</p>
<p>   </p>
<p>    Both:</p>
<p>        Feature Selection</p>
<p>        Independent Component Analysis</p>
<p>        Multidimensional Scaling</p>
<p>        Weighted MDS</p>
<p>    Non-Linear:</p>
<p>        Glimmer</p>
<p>        Isomap</p>
<p>        LDA</p>
<p>        T-SNE</p>
<p>Manhattan distances are preferable to Euclidian distances for high-dimensional data.</p>
<p>They define several tasks performed over Dimensionality reduction and clustering:</p>
<p>   Change the projection</p>
<p>   Identify similarity based on distance</p>
<p>   Positioning elements close to each other is a clustering task.</p>
<p>They mention that in order to create this interaction between both algorithms the pipeline order is important in order to understand how the interaction should affect the model:</p>
<p>    1. Independent Algorithms</p>
<p>    2. Dimension reduction then clustering</p>
<p>    3. Clustering preprocessing for dimension reduction</p>
<p>    4. Execute one of them cluster or reduce the dimensions on the result.</p>
<p>    5. Global and local combinations</p>
<p>    6 Iterative, Alternating Algorithms</p>
<p>But they also mention that this is hidden from the user.</p>
<p> </p>
<p>They have a lot of citation and papers about interaction with projection and clusters.</p>
<p>They define <strong>OLI</strong>: Observation Level interaction that refers to the direct manipulation of the observations which in turn triggers a black solving routing o learn new parameter.</p>
<p>This OLI interaction adds another level of complexity which is expressed by the authors as ""With respect to what"" detailed in citation 80.</p>
<p>For example for clusters:</p>
<p>       1. Moving an observation into a cluster</p>
<p>        2. Moving an observation out of a cluster</p>
<p>        3. Moving an observation from a cluster into another</p>
<p>        4. Moving an observation without changing cluster membership.</p>
<p><strong>The outcome of dimension reduction algorithm helps to explain the meaning of the clusters in the space, while the cluster themselves help to explain the meaning of the space. </strong>This is how in data context map they are able to add the context into the space by using the clusters.</p>",Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics,ENF2YTJG,False,False
7N8IFD7Z,WSWI9XMV,"See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/6451414

Value	and	Relation	Display:	Interactive	Visual
Exploration	of	Large	Data	Sets	with	Hundreds	of
Dimensions

Article		in		IEEE	Transactions	on	Visualization	and	Computer	Graphics	·	May	2007

DOI:	10.1109/TVCG.2007.1010	·	Source:	PubMed

CITATIONS
37

5	authors,	including:

READS
78

Matthew	Ward
Worcester	Polytechnic	Institute

133	PUBLICATIONS			3,082	CITATIONS			

Elke	Rundensteiner
Worcester	Polytechnic	Institute

567	PUBLICATIONS			7,531	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Matthew	Ward	on	12	March	2014.

The	user	has	requested	enhancement	of	the	downloaded	file.

Value and Relation Display: Interactive Visual
Exploration of Large Datasets with Hundreds of

Dimensions

1

Jing Yang

Dept of Computer Science

UNC Charlotte

jyang13@uncc.edu

Daniel Hubball
Dept of Computer Science
University of Wales Swansea

csdan@swansea.ac.uk

Matthew Ward
Dept of Computer Science

Worcester Polytechnic Institute

matt@cs.wpi.edu

Elke Rundensteiner
Dept of Computer Science

Worcester Polytechnic Institute

rundenst@cs.wpi.edu

William Ribarsky
Dept of Computer Science

UNC Charlotte

ribarsky@uncc.edu

Abstract— Few existing visualization systems can handle large
datasets with hundreds of dimensions, since high dimensional
datasets cause clutter on the display and large response time in
interactive exploration. In this paper, we present a signiﬁcantly
improved multi-dimensional visualization approach named Value
and Relation (VaR) display that allows users to effectively and
efﬁciently explore large datasets with several hundred dimen-
sions. In the VaR display, data values and dimension relationships
are explicitly visualized in the same display by using dimension
glyphs to explicitly represent values in dimensions and glyph
layout to explicitly convey dimension relationships. In particular,
pixel-oriented techniques and density-based scatterplots are used
to create dimension glyphs to convey values. Multi-dimensional
scaling, Jigsaw map hierarchy visualization techniques, and
an animation metaphor named Rainfall are used to convey
relationships among dimensions. A rich set of interaction tools
have been provided to allow users to interactively detect patterns
of interest in the VaR display. A prototype of the VaR display
has been fully implemented. The case studies presented in this
paper show how the prototype supports interactive exploration of
datasets of several hundred dimensions. A user study evaluating
the prototype is also reported in this paper.

Index Terms— Multi-dimensional visualization, high dimen-

sional datasets, visual analytics.

datasets in the Information Visualization ﬁeld. They include:
(cid:129) Using condensed displays to provide as much information
as possible to users. Typical approaches include pixel-
oriented techniques [12], [13] and density-based displays
[9], [24]. For example, in pixel-oriented techniques, infor-
mation is so condensed that each pixel presents a single
data value.

(cid:129) Examining relationships among dimensions to discover
lower dimensional spaces with signiﬁcant features. Ex-
ample approaches include ranking low dimensional pro-
jections by their features such as linear relationships [19],
and placing dimensions in a layout revealing their rela-
tionships to help users construct meaningful subspaces
[28].

(cid:129) Providing a rich set of interactions to allow users to
explore datasets from multiple coordinated views. In
these views, different subsets of dimensions and/or data
items can be examined at different levels of detail us-
ing different visualization techniques. Examples of such
approaches include the Hierarchical Parallel Coordinates
[10] and the VIS-5D system [11].

I. INTRODUCTION

Large datasets with hundreds of dimensions are common
in applications such as image analysis, ﬁnance, bioinformatics
and anti-terrorism. For example, in order to detect the semantic
contents of large image collections, it is common to analyze
hundreds of low level visual attributes of the images. It is
a challenge to make decisions based on these datasets, since
they are hard to analyze due to the dimensionality curse [5],
i.e., the lack of data separation in high dimensional space.
Using multi-dimensional visualization techniques to present
this data to analysts and allowing them to interactively explore
and understand the datasets are an important approach to
addressing this challenge. However, most traditional multi-
dimensional visualization techniques suffer from visual clutter
and only scale up to tens of dimensions. Up to now, few multi-
dimensional visualization systems have claimed to be scalable
to datasets with hundreds of dimensions. In this paper, we
present such a system, called the Value and Relation (VaR)
display, which is an improved version of a technique reported
in an earlier paper [27].

Our work is based on multiple concepts proposed and
explored in prior efforts toward visual exploration of large

The concepts above are signiﬁcant features of the VaR
display since its initial version [27]. In the ﬁrst version (see
ﬁgures 1a and b), pixel-oriented displays were used to show
data values and group them into dimension glyphs representing
individual dimensions. The dimension glyphs were then po-
sitioned on the screen using a fast Multi-dimensional scaling
(MDS) algorithm [4] according to dimension correlations to
reveal their inter-relationships (dimension correlation is used
since it is a typical measure of dimension relationships, but
other relationship measures can also be used). A rich set
of interactions were provided to facilitate navigation in the
display and generate lower dimensional spaces of interest. To
differentiate the ﬁrst version from the improved version, we
call it the Pixel MDS VaR display.

In the improved version of VaR presented in this paper,
these features are signiﬁcantly strengthened. A density-based
scatterplot [9], [24] has been added to the system as an alter-
nate approach to generating dimension glyphs. A Jigsaw map
layout [23] and the Rainfall metaphor have been added into the
system as alternate dimension glyph layout approaches. The
new version also supports a broader range of interaction tools
than the original version, including a new data item selection

2

Fig. 1.
(a) Illustration of the VaR display. On the left is the spreadsheet of a 4-dimensional dataset with each column representing a dimension. On the
bottom is a matrix that records the pair-wise relationships (such as correlations) among the dimensions. In the middle is the glyph of the fourth dimension.
On the right is the VaR display of the dataset. (b) The Pixel MDS VaR display of the Image-89 dataset (89 dimensions, 10,417 data itmes). (c) The X-Ray
scatterplot MDS VaR display of the same dataset.

and highlighting tool. The labeling issue, which was ignored in
the initial version, is addressed in this version. A case study is
included in this paper involving the visual analysis of a dataset
with 838 dimensions. A user study comparing the VaR display
with the rank-by-feature framework [19], [20] is also reported.
This paper is organized as follows. Section II reviews
related work. Section III brieﬂy introduces the original Pixel
MDS VaR display. Section IV presents the approach of us-
ing density-based scatterplots to generate dimension glyphs.
Section V describes the new Jigsaw and Rainfall dimension
glyph layout strategies. Section VI summarizes the correlation
calculation algorithm used in the VaR display. Section VII
presents the interaction tools. Section VIII addresses the
labeling issue. Section IX describes the implementation of
the VaR display and addresses the scalability issue.Section X
discusses visual exploration approaches with the VaR display.
Section XI presents a case study and Section XII presents
a user study for the VaR display. Section XIII presents our
conclusions and future work.

II. RELATED WORK

Many techniques exist for generating condensed displays
for large datasets. The work most related to our work is pixel-
oriented techniques and scatterplots. Pixel-oriented visualiza-
tion techniques [12], [13] are a family of multi-dimensional
display techniques that map each data value to a pixel on
the screen and arrange the pixels into subwindows to convey
relationships. The patterns of the subwindows may reveal
clusters, trends, and anomalies. Pixel-oriented techniques are
one among several options to create the dimension glyph in
the VaR display.

Scatterplots visualize 2-D datasets or 2-D projections of
multi-dimensional datasets. In a scatterplot, there is a hori-
zontal axis and a vertical axis, which are associated with two
dimensions (X and Y). The data items are plotted onto the
display according to their coordinates on X and Y. Scatterplots
are widely used since they provide rich information about the

relationship between two dimensions, such as strength, shape
(line, curve, etc), direction (positive or negative), and presence
of outliers [18]. Density-based scatterplots [24], [9] scale to
large datasets by using intensity of the spot in a scatterplot
to indicate the data density in that spot. We use the density-
based scatterplot as an option for generating the dimension
glyph and treat the areas with no data items in a scatterplot in
a different way from existing approaches due to the possible
overlaps among the scatterplots.

Scatterplots of multi-dimensional datasets are often orga-
nized together to show multiple 2D projections of the datasets.
Scatterplot matrices [7] organize the scatterplots of all N x (N-
1)/2 2-D projections of an N-dimensional dataset into a matrix.
Scatterplot matrices easily get cluttered when the number of
dimensions increases. Rather than displaying all 2D projec-
tions, we display N scatterplots between all dimensions and
a focus dimension and position them in a manner conveying
dimension relationships in our density-based scatterplot VaR
option.

There exist multiple visualization approaches to examining
relationships among dimensions to discover lower dimen-
sional spaces with signiﬁcant features. The rank-by-feature
framework [19] ranks 1D or 2D axis-parallel projections of
multi-dimensional datasets using statistical analysis to help
users detect 1D or 2D projections with desired features such
as linearly related dimensions. [16] visualizes correlations
between each pair of dimensions in a matrix and allows users
to interactively select dimensions from the matrix to con-
struct lower dimensional spaces. The interactive hierarchical
dimension reduction approach [28] visually conveys dimension
relationships using a dimension hierarchy to facilitate lower
dimensional space construction. The VaR display is different
from these approaches since it integrates data value visual-
ization with dimension relationship visualization in the same
display to use screen space more efﬁciently.

Multi-dimensional Scaling (MDS) [4], [15] is an itera-
tive non-linear optimization algorithm for projecting multi-

dimensional data down to a reduced number of dimensions.
It is often used to convey relationships among data items
of a multi-dimensional dataset. For example, IN-SPIRE [25]
uses MDS to map data items from a document dataset to
a 2D space. It generates a Galaxies display as a spatial
representation of relationships within the document collection.
In our approach, MDS is used in a different way, namely to
convey relationships among dimensions rather than data items.
The Jigsaw map [23] is a recent space ﬁlling hierarchy
layout method. By placing the leaf nodes of a hierarchy into
a 1D layout using a depth ﬁrst traversal and mapping the
1D layout
into a rectangular 2D mesh using space-ﬁlling
curves, this method creates hierarchy displays of nicely shaped
regions, good continuity and stability. When all leaf nodes are
of the same size, a Jigsaw map can draw all leaf nodes without
any distortion in shape, namely, they can be all equal-sized
squares. This property of the Jigsaw map makes it a perfect
option for us to lay out dimensions organized into a hierarchy
on a 2D mesh, with each dimension drawn as a square glyph.
The similarity based dimension arrangement proposed in
[1] also addressed the problem of arranging pixel oriented
subwindows (dimensions) on a 2D mesh. It aimed to place
similar dimensions close to each other on the 2D mesh. The
Jigsaw map dimension layout is different in that it aims to
use the dimension layout to convey the hierarchical structure
among the dimensions. As a consequence, not only similar
dimensions but also outlier dimensions are revealed.

[29] presents a multi-dimensional visualization technique
called Dust & Magnet. It represents dimensions as magnets
and data items as dust particles and attracts dust particles using
magnets to reveal data item values in the dimensions. The
Rainfall metaphor proposed in this paper was inspired by Dust
& Magnet. The difference is that the Rainfall metaphor attracts
dimensions using dimensions, while Dust & Magnet attracts
data items using dimensions.

III. PIXEL MDS VAR DISPLAY

Figure 1a illustrates the approach to generating a Pixel
MDS VaR display. First, a dimension glyph, called a glyph in
short, is generated to represent data values in each dimension,
i.e., values in the same column in the spreadsheet, using
pixel oriented techniques [13]. In particular, each value is
represented by a pixel whose color indicates a high or low
value, and pixels representing values from the same dimension
are grouped together to form a glyph. In a glyph, each pixel
occupies a unique position without overlap. In the original
version, a spiral pixel layout was used. Rows in the spreadsheet
are ordered according to their values in one dimension (Note:
actually any 1D order can be used). Data values in each
column are positioned into a spiral according to this order. In
all glyphs, pixels representing values in the same row occupy
the same position so that glyphs can be associated with each
other.

Second, the correlations among the dimensions are cal-
culated and recorded into an N x N matrix (where N is
the dimensionality of the dataset). In order to calculate the
correlations, different approaches can be used according to

3

different purposes. For example, if users are most interested
in linearly related dimensions, Pearson’s correlation coefﬁ-
cient can be used to capture the linear relationships among
dimensions. We proposed a scalable and ﬂexible correlation
calculation algorithm [27] and applied it in the VaR display.
We will brieﬂy introduce it in Section VI for the purpose of
completeness.

Third, the N x N relationship matrix is used to generate N
positions in a 2-D space, one position for each dimension. The
proximity among the positions reﬂects relationships among
the dimensions, i.e., closely related dimensions are spatially
close to each other, and unrelated dimensions are positioned
far way from each other. In particular, a multi-dimensional
scaling algorithm [4] is used to create the 2-D positions upon
the relationship matrix.

Finally, the dimension glyphs are placed in the 2-D space
in their corresponding positions to form the VaR display.
Figure 1b shows an example of the VaR display. It shows
the Image-89 dataset of 89 dimensions and 10,417 data items.
It is a real dataset containing 88 low level visual attributes
and classiﬁcation information for 10,417 image segments
generated by an image analysis approach [8]. In the ﬁgure,
each block is a dimension glyph and there are 89 glyphs.
In each glyph, data values of the dimension are mapped to
colors of pixels, and pixels are ordered in a spiral manner. The
closeness of the glyph positions reveals the correlations among
the dimensions calculated by the underlying algorithm. For
example, several clusters of closely correlated dimensions and
a few dimensions that are distinct from most other dimensions
can be detected from the glyph positions in Figure 1b.

The above approach can be summarized as dimension glyph
generation and layout. Glyphs explicitly convey data values
and their layout explicitly conveys dimension relationships.
Moreover, dimension relationships are also revealed by the
patterns of the glyphs. Similarity among glyph patterns in-
dicates dimension relationships, whether there is a linear or
non-linear relationship, or they are partially correlated (such
as dimensions for which a subset of the data items is closely
related). Since humans are good at pattern recognition, the
patterns of the glyphs provide straightforward and intuitive
comparison of the dimensions. On the one hand, the layout
approach brings related dimensions close to each other to make
the pattern comparison easier. On the other hand, the patterns
allow users to conﬁrm or refute the relationships suggested by
the layout using their eyes, and reveal how the dimensions are
related in detail.

Besides the techniques used in the original VaR display,
there are other approaches to creating glyphs and laying them
out, which will be introduced in the following sections. Since
glyph generation and layout are independent from each other,
they can be combined freely to form various VaR displays.

IV. DIMENSION GLYPH ALTERNATIVE: X-RAY

SCATTERPLOTS

The glyph generation approach used in the original VaR
display is not the only approach for creating dimension glyphs.
For example, different layouts of the pixels within a glyph

reveal different patterns. As an example, organizing pixels
into a calendar pattern according to the time stamps of the
data items can reveal time-dependant patterns among the data
items. Since these techniques have been widely studied in
pixel-oriented techniques [12] and they can be integrated
into the VaR display easily by replacing the original pixel-
oriented dimension glyph generation approach, they will not
be discussed in this paper. Instead, we present our work
on customizing a density-based scatterplot glyph (called an
X-Ray glyph) generation approach. This approach has been
introduced into the improved version (see Figure 1c for an
example VaR display using the scatterplot approach).

In the VaR display, a scatterplot

is generated for each
dimension. The Y dimension of a scatterplot dimension glyph
is the dimension it represents, while all of the glyphs have the
same X dimension. We choose to use the same X dimension
since it will be hard for users to associate different dimension
glyphs if both X and Y dimensions change from one glyph
to another. Although this causes information loss, users can
always interactively change the X dimension guided by the
semi-automatic selection tool (see Section VII) and their visual
exploration (see Section XI).

The VaR display is targeted at large datasets. It is time
consuming to draw the projection of each data item on each
of the N scatterplots. Also, the large number of projections
would clutter the glyph. In order to avoid clutter and increase
scalability, we store each glyph as an M X M pixel matrix,
where M is an adjustable integer, and divide the 2D space
within the value range of the dataset into M X M equal-
size bins. The number of projections falling into each bin
is recorded and translated into the color of its corresponding
pixel in the pixel matrix. In particular, the intensity of the pixel
is proportional to data density of the area it represents.

Fig. 2. X-Ray Scatterplots (a) The ﬁrst solution (b) The second solution (c)
The X-Ray scatterplot solution.

The ﬁrst image (Figure 2a) we generated is disappointing,
since it is hard to differentiate unoccupied area (areas with
zero data items) from areas with a few data items. In order
to solve this problem, we assign a different hue to the pixels
representing unoccupied areas. In the image generated (Figure
2b), there are no data items in the blue area. We then observed
that,
to glyphs generated using pixel-oriented
techniques where every pixel represents a data value, there
are often large contiguous unoccupied areas in a scatterplot
glyph, especially when the X and Y dimensions are closely
related. Recalling that some glyph layout approaches, such

in contrast

4

as MDS, could cause overlaps among different glyphs, we
made the unoccupied areas semi-transparent so that users can
see hidden glyphs through the unoccupied areas of the hiding
glyphs. Figure 2c shows this ﬁnal solution. Since in the ﬁgure
the glyphs look very much like X-Ray photos, we named
this VaR display the X-Ray scatterplot VaR display. To give
users more ﬂexibility, we allow them to interactively choose
the color and transparency of the unoccupied areas. If users
dislike the semi-transparent unoccupied areas, they are able to
set them to opaque.

V. DIMENSION LAYOUT ALTERNATIVES: JIGSAW MAP

LAYOUT AND RAINFALL

A. Jigsaw Map Glyph Layout

The MDS approach is effective in conveying dimension
relationships. However, using the MDS approach, the positions
of two glyphs could be very close to each other if they are
closely related. Glyphs might overlap in this case, which is
sometimes undesired by the users. Besides allowing the users
to reduce overlaps in the MDS layout using interactions (see
Section VII), we propose a Jigsaw Map dimension layout
based on the recently proposed Jigsaw map [23]. In this
approach, dimensions are grouped into a dimension hierarchy.
The Jigsaw map, which is a space-ﬁlling hierarchy visualiza-
tion method, is then used to lay the dimensions on a grid.
This approach not only prevents glyphs from overlapping, but
also conveys the hierarchical structure among the dimensions.
Figure 3 shows VaR displays with a Jigsaw layout.

The motivation of this approach is that grouping dimensions
of high dimensional datasets into dimension hierarchies makes
it easy to capture the relationships among the dimensions.
In a dimension hierarchy, dimensions are organized into a
hierarchy of clusters. Dimensions within a cluster have closer
relationships among each other than with dimensions outside
the cluster. Clusters in different levels of the hierarchy divide
the dimensions into groups of different granularity. With the
dimension relationship matrix, it is convenient to generate
a dimension hierarchy using existing hierarchical clustering
approaches. In the hierarchy, each leaf node is a dimension in
the high dimensional dataset.

In order to turn the dimension hierarchy into the dimension
layout, we examined existing hierarchy visualization tech-
niques. The basic requirements are 1) the layout should be
space efﬁcient since our target is high dimensional datasets
and 2) each dimension should be assigned a space of the
same size, shape and orientation since it is difﬁcult for users
to compare and associate glyphs with different sizes, shapes,
or orientations. Since node-linked diagrams do not use space
efﬁciently, we only considered the space-ﬁlling hierarchy
visualization techniques [3], [21], [23]. Among them, only
the Jigsaw map [23] and quantum treemaps [3] are capable
since all other techniques assign areas of different shapes or
orientations to leaf nodes. We chose the Jigsaw map since it
generates layouts of nicely shaped regions and is stable with
regards to changing tree structures and leaf nodes [23].

To generate the Jigsaw map layout, we ﬁrst hierarchically
cluster the N dimensions in a dataset based on their pair-
wise distances (a pair of more closely related dimensions

5

Fig. 3. The Image-838 dataset (838 dimensions, 11,413 data items). (a) The Pixel Jigsaw map VaR display with separated dimensions selected and labeled
(b) The X-Ray scatterplot Jigsaw map VaR display with dimension Coarseness as the X dimension. The X dimension is in a pink frame and labeled. (c)
The X-Ray scatterplot Jigsaw map VaR display with dimension angle 135 as the X dimension. The X dimension is at the left bottom corner of the map and
dimensions closely related to it are in red frames. (d) A zoomed in display of the selected dimensions with their labels shown.

has a smaller distance than a pair of less related dimensions)
using the minimum single linkage metrics [17]. Then, the N
dimensions are ordered into a 1-D sequence according to their
positions in the hierarchy using a depth-ﬁrst traversal of the
hierarchy, and then the sequence is mapped to a 2-D L x K (
L x K >= N) mesh by applying a space-ﬁlling curve called
an H curve (please refer to [23] for more details). Figure 3a
shows an example of the Jigsaw layout. In this ﬁgure, similar
dimensions are close to each other and signiﬁcant boundaries
of groups of closely related dimensions, such as the group of

dimensions in the left bottom part of the map, can be detected.
Outlier dimensions, such as the dimensions on the left top part
of the map, are also distinguishable since their textures look
different from their neighbors.

B. Rainfall Metaphor

When exploring a high dimensional dataset, users are often
interested in the relationships between a single dimension of
interest with all other dimensions. Beside the X-Ray scatter-
plot, which reveals the relationships using glyph textures, we

6

Fig. 4. The Rainfall Metaphor. (a) At the beginning of the rain. Dimensions more closely related to the dimension of interest in the bottom are falling in a
faster acceleration than less related dimensions. (b) The rain continues. The dimensions with different correlations to the dimension of interest are separated.
It can be seen that there are roughly three levels of association between the dimension of interest and other dimensions. (c) The Rain is close to its end.
Dimensions signiﬁcantly distinct from the dimension of interest are revealed. The dataset is the Image-89 dataset. The glyphs are pixel-oriented glyphs (pixels
are ordered in a line by line (horizontal lines) manner.

provide a simple animation approach to dynamically illustrate
the relationships by changing glyph positions. This approach is
named the Rainfall Metaphor since it imitates rain (see Figure
4 for an example).

In the beginning of the animation, the dimension of interest
is placed in the center bottom of the display (the ground) and
all other dimensions (raindrops) are placed in the top of the
display (the sky). The horizontal positions of the raindrops
are randomly generated. After the rain starts, a raindrop falls
toward the ground in an acceleration that is proportional to
its correlation with the dimension of interest. Thus, a raindrop
moves toward the ground faster than another raindrop if it has
a closer relationship to the dimension of interest. A raindrop
stops its movement after it hits the ground. There is a timer
that starts from the beginning of the rain and ends when
all raindrops hit the ground. Users can interactively play the
animation by moving the slider representing the timer. Users
can also interactively select the dimension of interest for the
animation.

Figure 4a-c shows some screen captures of the Rainfall
layout. Using this metaphor, users can focus on the relation-
ships between the dimension of interest and other dimensions,
without being distracted by relationships among the other
dimensions. In different moments of the rain, either similar
dimensions or distinct dimensions to the dimension on the
ground attract the users’ attention.

VI. CORRELATION CALCULATION

In the VaR display, a binning based correlation calculation
algorithm is used. We only brieﬂy introduce it here since
it has been presented in full detail in [27]. We claim that
any relationship calculation algorithm can be used in the VaR
display as long as it scales to large datasets. The layout of
the glyphs reﬂects the type of relationship calculated by the
underlying algorithm.

In our algorithm, distribution of the value differences (be-
tween the different dimensions for the same data item) is
recorded into bins. In particular, the possible range of value
differences between a pair of dimensions is divided into a
sequence of bins. The number of data items whose value
differences between these two dimensions fall into the bins is
recorded. For an N dimensional dataset, N x (N-1)/2 sequences
of bins (one sequence for each pair of dimensions) are created.
A pair of dimensions is considered to be closely related if
a large number of data items fall info a small number of
bins (K) in its sequence. With a given K, the correlations
can be calculated in this way: sort the bins in the sequence
according to their populations, and sum up the populations
of K bins with the highest populations. The sum divided by
the total population of the data items is proportional to the
correlation between the dimensions. K is selected to be the
number of bins that make the global variance of correlations
for all dimensions maximum. This algorithm scales to a large
number of data items. Except for the ﬁrst scan, which can
be done with minimal cost when inserting the dataset into
the database, its efﬁciency is only related to the number of
dimensions.

The above algorithm is a heuristic approach whose purpose
is to maximize the visibility of the structure of the MDS and
Jigsaw layout. There are many other optimization problems
in the VaR display, such as selecting a dimension ordering
the pixel-oriented display in the initial view to provide the
maximum information to users at a ﬁrst glance. A detailed
discussion of such problems is presented in [27] and not
repeated here.

VII. INTERACTIVE TOOLS IN THE VAR DISPLAY

A rich set of interaction tools has been developed for the
VaR display. Navigation tools help users reduce clutter in the
display and discover information about the dataset. Automatic
and manual dimension selection tools allow users to perform

human-driven dimension reduction by selecting subsets of
dimensions for further exploration in the VaR display as well
as other multi-dimensional visualizations. Data item selection
tools allow users to select subsets of data items for further
exploration. In addition, the data item masking tool allows
users to examine details of selected data items within the
context of unselected data items.

Most of the interactive tools make no special assumption
about the glyph positioning and generation strategies, i.e., they
can be applied to any realization of the VaR display. These
tools are called general tools. Unless speciﬁcally noted, an in-
teraction tool is a general tool in the following sections, where
details of each navigation and selection tool are presented.

A. Tools for Glyph Layout

The MDS dimension layout causes overlaps among the
glyphs. Overlaps emphasize close relationships among the
dimensions because glyphs overlap only if their dimensions
are closely related. However, overlaps can prevent a user
from seeing details of an overlapped glyph. We provide the
following operations to overcome this problem (see [27] for
more detail).

(cid:129) Showing Names: By putting the cursor on the VaR
display, the dimension names of all glyphs under the
cursor position are shown in a message bar. Thus a user
can be aware of the existence of glyphs hidden by other
glyphs.

(cid:129) Layer Reordering: With a mouse click, a user can force
a glyph to be displayed in front of the others. In this
way he/she can view details of a glyph that is originally
overlapped. Users can also randomly change the ordering
of all dimension glyphs by clicking a button in the control
frame. In addition, selected dimensions are automatically
brought to the front of the display.

(cid:129) Manual Relocation: By holding the control key, a user
can drag and drop a glyph to whatever position he/she
likes. In this way a user can separate overlapping glyphs.
(cid:129) Extent Scaling: Extent scaling allows a user to interac-
tively decrease the sizes of all the glyphs proportionally to
reduce overlaps, or to increase them to see larger glyphs.
(cid:129) Dynamic Masking: Dynamic masking allows users to
hide the glyphs of unselected dimensions from the VaR
display.

(cid:129) Automatic Shifting: This operation automatically re-
duces the overlaps among the glyphs by slightly shift-
ing the positions of the glyphs. There are many more
advanced overlap reducing algorithms that can be used,
such as those listed in [22].

(cid:129) Distortion: Users can interactively enlarge the size of
some glyphs while keeping the size of all other glyphs
ﬁxed. In this way users are allowed to examine details
of patterns in the enlarged glyphs within the context
provided by the other glyphs.

(cid:129) Zooming and Panning: Users can zoom in, zoom out
and pan the VaR display. For example, in order to reduce
overlaps, sometimes the size of the glyphs has to be set
very small when there are a large number of dimensions.

7

Zooming into the display will enlarge the glyphs so that
the user can have a clear view of the patterns in the
glyphs.

(cid:129) Reﬁning: A reﬁned VaR display can be generated for
a selected subset of dimensions and a selected subset
of data items. The selected dimensions and data items
are treated as a new dataset. The relationship calculation,
glyph generation and positioning are applied to the new
dataset.

B. Tools for Glyph Regeneration

In the Pixel-Oriented dimension glyphs, the dimension used
to sort the data items affects the glyph patterns signiﬁcantly.
Clusters in subspaces including this dimension can be easily
detected while clusters in other subspaces are not. Similarly, in
the X-Ray scatterplot dimension glyphs, relationships between
other dimensions and the X dimension are easier to detect
than relationships among other dimensions. We allow users to
interactively select the sorting dimension in the pixel-oriented
mode and the X dimension in the X-Ray scatterplot mode
by clicking the mouse button on the glyph of the desired
dimension or selecting from a combo-box.

In addition, a comparing mode can be used in the pixel-
oriented glyphs in order to compare the dimensions with a
dimension of interest. In this mode, except the glyph of the
base dimension, the pixels of all other glyphs will be colored
according to the differences between the values of the base
dimension and their dimensions. A ﬁgure of the comparing
mode can be found in [27].

C. Dimension Selection Tools

Dimension selection tools enable users to select dimen-
sions of interest for further exploration using other multi-
dimensional visualization techniques. They can also be used
as a ﬁlter to reduce the number of glyphs displayed in a VaR
display, since we allow users to hide glyphs of unselected
dimensions using dynamic masking (see Section VII-A). The
selection tools we provide to users include automatic selec-
tion tools for closely related dimensions and well separated
dimensions, in addition to manual selection.

The automatic selection tool for related dimensions
takes a user-assigned dimension and correlation threshold as
input. Here we assume that a pair of more closely related
dimensions has a larger correlation measure than a pair of
less related dimensions. Users pick the assigned dimension by
clicking its glyph and adjust the threshold through a slider. The
tool automatically selects all dimensions whose correlation
measures to the input dimension are larger than the threshold
by traversing the dimension relationship matrix. This tool
enables the users to select a set of closely related dimensions.
The automatic selection tool for separated dimensions
takes a user-assigned dimension and correlation threshold
as input and returns a set of dimensions that describe the
major features of the dataset. The assigned dimension will
be included in the returned set of dimensions. Between each
pair of dimensions in the result set, the correlation measure is
smaller than the threshold. For any dimension that is not in

8

Fig. 5. Masking of Unselected Data Items. Unselected data items are covered by a mask with adjustable color and transparency. (a) No mask or fully
transparent mask. (b) Opaque mask. (b) Semi-transparent mask. The dataset is the Image-89 dataset. The glyphs are pixel-oriented glyphs (pixels are ordered
in a line by line (vertical lines) manner.

the result set, there is at least one dimension in the result set
whose correlation measure with it is larger than the threshold.
Using this tool, a user is able to select a set of dimensions
to construct a lower dimensional subspace revealing the major
features of the dataset without much redundancy. In Figure 1b
separated dimensions selected automatically are labeled.

The following algorithm can be used for automatic selection

of separated dimensions:

1) Get the assigned dimension and the selection threshold.
2) Set the assigned dimension as “selected” and all other

dimensions as “unselected”.

3) Find all unselected dimensions whose correlation mea-
sures to all existing selected dimensions are smaller than
the threshold. Mark them as “candidates”.

4) If there is no candidate dimension, go to step 5. Else, set
one candidate dimension as “selected” and every other
candidates as “unselected”. Go back to step 3.

5) Return all dimensions marked as “selected”.
It is interesting that it is not deﬁned how to pick one dimen-
sion among the candidate dimensions in step 4. Thus it can
be customized according to the task of interest. For example,
in Section VIII, this approach is customized to reduce the
clutter among the labels of the selected dimensions for a good
labeling result. Here we present another customization.

When users start to explore an unknown dataset, it is often
desired to ﬁnd dimension groups containing large numbers of
closely related dimensions. Thus a heuristic approach can be
used in step 4: setting a threshold, for each candidate dimen-
sion counting the number of dimensions having correlation
measures to it that are larger than the threshold, and selecting
the dimension with the highest count. Using this approach
dimensions with a larger number of closely related dimensions
have higher priority to be selected.

Manual selection allows a user to manually select a dimen-
sion by clicking its corresponding glyph. The user can unselect
the dimension by clicking the glyph again. The combination of
manual and automatic selection makes the selection operation

both ﬂexible and easy to use.

D. Data Item Selection and Masking Tools

Rather than allowing a user to select data items directly from
the glyphs in the VaR display (which is hard when glyphs
are small), we allow the user to select data items from a
dialog. Firstly, the user selects a dimension name from a name
list in the dialog. Then a brief summary of the dimensions
will be provided to help the user set up the selection criteria
for the selected dimension. If the dimension is a categorical
dimension, the distinct values in that dimension as well as the
number of data items for each value will be provided. The user
can then select the desired distinct values. If the dimension is
a numeric dimension, a histogram of the dimension will be
provided. The user then set up a minimum value and maximum
value for the selection using two sliders. The user can set the
selection ranges for multiple dimensions.

After the user sets the selection criteria, he/she can click a
button in the dialog to trigger the selection. A problem here is
how to highlight the selected data items. In most visualization
systems, selected data items are highlighted using either a
special color, or a surrounding box around the selected items.
However, in the VaR display with pixel-oriented techniques,
color has been used to represent the values, and it is hard
to put a surrounding box in a condensed glyph, especially if
the selected data items are not adjacent to each other in the
glyphs.

A straightforward solution to this problem is to display only
the selected data items. This is a general solution suitable for
all realizations of the VaR display. However, a drawback of
this approach is that the context provided by unselected data
items is lost. Such a context is often useful. For example, the
users might want to compare the selected data items with the
unselected data items among the dimensions.

In order to overcome this drawback, we developed an
approach called data item masking. This approach is only
useful for VaR displays using pixel-oriented techniques. In

9

Fig. 6. Labeling Solutions (a) All dimensions are labeled with names (b) Dimensions selected by the labeling algorithm are labeled. Clutter is reduced. (c)
Angled text is used to label all dimensions in the Jigsaw map layout. The dataset is the Image-89 dataset.

according to the following two heuristic criteria: 1) they should
be distinct dimensions, i.e., two similar dimensions should
not be labeled at the same time. Dimensions distinct from all
other labeled dimensions should be labeled. 2). they should be
separated from each other as much as possible to avoid clutter
on the screen. In addition, we allow users to interactively
change the number of dimensions labeled to get a less cluttered
view or to see more labels.

Criterion 1 is exactly the criterion used for automatic se-
lection of separated dimensions (see Section VII-C). Criterion
2 adds more constraints to the selection. Recall that there is
some freedom in step 4 of the selection algorithm, i.e., any
dimensions in the candidate dimension set can be selected; we
modiﬁed the algorithm for labeling as follows:

1) Assign a dimension and a selection threshold.
2) Set the assigned dimension as “selected” and all other

dimensions as “unselected”.

3) Find all unselected dimensions whose correlations with
all existing selected dimensions are smaller than the
threshold. Mark them as “candidates”.

4) If there is no candidate dimension, go to step 5. Else, set
the candidate dimension which is the most far way on
the screen from its closest existing selected dimension
as “selected” and other candidates as “unselected”. Go
back to step 3.

5) Return all dimensions marked as “selected” and label

this approach, both selected and unselected data items are
drawn on the screen. Unselected data items are covered by
a mask. Users can interactively change the color of the mask,
and adjust the transparency of the mask though a slider. When
the mask is opaque, as shown in Figure 5b, unselected data
items are hidden. When the mask is fully transparent, as shown
in Figure 5a, the selected data items are not highlighted. When
the mask is semi-transparent, as show in Figure 5c, the selected
data items are highlighted within the context provided by
the unselected data items. Users can interactively change the
transparency of the mask to adjust the strength of the context.
The implementation of this masking operation is simple.
First, a mask is generated using an approach similar to the
generation of a normal dimension glyph. The only difference
is that the pixels are set to be transparent for selected data
items and with user assigned color and transparency for
unselected data items. Our mask generation mechanism has
no dependency on the order of the data items,
is
not necessary for the selected data items to be adjacent to
each other in the glyphs. Since the color and shape of the
masks are the same for all
the mask is only
generated once, stored as a texture object, and pasted in the
front of all the glyphs. Since the texture mapping operation is
efﬁcient in OpenGL, displaying masks has minimal effect on
the rendering of a VaR display.

the glyphs,

i.e.,

it

VIII. LABELING

them.

In the original version of the VaR display, dimension names
are labeled horizontally in the middle top region above the
dimension glyph for all dimensions shown on the screen (see
Figure 6a). The labels clutter the screen seriously for a high
dimensional dataset, thus we did not provide the labeling
option to users. Rather, when users moved the cursor over
a glyph, the glyph name showed in the message bar below
the display. However, users complained that ﬁnding dimension
names in this way was tiring. They argued that the VaR display
without dimension labels is much less meaningful than one
with names labeled. In order to solve this problem, we chose to
label a subset of dimensions on the screen for the MDS layout
(see Figure 6b). The dimensions to be labeled are selected

When calculating the screen distance between two dimen-
sions in step 4, we must consider the fact that horizontal labels
are used. Their lengths are much larger than their widths.
Assume that labels have 5 characters on average and the
characters have equal height and width, the screen distance
between two dimensions d1 and d2 D(d1, d2) = fabs((d1.x
- d1.x)) + 5 * fabs((d1.y - d2.y)), where x and y are the
screen coordinates of the dimensions. The equation means that
we prefer dimensions separated in the vertical direction than
the horizontal direction. Figure 6b shows the same display as
Figure 6a with selected dimensions labeled using the above
algorithm.

The same labeling approach can be applied to the Jigsaw

map layout. In addition, since the glyphs are placed in a regular
mesh in the Jigsaw map, applying an angle on all the labels
greatly reduces the clutter on the screen even when all labels
are shown. Figure6c shows the Image-89 datasets in the Jigsaw
map layout with all dimension names displayed at a 20 degree
angle. Almost all of the dimension names can be distinguished
from this display.

In our prototype we bind labeling with selections,

i.e.,
users have the option to show labels of selected dimensions
only. When a user chooses this option and uses the automatic
selection tool for separated dimensions, it is exactly the above
clutter-reducing labeling approach. When a user uses the
selection tool for related dimensions, the dimensions closely
related to the user-assigned dimensions are labeled (see Figure
3d for an example).

IX. IMPLEMENTATION AND SCALABILITY ISSUE

When there are several hundred dimensions, the datasets
can easily contain millions of data values even if they only
contain thousands of data items. Datasets often have a higher
number of data items. Such large datasets not only cause large
response time during interactions and problems in storing the
data structures in a visualization system, but also cause clutter
on the display. Scalability is a critical issue for visualization
systems aimed at high dimensional datasets.

We have implemented a fully working prototype of the VaR
display. The biggest dataset that has been successfully loaded
into the VaR display so far is an image classiﬁcation dataset
containing 838 dimensions and 11413 data items, which means
over 9 million data values (see Figure 3 for its VaR display).
Most interactions can be processed within a few seconds on a
typical PC for this dataset. This dataset is the biggest dataset
we currently have. In the future, we will test larger datasets
on the prototype.

The critical techniques we used in the prototype for increas-
ing scalability are texture mapping, binning, and sampling
techniques. Using the texture mapping techniques provided by
OpenGL, our prototype stores all dimension glyphs (including
the mask in the masking operation) as texture objects and
pastes them on the screen as needed. As long as the glyph
textures do not change,
the dataset does not need to be
rescanned, which is time consuming for large datasets. By
keeping the texture objects small (such as hundreds of pixels),
which is reasonable since each dimension glyph will not be
too big on the screen in order to reduce clutter, the system can
draw hundreds of dimension glyph textures on the screen in
almost real time. This approach greatly reduces the response
time for most interactions because, except for reordering for
pixel-oriented glyphs and resetting the X dimension for X-Ray
scatterplot glyphs, almost all other interactions do not change
the glyph textures. Rather, they refresh, resize, reposition, or
reorder the glyphs.

According to our experience, drawing fonts in OpenGL is
a time consuming task. Our prototype stores all dimension
name labels as texture objects. These texture labels are created
one time, and can be quickly pasted on the screen until users
change the contents or colors of the labels. The texture labels
can be scaled and rotated easily on the screen.

10

Binning, i.e., using buckets to stored statistic information
about groups of values rather than recording them individually,
is an approach widely used in data mining techniques for large
datasets. We use binning techniques to increase the speed of
the correlation calculation algorithm (see section VI) and the
X-Ray scatterplot glyph generation (see section IV).

The prototype stores datasets in an Oracle database server.
It dynamically requests data from the server when needed,
making use of the sorting and query functions provided by
the database server. When generating a VaR display for a
dataset containing a large number of data items, we use a
random sampling approach to reduce the response time for
fetching data items from the server, as well as the number
of values to be processed. In particular, the system keeps a
default maximum number. When the number of data items
contained in a dataset exceeds it, a uniform random sampling
is performed on the dataset to only fetch the maximum number
of data items. Users are allowed to interactively adjust the
maximum number in order to trade between the response time
and visualization accuracy.

Random sampling is easy to implement. However, it has
the big drawback that a large sampling rate is needed in order
to reduce small group loss in the samples [6]. In order to
overcome this problem, many solutions have been proposed,
such as biased sampling [14] or dynamic sample selection
[2]. It has been shown in the literatures that these approaches
successfully reduce small group loss. We will explore these
approaches in the future.

X. DISCUSSION

The VaR display can serve as an overview tool for a high
dimensional dataset. Starting from the VaR display, other
visualization techniques can be used for more detailed visual
analysis. For example, the VaR display is coordinated with
parallel coordinates, star glyphs, and scatterplot matrix views
in our prototype. Although these techniques could not handle
hundreds of dimensions, they work well in examining data
items and dimensions selected by the VaR display. Recently,
we completed an interesting project in coordinating the VaR
display with an image exploration interface. The VaR display
was used to show the high dimensional image content anno-
tations. Users were allowed to select images by contents from
the VaR display. The images were then examined in detail in
an image exploration interface. This work is described in [26].
The MDS and Jigsaw map glyph layout approaches have
their advantages and disadvantages. From its nature, MDS
is better in capturing high dimensional relationships than the
hierarchical approach. However, the non-overlap feature of the
Jigsaw map layout makes it a popular approach for users of
the VaR display thus far.

Although the pixel-oriented glyphs are mentioned less than
the X-Ray scatterplot glyphs in this paper, this is only because
the usage of the pixel-oriented techniques has been widely
studied and their effectiveness has been shown in many papers.
Compared to scatterplots, the pixel-oriented glyphs are more
effective in pixel usage since they make use of each pixel.
However, it is easier to compare the relationship between a

11

Fig. 7.
(a) The Pixel MDS VaR display of the Image-838 dataset with separated dimensions selected and labeled. (b)(c) The X-Ray scatterplot Jigsaw map
VaR display of the Image-89 dataset. The dimension in a yellow frame is non-linearly related to the X dimension. (c) The X-Ray scatterplot Jigsaw map VaR
display with another X dimension (the dimension highlighted by the yellow frame in (b)).

dimension of interest and all other dimensions using the scat-
terplot glyphs. Users ﬁnd it difﬁcult to compare the patterns
of pixel-oriented glyphs if they are far from each other.

Compared to scatterplot matrices, the X-Ray scatterplot VaR
display has its advantages and disadvantages. For datasets with
a small number of dimensions, scatterplot matrices might be
preferred since all possible axis-parallel 2-D projections are
provided in them. However, for datasets with tens, hundreds
or thousands of dimensions, the X-Ray scatterplot VaR display
might be preferred since it causes less clutter. Its disadvantage
that only part of possible 2-D projections are displayed is
leveraged by two facts: ﬁrst, dimension relationships conveyed
by the VaR display give strong hints on the shapes of the
undisplayed 2-D projections; second, users can interactively
access 2-D projections of interest through interactions.

Compared to approaches that rank the 1D or 2-D projections
according to their features and allow users to examine detail
of a projection by selecting it from diagrams or lists conveying
the ranking (such as the rank-by-feature framework [19]),
the VaR display also has its advantages and disadvantages.
Obviously for tasks such as ﬁnding the most linearly corre-
lated dimensions the ranking approaches are better choices.
However, the VaR display is better in helping users grasp the
global relationships among the dimensions.

XI. CASE STUDY

We have explored several real datasets using the VaR dis-
play, including the Image-838 dataset [8] with 838 dimensions
and 11,413 data items and the Image-89 dataset [8] with 89
dimensions and 10,471 data items. They all contain low level
visual attributes for image classiﬁcation. Image analysts are
interested in ﬁnding outlier dimensions that are uncorrelated to
most other dimensions, and dimensions representing a group of
correlated dimensions (a dimension cluster) in order to reduce
the number of low level visual attributes used in the image
classiﬁcation process.

For both datasets, we selected a Pixel MDS VaR display
with all dimensions displayed as the initial view, since the

pixel-oriented glyphs have a higher pixel usage efﬁciency
and the MDS display conveys dimension relationships more
accurately than the Jigsaw map layout. Figure 7a and Figure 1b
show the Pixel MDS VaR displays of the Image-838 dataset
and the Image-89 dataset respectively. From the ﬁgures, we
found that there are dimension outliers and clusters in both
datasets. We then applied automatic selections for separated
dimensions. Both outlier dimensions and dimensions repre-
senting dimension clusters were selected.

Then, we switched to the Jigsaw map layout. Figure 3a
shows the Pixel Jigsaw map VaR display of the Image-838
dataset. There are several distinguishable regions that can be
seen in the map where adjacent glyphs in the regions have
similar patterns. For example, there is a distinguishable region
composed of bright blue glyphs at the left bottom of the
map. If only one dimension is selected in such a region, it
means that the neighbors of the selected dimension are closely
related to it, since selection for separated dimensions was used.
Thus they are a dimension cluster and the selected dimension
can represent the cluster. The selected and labeled dimension
angle 135 at the left bottom corner is such a representative
dimension. Meanwhile, selected dimensions crowded together,
such as the selected dimensions in the left top of the map,
are potential outliers since they are distinct from their closest
neighbors. The selected and labeled dimension Coarseness at
the left top corner is such suspicious outlier.

In order to examine if dimension Coarseness is an outlier,
an X-Ray scatterplot VaR display was created using it as
the X dimension (see Figure 3b). From scatterplots in Figure
3b it can be seen that no other dimensions show strong
correlations with dimension Coarseness. Thus it is conﬁrmed
that dimension Coarseness is an outlier dimension.

Figure 3c examines if dimension angle 135 is a repre-
sentative dimension. The X dimension of the scatterplots is
dimension angle 135 and dimensions closely correlated to
dimension anagle 135 are selected and highlighted. It can be
seen that a large number of dimensions are selected and they
all contain a clear diagonal pattern which indicates a strong

linear correlation. Figure 3d shows a zoomed in display of the
selected dimensions in which their labels are shown.

A similar exploration approach was conducted for the
Image-89 dataset. An interesting pattern in this dataset was
found when we were examining dimension Channel Energy 5
using the X-Ray scatteplot Jigsaw map VaR display (Figure
7b): there was a glyph with a curved band (the glyph with a
yellow frame, the frame was manually added into the ﬁgure
for highlighting). It seemed that
this dimension was non-
linearly related to the target dimension. It raised our interest
and became our next target.

We clicked this dimension to set it as the X dimension in
the X-Ray scatterplots and got Figure 7c. It is labeled in
Figure 7c as Texture Brightness DC. Figure 7c shows that
dimension Texture Brightness DC is non-linearly related to
most dimensions in this dataset. The curved bands are fairly
thin in some dimensions, which means strong non-linear
relationships.

XII. USER STUDY

A user study has been conducted to evaluate the VaR display
by comparing it to the Rank-by-Feature feature of HCE [19].
To form a comparable study, we considered the X-Ray scat-
terplot glyph style of VaR and the scatterplot prism from the
HCE system, namely its 2D projection ranking, selection and
visualization feature. In HCE, 2D projections are ranked by
features such as strength of linear relationship or least square
error for curvilinear regression. The ranking is visualized in
both a matrix and a list. A window beside the ranking windows
shows the scatterplot of the 2D projection selected by the user.
Our assumption was that the VaR display would better help
users grasp global relationships among the dimensions in a
high dimensional dataset. The reason is that VaR provides a
detailed view of all dimensions at the same time while users
of HCE need to take efforts to associate multiple dimensions
since they can only examine a few detailed views at the same
time.

Eight subjects participated in the user study. The subjects
vary in educational backgrounds: one was a psychology grad-
uate student, two were computer science undergraduate stu-
dents, three were graduate students in the ﬁeld of visualization,
and two were researchers/post-doctorates in visualization. The
subjects completed the user study one by one on the same
computer with the same instructor. Each subject tested both
systems. The order of using VaR and HCE was alternated for
the subjects.

The study began with a 10 minute training session using
both VaR and HCE and a further 10 minutes to allow subjects
to explore the tools and ask the instructor questions. A set of
tasks were then completed by the subjects using both tools. A
post-test survey to ﬁnd user preferences and a discussion were
conducted immediately following the completion of the tasks.
We used the Image-89 dataset of 89 dimensions and 10,471
data items. As shown in the case study (Section XI), there
are some strong linearly related dimensions and some strong
non-linearly related dimensions in the Image-89 dataset.

The ﬁrst

task was to describe relationships between a
given dimension and each of the other dimensions using the

12

scatterplot displays by approximating the numbers of different
scatterplot shapes involved with the given dimension. Samples
of typical shapes, such as diagonal thin straight bands for lin-
ear relations, curved bands for non-linear relations, and evenly
distributed scatterplot indicating unrelated dimensions were
provided to users. The second task required users to describe
relationships among ﬁve randomly assigned dimensions using
their scatterplot shapes.

The majority of users performed the ﬁrst task quicker and
evaluated the task to be easier using the VaR display. The
average time was 3.2 minutes and the standard deviation was
0.5 minutes for VaR, and the average time was 4.7 minutes
and the standard deviation was 3.2 minutes for HCE. On a
scale of 0 (hard) to 5 (easy), the mean scores of 3.5 and 2.1
were given to VaR and HCE respectively. A similar trend was
identiﬁed in the second task: the average time was 3.5 minutes
and the standard deviation was 0.4 minutes for VaR, and the
average time was 8.5 minutes and the standard deviation was
2.9 minutes for HCE. The scores are 3.6 for VaR and 1.0 for
HCE. Results from these tasks highlighted the advantage of
the VaR display in providing a global view of the dimension
relationships.

Qualitative results and qualitative feedback from the post-
test survey were also encouraging. Users typically preferred
using VaR over HCE for the given tasks. The reasons given
by each user were generally similar and can be summarized by
the ability to examine details of multiple relations on a single
display. One user in the study preferred HCE over VaR due to
the more detailed and visible scatterplots in the HCE system.
Users were also asked if they agreed with the statement “this
tool is useful for exploring high dimensional data”. On a scale
of 0 (disagree) to 5 (agree), users responded with a mean score
of 4.3 and 3.5 for VaR and HCE respectively.

A number of comments and suggestions were made by the
users regarding both systems. Positive feedback from VaR
included an intuitive interface, the instantaneous global view
and ability to quickly select the X dimension of all scatterplots.
Improvements suggested by the users involved ranking the
dimensions by features, and using color and best-ﬁt-lines to
enhance the scatterplot displays which were considered too
dense. In addition, users suggested ordering the dimension
glyphs according to the shapes of the scatterplot using au-
tomatic image analysis techniques. For the HCE system, users
preferred the ranking features and the scatterplot display with
rich features and interactions. Users suggested that the global
view provided by the prism in HCE lacked details compared
to the VaR display. Future work may beneﬁt by combining the
best features of these two systems.

XIII. CONCLUSION

In this paper, the VaR display, which allows users to inter-
actively explore large datasets with hundreds of dimensions,
was presented. The essential
idea of the VaR display is
to represent each dimension in a high dimensional dataset
using an information-rich glyph, and arranging the glyphs to
reveal the relationships among the dimensions. By integrating
existing techniques such as MDS, Jigsaw map, pixel-oriented

13

[12] D.A. Keim. Designing pixel-oriented visualization techniques: Theory
IEEE Transactions on Visualization and Computer

and applications.
Graphics, 6(1):1–20, January-March 2000.

[13] D.A. Keim, H.-P. Kriegel, and M. Ankerst. Recursive pattern: a
technique for visualizing very large amounts of data. Proc. IEEE
Visualization ’95, pages 279–286, 1995.

[14] G. Kollios, D. Gunopulos, N. Koudas, and S. Berchtold. Efﬁcient
biased sampling for approximate clustering and outlier detection in large
IEEE Transactions on Knowledge and Data Engineering,
data sets.
15(5):1170–1187, 2003.

[15] J.B. Kruskal and M. Wish. Multidimensional Scaling. Sage Publications,

1978.

[16] A. MacEachren, X. Dai, F. Hardisty, D. Guo, and G. Lengerich. Explor-
ing high-d spaces with multiform matrices and small multiples. Proc.
IEEE Symposium on Information Visualization, pages 31–38, 2003.

[17] F. Murtagh. A survey of recent advances in hierarchical clustering

algorithms. Computer Journal, 26(4):354–359, 1983.

[18] NetMBA.

http://www.netmba.com/statistics/plot/scatter/.

[19] J. Seo and B. Shneiderman. A rank-by-feature framework for un-
supervised multidimensional data exploration using low dimensional
projections. Proc. IEEE Symposium on Information Visualization, pages
65–72, 2004.

[20] J. Seo and B. Shneiderman. A rank-by-feature framework for inter-
active exploration of multidimensional data. Information Visualization,
4(2):96–113, 2005.

[21] B. Shneiderman. Tree visualization with tree-maps: A 2d space-ﬁlling

approach. ACM Transactions on Graphics, 11(1):92–99, Jan. 1992.

[22] M.O. Ward. A taxonomy of glyph placement strategies for multidi-
mensional data visualization. Information Visualization, 1(3-4):194–210,
2002.

[23] M. Wattenberg. A note on space-ﬁlling visualizations and space-ﬁlling
curves. Proc. IEEE Symposium on Information Visualization, pages 181–
186, 2005.

[24] E.J. Wegman and Q. Luo. High dimensional clustering using parallel
coordinates and the grand tour. Computing Science and Statistics,
28:361–368, 1997.

[25] J.A. Wise, J.J. Thomas, K. Pennock, D. Lantrip, M. Pottier, A. Schur,
and V. Crow. Visualizing the non-visual: Spatial analysis and interaction
with information from text documents. Proc. IEEE Symposium on
Information Visualization, pages 51–58, 1995.

[26] J. Yang, J. Fan, D. Hubball, Y. Gao, H. Luo, W. Ribarsky, and
M. Ward. Semantic image browser: Bridging information visualization
with automated intelligent image analysis. Proc. IEEE Symposium on
Visual Analytics Science and Technology, pages 191–198, 2006.

[27] J. Yang, A. Patro, S. Huang, N. Mehta, M. Ward, and E. Rundensteiner.
Value and relation display for interactive exploration of high dimensional
datasets. Proc. IEEE Symposium on Information Visualization, pages
73–80, 2004.

[28] J. Yang, M.O. Ward, E.A. Rundensteiner, and S. Huang. Visual
hierarchical dimension reduction for exploration of high dimensional
datasets. Eurographics/IEEE TCVG Symposium on Visualization, pages
19–28, 2003.

[29] J. Yi, R. Melton, J. Stasko, and J. Jacko. Dust & magnet: Multivariate
information visualization using a magnet metaphor. Information Visual-
ization, 4:239–256, 2005.

techniques, and scatterplots, and allowing users to interactively
explore large datasets according to their interests, the VaR
display provides a rich metaphor for interactive exploration
of high dimensional datasets. The case studies and user study
conducted proved that the VaR display is an effective approach
with high scalability.

Although work presented in this paper has greatly extended
the functionality of the original VaR display [27], we believe
that
the VaR display still has much potential for further
development. Time-dependant dimension glyph generation or
layout, the ability to convey spatial information, and the ability
to visualize dynamically changing data streams, are future
directions we want to explore in the VaR display. In addition,
detecting features by analyzing and comparing textures of
dimension glyphs using automatic image analysis techniques
is also an appealing future work. Another important future
work is to conduct user studies to evaluate different options
provided by the VaR display.

ACKNOWLEDGMENT

We gratefully thank Dr. Daniel A. Keim for giving many
valuable suggestions for this work, Dr. Jianping Fan, Yuli Gao,
and Hangzai Luo for providing us the datasets, and the users
who participated in the user study.

This work was performed with partial support from NSF
grant IIS-0119276 and the National Visualization and Ana-
lytics Center (NVAC(tm)), a U.S. Department of Homeland
Security Program, under the auspices of the Southeastern Re-
gional Visualization and Analytics Center. NVAC is operated
by the Paciﬁc Northwest National Laboratory (PNNL), a U.S.
Department of Energy Ofﬁce of Science laboratory.

REFERENCES

[1] M. Ankerst, S. Berchtold, and D.A. Keim. Similarity clustering of
dimensions for an enhanced visualization of multidimensional data.
Proc. IEEE Symposium on Information Visualization, pages 52–60,
1998.

[2] B. Babcock, S. Chaudhuri, and G. Das. Dynamic sample selection
for approximate query processing. Proc. ACM SIGMOD International
Conference on Management of Data, pages 539–550, 2003.

[3] B. Bederson, B. Shneiderman, and M. Wattenberg. Ordered and quantum
treemaps: Making effective use of 2d space to display hierarchies. ACM
Transactions on Graphics, 21(4):833–854, 2002.

[4] C.L. Bentley and M.O. Ward. Animating multidimensional scaling
to visualize n- dimensional data sets. Proc. IEEE Symposium on
Information Visualization, pages 72–73, 1996.

[5] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. When is “nearest
neighbor” meaningful? Lecture Notes in Computer Science, 1540:217–
235, 1999.

[6] S. Chaudhuri, R. Motwani, and V. Narasayya. Random sampling for
histogram construction: how much is enough? Proc. ACM SIGMOD
International Conference on Management of Data, pages 436–447, 1998.
[7] W.S. Cleveland and M.E. McGill. Dynamic Graphics for Statistics.

Wadsworth, Inc., 1988.

[8] J. Fan, Y. Gao, and H. Luo. Multi-level annotation of natural scenes
using dominant image components and semantic image concepts. Proc.
ACM international conference on Multimedia, pages 540 – 547, 2004.
Interactive information visualization of
a million items. Proc. IEEE Symposium on Information Visualization,
pages 117–124, 2002.

[9] J.-D. Fekete and C. Plaisant.

[10] Y. Fua, M.O. Ward, and E.A. Rundensteiner. Hierarchical parallel
coordinates for exploration of large datasets. Proc. IEEE Visualization,
pages 43–50, Oct. 1999.

[11] B. Hibbard and D. Santek. The vis-5d system for easy interactive

visualization. Proc. IEEE Visualization, pages 28–35, 1990.

View publication stats
View publication stats

",False,2007.0,{},False,False,journalArticle,False,7N8IFD7Z,"[{u'tag': u'metafor'}, {u'tag': u'multidimensional data'}, {u'tag': u'visualization'}]",self.user,False,False,False,False,http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4135655,"<p>They show a system to visualize hundreds of dimensions using metafors.</p>
<p>&nbsp;</p>
<p>Could be a citable paper or could be a paper to get some ideas from for an attraction 2 paper.</p>",Value and Relation Display: Interactive Visual Exploration of Large Data Sets with Hundreds of Dimensions,7N8IFD7Z,False,False
UQ2U6GFE,D9MCDRG2,"Observation-Level Interaction with Statistical Models for Visual Analytics 

Alex Endert+      Chao Han*      Dipayan Maiti *     Leanna House*      Scotland Leman*      Chris North+ 

+ Department of Computer Science  

* Department of Statistics 

Virginia Tech 

 

 

ABSTRACT 

specifically  designed  for  visualizations  of  this  purpose.  Thus, 
many  visual  analytic  systems  are  fundamentally  based  on 
interaction  with  statistical  models  and  algorithms,  using 
visualization as the medium for the communication (i.e. where the 
interaction occurs). This communication is performed via direct 
interaction  with  the  parameters  of  the  model.  For  example, 
Interactive  Principal  Component  Analysis,  iPCA [3],  allows  the 
user to change the weight for each dimension in calculating the 
direction  of  projection  using  multiple  sliders  (one  slider  per 
dimension). Also, in an interactive visualization using MDS [4], 
the  user  can  weight  the  dissimilarities  in  the  calculation  of  the 
stress function through similar visual controls.  
In both instances, the model is made aware of the user input 
through  a  formal  and  direct  modification  of  a  parameter  (i.e. 
parameter  level  interaction).  The  drawback  of  this  type  of 
interaction  is  that  users  are  expected  to  be  experts  in  the 
underlying  model  that  generates  the  visualization.  Moreover,  as 
datasets continue to increase in size and dimensionality, directly 
adjusting dimensions or parameters creates an issue of scalability. 
Both interactive MDS [4] and object-centered MDS [5] also allow 
interactions such as “anchoring” points to provide the algorithm 
with user specified starting positions, either to test the sensitivity 
of the current visualization or to obtain an alternate spatial layout 
based  on  the  anchored  observations.  In  both  cases,  the  visual 
analytic system does not leverage the observation level interaction 
to obtain information about the parameters of the model. 
In  this  paper,  we  reevaluate  interaction  with  such  models, 
moving  away  from  parameter  level  interactions,  and  propose  to 
focus on interacting with data (i.e. observation level interaction). 
In contrast to parameter level interactions, users are familiar and 
comfortable  interacting  directly  with  the  data  in  a  spatial 
visualization, freely organizing and relocating observations as an 
integral  part  of  their  sensemaking  process  [6].  Thus,  it  is 
necessary for us to design models that are more tightly integrated 
with interaction at the observation level, rather than through visual 
controls of parameters. 
Our  framework  shields  users  from  the  technicalities  of  the 
model  and  allows  them  to  interact  freely  with  the  data  in  the 
visual  space. The  typical  steps  in  a  discovery  process  based  on 
such a framework will be as follows: 1) the visual analytic system 
provides  a  visualization  based  on  initial  values  of  model 
parameters,  2)  users 
inject 
understanding  and  semantic  reasoning  of  the  data,  3)  under  a 
certain  predefined  mapping  of 
the  user's  observation-level 
interaction to analytic reasoning, the parameters of the model are 
tuned  or  re-weighted  to  reflect  the  user's  understanding  of  the 
data,  and  finally  4) 
the  system  regenerates  an  updated 
visualization  based  on  the  new  parameter  values  of  the  model. 
The  process  continues  iteratively,  as  does  sensemaking,  for  the 
duration of the analytic process.  
We  show  examples  that  such  a  framework  can  be  applied  to 
dimension  reduction  algorithms  for  visual  analysis  of  high-
dimensional data. Our framework of incorporating user interaction 
can be applied to either deterministic or probabilistic methods. We 
demonstrate  this  on:  PPCA  (a  probabilistic  projection-based 

interact  with  observations 

to 

is 

facilitated 

In  visual  analytics,  sensemaking 

through 
interactive  visual  exploration  of  data.  Throughout  this  dynamic 
process, users combine their domain knowledge with the dataset 
to  create  insight.  Therefore,  visual  analytic  tools  exist  that  aid 
sensemaking  by  providing  various  interaction  techniques  that 
focus  on  allowing  users  to  change  the  visual  representation 
through adjusting parameters of the underlying statistical model. 
However,  we  postulate  that  the  process  of  sensemaking  is  not 
focused on a series of parameter adjustments, but instead, a series 
of perceived connections and patterns within the data.  Thus, how 
can models for visual analytic tools be designed, so that users can 
express  their  reasoning  on  observations  (the  data),  instead  of 
directly  on  the  model  or  tunable  parameters?  Observation  level 
(and  thus  “observation”)  in  this  paper  refers  to  the  data  points 
within  a  visualization.  In  this  paper,  we  explore  two  possible 
observation-level interactions, namely exploratory and expressive, 
within  the  context  of  three  statistical  methods,  Probabilistic 
Principal Component Analysis (PPCA), Multidimensional Scaling 
(MDS),  and  Generative  Topographic  Mapping  (GTM).  We 
discuss  the  importance  of  these  two  types  of  observation  level 
interactions, in terms of how they occur within the sensemaking 
process.  Further,  we  present  use  cases  for  GTM,  MDS,  and 
PPCA,  illustrating  how  observation  level  interaction  can  be 
incorporated into visual analytic tools. 
 
KEYWORDS:  observation-level 
statistical models. 
 
INDEX TERMS: H.5.0 [Human-Computer Interaction] 
1 

interaction,  visual  analytics, 

INTRODUCTION 
Visual  analytics 

is  “the  science  of  analytical  reasoning 
facilitated by interactive visual interfaces” [1]. The goal of visual 
analytics  (VA)  is  to  extract  information,  perform  exploratory 
analyses,  and  validate  hypotheses 
interactive 
exploration  process  known  as  sensemaking 
this 
sensemaking loop, users proceed through a complex combination 
of proposing and evaluating hypotheses and schemas about their 
data, with the ultimate goal of gaining insight (i.e. “making sense 
of”  the  data).  A  wide  variety  of  statistical  models  have  been 

through  an 

[2]. 

In 

{aendert, chaohan, dipayanm, lhouse, leman, north}@vt.edu 
 Blacksburg, VA 24061 

model),  MDS  (a  deterministic  stress  minimization  model),  and 
GTM  (a  probabilistic  manifold  learning  model).  However,  the 
fundamental framework can be applied to numerous other models. 
Finally,  we  discuss  the  tradeoffs  between  these  models  for 
observation-level interaction. 
2 

RELATED WORK 

these  algorithms. 

The three methods in this paper were chosen either because of 
their  wide  usage  or  flexibility  in  modelling  non-linear  data.  A 
large and growing body of literature has shown their successful 
applications in visualization. For example, PCA has gained a lot 
of  success  in  the  area  of  image  classification,  with  applications 
such  as  face  recognition  [7-10].  MDS  has  been  used  in  graph 
layout for network visualization [11-13] due to its rich distance 
information.  GTM  is  good  at  visualizing  unstructured  data  like 
newsgroup  text  collections,  web  navigation  datasets  [14],  and 
datasets  which  have  complicated  structure,  for  instance,  protein 
sequences [15] and the standard Swiss-Roll dataset [16].   
Research  has  gone  into  creating  systems  that  allow  for 
interaction  with 
iPCA  [3]  allows  direct 
interaction with the parameters of PCA, through the use of visual 
controls.  In  adjusting  these  parameters,  users  can  observe  the 
corresponding change in the visualization. Buja et al. demonstrate 
an  interactive  version  of  MDS  in  which  users  can  define  static 
locations of a number of observations, and the algorithm positions 
the remaining observations into the layout [4]. We would consider 
this an example of an observation-level interaction, as users can 
“test” the location of specific observations, and see how the layout 
(and  thus  the  algorithm)  responds.  However,  the  interaction  is 
directly on pairwise dissimilarities, instead of updating of global 
dimension  weights  based  on  the  user’s  positioning  of  the 
observations. 
interactive  MDS 
algorithm  using  “object-centric  interaction”,  where  users  can 
explore  alternative  positions  of  observations  by  moving  them 
within  the  spatialization  [5].  This  is  similar  to  our  concept  of 
observation-level interaction, in that the interaction is occurring in 
the spatialization. However, the movement of an observation is to 
discover the proportional error contribution, and not to adjust the 
parameters  of  MDS.  Another  example  of  interacting  directly  in 
the spatialization is “Dust & Magnet”, an interactive visualization 
allowing users to understand large, multivariate datasets [17]. It is 
based on the metaphor of magnets, which can attract observations 
that share the attributes of the magnet. Thus, in placing multiple 
magnets into specific locations in the space, users can gain insight 
into  the  structure  of  the  data  through  seeing  how  observations 
respond to the attractors. Therefore, the interaction is performed 
on attractors (i.e., parameters), not on the observations. 
From  this  work,  we  learn  that  statistical  methods  are  widely 
used in visual analytics, and approaches to making these methods 
interactive  have  been  proposed.  However,  interactivity  in  these 
cases mainly refers to direct manipulation of model parameters. 
With observation-level interaction, we focus on interacting with 
the  observations  within  the  spatial  metaphor,  and  handle  the 
corresponding parameter updates through our methods. 
3 

Similarly,  Broekens  et  al.  describe  an 

OBSERVATION-LEVEL INTERACTION 

In  general,  observation-level  interaction  refers  to  interactions, 
occurring  within  a  spatialization,  that  enable  users  to  interact 
directly  with  data  points  (i.e.,  observations).  A  spatialization  in 
this  context  refers  to  a  two-dimensional  layout  calculated  from 
high-dimensional  data  where  the  metaphor  of  relative  spatial 
proximity represents similarity between documents. That is, data 
points placed closer together are more similar. Observation-level 

in 

interaction 

interaction  may  exhibit 

the  algorithm  computes  similarity.  Thus,  when 

interactions  are  therefore  tightly  coupled  with  the  underlying 
mathematical  models  creating  the  layout,  thus  allowing  the 
models to update parameters based on the interaction occurring. 
While  numerous  forms  of 
these 
characteristics  (e.g.,  moving  clusters  of  documents,  marking 
regions of interest within the spatialization, etc.), in this paper we 
will  focus  on  one  –  movement  of  observations.  From  previous 
studies, we found that movement of observations (in those cases 
documents) closer together is one way for the user to externalize 
the  analytical  reasoning  that  those  documents  are  somehow 
similar [6]. In this study, the spatial rearrangement of documents 
was  an  integral  part  of  each  intelligence  analysts’  sensemaking 
process.  Further,  this  study  points  out  that  users  perform 
observation-level 
two  ways,  exploratory  or 
expressive, based on the particular analytical reasoning associated 
with the interaction, and also how the system responds.  
During an exploratory interaction, users utilize the algorithm to 
explore the data and the space. For example, through dragging one 
observation within the layout, users gain insight into the structure 
of  the  data  by  observing  how  other  data  reacts  given  the 
algorithm. While an observation is dragged through the layout, the 
algorithm  adjusts  the  layout  of  the  remaining  data  according  to 
how 
the 
observation  is  dragged  towards  a  cluster  of  data,  similar  data 
points attract, while dissimilar ones repel. Additional information 
such  as  a  list  of  similar  and  dissimilar  parameters  can  also  be 
displayed.  Through  this  process,  users  learn  about  a  single 
observation,  and  how  it  relates  to  the  other  observations  in  the 
dataset.  
An expressive interaction is different, in that it allows users to 
“tell” the model that the criteria (i.e. the parameters, weights) used 
for  calculating  the  similarity  need  to  be  adjusted  globally.  For 
example,  as  a  user  reads  two  documents,  she  denotes  they  are 
similar by dragging them close together. If this were exploratory, 
the two documents would repel again. However, in an expressive 
form of this interaction, it is the responsibility of the underlying 
mathematical  model  to  calculate  and  determine  why  these 
documents  are  similar,  and  update  the  model  generating  the 
spatial layout accordingly. Using the methods below, we illustrate 
how both expressive and exploratory forms of observation-level 
interaction  are  enabled  through  modifications  made  to  three 
common statistical methods (PPCA, MDS, and GTM).  
4  METHODS INTEGRATING OBSERVATION-LEVEL INTERACTION 
A probabilistic model assumes a sampling distribution for the 
observed data and an uncertainty over the model parameters (e.g. 
PPCA and GTM discussed in Section 4.1 and 4.3 respectively). A 
deterministic method makes no such assumptions about the data 
or the parameters (e.g. Weighted MDS, discussed in Section 4.2). 
House  et  al.  describe  in  detail  the  underpinnings  of  the 
probabilistic framework, termed as “Bayesian Visual Analytics” 
(BaVA) [18]. The BaVA process begins with an initial display of 
the data. In turn the user may assess the display and decide if it 
matches her mental model of the data. If it does not, the user may 
convey  her  cognitive  feedback  f(c)  by  adjusting  the  locations  of 
two  observations  to  convey  her  mental  model  about  the  two 
observations.  The  user  might  also  explore  an  alternative  spatial 
location  of  an  observation  and  see  how  the  other  observation 
responds  to  such  an  interaction.  In  short,  iterations  of  user 
interaction  and  subsequent  regeneration  of  the  visualization  are 
modelled  as  sequential  updating  of  maximum  a  posteriori 
estimates  of  parameters.  The  deterministic  version  of 
the 
framework, termed as “Visual to Parametric Interaction” (V2PI), 
also  starts  with  an  initial  display  and  upon  obtaining  a  user 
feedback  sequentially  updates  the  parameters,  but  the  updated 

values  of  the  parameters  are  such  that  they  minimize  some 
measure of discrepancy between the expected configuration of the 
data under the user’s reasoning and the original data [19].  
For each of the models discussed in this paper, we present an 
overview of the model, describe the modifications made to allow 
observation-level interaction, and show a use case demonstrating 
how an end-user can interact with each model. Given that each of 
these models is designed for different types of data (varying  in 
structure,  size,  and  nature  of  the  data),  the  example  use  cases 
below each use different datasets to match the intended use of the 
models  with  the  use  case.  The  use  cases  are  performed  in 
prototype visualizations to show a proof of concept, and we are 
actively  working  to  incorporate  these  models  into  more  fully 
featured tools.  
PPCA 
4.1 

4.1.1  Overview 

Principal  Component  Analysis  (PCA)  [20-22]  is  a  common, 
deterministic  method  used  to  summarize  data  in  a  reduced 
dimensional  form.  The  summary  is  a  projection  of  a  high-
dimensional  dataset  in  the  directions  with  the  largest  variance. 
When only two directions are chosen, PCA may produce a spatial 
representation or map of the data that is easy to visualize. One 
problem with PCA is that important structures (e.g., clusters) in 
data may not correlate with variance. Thus, PCA spatializations 
may mask information in the data that analysts may find useful.    
Probabilistic PCA [23] is, simply, a probabilistic form of PCA. 
This  means  that  PPCA  is  not  a  deterministic  algorithm,  but  a 
statistical  modeling  approach  (specifically,  a  factor  modeling 
approach) that estimates low-dimensional representations of high-
dimensional  data.  Let  d=[d1,…,dn]  represents  a  p×n  high-
dimensional  data  matrix,  where  n  represents  the  number  of 
observations, p represents the number of columns, and di (for i∈ 
{1,…,n})  represents  a  p×1  vector  for  observation  i.  Also,  let 
r=[r1,…,rn] represent a low-dimensional analogy of d, such that r 
is q×n and q<p.  For our purposes, we set q=2.  PPCA models d as 
a function of r,  

d W r
,
i
i

,

,
µσ

2

=

 
No Wr
i

(

I
µ σ
+
p

,

 

2

)

  
where, No(.,.) represents the Multivariate Normal Distribution; µ 
represents  a  p×1  mean-vector  of  d;  W  is  a  p×q  transformation 
matrix  known  as  the  factor  loadings  of  d;  Ip  is  a  p×p  identity 
matrix; and σ2 represents the variance of each dimension in d.  By 
convention,  PPCA  models  each  ri  with  a  Multivariate  Normal 
distribution centered at zero and with unit variance: ri~No(02, I2).  
In  turn,  the  conditional  posterior  distribution  for  ri  is  No(η,Σr), 
where 
 

η
=

2

W W I
W d
(
)
1
(
σµ−
ʹ′
+
−
2
r W W
Iσ
2
2
−
ʹ′
Σ=
σ
+
2

ʹ′

i

(

−

) 1

)
                               (1) 

 
A spatialization of data d that relies on PPCA plots the posterior 
expectation  η.  Similar  to  PCA,  the  coordinates  η  rely  on  the 
variability observed in d. To see this, let Σd represent the marginal 
variance of di, (Σd=V[di |W,µ,σ2]). Since Σd=W´W+I2σ2, we can 
-1W(di  -µ)  which  shows  that  the  relationship 
rewrite  η  as  η=Σd
between Σd and η is well defined.   
The  final  step  in  PPCA  is  to  estimate  the  model  parameters, 
{W, µ, σ2, Σd}. We take a Bayesian approach. We specify either 

 

 

 

reference or flat priors for each unknown (as suggested by [23] 
and use Maximum A Posteriori (MAP) estimators to assess (and 
plot)  η.  For  example,  when  we  assign  π(Σd)  ∝1,  the  posterior 
distribution for Σd  is an Inverse Wishart (IW) distribution,  

d

d

,

,

)

d

∝

1)

IW(

(
π Σ

nS p n p

− −                             (2) 
 
Where  Sd  represents  the  empirical  variance  of  d.  The  MAP 
estimate of Σd is Sd.   
4.1.2 

User Guided PPCA 

To enable analysts to guide PPCA via the data visualization, we 
take  advantage  of  the  relationship  between  Σd  and  η.  Namely, 
changes in Σd will effect η, and changes in η will effect Σd, when 
we invert Equation (1).   
After  obtaining  an  initial  PPCA  display,  the  user  adjusts  the 
locations of two observations; i.e., adjusts two columns in η. If 
the two observations are moved close to one another, the analyst 
is conveying that in her mental map, the observations are more 
similar  than  what  they  appear  in  the  display;  and,  if  the 
observations are dragged apart, the analyst is conveying that the 
observations differ more than what they appear.    
The  challenge  in  BaVA  is  to  parameterize  the  cognitive 
feedback  and  update  the  visualization  [18].  First,  we  determine 
the dimensions of the data d for which the adjusted observations 
are similar and different. Second, we transform the adjustments to 
η into a hypothetical p×p variance matrix. We denote this matrix 
by f(p), as it is a quantified version of f(c). In f(p), the dimensions for 
which the adjusted observations are similar have small variances 
and  the  dimensions  for  which  adjusted  observations  differ  have 
large variances. Third, we consider the hypothetical variance f(p) to 
be a realization of a Wishart distribution that has an expectation 
equal to Σd. Finally, we apply Bayesian sequential updating [24, 
25] to adjust Equation (2) by the parametric feedback f(p),  
 

(

π Σ=
d

d f
,

(

p

)

)

IW(
+

pS
− −
d

vf
+

(

p

)

,

p n v p
,

 

1)

where, υ is solved from a specification κ(κ ∈ [0,1]) made by the 
analyst  that  states  how  much  weight  to  place  on  the  feedback 
relative to the data. Namely, the updated MAP estimate for Σd is a 
weighted average of the empirical variance Sd and feedback f(p) 
 

MAP

(

)
Σ=
d

f

)

p
(
+

v
v n
+
 

 

S

d

n
v n
+

thus υ= nκ/(1-κ). Now, the PPCA projection of the data d that is 
based on MAP(Σd) will portray both information in the data and 
expert feedback. 
4.1.3 

Example 

A sensitive issue for taxpayers, parents, children, educators, and 
policy  makers  is  whether  an  increase  in  money  devoted  to 
education  will  increase  education  quality.  Money  provides  a 
means  to  buy  modern  textbooks,  employ  experienced  teachers, 
and provide a variety of classes and/or extra curricular activities. 
Although,  do  the  students  who  benefit  from  these  high-priced 
resources actually improve academically? 
In 1999, Dr. Deborah Guber compiled a dataset for pedagogical 
purposes  to  address  this  question  [26].  Based  on  the  following 
variables, 
the  academic  success, 
educational expenses, and other related variables in 1997 for each 
U.S. state: the average exam score on the Standard Aptitude Test 

the  dataset  summarizes 

from 

the  National  Center 

(SAT);  the  average  expenditure  per  pupil  (EXP);  the  average 
number of faculty per pupil (FAC); the average salary for teachers 
(SAL); and the percentage of students taking the SAT (PER). To 
increase  the  complexity  of  the  dataset  slightly,  we  added  two 
variables 
for  Education 
Statistics(www.http:nces.ed.gov):  the  number  of  high  school 
graduates  (HSG)  and  the  average  household  income  (INC). We 
hypothesize that states that spend more on education will cluster 
with states with high SAT averages.   
To assess the hypothesis and explore the data, we implement 
the  BaVA  process  using  PPCA.  Figure  1a),  displays  our  initial 
view of the data. Notice that the visualization does not present any 
structure in the data.  Analysts in the field of education, notice that 
two  states  with  different  expectations  for  SAT  scores  are 
displayed  close  to  one  another.  Thus,  we  select  the  appropriate 
observations and drag them apart as an expressive interaction to 
obtain an updated view that is displayed in Figure 1b). There are 
two clusters in 1b). These clusters correspond with SAT scores 
above and below the national median.  
Based  on  our  hypothesis,  we  suspect  that  the  clustering 
structure in SAT relates to EXP. However, when we re-plot 1b) 
and label the upper and lower EXP 50% quantiles in Figure 1c), 
EXP  does  not  explain  the  clusters.  Thus,  we  used  a  bi-plot  to 
identify  which  variables  explain  the  structure  we  see  in  Figure 
1b).  When  we  mark  the  observations  above  and  below  the 
empirical PER median in Figure 1d), we see that PER and SAT 
clearly  relate  to  the  formation  of  clusters  in  the  dataset.  Thus, 

(a)  Initial                            (b) SAT Scores 

 

 

              (c) EXP                                  (d) PER      

 
Figure  1.  After  injecting  expert  feedback  into  Figure  1a),  we 
obtain  Figures  b)-c).  For  frame  of  reference,  we  marked  the 
two  points  moved  to  inject  feedback  by  `x'  in  Figure  b).  The 
configuration  of  points  in  each  graph  are  identical,  but  the 
observations  are  labeled  differently.  In  Figure  b),  symbols  `●’ 
and  `○’  mark  the  upper  and  lower  50%  quantiles  for  SAT 
scores respectively; in Figure c), symbols `●’ and `○’ mark the 
upper  and  lower  50%  quantiles  for  EXP  scores  respectively; 
and in Figure d), symbols `●’ and `○’ mark the upper and lower 
50%  quantiles  for  the  percentage  of  students  taking  the  SAT 
the  clusters 
(PER) 
in  each  graph 
correspond with SAT and PER, but not EXP. 

respectively.  Notice 

further analyses of SAT and EXP must control for PER.   

 

4.2  MDS 

We  extend  our  framework  to  another  deterministic  method, 
which  forms  the  basis  for  a  large  number  of  visualization 
techniques: Multi-Dimensional Scaling (MDS).  
4.2.1  Overview 

All complex data visualizations are based on high-dimensional 
datasets, which contain features corresponding to dimensions, and 
the relative importance of such features through a set of weights 
(wi).  Classically  weighted  multidimensional  scaling  deals  with 
mapping  a  high  dimensional  dataset  d=[d1,…,dn]  into  a  low 
dimensional (in our case two-dimensional) space r, by preserving 
pairwise distances between observations in the low dimensional 
representation.  Let  w  represent  the  p-vector  of  feature  weights:  
w={w1,…,wp}. Given a set of feature weights, the low dimensional 
spatial coordinates are found by solving:  

 

where 

min
r
r i
,...,
1

n

, 

r
i

−

r δ
w
(
i j
j
,

−

)

∑

j n

<≤

)

w
(
δ
i j
,

w
k

dist(

d

ik

d

)

jk

 

p

=−∑

k

1
=

such  that  ∑k  wk=1.  dist()  represents  any  distance  function  for 
measuring  individual  features  in  the  high  dimensional  space. 
Because  it  is  not  possible  to  estimate  weights  and  the  set  r 
simultaneously,  we  provide  a  uniform  weighting  of  the  space 
wi=1/p for our first iteration. 
User Guided MDS 
4.2.2 

the  display  and 

learn  from  certain  aspects  of 

Once  a  visualization  is  generated,  the  user  may  either  agree 
with 
the 
visualization, or disagree, based on their domain expertise. Hence, 
the  user  may  wish  to  interact  and  rearrange  a  few  of  the 
observations in the visualization. Given a spatial interaction in the 
form  of  adjusting  the  relative  position  of  a  set  of  points,  we 
compute a set of feature weights, which are consistent with both, 
the  users  adjustment  and  the  underlying  mathematical  model. 
These are computed by inverting the optimization, by fixing the 
locations  of  the  adjusted  points  and  finding  an  optimal  set  of 
weights,  which  are  consistent  with  the  visualization.  Explicitly, 
we solve for w such that  

Figure  2.  Visualization  of  the  1990  census  dataset  using 
classical MDS. 
 
 

 

%  

r
( )
δ
−
i j
,

w
k

dist

d

(

i k
( )

−

r
j k
( )

)

min
w
w i
,...,
1

p

j

p

∑ ∑

l k
<≤ =

1

 
and  ∑k  wk=1,  where  d(i)k  represents  the  kth  element  in  the 
observation of d that maps to ri in the adjusted visualization and l 
is  the  total  number  of  manipulated  observations.  It  should  be 
noted  that  computing  the  new  weights  is  extremely  fast,  and  is 
then followed by a full MDS step. Thus, the entire generation of a 
new view can be performed in real time, depending on the size of 
the dataset and the specific hardware used. 
4.2.3 

Example 

inconsistencies  with 

their  mental  model 

Consider  for  example  a  visualization  produced  by  a  standard 
MDS  technique.  In  this  example  we  focus  on  the  1990  census 
dataset [27] under a Classical Metric Scaling (CMS) [28], using a 
Hamming distance (due to the categorical nature of the dataset) 
for  measuring  features  in  the  high  dimensional  space.  Figure  2 
illustrates  results  obtained  under  a  Classical  Metric  Scaling 
(CMS).  
Given  this  visualization,  a  user  may  distinguish  3-5  main 
clusters, and inquire what they mean. We see two major ways a 
user  can  interact  with  the  visualization,  in  order  to  explore  the 
space, and learn about the underlying dataset. The first of these is 
by highlighting a subset of the data, based on some question the 
user seeks to answer, and then rearranging the visualization based 
on 
(expressive 
interactions). 
The second approach is to hone in on visual structure, and move 
points  in  the  visual  space  in  order  to  learn  what  the  structure 
relates to in terms of the feature space (exploratory interactions). 
Both  of  these  interactions  are  nearly  identical,  however  the 
motivation for the interactions will differ. We will illustrate both 
types of visual reasoning through an example based on the 1990 
census dataset. 
The user may wish to interact expressively and identify points 
in the space that pertain to high and low income groups. The user 
highlights individuals with incomes below 15K and over 60K, as 
shown  by 
  in  leftmost  panel  of  Figure  3,  respectively. 
Because of the close proximity of the highlighted groups in the 
main clusters, the user drags (denoted by ⊗) a few representative 
low and high-income individuals into sets of groups in each of the 
3  main  sub-clusters.  The  system  reports  back  a  set  of  weights, 
which  explain  how  much  a  particular  feature  explains  the 
arrangement of points suggested by the user. High weights relate 
to 
their 

low  weights  suggest 

features,  while 

important 

and 

 

Figure 4. A user performing an exploratory interaction to learn 

to 

this 

information, 

the  system  updates 

    what distinguishes two clusters. 
corresponding  features  do  not  relate 
the  user's  visual 
rearrangement. For our example, we learn not only that income 
level  (29%),  but  also  by  their  means  of  transportation  to  work 
(20%), whether or not they worked the full year (25%), and their 
level of education (10%) are related to the user's repositioning of 
points.  Given 
the 
visualization, as shown in center panel of Figure 3. We notice that 
in  the  resulting  visualization,  the  income  groups  are  clearly 
separated.  The  resulting  visualization  displays  a  much  richer 
spatialization than simply showing clusters relating to the income 
groups.  For  example,  we  highlight  individuals  that  actually 
worked  in  the  right  most  panel  of  Figure  3,  and  notice  these 
individuals are shown in distinct sub-clusters. 2 of the 4 clusters in 
which  individuals  work  pertain  to  low-income  groups,  and  the 
other 2 pertain to high-income groups (as illustrated by the 
 and 
 symbols).  
Figure  4  shows  how  the  user  might  perform  an  exploratory 
interaction in order to learn what explains the clustering structure 
between the working/low income groups. To suggest the clusters 
could be moved further away from each other than they appear in 
the  current  visualization,  the  system  reports  back  the  weights, 
which explains the differences in the groups. For this example, the 
user learns that one of these clusters contains individuals that have 
a reliable mode of transportation to work (93% explained). The 
visualization could be updated based on this information, or the 
user could simply document this fact and proceed by explaining 
other areas of the spatialization. As always, there are an endless 

 
Figure 3. A sequence of visualizations derived through observation-level interaction with a modified MDS method. (Left) The user moves a 
set of points into new locations, communicating his intuition that there may be additional structure within each cluster. (Middle) The updated 
visualization showing new clusters. (Right) Highlighting showing the separation of income groups in the updated visualization. 
 

 

 

number  of  possibilities  for  learning  about  a  high  dimensional 
dataset via visual expression/exploration. Another example of an 
exploratory interaction with MDS is demonstrated by Buja et al. 
in  which  users  can  constrain  observations  to  specific  spatial 
locations [4]. 
GTM 
4.3 

4.3.1  Overview 

Introduced  by  Bishop  et  al,  [29]  Generative  Topographic 
Mapping (GTM) is a nonlinear latent variable modeling approach 
for  high-dimensional  data  clustering  and  visualization.  It  is 
considered  to  be  a  probabilistic  alternative  for  both  the  Self-
Organizing  Map  (SOM)  algorithm  [30]  and  Nonlinear  PCA. 
Similar  to  PPCA,  GTM  estimates  a  latent  variable  r=[r1,…,rn] 
(q×n  matrix)  that  is  a  low-dimensional  representation  of  high-
dimensional  data  d=[d1,…,dn]  (p×n  matrix  such  that  p>q). 
However, unlike PPCA, the q-dimensional coordinates r in GTM 
map  nonlinearly  to  a  complex  manifold  m=[m1,…,mn]  that  is 
embedded in the high-dimensional space.  This manifold, ideally, 
in  data  d  and  represents 
characterizes 
geometrically the expected value for d in the Gaussian model,   
                               (3) 

important  structure 

    

d N W r
( ),
i
i

Φ

(

I β−
1
p

)

:

 

j

2

)

=Φ

    

exp(

r
( )
Φ=−
i

µ
−
j
2
2
σ

m W r
( )
i
i
r
i

To estimate a coordinate mi, GTM takes a weighted average of J 
radial  basis  functions  {Φ1(),…,ΦJ()}  (Φj()  represents  a  radially 
symmetric Gaussian kernel) given ri and parameters there in,  
 

,                                      (4) 
,                            (5) 
 
where  W  is  a  p×J  transformation  matrix;  Φ(ri)  is  a  J×1  vector 
such that Φ(ri)=[Φ1(ri),Φ2(ri)…,ΦJ(ri)]ʹ′; and µj is a q×1vector that 
centers  the  basis  functions.  The  center  coordinates  µ=[µ1,…,µJ] 
cover the q-dimensional latent space uniformly. Model parameters 
are estimated using the EM algorithm [31]. 
One  advantage  of  GTM  is  that,  by  construction,  it  lacks 
sensitivity to outliers.  For tractability, the coordinates of each ri 
are  limited  a  priori  to  a  finite  set  g  of  K  possibilities,  ri 
∈g={g1,…,gK} 
latent  space 
uniformly.    To  decide  which  value  for  ri  generates  di,  GTM 
estimates the posterior probability, i.e., responsibility, that ri=gk. 
Given a prior probability that ri=gk is 1/K for all k ∈{1,…,K}, let 
Rik  represent  the  posterior  responsibility  that  latent  variable  ri 
generates di, when ri=gk, 
 

the  q-dimensional 

that  covers 

 ,                            (6) 

R
ik

=

d r
(
π
=Φ
i
i
K
d r
(
∑
π=
i
i
l
1

g W
,
,
k
g W
,
=Φ
l

())
,

())

 
In  turn,  GTM  plots  the  posterior  mode,  expectation,  or  any 
quantile of ri given specifications g and estimates for {Ri1,…RiK}.    
4.3.2 

User Guided GTM 

GTM  is  a  complex  modeling  approach  that  relies  on  many 
tunable parameters that are hard to interpret. User Guided GTM 
(ugGTM)  will  allow  analysts  to  both  take  advantage  of  the 
benefits  of  GTM 
complicated  GTM 
parameterization.  Specifically,  analysts  may 
tag 
clusters,  tag  regions  of  the  visualization  space,  and  query 
differences in documents.  

and  guide 

label,  i.e., 

the 

Here, we illustrate ugGTM within the context of an example. 
We have a collection of 54 abstracts from proposals funded by the 
National Institute for Health (NIH). After standard preprocessing, 
we apply a ranking system that we will call an Importance Index 
(ImpI),  which  is  based  on  the  Gini  coefficient.  ImpI  considers 
both the frequency and uniqueness of words that are shared across 
documents and assigns a metric between 0 and 1.  Entities that 
occur  equally  frequently  in  all  the  documents  have  ImpI=0  and 
entities that occur in only one document has ImpI=1.  We selected 
the 1000 entities with the highest ImpI. One advantage of ImpI is 
that we can measure document similarity using Euclidean distance 
between  proposals.  Pairs  of  documents  with  small  Euclidean 
distances have comparable terms with similar frequency; and pairs 
of  documents  with  large  Euclidean  distances  have  few,  if  any, 
words in common. 
We apply GTM for J=16 and K=400 to obtain an initial display 
of the proposals, shown in Figure 5. Notice four clusters appear in 
Figure 5 that we labeled A, B, C, and D.   
Tagging  the  Clusters  and  the  Space.  To  understand  the 
meaning of the clusters, we determine the words that both overlap 
the  least  within  each  cluster  and  have  the  highest  ImpI’s.  
Specifically, we apply k-means [32] to the low-dimensional data 
coordinates to determine cluster memberships.  For each cluster 
we  sum  the  ImpI  vectors  across  the  documents  and  rank  the 
entities based on the ImpI sum. Entities ranked highest are those 
that 1) have importance in the corpus (as determined by the ImpI) 
and 2) have occurred most frequently.  Given top rankings from 
each cluster, we delete those shared by all four clusters. Table 1 
lists  the  unique  key  words  that  describe  each  cluster.  Group  A 
represents proposals that include brain related cancer studies and 
their clinical applications. Group B represents proposals related to 
human neural systems. Group C represents proposals that address 
genomic  and  transcriptomic  research  problems. 
  Group  D 
represents  proposals  about 
such  as 
infectious  diseases, 
tuberculosis, and immunity.  

 
Table 1. Cluster tags (top 10 keywords) for NIH abstract groups. 
 

    
As  described  previously  in  Equation  (3),  GTM  characterizes 
high-dimensional  data  as  random  perturbations  from  a  complex 
manifold m; E[di] = mi for all i ∈ [1,…,n]. To tag the visualization 
space, we select any spot, r+, in the visualization and use Equation 
(4) to estimate its corresponding location on the manifold, m+. The 
estimate m+ will be a 1000×1 vector of ImpI’s that we may use to 
rank  the  entities.  We  report  the  top  ranked  entities  to  tag  the 
space. For example, in Figure 5, we pick up a spot r+ (represented 

Group A 

Group B 

Group C 

Group D 

Shared by All 

Groups 

tumors, brains, stem, treatments, patients, 
generations, drugs, ordering, controlling, 
therapeutics 
stem, neuronal, brains, proteins, deliveries, 
regulations, neural, patients, differentiation, 
expression, treatments 
stem, genetically, regulations, drugs, 
structurally, proteins, genomics, epigenetics, 
RNAs, complexities 
Infections, treatments, tuberculosis, 
expression, patients, drugs, strains, 
resistance, vaccination, immunity 
cells, functionalization, diseases, 
developments, genes, cancerous , studying, 
researchers, proposing, mechanisms, 
specification 

by a pink circle) that locates roughly at the center of cluster D.  
Several  of  the  tagged  top  keywords  overlap  with  the  words 
describing cluster D. 
    
Document-Based  Query  and  Cluster  Reorganization.  It  is 
common for users to assess documents by searching for keywords. 
However,  keyword  searching  may  be  a  tedious  task  and  fail  to 
reveal  document  clusters  of  interest.  For  example,  keyword 
searches may identify documents with similar keywords, but used 
in different contexts; miss documents that contain combinations of 
the  keywords;  or  prioritize  words  that  have  little  relative 
importance for the user.  In response to the challenges of keyword 
searching,  many  analysts  rely  on  document  matching.  For 
document  matching,  entire  documents  can  be  used  to  identify 
which of the remaining documents in the corpus are most similar 
(to the chosen document). Hence such a matching algorithm is a 
document-based query of a corpus. 
In our ugGTM, users may query documents in the corpus by 
dragging a document of interest directly in the visualization and 
watching  how  the  remaining  documents  respond;  e.g.,  similar 
documents will follow the document being dragged and dissimilar 
documents will repel. The behaviour of the documents is similar 
in spirit to Dust and Magnets (DnM) [17]. In DnM, analysts may 
drag or shake magnets that represent variables in the dataset and 
watch  as  relevant  documents  follow  the  magnets.  However,  a 
major  difference  between  DnM  and  ugGTM  is  that  when  users 
drag  documents  (not  variables)  and  watch  how  the  remaining 
react, they are comparing documents based on all of the variables 
in  the  dataset  simultaneously.  In  turn,  users  may  learn  which 
variables are important for comparisons, based on tags within the 
visualization space. 
The interaction is possible because ugGTM gives control to the 
users of some parameters in the model via the visualization.  Let 
r* represent the low-dimensional coordinates for a document that 
an  analyst  has  chosen  to  drag.  Given  r*,  we  add  to  the  model 
described in Equations (3)-(6) by expanding sets g and Φ so that 
g={g1,…,  gK,  g*}  and  Φ={Φ1,…,ΦJ,Φ*},  where  Φ*  =  exp{-||ri-
µ*||2/2σ2}  and  g*=µ*=r*.  In 
the  posterior 
responsibility (Equation 6) that r* generates d* via m* to 1 (where, 
m* is defined by Equation (4) so that the mapping between the 
low-  and  high-  dimensional  coordinates 
the  moving 
observations is deterministic. 
To  propagate  the  effect  of  moving  r*  to  the  remaining 
visualization,  we  take  a  local  regression  approach  [33]  to 
characterize high-dimensional data di |{ ri=g*,m*} in that we scale 
di-m* by the square-root of function V given scaled distance Δi = 
||d*-di||/c so that,  

turn,  we  assign 

for 

(
π

d r
=Φ=
i
i

g W
*

,

,
−

)

⎛
⎜
⎝

β
2
π

⎞
⎟
⎠

exp{

V
β

)

i

(
Δ
2

d m
i

*

 
}

2

p
/2
−
−
 

 

 

2 and c=0.5. In turn, both 
where c is user-defined; e.g., V(Δi)=Δi
posterior responsibility estimates (Equation 6) and estimates for m 
(Equation 4) change.  Let mi
(c) and mi
(u) represent the current and 
user-adjusted manifold estimates for observation i.  We define the 
BaVA-GTM estimate for the manifold, mi
 

(c+1), by 

 

m
(
i

c

1)
+ =

δ
i

m
c
( )
+−
i

(1

δ
i

)

m
u
( )
i

, 

where δi=||ri-r*||/b and b=max{||r1-r*||,…,||rn-r*||} so that δi∈ [0,1].  
(c+1) controls the visualization so that only the 
This definition for mi

regions  of  interest  respond  to  user  interactions;  areas  that  are 
distant from the dragged observations do not change.   
Parameters g*, Φ*, V(Δi), δ  and m(c+1) in ugGTM work together 
in the following way. When a data point di is far from d*, V(Δi) 
will  be  large  and  thus  decrease  the  posterior    responsibility 
(Equation 6) that ri=g* generates di. Similarly, when di is near d*, 
the  corresponding  responsibility  will  increase.  Increases  in  the 
responsibility  for  ri=g*  will  cause  the  coordinates  for  ri  to 
gravitate toward r*. Thus, analysts may specify constant c in our 
definition Δi, depending upon how many document matches they 
seek  for  the  moving  document.  Also,  the  degree  to  which  the 
observations  gravitate  toward  r*  is  determined  by  δ  and  m(c+1). 
When the manifold shifts from m(c) to m(c+1), the meaning of the 
visualization space changes, as we demonstrate in our example.  
4.3.3 

Example 

For our NIH example, we apply ugGTM. We display an initial 
GTM view of the documents (the 54×1000 dataset) in Figure 5. 
Suppose a user identifies a specific document of interest, e.g., Doc 
7 (highlighted in yellow in Figure 5) to investigate. A preliminary 
investigation might involve a sequence of non-spatial interactions, 
such as, searching of multiple keywords, reading all or part of the 
document  etc.  However,  a  comprehensive  assessment  of  the 
document may require spatial interactions as well. The user might 
explore  space  tags  across  the  screen  and  determine  a  more 
appropriate location for the document of interest. In this case, Doc 
7 is closer to Group A, and is about developing new brain tumor 
therapies  and  tumor  stem  cell  quiescence.  The  keywords  this 
document shares with group A include tumors, brains, cancerous, 
therapeutics and chemotherapy. However, since Doc 7 relates to 
therapy developments for disease, it shares some keywords with 
Group  D;  e.g.,  treatments,  strategies,  patients,  drugs,  resistance, 
clinically.   
As an exploratory spatial interaction, the user drags Doc 7 to 
the  lower  left  corner  of  the  display  and  watches  how  the 
remaining  documents  react.  By  repositioning  Doc  7,  the  user 
redefines the spatialization of the screen, i.e., modifies the space 
tag  corresponding  to  a  location.  For  example,  when  we  tag  the 
same  coordinates  r+  in  Figure  6  (r+  are  the  coordinates  of  the 
space tagged in Figure 5), we learn that the top keywords include 
treatments  and  tumors  as  well  as  those  that  were  there  earlier. 
Recall that ugGTM uses every variable in the dataset to compare 
documents.  For  this  reason,  documents  that  mention  stem  cells 
and  other  important  keywords  in  Doc  7  follow  Doc  7.  As 
expected, many documents in Group D gravitate toward Doc 7. 
However,  a  few  documents  in  Group  B  also  followed.  Future 
work will allow users to weight the keywords in Doc 7, if desired. 
Also  Documents  with  ID  20,  22,  32  and  39  change  locations. 
Important  keywords  for  these  documents  include  the  following: 
Doc 20 discusses diagnosis of HIV infection in patients who live 
with  limited  access  to  therapeutic  treatments;  Doc  22  discusses 
expression characteristics of a drug-resistant gene; Docs 32 relates 
to  varying  yeast  strains;  and  Doc  39  relates  to  Lymphocyte 
Homing.  Docs  20  and  22  repelled  against  Doc  7  because  the 
redefined-manifold down-weighted their important entities in the 
lower left corner and up-weighted the entity tumor. Thus, Doc 20 
and Doc 22 shifted to Groups A and C respectively. Docs 32 and 
39  are  separated  slightly  from  Group  D  and  gravitated  toward 
Group C because they have a few words in common with each 
group, but not enough to place them in either corner. 
An interesting note about the updated manifold is the change in 
shape or magnification factor [34]. The colour in the background 
is  plotted  based  on  the  logarithm  of  the  magnification  factor 

Figure  5.  GTM  display  of  the  NIH  abstracts.  Black  dots  mark 
documents and labeled by their document ID. 
 
evaluated on a fine grid that covers the visualization space. Due to 
the  nonlinear  mapping  from  ri  to  mi,  equal  distances  in  the 
visualization do not necessarily imply equal distances in the high-
dimensional space. The magnification factor describes the rate of 
change  between  distance  or  area  in  the  latent  space  and  the 
corresponding  distance  or  area  on  the  manifold  and  can  be 
interpreted  as  a  description  of  how  wiggly  the  manifold  is. 
Overall,  the  magnification  factor  is  lower  in  Figure  6  than  in 
Figure 5 and the clusters formed in Figure 6 are mainly in low 
magnification  areas.  This  means  the  clusters  in  Figure  6  are  in 
flat, stable regions of the estimated manifold. Thus, observations 
in these clusters are closer to one another than observations shown 
in clusters within Figure 5. 
5 

DISCUSSION 

We present a comparison of key characteristics of the methods 
used in this paper in Table 2. Again, the purpose of this work is 
not to make a direct comparison of these three methods, but rather 
to  present  how  to  apply  observation-level  interaction  to  each 
method, and summarise our findings in the table.  
Mappings. The three methods discussed in the paper provide 
us  with  a  spatialization  of  the  data  within  the  bounds  of  their 

 

 

 

Figure 6. The updated view after moving doc 7 from top left to 
bottom left. 
algorithmic  complexity.  Points  that  are  close  in  the  higher 
dimensional space remain close to each other in the visualization 
in  all  the  algorithms  although  the  concept  of  proximity  varies 
depending on the algorithm. As an artifact of the algorithms, in 
both PPCA and MDS, the high dimensional data is assumed to be 
a linear mapping of the visualized representation while GTM is a 
non-linear mapping of the same. Hence, the same dataset might 
provide  widely  disparate  visualizations  for  different  algorithms. 
Spatially  this  might  translate  to  the  fact  that  based  on  the 
algorithm, the user’s spatial interaction might target different sets 
of observations. Each algorithm can potentially have its own set 
of diagnostics overlaid with the visualization that might aid the 
user  in  understanding  the  proximity  of  the  data  in  the  higher 
dimensions;  e.g.  visualizing  the  magnification  factor  along  with 
the data in GTM indicates the level of distortion. The goal of the 
user is to obtain a view in multiple steps that matches with his 
mental model irrespective of the algorithm used to visualize the 
data.  The  specific  steps  that  the  user  goes  through  should  be 
immaterial in so far as the final visualization is concerned and all 
the algorithms discussed here have the flexibility to provide that.  
PPCA relies on the assumption that a single linear projection 
exists  that  can  reveal  useful  structure.  MDS  provides  a  two- 
dimensional representation of the observations via penalization of 
any  distance  distortion  that  happens  in  the  two-dimensional 

Table 2. Comparison of the methods used in this paper.  

  
Mapping Type 
Method Characterisation  
Distribution Assumption 
Scalability (Observations) 
Scalability (Dimensions) 
Conceptual Clarity 
Running Time 
Outlier Robustness 

PPCA 
Linear 
Variance 
Probabilistic 

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174) = Good(cid:1) (cid:174)(cid:174) = Average(cid:1)

MDS 
Linear 
Similarity 
Deterministic 

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174) = Poor 

GTM 

Non-linear 
Manifold 
Probabilistic 

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

representation  using  a  stress  function.  However,  the  linear 
projection assumption may not hold for complex datasets or, the 
visualization based on minimizing stress in MDS might not reveal 
all the information in the data. In PPCA, using variance to select 
the  direction  in  which  to  project  data  makes  sense  for  datasets 
with a global linear structure [23]; the projection will minimize 
the number of observations that overlap so that they are as visible 
as possible.  
However,  variance  estimates  and  hence  PPCA  visualizations 
are  sensitive  to  outliers  and  it  is  not  uncommon  for  PPCA  to 
display one or two outliers and a cloud of occluded points. Under 
Euclidean  distance,  MDS  is  algorithmically  the  same  as  PPCA 
and  will  suffer  from  the  same  sensitivity  to  outliers.  Assessing 
such  a  visualization  and  making  appropriate  adjustments  would 
be,  at  best,  challenging.  Thus,  a  more  complex  methodology  is 
often needed to summarize datasets, e.g., mixture PPCA or GTM. 
GTM being a topographic mapping places the outliers at one end 
of the screen or at a position that is distant from the region that 
has more structure. In our interactive framework, outliers can be 
brought closer to existing user defined clusters through redefining 
the principal components in PPCA, reweighting of the dimensions 
in MDS and constraining responsibilities in GTM; in all the cases 
the  user’s  observation-level  interaction  initiates  the  parameter 
update. 
Scalability. In terms of time complexity, GTM is O(KND) (K 
number  of  latent  points,  N  number  of  observations,  D  data 
dimensionality),  PPCA  is  O(qND)  (q  is  the  dimension  of  the 
latent space, usually equals 2) and MDS varies from O(qND) to 
O(N3).  The  effect  of  high  dimensionality  (i.e.  the  number  of 
columns for every observation) on the run-time will be similar for 
all three algorithms. The challenge in scalability (large N) is also 
of  the  same  order  for  the  three  algorithms  when  Euclidean 
distance is used.  
However  in  the  design  of  a  visual  analytic  system  that 
incorporates user interaction in the framework, the choice of the 
algorithm  should  be  based  not  only  on  the  run-time  of  the 
algorithm  but  also  on  the  cost  incurred  in  converting  the 
observation-level interaction or feedback to updated values of the 
parameters for the method.  In PPCA, it is the cost of evaluating 
the feedback matrix f(p); in MDS it is the cost of obtaining optimal 
feature weights w based on pair-wise distances of the observations 
that the user has moved; and in GTM, it is the cost of computing 
distances between data points and reference vectors. Under such 
considerations, we think MDS provides the quickest and easiest 
two-dimensional visualization of the data, followed by PPCA and 
GTM.  
We  maintain  a  probabilistic  framework  in  PPCA  and  GTM. 
Specifically  for  PPCA,  computation  is  quick  since  the  primary 
parameter  of  interest  Σd  has  a  posterior  distribution  and  a 
conjugate feedback distribution, and MAP(Σd) can be computed 
without MCMC.  Thus, analysts can explore the data in real time.  
GTM (although being most flexible in handling more complicated 
data occlusion issues that challenge MDS or PPCA) is based on 
an expectation-maximization algorithm and hence needs more run 
time to converge to the optimal parameter value. 
Sensitivity. The methods described in this paper will respond 
based on the interaction performed (i.e., number of observations 
moved, distance the observations were moved, etc.). For example, 
moving a single observation will generally result in a less drastic 
change in the layout compared to a similar interaction performed 
on a cluster of observations. Thus, the sensitivity of the models in 
terms of responding to the user’s intuition is dependent on how 
large the change or update is provided by the user’s interaction, 

the size of the dataset, as well as if the data supports the suggested 
updated  layout.  The  methods  will  attempt  to  find  the  “best  fit” 
given the user feedback, but will maintain mathematical validity 
(i.e., users cannot force the layout if the data does not support it). 
The  result  is  such  that  the  system  balances  the  user’s  intuition 
with  the  structure of  the  data  to  reduce  bias.  The  goal  of  these 
techniques is not to converge on a single structure or layout, but 
rather to allow exploration of many possible structures. 
Interaction. The examples of how observation-level interaction 
can occur within spatializations in this paper show only one form 
of interaction available to users within spatializations – movement 
of individual observations. The methods are expandable to allow 
more  complex 
interactions,  such  as  moving  clusters  of 
observations, annotating a region of the spatialization, and other 
interactions used for communicating the intuition of the user to 
the system. In a fully implemented visual analytics system, these 
interactions  may 
include  queries,  highlighting,  and  other 
interactions  from  which  analytical  reasoning  of  users  can  be 
interpreted.  
Implementation.  The  prototype  visualizations  shown  in  this 
paper are intended to provide working examples of the modified 
methods. Through the use cases, we highlighted how an end-user 
might  interact  with  such  systems.  We  plan  to  integrate  these 
methods  into  more  fully  functional  visual  analytics  tools.  That 
will allow us to perform a series of user studies to evaluate the 
usability  and  effectiveness  of  observation-level  interaction  in 
terms  of  providing 
the 
sensemaking process.  
6 

to  users,  and  supporting 

insight 

CONCLUSION 

In  this  paper,  we  described  how  modifications  of  powerful 
statistical methods allow user interaction at the observation-level. 
By  interacting  within  the  visualization  through  movement  of 
observations, users are able to perform exploratory and expressive 
interactions. Thus, users are able to perform sensemaking tasks, 
such as hypothesis validation, directly within the spatial metaphor. 
By keeping the interaction at the observation level, users are not 
required  to  transform  their  sensemaking  into  a  combination  of 
statistical parameter updates.  
In particular, we modified PPCA, MDS, and GTM using BaVA 
[18] and V2PI [19] approaches, so that users can focus on their 
spatial  analysis  of  data  rather  than  directly  updating  statistical 
parameters of models. We present three examples (one for each 
modified  method)  that  illustrate  the  effectiveness  of  these  new 
models. Based on the positive results in this paper, as well as the 
lessons  learned,  coupling  interaction  with  statistical  models 
provides  an  opportunity  to  explore  additional  forms  of  spatial 
interaction for visual analytic applications. 
ACKNOWLEDGEMENTS 
This research was funded by the National Science Foundation, 
Computer and Communications Foundations, grant #0937071. 
REFERENCES 
[1]  Thomas,  J.  J.,  Cook,  K.  A.,  National,  V.  and  Analytics,  C. 
Illuminating the path. IEEE Computer Society, 2005. 
[2] Pirolli, P. and Card, S. Sensemaking Processes of Intelligence 
Analysts  and  Possible  Leverage  Points  as  Identified  Though 
Cognitive  Task  Analysis  Proceedings  of  the  2005  International 
Conference on Intelligence Analysis, McLean, Virginia, 2005. 
[3] Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. and 
Chang,  R.  iPCA:  An  Interactive  System  for  PCA-based  Visual 
Analytics. Computer Graphics Forum, 28, 2009. 

[24]  Spiegelhalter,  D.  and  Lauritzen,  S.  Sequential  updating  of 
conditional  probabilities  on  directed  graphical 
structures. 
Networks, 20, 1990, 275-605. 
[25]  West,  M.  and  Harrison,  J.  Bayesian  Forecasting  and 
Dynamic Models (Springer Series in Statistics). Springer, 1997. 
[26]  Guber,  D.  Getting  What  You  Pay  For:  The  Debate  Over 
Equity  in  Public  School  Expenditures.  Journal  of  Statistics 
Education, 7, 2, 1999. 
[27]  Blake,  C.  and  Merz,  C.  J.  {UCI}  Repository  of  machine 
learning databases. 1998. 
[28] Schiffman, S., Reynolds, L. and Young, F. Introduction to 
Multidimensional  Scaling:  Theory,  Methods,  and  Applications. 
Academic Press, 1981. 
[29]  Christopher,  M.  B.  GTM:  The  generative  topographic 
mapping. 1998. 
[30] Kohonen, T., Kaski, S., Lagus, K., Salojarvi, J., Honkela, J., 
Paatero,  V.  and  Saarela,  A.  Self  Organization  of  a  Massive 
Document  Collection.  Transactions  on  Neural  Networks,  11,  3 
2000. 
[31] Dempster, A. P., Laird, N. M. and Rubin, D. B. Maximum 
likelihood from incomplete data via the EM algorithm. City, 1977. 
[32] MacQueen, J. Some methods for classification and analysis 
of  multivariate  observations.  Proceedings  of 
the  Berkeley 
Symposium  on  Mathematical  Statistics  and  Probability,  1,  281-
297, 1967, 14. 
[33] Hastie, T., Tibshirani, R. and Friedman, J. H. The Elements 
of Statistical Learning. Springer, 2003. 
[34]  Svensen,  J.  F.  M.  GTM:  the  generative  topographical 
mapping. Aston University, Birmingham, 1998. 
 

 

L. 

the  expert. 

in  Face  Verification.  In  Proceedings  of 

[4] Buja, A., Swayne, D. F., Littman, M., Dean, N., Hofmann, H. 
and  Chen, 
Interactive  Data  Visualization  with 
Multidimensional  Scaling.  Journal  of  Computational  and 
Graphical Statistics, 17, 2, 2008. 
[5]  Broekens,  J.,  Cocx,  T.  and  Kosters,  W.  A.  Object-centered 
interactive  multi-dimensional  scaling:  Ask 
In 
Proceedings  of  the  Eighteenth  Belgium-Netherlands  Conference 
on Artificial Intelligence, 2006. 
[6] Andrews, C., Endert, A. and North, C. Space to Think: Large, 
High-Resolution Displays for Sensemaking. In Proceedings of the 
CHI, 2010.  
[7] Conde, C., Ruiz, A. and Cabello, E. PCA vs Low Resolution 
Images 
the  12th 
International  Conference  on  Image  Analysis  and  Processing 
(2003). IEEE Computer Society.  
[8]  Gottumukkal,  R.  and  Asari,  V.  K.  An  improved  face 
recognition technique based on modular PCA approach. Pattern 
Recogn. Lett., 25, 4, 2004. 
[9]  Imran,  S.,  Bajwa,  S.  and  Hyder,  I.  PCA  based  Image 
Classification of Single-layered Cloud Types. Journal of Market 
Forces, 1, 2, 2005. 
[10]  Du,  Q.  and  Fowler,  J.  E.  Low-Complexity  Principal 
Component Analysis for Hyperspectral Image Compression. Int. 
J. High Perform. Comput. Appl., 22, 4, 2008. 
[11] Battista, D., Eades, P., Tamassia, R. and Tollis, I. Algorithms 
for Drawing Graphs: An Annotated Bibliography. Computational 
Geometry, 1994. 
[12] Zigelman, G., Kimmel, R. and Kiryati, N. Texture Mapping 
Using  Surface  Flattening  via  Multidimensional  Scaling.  IEEE 
Transactions  on  Visualization  and  Computer  Graphics,  8,  2, 
2002. 
[13]  Chen,  L.  Local  multidimensional  scaling  for  nonlinear 
dimension  reduction,  graph  layout  and  proximity  analysis. 
ScholarlyCommons@Penn, 2006. 
[14] Kaban, A. A Scalable Generative Topographic Mapping for 
Sparse Data Sequences. 2005. 
[15]  Olier,  I.,  Vellido,  A.  and  Giraldo,  J.  Kernel  generative 
topographic mapping. In Proceedings of the European Symposium 
on Artificial Neural Networks – Computational Intelligence and 
Machine Learning, 2010.  
[16] Cruz-Barbosa, R. and Vellido, A. Unfolding the Manifold in 
Generative  Topographic  Mapping.  In  Proceedings  of  the  3rd 
international workshop on Hybrid Artificial Intelligence Systems 
(Burgos, Spain, 2008). Springer-Verlag.  
[17]  Yi,  J.  S.,  Melton,  R.,  Stasko,  J.  and  Jacko,  J.  A.  Dust  & 
magnet:  multivariate  information  visualization  using  a  magnet 
metaphor. Information Visualization, 4, 4, 2005. 
[18]  House,  L.,  Leman,  S.  C.  and  Han,  C.  Bayesian  Visual 
Analytics  (BaVA).  In  revision,  Technical  Report:  FODAVA-10-
02, http://fodava.gatech.edu/node/342010). 
[19] Leman, S. C., House, L., Maiti, D., Endert, A. and North, C. 
A  Bi-directional  Visualization  Pipeline  that  Enables  Visual  to 
Parametric  Interation  (V2PI). NFS FODAVA Technical Report 
(FODAVA-10-41),  2011.  
[20] Pearson, K. On Lines and Planes of Closest Fit to Systems of 
Points in Space. City, 1901. 
[21]  Jolliffe,  I.  Principal  Component  Analysis.  John  Wiley  and 
Sons, Ltd, 2002. 
[22]  Torokhti,  A.  and  Friedland,  S.  Towards  theory  of  generic 
Principal Component Analysis. 
[23]  Tipping,  M.  E.  and  Bishop,  C.  M.  Probabilistic  Principal 
Component  Analysis.  Journal  of  the  Royal  Statistical  Society, 
SeriesB: Statistical Methodology, 61, 1999. 

",False,2011.0,{},False,False,conferencePaper,False,UQ2U6GFE,[],self.user,False,False,False,False,,"<p>The paper postulates that the process of sensemaking is not focused on a series of parameter adjustements, but instead, a series of perceived connections and patterns within the data.</p>
<p>So they discuss how can models for visual analytics tools be designed, so that users can express their reasoning in observations (the data) directly rather than of on the model parameters.</p>
<p>The main idea is that users are able to perform sensemaking tasks, such as hypothesis validation, directly within the spatial metaphor.</p>
<p>They also define two types of interactions exploratory and expressive.</p>
<p> </p>
<p>Exploratory: Users use the algorithm to explore the data and the space. For example moving one data point and looking how the other data reacts to it.</p>
<p>Expressive: Users are able to tell the model that the criteria used to calculate the parametres must be updated globally.  Dragging two documents together because they are similar.</p>
<p>Visual analytics definition: <strong>"" The science of analytical reasoning facilitated by interactive visual interfaces.""</strong> [1] <strong>The goal of visual analytics (VA) is to extract informaion, perform anayses, and validate hypotheses through an interactive exploration process know as sensemaking </strong>[2]</p>
<p>Incremental Principal Components analysis, iPCA [3], allows the user to change the weight for each dimension in calculating the direction of projection using multiple sliders.</p>
<p>Users are familiar and confortable interacting directly with the data in a spatial visualization, freely organizing and relocating observations as an integral part of their sensemaking process [cite 6]</p>
<p> They are two approaches for the feedback into the algorithm:</p>
<p>Bayesian Visual Analytics</p>
<p>And the determinist version Visual to parametric interaction</p>
<p> </p>
<p><strong>Visual analytics is the science of analytical reasoning facilitated by interactive visual interfaces. [1] The goal of visual analytics is to extract information, perform exploratory analyses and validate hypotheses through an interactive exploration process known as sensemaking [2]</strong></p>
<p>In this paper the authors define the differences between exploratory and expressive interactions.</p>
<p>Exploratory: Move an element and the similar elements to that one move towards it.</p>
<p>Expressive: Allows users to tell the model that the criteria used for calculating the similarity should be adjusted globally.</p>
<p>They mention their method is different from dust and magnet because in dust and magnet the interaction is performed on the attractors not on the observations directly</p>
<p>With this type of systems users are able to perform sensemaking tasks such as hypothesis validation, directly within the spatial metaphor.</p>
<p> </p>",Observation-level interaction with statistical models for visual analytics,UQ2U6GFE,False,False
DCXVS8DD,SGF9UCIG,"Co-located Collaborative Sensemaking on a Large  
High-Resolution Display with Multiple Input Devices 

Katherine Vogt1, Lauren Bradel2, Christopher Andrews2, Chris North2,  

Alex Endert2, and Duke Hutchings1 

1 Department of Computing Sciences 
Elon University, Elon, NC 27244, USA 
{kvogt,dhutchings}@elon.edu 

2 Department of Computer Science 

Virginia Tech, Blacksburg, VA 24060, USA 

{lbradel1,cpa,north,aendert}@cs.vt.edu 

Abstract. This study adapts existing tools (Jigsaw and a text editor) to support 
multiple input devices, which were then used in a co-located collaborative intel-
ligence analysis study conducted on a large, high-resolution display. Exploring 
the sensemaking process and user roles in pairs of analysts, the two-hour study 
used a fictional data set composed of 50 short textual documents that contained 
a  terrorist  plot  and  subject  pairs  who  had  experience  working  together.  The 
large display facilitated the paired sensemaking process, allowing teams to spa-
tially arrange information and conduct individual work as needed. We discuss 
how the space and the tools affected the approach to the analysis, how the teams 
collaborated, and the user roles that developed. Using these findings, we sug-
gest design guidelines for future co-located collaborative tools. 

Keywords:  Visual  analytics,  sensemaking,  co-located,  CSCW,  large  high-
resolution display. 

1   Introduction 

As analysts sort through the growing amounts of data every day, tools that can display 
the information in a useful manner without overwhelming their sensemaking process are 
a beneficial component of their workflow. Visual analytics works to improve analysts’ 
experience in their work and productivity. As such, exploring what collaborative visual 
analytics may contribute to this challenge has become a key area of research within vis-
ual  analytics  [1].  Through  the  support  of  collaboration  within  the  analytic  process,  
designers can improve the effectiveness though leveraging various social and group dy-
namics [2]. Various design guidelines and structured collaborative techniques exist [3], 
but, as the culture of intelligence analyst working within an agency can be described as 
“competitive”, where sharing of knowledge may adversely affect their job security, col-
laboration occurs at a much less formal level, if at all [4]. The study presented here does 
not  present  a  formal  collaborative  method,  but  places  the  users  in  a  setting  where  
information and knowledge is inherently shared through the use of a shared workspace.  

P. Campos et al. (Eds.): INTERACT 2011, Part II, LNCS 6947, pp. 589–604, 2011. 
© IFIP International Federation for Information Processing 2011 

590 

K. Vogt et al. 

Fig. 1. Study setup, two users with their own input devices in front of the large display 

 

Large, high-resolution workspaces (such as the one shown in Fig. 1) are beneficial 
to intelligence analysis in that they allow for spatial information organization to act as 
an external representation or memory aid [5]. This advantage was shown to help indi-
vidual intelligence analysts in their task, in that they were able to spatially organize 
and  reference  information.  This  work  explores  how  such  a  workspace,  allowing  for 
these spatial strategies, can impact the strategy and workflow of a team (of 2) users 
working  collaboratively  on  an  intelligence  analysis  task.  In  this  environment,  we  
provide  users  with  a  social  setting  in  which  to  perform  their  analysis,  and  a  shared 
representation in which to organize their thoughts. We analyze their process in terms 
of their activities and roles exemplified during their task, their use of space, and level 
of collaboration. 

In  such co-located  settings (versus remote  settings), it  has been  shown  that  teams 
experience a greater quality of communication because of subtle physical interaction 
cues and a stronger trust that develops with the shared experience [6]. Also, given that 
analysts often work with large collections of electronic documents, it is worthwhile to 
explore how the design of tools on large, high-resolution displays could facilitate col-
laboration  during  analysis.  Further,  if  this  environment  supports  collaborative  work, 
then the ability to make sense of documents develops great potential. To investigate the 
collaborative use of a large, high-resolution display environment, we have completed 
an  exploratory  study  of  two  visual  analytic  tools:  Jigsaw  [7],  and  a  simple  multi-
window text editor. The study we present involves synchronous, co-located collabora-
tive  sensemaking.  Here,  we  define  co-located  work  as  multiple  users  working  each 
with his or her own input devices (mouse and keyboard) on the same computer display. 

2   Related Work 

Design tensions exist in collaborative tools between “individual control of the appli-
cation,  and  support  for  workspace  awareness”  [8].  Some  previous  groupware  tools 
have had difficulty achieving a balance between these extremes, either supporting the 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

591 

group through consistent view sharing (“What You See Is What I See” – WYSIWIS) 
or the individual through relaxed view sharing [9]. However, Gutwin and Greenberg 
feel that a solution to this tension exists, stating that “the ideal solution would be to 
support both needs – show everyone the same objects as in WYSIWIS systems, but 
also  let  people  move  freely  around  the  workspace,  as  in  relaxed-WYSIWIS  group-
ware” [8]. Single display groupware provides an interface to achieve this balance. 

Single Display Groupware (SDG) concerns face-to-face collaboration around a single 
shared  display  [10].  Early  SDG  systems  include  Liveboard  [11],  Tivoli  [12],  and  the 
Digital Whiteboard [13]. When compared to co-located multi-display groupware, SDG 
resulted in increased collaborative awareness [14]. Stewart et al. continued to investigate 
SDG systems  in subsequent work ([15, 16]). They proposed that  the multi-user nature 
of SDG systems on early displays with limited screen size “may result in reduced func-
tionality compared with similar single-user programs” [16], although this concern can be 
alleviated by increasing the physical size (and resolution) of the SDG display. 

SDG systems using multiple input devices have been found to increase interaction 
between  participants  and  keep  participants  “in  the  zone”  [15].  Providing  a  separate 
mouse and keyboard to each participant has been shown to allow users to complete 
more  work  in  parallel  than  if  they  were  restricted  to  a  single  mouse  and  keyboard 
[17]. Multiple input devices provide the benefit of allowing reticent users to contrib-
ute to the task [18, 19]. As a result of our desire to keep participants in the “cognitive 
zone” [20], given the cognitively demanding nature of sensemaking tasks, we chose 
to implement multiple input devices for our set-up. 

The sensemaking process has been illustrated by Pirolli and Card (Fig. 2) to outline 
the cognitive process of “making sense” of documents throughout their investigation 
in  order  to  produce  a  cohesive  and  coherent  story  of  interwoven  information  found 
across document sources [21]. This process can be broken down into two broad cate-
gories: foraging and sensemaking. The foraging loop involves extracting and filtering 
relevant  information.  The  sensemaking  loop  represents  the  mental  portion  of  sense-
making where a schema, hypothesis, and presentation are iteratively developed. The 
analyst is not restricted to a single entry point to this loop, and instead can enter at the 
top or bottom before looping through the various steps [21]. The sensemaking process 
has been studied and observed on large, high-resolution displays as well as multiple 
monitor set-ups for individual users [5, 7, 22].  

Paul and Reddy observed, through an ethnographic study concerning collaborative 
sensemaking of healthcare information, that collaborative sensemaking should focus 
on the following factors: prioritizing relevant information, the trajectories of the sen-
semaking activity, and activity awareness [23]. We believe that the large display used 
in our study provides users with the opportunity for this awareness and prioritization. 
Collaborative sensemaking has also been studied in terms of web searches [24, 25], as 
well  as  remote  collaborative  sensemaking  for  intelligence  analysis  [26].  Furthermore, 
collaborative sensemaking has been observed in co-located tabletop settings [27-29], al-
though, to the best of our knowledge, co-located collaborative sensemaking applied to  
intelligence analysis has not been investigated on large, high-resolution vertical displays. 

 
 

592 

K. Vogt et al. 

Fig. 2. Adapted from sensemaking loop, Pirolli and Card [21] 

 

User performance on simple tasks, such as pattern matching, has been shown to im-
prove when using a large, high-resolution vertical display when contrasted with a stan-
dard single monitor display [30]. In addition to quantitative improvement, users were ob-
served  using  more  physical  navigation  (e.g.  glancing,  head/body  turning)  than  virtual  
navigation  (e.g.  manually  switching  windows  or  tasks,  minimizing/maximizing  docu-
ments) when using large, high-resolution displays, such as the one shown in Fig. 1. 

Andrews  et  al.  expanded  the  benefits  of  using  large,  high-resolution  display  to 
cognitively demanding tasks (i.e., sensemaking) [5]. We chose to use these displays to 
explore collaborative sensemaking on large vertical displays, especially the user roles 
that develop throughout the sensemaking process and how the sensemaking process is 
tackled by teams of two. 

3   Study Design 

We  have  conducted  an  exploratory  study  examining  the  collaborative  sensemaking 
process on a large, high-resolution display. Teams of two were asked to assume the 
role of intelligence analysts tasked  with analyzing a collection of text documents to 
uncover a hidden plot against the United States. The teams were provided with one of 
two tools, Jigsaw or a multi-document text editor, with which they were asked to con-
duct  their  analysis.  While  each  team  was  told  that  they  were  expected  to  work  col-
laboratively, the nature of that collaboration was left entirely up to the participants.  

3.1   Participants 

We recruited eight pairs of participants (J1-J4 used Jigsaw, T1-T4 used the text editor). 
All pairs knew one another and had experience working together prior to the study. Six 
of the eight pairs were students and the other two pairs consisted of research associates 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

593 

and faculty. There were four all male groups, one all female, and three mixed gender. 
Each participant was compensated $15 for participation. As a form of motivation, the so-
lutions generated by the pairs of participants were scored and the participants received an 
additional financial award for the four highest scores. The rubric for evaluating the par-
ticipants’  verbal  and  written  solutions  was  based  on  the  strategy  for  scoring  Visual  
Analytics  Science  and  Technology  (VAST)  challenges  [22].  The  participants  earned 
positive points for the people, events, and locations related to the solution and negative 
points for those that were irrelevant or incorrect. They also received points based on the 
accuracy of their overall prediction of an attack.  

3.2   Apparatus 

Each pair of users sat in front of a large display consisting of a 4x2 grid of 30” LCD 
2560x1600  pixel  monitors  totaling  10,240x3,200  pixels  or  32  megapixels  (Fig.  1). 
The display was slightly curved around the users, letting them view the majority, if 
not all, of the display in their peripheral vision. A single machine running Fedora 8 
drove the display. A multi-cursor window manager based on modified versions of the 
IceWM and x2x was used to support two independent mice and keyboards [31]. Thus, 
each user was able to type and use the mouse independently and simultaneously in the 
shared workspace. This multi-input technology allowed two windows to be “active” 
at  the  same  time,  allowing  participants  to  conduct  separate  investigations  if  they 
chose. A whiteboard, markers, paper, and pens were also available for use. These ex-
ternal artifacts were provided as a result of a pilot study where participants explicitly 
requested to use the whiteboard or write on sheets of paper. Each participant was pro-
vided  with  a  rolling  chair  and  free-standing,  rolling  table  top  holding  the  keyboard 
and  mouse  so  that  they  could  move  around  if  they  chose  to  do  so.  The  desks  and 
chairs were initially positioned side-by-side in the central area of the screen space. 

3.3   Analytic Environment 

During this exploratory study, four of the pairs (J1-J4) examined the documents within 
Jigsaw, a recent visual analytics tool, while the other four (T1-T4) used a basic text edi-
tor, AbiWord [32], as a contrasting tool. We chose to investigate these two tools due to 
the different analytical approaches the tools inherently foster. Jigsaw supports a function-
based  approach  to  analysis,  allowing  the  tool  to  highlight  connections  between  docu-
ments and entities. The Text Editor instead forces the participants to read each document 
first, and then draw connections themselves without any analytical aid. We do not intend 
for these two tools to be representative of all visual analytics tools. Instead, we sought to 
explore  co-located  collaborative  sensemaking  in  two  different  environments.  This  text 
editor allows the user to highlight individual document sections and edit existing docu-
ments or create text notes. Teams using this text editor were also provided with a file 
browser in which they could search for keywords across the document collection. Jigsaw 
[7, 33] is a system that has been designed to support analysts; it visualizes document col-
lections  in  multiple  views  based  on  the  entities  (people,  organizations,  locations,  etc.) 
within those documents. It also allows textual search queries of the documents and enti-
ties. The views are linked by default so that exploring an entity in one visualization will 
simultaneously  expand  it  in  another. This  feature  is  controlled  by  the  user  and  can  be 
turned on or off within each view. We were not able to change Jigsaw’s source code to 

594 

K. Vogt et al. 

allow  windows  to  be  linked  separately  for each participant, therefore  all Jigsaw  views 
were connected unless the linking feature was disabled by the participant teams. Jigsaw 
can sort documents based on entity frequency, type, and relations. This information can 
be displayed in many different ways, including interactive graphs, lists, word clouds, and 
timelines. Jigsaw also comes  equipped  with a recently added Tablet view  where users 
can  record  notes,  label  connections  made  between  entities,  identify  aliases,  and  create 
timelines. As a result of the complexity of the visualizations available in Jigsaw, pairs  
using this visual analytics tool were given a thirty minute tutorial prior to the start of the 
scenario, while pairs using the text editor only required a five minute tutorial. 

3.4   Task and Procedure 

After a tutorial on Jigsaw or the text editor with a sample set of documents, each pair was 
given two hours to analyze a set of 50 text documents and use the information gathered 
to predict a future event. This scenario comes from an exercise developed to train intelli-
gence analysts and consists of a number of synthetic intelligence reports concerning vari-
ous incidents around the United States, some of which can be connected to gain insight 
into  a  potential  terrorist  attack.  This  same  scenario  was  also  used  in  a  previous  study 
evaluating  individual  analysts  with  Jigsaw  [33].  Following  the  completion  of  the  sce-
nario, each  participant  filled out  a  report  sheet  to  quantitatively  assess  their  individual 
understanding of the analysis scenario, then verbally reported their final solution together 
to  the  observers.  Finally,  individual  semi-structured  interviews  were  conducted  where 
each participant commented on how they solved the scenario, how this involved collabo-
ration, and their sense of territoriality.  

3.5   Data Collection 

During each scenario, an observer was always present taking notes. Video and audio 
of every scenario, debriefing, and interview was recorded. The video was coded using 
PeCoTo  [34].  We  also  collected  screenshots  in  fifteen  second  intervals  and  logged 
mouse actions and active windows. The screenshots played two roles in our analysis. 
Their primary role was to allow us to “play back” the process of the analysis so that 
we could observe window movements and the use of the space. Furthermore, we ap-
plied the previously described point system in order to evaluate the accuracy of their 
debriefing, providing a way to quantitatively measure their performance. The scores 
can  be  seen  below  in  Table  1.  There  was  no  significant  difference  between  overall 
performance between the Jigsaw and Text Editor tool conditions when evaluated with 
a t-test, although statistical significance is difficult to show with small sample sizes. 

Table 1. Overall team scores grouped by tool used comparing aggregated performance 

Jigsaw 

J1 
J2 
J3 
J4 

11 
-1 
-2 
-7 

Text Editor 
T1 
13 
-1 
T2 
10 
T3 
T4 
14 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

595 

4   Analysis 

4.1   User Activities 

Each group exhibited a variety of activities depending on their amount of progress to 
achieve a satisfactory solution. After analyzing the video, interviews, and solution re-
ports, we have concluded that five major activities that were used by the participants, 
which together formed a strategy for analyzing the data. These were not usually ex-
plicitly  identified  by  the  participants,  but  rather  tasks  that  the  participants  naturally 
took  on  in  order  to  uncover  the  underlying  terrorist  plot.  The  five  activities  are  ex-
tract, cluster, record, connect, and review and will be described in greater detail be-
low.  Although  each  group  exhibited  the  execution  of  each  activity  (one  exception  
being cluster which we will discuss later), the groups used different methods to im-
plement that activity that were often based on the interface condition (Jigsaw or text 
editor) of the group [Table 2]. 

Extract. The groups had no starting point or lead to begin with - just fifty text docu-
ments and the knowledge that there was a terrorist threat to the nation. Therefore, they 
needed to familiarize themselves with the information presented within the documents 
and then extract that  which  seemed important. In Jigsaw, the visualizations allowed 
for participants to begin this process by looking at frequently occurring entities and 
the other entities and documents to which they connected. With the text editor, these 
features  were  not  available  therefore  the  participants  were  forced  to  open  and  read 
each  document.  They  then  all  used  color-coded  highlighting  to  distinguish  entities 
and/or important phrases. The coloring scheme was decided upon by the participants, 
whereas Jigsaw maintains a set color scheme for entities. In the text editor groups, the 
subjects opened documents in consistent locations to read them, but soon moved the 
opened  documents  into  meaningful  clusters  (see  next  activity).  The  Extract  activity 
required little display space to complete in either study condition. This activity  was 
done together in some groups with both participants simultaneously reading the same 
document  and  in  parallel  in  others  with  each  participant  reading  half  of  the  docu-
ments, often split by document number. 

Table 2. Five sensemaking activities and their methods for corresponding tool 

Activity 
Extract 

Cluster 

Record 

Connect 

Review 

Method 
Tool 
Jigsaw 
Look over frequently occurring entities and related documents 
Text Editor  Read all of the documents, together or separately, and highlight 
Jigsaw 

(did not occur with this tool as Jigsaw automatically color codes 
and groups the entities by type) 

Text Editor  Group document windows by related content 
Jigsaw 
Text Editor  Whiteboard, paper 
Jigsaw 

Tablet (3 groups), whiteboard & paper (1 group) 

Loop  between 
per/whiteboard), together or separately 

list,  document  view,  and  Tablet  (or  pa-

Text Editor  Search function; reread paper, whiteboard, and documents 
Jigsaw 
Text Editor  Reread, possibly close windows after reviewing 

Reread, search for unviewed documents (2 groups) 

 

596 

K. Vogt et al. 

Cluster.  With the text editor, all of the groups found a need to cluster and organize the 
documents. The groups clustered by grouping the document windows by content in the 
space (they resembled piles), using whitespace between clusters as boundaries. The clus-
ters eventually filled the display space, allowing the participants to view all documents at 
once  in  a  meaningful  configuration.  Even  when  only  one  partner  organized  the  docu-
ments into clusters, the other partner could easily find documents relevant to a certain 
topic due to their agreed upon clustering scheme (e.g. chronological order, geographical 
as shown in Fig. 3). Most text editor groups used the multi-mouse functionality to simul-
taneously  organize  the  display  space.  Three  of  the  four  groups  eventually  re-clustered 
their documents after some analysis. The cluster activity as defined above (spatially ar-
ranging document windows) was not present in any of the Jigsaw groups, because Jigsaw 
organizes  the  entities  and  documents  through  its  various  functionalities.  Many  Jigsaw 
groups,  however,  clustered  relevant  entities  within  their  Tablet  views,  giving  spatial 
meaning to the information recorded. 

 
Fig. 3. Geographical clustering of documents on the large display screen, done by group T4 (T4-B, 
the forager, arranged the space while T4-A, the sensemaker, instructed document placement) 

Record. Recording important information proved to be a useful strategy for all groups. 
Through interviews the participants revealed that this not only served as a memory aid, 
but also a way to see what events, dates, people, and organizations related. In the scenar-
ios with the text editor, with two of the groups using the whiteboard and three using scrap 
paper (one used both), all groups found a need to use an external space to record impor-
tant information regardless of how much of the display was filled by clusters. This al-
lowed them to preserve the cluster set-up and keep the documents persistent. Three of the 
Jigsaw  groups  used  the  Tablet  view  to  take  notes  and  one  group  used  paper  and  the 
whiteboard. Thus all participants devoted a separate space to keep track of pertinent in-
formation. Groups also recorded important information verbally to alert their partner to a 
potential lead, allowing their partner to create a mental record. 
Connect. In order to make connections and look for an overall plot, the Jigsaw partici-
pants would often loop through the list view, document view, and the Tablet, connecting 
the information they discovered. Two groups worked on this separately and two did this 
together. With the text editor, participants searched for entities and reread their notes. In 
comparison to their discourse during the other activities, the groups were more talkative 
when making connections. Text editor group T1 cleared a screen to use as a workspace 
for  their  current  hypotheses.  They  opened  relevant  documents  in  their  workspace  and 
closed irrelevant documents or documents from which they had extracted all information.  
 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

597 

In all text editor cases, the meaning conveyed by clustered documents on the display was 
helpful in drawing connections. 
Review. This appeared to be a very important element in the groups’ analyses. Often 
when  one  or  both  partners  reread  a  document  for  the  second,  third,  or  even  fourth 
time, it took on a new meaning to them after they understood the greater context of 
the scenario. This element of review could also help the participants as they worked to 
Connect. Two of the Jigsaw groups chose to search for unviewed documents to ensure 
that they had encountered all potentially important information. Two of the text editor 
groups  began  closing  windows  after  they  had  reread  them.  Sometimes  this  was  be-
cause the document  was considered irrelevant. For example, group T3 moved  unre-
lated documents to what they called the “trash window”. They later reread all of the 
trash window documents and closed those which still seemed irrelevant. The Review 
activity also included discussing current and alternative hypotheses. 

While the activities listed in the table can be loosely defined in this sequential or-
der, the order is certainly not set nor were they visited only once within each scenario. 
Rather, there was often rapid but natural movement between these activities and their 
methods depending on the current needs of the analysis. In particular, the middle three 
activities were present many times throughout the study. Extract was only necessary 
during  the  first  part  of  each  scenario  and  review  was  usually  only  seen  after  a  
significant portion of the first activity had been completed. 

4.2   Comparison between Sensemaking Loop and Activities 

The processes we observed closely reflect the Pirolli and Card [21] sensemaking model 
(Fig. 2)   which  was developed for individual  analysts. We  have  found  that it  may  also 
generally be applied to collaborative pairs, although the loop is utilized differently because 
of the roles that developed. Extract and cluster relate to steps two through seven. The Evi-
dence  File  and  Schema  steps  were  combined  by  the  pairs  due  to  the  available  display 
space. They were able to sort evidence into a meaningful schema by placing documents in 
different areas of the display. Record is very similar to schematizing and connect is a part 
of developing hypotheses. Review does not directly map to one stage of the sensemaking 
loop,  but  rather  it  is  the  equivalent  of  moving  back  down  the  loop,  analyzing  previous 
work, and returning to the shoebox and evidence file. Note that External Data Sources is 
not mentioned here because the participants were only presented with fifty documents so 
we are assuming that prior analysis has moved through this step. The cumulative Presen-
tation directly links to the debriefing following the scenario.  

Fig. 4. Screenshot of one of the scenarios, group J2, using Jigsaw, illustrating one way in which 
the users partitioned the display to conduct individual investigations 

 

598 

K. Vogt et al. 

While the activities described above and the sensemaking loop hold parallel ideas, 
we do want to distinguish the two concepts. The overall strategy we propose has been 
condensed to  five activities as a result of the collaboration and  space.  Additionally, 
we have given the idea of review new emphasis. This is a very important element in 
the sensemaking process, but is not explicitly identified in the sensemaking loop. 

All  of  the  activities,  excluding  Cluster,  were  present  in  both  scenarios.  This  is  
notable considering the vast differences of the scenarios based on tool type. Since the 
activities  we  observed  correspond  to  the  Pirolli  and  Card  sensemaking  model  [21], 
with the primary difference in user behavior being the tool-specific methods adopted to  
fulfill those activities, we propose that these activities are very likely to be universal. 

4.3   Collaboration Levels 

The amount of time spent working closely together appears to have impacted the scores. 
We applied the video coding code set from Isenberg et al. [29] to determine how much 
time was spent closely coupled (collaborating together) versus loosely coupled (working 
individually). Closely coupled is defined by Isenberg et al. active discussion, viewing the 
same document, or working on the same specific problem [29]. Loosely coupled is de-
fined as working on the same general problem, different problems, or being disengaged 
from the task. Upon graphing this data (Fig. 5), two clusters appear separating the high-
scoring groups from the low-scoring ones. The high scoring cluster worked closely over 
89% of the time spent on the scenario. The low scoring cluster only worked closely in be-
tween 42% and 67% of the time. All but one group at least collaborated closely during 
the remaining half hour of the scenario in order to synthesize their hypotheses. The corre-
lation  coefficient  between  the  amount  of  time  spent  collaborating  closely  and  score  is 
.96105, suggesting that there is a strong correlation between these variables. This rein-
forces the result from [29] that strongly links collaboration levels with performance.  

Fig. 5. Jigsaw (dark blue) and Text Editor (light green) scores versus collaboration levels 

 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

599 

4.4   User Roles 

All  groups  divided  the  responsibilities  of  the  collaborative  sensemaking  task.  The 
roles  could  be  observed  during  the  study  because  of  actions  and  conversation,  but 
they  were  also  evident  during  the  interviews  following  the  study.  Five  of  the  eight 
groups established clearly defined collaborative roles (measured through  video cod-
ing). This appeared to be because the three groups were going through the steps of the 
analysis  independently,  but  in  parallel.  Therefore  various  team-related  roles  and  
responsibilities in the analysis were less likely to develop. 

For the five groups who established clearly defined roles, the two broad roles we 
identified  through  this  analysis  are  sensemaker  and  forager.  These  high-level  roles 
were primarily established after a considerable amount of the investigation had been 
completed, normally after the half-way point of the study session. Primarily, the sen-
semaker  tended  to  be  the  dominant  partner,  often  dictating  what  the  forager  did. 
Common activities for the sensemaker included standing, writing on the whiteboard, 
using  a  hand  to  point  to  information  (instead  of  using  a  cursor),  and  rarely  using  a 
mouse, instead requesting the forager to perform various activities. The forager’s role 
consisted of questioning the current hypotheses, finding information, and maintaining 
a  better  awareness  of  where  the  information  was  located.  For  example,  the  sense-
maker would request actions such as “can you open [a particular document]?” and the 
forager would perform the action.  

These two roles closely match the two primary sub-loops (Fig. 2) in the Pirolli and 
Card model [21]. The first loop, foraging, involves sorting through data to distinguish 
what is relevant from the rest of the information. The second loop, sensemaking, in-
volves utilizing the information pulled aside during the foraging process to schema-
tize and form a hypothesis during the analysis. Thus, the sensemaker was more con-
cerned with the synthesizing of the information, while the forager was more involved 
in the gathering, verifying, and organizing of the information. While the sensemaker 
and forager each spent the majority of their time at their respective ends of the loop, 
they did not isolate themselves from the rest of the sensemaking process. 

To illustrate the distribution of responsibilities prompted by the roles adopted, we 
will  explain  in  detail  two  of  the  pairs  where  the  participants  formed  distinct  roles. 
These  are  the  two  groups  in  which  the  roles  are  most  clearly  defined,  and  are  
therefore the most interesting to talk about.  

In  group  T1,  the  team  with  the  second-highest  score,  both  participants  spent  the 
first  hour  foraging  (i.e.,  exposing,  clustering)  for  information  while  taking  a  few 
breaks  to  engage  in  sensemaking  activities  (i.e.,  connecting).  Participant  T1-A  (the 
subject who sat on the left) at times led T1-B’s (the participant who sat on the right) 
actions by initializing activities or finalizing decisions.  At the 68-minute  mark, par-
ticipant T1-B moved to the whiteboard (never to return to the computer input devices) 
and established a clear, dominant role as sensemaker while T1-A continued to forage 
for information. Specifically, T1-A organized the documents, searched, and provided  
 

600 

K. Vogt et al. 

dates, locations, relevant events, etc., but T1-B drew a picture connecting the relevant 
events working to form a hypothesis and requested information from T1-A. T1-B be-
gan focusing on Record and Connect, but they both engaged in the Review activity to-
gether.  The  Review  activity  was  interspersed  throughout  the  scenario  as  pieces  of  
information inspired participants to revisit a document. During the interviews, T1-B 
revealed that he wanted to build a chart or timeline to organize their thoughts better. 
Although  interviewed  separately,  they  seemed  to  have  similar  views  on  their  roles. 
T1-B stated, “I basically just tried to stand up there and construct everything while he 
finds evidence,” while T1-A said, “I was just trying to feed him the data, that was my 
skill, find it, and he can put it in a flow chart.” 

The other pair is group T4, the group with the highest score, where T4-A was the sen-
semaker and T4-B the forager. Again, the sensemaker is the participant (T4-A) who built 
a picture on the whiteboard, meaning he drove Record and Connect. In fact, T4-A barely 
touched his mouse after the first fifteen minutes of the scenario. He only had 104 mouse 
clicks while T4-B had 1374. They worked through Extract and Cluster together, but T4-
A verbally dictated the clustering while T4-B controlled it with the mouse. While T4-A 
worked on the whiteboard, T4-B fed him details as needed. As T4-A stated, “We ended 
up splitting the tasks into organization and story-building… I would say I built most of 
the story.” Both participants worked on the Review activity, but during this T4-B ques-
tioned T4-A’s  hypotheses  which  forced  him  to  justify  and  support  his  thoughts.    This 
lopsided mouse usage is not a new method of interaction [35], however, it is interesting 
that T4-A abandoned his mouse in favor of instructing his partner. 

5   Design Implications 

Viewing  all  documents  simultaneously  appeared  to  be  an  effective  strategy,  
given the added space provided by the large display. All 50 documents comfortably 
fit into user-defined clusters. No Jigsaw groups chose this approach, instead relying 
on the specialized views available. Visual analytics tools designed for large displays 
should  take  this  into  consideration  by  allowing  users  to  open  many  documents  and 
flexibly rearrange the clusters as needed. This may not be feasible after the document 
collection becomes large enough, in which case a tool such as Jigsaw would be valu-
able  in  narrowing  down  the  document  collection.  We  recommend  that  developers 
combine  these  two  analysis  approaches  to  perform  well  on  all  document  collection 
sizes. 

Because the highest scoring groups had clearly defined user roles while the lowest 
scoring groups did not, we recommend that co-located collaborative visual analytics 
tools  support  the  division  of  responsibilities.  One  way  to  achieve  this  would  be  to  
implement specialized views for foragers and sensemakers. 

Some sensemakers stood and used a physical whiteboard to record their thoughts. 
All text editor groups used the whiteboard or paper to record their thoughts. One Jig-
saw  group used the  whiteboard while the rest used Jigsaw’s Tablet view. From this 
we can see a clear need for tools that integrate evidence marshaling and sensemaking  
 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

601 

into the analytic process. The Tablet view in Jigsaw and other integrated sensemaking  
environments such as the Sandbox in the nSpace suite [36] are one approach. Another 
approach, suggested by  the studies conducted by Robinson [18] and  Andrews et al. 
[5] as well as our observations of the text editor group would be to integrate sense-
making tools right into the document space. As we observed in this study, the users of 
the  text  editor  already  were  arranging  documents  into  structures  based  on  their  
content.  A  logical  continuation  of  this  would  be  to  integrate  sensemaking  tools  and 
representations into this space directly, so that the sensemaking is done directly with 
the  documents,  allowing  the  user  to  maintain  the  context  of  the  original  source  
material.  

We have also considered some frustrations expressed by the users while develop-
ing design implications. One issue involved the presence of the taskbar on only one of 
the  eight  monitors,  an  issue  recognized  in  the  past  (for  example  GroupBar  [37]).  It 
became difficult and inconvenient for the users to locate windows in the taskbar, es-
pecially  with over fifty  windows opened  simultaneously.  For future  visual analytics 
tools,  we  recommend  implementing  a  feature  that  allows  easier  location  of  docu-
ments. This could be done through a better search feature, such as flashing document 
windows to make locating them easier.  

6   Conclusion 

We have conducted a study which explores an arrangement for co-located collabora-
tive sensemaking and applied it to intelligence analysis, an application that, to the best 
of our knowledge, has not yet been seen for this specific set-up and application. We 
extracted five common activities which the participants used in their overall strategy 
during collaborative sensemaking. While the activities were common with all groups, 
the execution of the activities varied based on the tool (Jigsaw or text editor). These 
activities reflected many of the steps in the Pirolli and Card sensemaking loop [21]. 
The participants also  moved through the loop by using the roles of sensemaker and 
forager  so  that  the  two  major  areas  of  sensemaking  could  be  performed  synchro-
nously. The groups that adopted these roles tended to score higher. Taking all of these 
findings  into  account,  we  have  developed  design  implications  for  systems  that  use 
multiple input devices collaboratively on a large, vertical display. 

The application of co-located collaboration to other visual analytics tools should be 
further investigated in order to develop a more accurate set of guidelines for designing 
co-located collaborative systems on large displays. We are also interested in studying 
the impacts of spatially arranged data on co-located collaborative analysis.  
 
Acknowledgments.  This  research  was  supported  by  National  Science  Foundation 
grants NSF-IIS-0851774 and NSF-CCF-0937133. 

602 

K. Vogt et al. 

References 

1.  Thomas, J., Cook, K.: Illuminating the Path: The Research and Development Agenda for 

Visual Analytics (2005) 

2.  Heer,  J.:  Design  considerations  for  collaborative  visual  analytics.  Information Visualiza-

tion 7, 49–62 (2008) 

3.  Heuer, R.J., Pherson, R.H.: Structured Analytic Techniques for Intelligence Analysis. CQ 

Press, Washington, DC (2010) 

4.  Chin, G.: Exploring the analytical processes of intelligence analysts, p. 11 (2009) 
5.  Andrews, C., Endert, A., North, C.: Space to think: large high-resolution displays for sen-
semaking.  In:  Proceedings  of  the  28th  International  Conference  on  Human  Factors  in 
Computing Systems. ACM, Atlanta (2010) 

6.  Waltz, E.: The Knowledge-Based Intelligence Organization. In: Knowledge Management 

in the Intelligence Enterprise. Artech House, Boston (2003) 

7.  Stasko, J.: Jigsaw: supporting investigative analysis through interactive  visualization. In-

formation Visualization 7, 118–132 (2008) 

8.  Gutwin,  C.,  Greenberg,  S.:  Design  for  individuals,  design  for  groups:  tradeoffs  between 
power and workspace awareness. In: Proceedings of the 1998 ACM Conference on Com-
puter Supported Cooperative Work. ACM, Seattle (1998) 

9.  Stefik,  M.,  Foster,  G.,  Bobrow,  D.G.,  Kahn,  K.,  Lanning,  S.,  Suchman,  L.:  Beyond  the 
chalkboard: computer support for collaboration and problem solving in meetings. Commu-
nications of the ACM 30, 32–47 (1987) 

10.  Stewart, J.E.: Single display groupware. In: CHI 1997 Extended Abstracts On Human Fac-

tors In Computing Systems: Looking to The Future, pp. 71–72. ACM, Atlanta (1997) 

11.  Elrod, S., Bruce, R., Gold, R., Goldberg, D., Halasz, F., Janssen, W., Lee, D., McCall, K., 
Pederson, E., Pier, K., Tang, J., Welch, B.: Liveboard: a large interactive display support-
ing  group  meetings,  presentations,  and  remote  collaboration.  In:  Proceedings  of  the 
SIGCHI Conference on Human Factors in Computing Systems, pp. 599–607. ACM, Mon-
terey (1992) 

12.  Pedersen, E.R., McCall, K., Moran, T.P., Halasz, F.G.: Tivoli: an electronic whiteboard for 
informal  workgroup  meetings.  In:  Proceedings  of  the  INTERACT  1993  and  CHI  1993 
Conference  on  Human  Factors  in  Computing  Systems,  pp.  391–398.  ACM,  Amsterdam 
(1993) 

13.  Rekimoto, J.: A multiple device approach for supporting whiteboard-based interactions. In: 
Proceedings  of  the  SIGCHI  Conference  on  Human  Factors  in  Computing  Systems,  pp. 
344–351. ACM Press/Addison-Wesley Publishing Co., Los Angeles, California (1998) 

14.  Wallace, J., Scott, S., Stutz, T., Enns, T., Inkpen, K.: Investigating teamwork and taskwork 
in single- and multi-display groupware systems. Personal and Ubiquitous Computing 13, 
569–581 (2009) 

15.  Stewart, J., Raybourn, E.M., Bederson, B., Druin, A.: When two hands are better than one: 
enhancing collaboration using single display groupware. In: CHI 1998 Conference Sum-
mary on Human Factors in Computing Systems, pp. 287–288. ACM, Los Angeles (1998) 

16.  Stewart, J., Bederson, B.B., Druin, A.: Single display groupware: a model for co-present 
collaboration. In: Proceedings of the SIGCHI Conference on Human Factors in Computing 
Systems: the CHI is the Limit, pp. 286–293. ACM, Pittsburgh (1999) 

 
 

 

Co-located Collaborative Sensemaking on a Large High-Resolution Display 

603 

17.  Birnholtz,  J.P.,  Grossman,  T.,  Mak,  C.,  Balakrishnan,  R.:  An  exploratory  study  of  input 
configuration and group process in a negotiation task using a large display. In: Proceedings 
of  the  SIGCHI  Conference  on  Human  Factors  in  Computing  Systems.  ACM,  San  Jose 
(2007) 

18.  Robinson, A.: Collaborative Synthesis of Visual Analytic Results. IEEE Visual Analytics 

Science and Technology, 67–74 (2008) 

19.  Rogers, Y.,  Lim, Y.-k., Hazlewood, W.R., Marshall, P.: Equal Opportunities: Do Share-
able  Interfaces  Promote  More  Group  Participation  Than  Single  User  Displays?  Human–
Computer Interaction 24, 79–116 (2009) 

20.  Green, T.M., Ribarsky, W., Fisher, B.: Building and applying a human cognition model for 

visual analytics. Information Visualization 8, 1–13 (2009) 

21.  Pirolli, P., Card, S.: The Sensemaking Process and Leverage Points for Analyst Technol-
ogy as Identified Through Cognitive Task Analysis. In: International Conference on Intel-
ligence Analysis (2005) 

22.  Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., O’Connell, T., Laskowski, S., Chien, 
L., Tat, A., Wright, W., Gorg, C., Liu, Z., Parekh, N., Singhal, K., Stasko, J.: Evaluating 
Visual Analytics at the 2007 VAST Symposium Contest. In: Computer Graphics and Ap-
plications, vol. 28, pp. 12–21. IEEE, Los Alamitos (2008) 

23.  Paul, S.A., Reddy, M.C.: Understanding together: sensemaking in collaborative informa-
tion seeking. In: Proceedings of the 2010 ACM Conference on Computer Supported Coop-
erative Work, pp. 321–330. ACM, Savannah (2010) 

24.  Paul, S.A., Morris, M.R.: CoSense: enhancing sensemaking for collaborative web search. 
In: Proceedings of the 27th International Conference on Human Factors in Computing Sys-
tems, pp. 1771–1780. ACM, Boston (2009) 

25.  Morris, M.R., Lombardo, J., Wigdor, D.: WeSearch: supporting collaborative search and 
sensemaking  on  a  tabletop  display.  In:  Proceedings  of  the  2010  ACM  Conference  on 
Computer Supported Cooperative Work, pp. 401–410. ACM, Savannah (2010) 

26.  Pioch, N.J., Everett, J.O.: POLESTAR: collaborative knowledge management and sense-
making tools for intelligence analysts. In: Proceedings of the 15th ACM International Con-
ference  on  Information  and  Knowledge  Management,  pp.  513–521.  ACM,  Arlington 
(2006) 

27.  Tobiasz,  M.,  Isenberg,  P.,  Carpendale,  S.:  Lark:  Coordinating  Co-located  Collaboration 
with Information Visualization. IEEE Transactions on Visualization and Computer Graph-
ics 15, 1065–1072 (2009) 

28.  Isenberg, P., Fisher, D.: Collaborative Brushing and Linking for Co-located Visual Ana-

lytics of Document Collections. Computer Graphics Forum 28, 1031–1038 (2009) 

29.  Isenberg, P., Fisher, D., Morris, M.R., Inkpen, K., Czerwinski, M.: An exploratory study 
of co-located collaborative visual analytics around a tabletop display. In: 2010 IEEE Sym-
posium on Visual Analytics Science and Technology, VAST, pp. 179–186 (2010) 

30.  Ball,  R.,  North,  C.,  Bowman,  D.A.:  Move  to  improve:  promoting  physical  navigation  to 
increase user performance with large displays. In: Proceedings of the SIGCHI Conference 
on Human Factors in Computing Systems, pp. 191–200. ACM, San Jose (2007) 

31.  Wallace, G., Li, K.: Virtually shared displays and user input devices. In: 2007 Proceedings 
of the USENIX Annual Technical Conference, pp. 1–6. USENIX Association, Santa Clara 
(2007) 

32.  http://www.abisource.com/ 
 

604 

K. Vogt et al. 

33.  Kang,  Y.-a.,  Gorg,  C.,  Stasko,  J.:  Evaluating  visual  analytics  systems  for  investigative 
analysis: Deriving design principles from a case study. In: IEEE Visual Analytics Science 
and Technology, Atlantic City, NJ, pp. 139–146 (2009) 

34.  http://www.lri.fr/~isenberg/wiki/pmwiki.php?n=MyUniversity.P

eCoTo 

35.  Pickens, J.: Algorithmic mediation for collaborative exploratory search, p. 315 (2008) 
36.  Wright,  W.,  Schroh,  D.,  Proulx,  P.,  Skaburskis,  A.,  Cort,  B.:  The  Sandbox  for  analysis: 
concepts and methods. In: CHI 2006: Proceedings of the SIGCHI Conference on Human 
Factors in Computing Systems, pp. 801–810. ACM, New York (2006) 

37.  Patrick, G.S., Baudisch, P., Robertson, G., Czerwinski, M., Meyers, B., Robbins, D., An-

drews, D.: GroupBar: The TaskBar Evolved. In: OZCHI, pp. 34–43 (2003) 

",False,2011.0,{},False,False,bookSection,False,DCXVS8DD,[],self.user,False,False,False,False,http://link.springer.com/10.1007/978-3-642-23771-3_44,,Co-located Collaborative Sensemaking on a Large High-Resolution Display with Multiple Input Devices,DCXVS8DD,False,False
PN7N37WR,VKS2ZMVM,"Visual Interaction with Dimensionality Reduction:

A Structured Literature Analysis

Dominik Sacha, Leishi Zhang, Michael Sedlmair, John A. Lee, Jaakko Peltonen,

Daniel Weiskopf, Stephen North, Daniel A. Keim

Abstract— Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be
useful in exploratory data analysis, they need to be adapted to human needs and domain-speciﬁc problems, ideally, interactively, and
on-the-ﬂy. Many visual analytics systems have already demonstrated the beneﬁts of tightly integrating DR with interactive visualizations.
Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual
analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven
common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant
features, or choosing among several DR algorithms. We investigate speciﬁc implementations of visual analysis systems integrating DR,
and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a “human in the loop”
process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and
classify several systems previously described in the literature, and to derive future research opportunities.
Index Terms—Interactive visualization, machine learning, visual analytics, dimensionality reduction

1 INTRODUCTION
Dimensionality Reduction (DR) is one of the major data abstraction
techniques in Visual Analytics (VA). In a typical setup, data is pro-
cessed by a DR algorithm, and the output is visualized and presented
to the analyst (Figure 1). DR aims at representing multidimensional
data in low-dimensional spaces, while preserving most of its relevant
structure, such as outliers, clusters, or underlying manifolds [36]. DR
is commonly applied to map data from many dimensions down to just
3 or 2, so that salient structures or patterns can be perceived while
exploring data visually, for example distances between data points in
a scatterplot. It is also used as preprocessing for other algorithms, to
improve performance by mitigating the curse of dimensionality [15].
Faced with a plethora of existing DR methods [54], it can be difﬁcult
for analysts to choose a good one, interpret the results, and apply DR
to the best advantage in a broader VA process. A common approach to
overcome this challenge is to involve analysts more closely, enabling
them to investigate and adapt standard methods through interactive
visualizations [39]. In such situations, tight integration of algorithmic
techniques and visualizations is essential. Contributing tools that sup-
port this duality is one of the major goals of VA [34]. Indeed, many VA

• Dominik Sacha is with the University of Konstanz, Germany.

E-mail: dominik.sacha@uni-konstanz.de

• Leishi Zhang is with the Middlesex University, UK.

E-mail: l.x.zhang@mdx.ac.uk

• Michael Sedlmair is with the University of Vienna, Austria.

E-mail: michael.sedlmair@univie.ac.at

• John A. Lee is a Research Associate with the Universit´e catholique de
Louvain (UCL/SSS/IREC/MIRO) and with the Belgian F.R.S.-FNRS.
E-Mail: john.lee@uclouvain.be

• Jaakko Peltonen is with Helsinki Institute for Information Technology HIIT,

Aalto University, and with the University of Tampere, Finland.
E-mail: jaakko.peltonen@aalto.ﬁ

• Daniel Weiskopf is with VISUS, University of Stuttgart, Germany.

E-mail: weiskopf@visus.uni-stuttgart.de

• Stephen C. North directs Infovisible LLC, Oldwick, U.S.A. and graphviz.org.

E-mail: s.c.n@ieee.org

• Daniel A. Keim is with the University of Konstanz, Germany.

E-mail: daniel.keim@uni-konstanz.de

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx

Fig. 1: A basic DR pipeline maps data to a DR algorithm. The results
are visualized and presented to the analyst. Interaction feeds back to
the pipeline components.

applications have been proposed that offer solutions for speciﬁc DR
methods and analysis problems. In these examples, the goal is usually
to support the analyst in steering the underlying algorithms through
effective interactions in a visual interface (e.g., [6]), a concept that has
become known as “semantic interaction” [18].

Despite these efforts, more general solutions that blend machine
learning and VA still do not exist. Yet, it is these more general tools
that are needed to deal successfully with real-world challenges [21,
48]. Aiming at a more general understanding of how to integrate
algorithmic and visual components, a wide variety of theoretical VA
models and frameworks have been proposed [12, 20, 34, 47, 48]. These
models, however, often focus on high-level, abstract views, and fail to
successfully characterize how a strong interplay between algorithms
and visualizations would be realized and exploited.

To better understand the integration of DR and visual user interfaces,
we formed an interdisciplinary group of VA and machine learning re-
searchers. The motivating questions considered were “Exactly how do
analysts interact with the DR pipeline?” and “How can we incor-
porate our ﬁndings into the interactive DR process?”. To answer
these questions, we conducted a semi-automated review of 1850 papers
from the visualization and VA literature. In the ﬁrst step, 377 relevant
papers were selected and subsequently reviewed to identify speciﬁc
examples of how DR interactions are realized, and to get a comprehen-
sive, well-grounded understanding of the overall area. We summarize
our main ﬁndings in the form of seven guiding scenarios that describe
ways of combining DR with visualization (to an extent, inspired by
previous work on guiding scenarios for visualization evaluation [35])
(Section 4). We also present some relevant statistics about DR and
interaction techniques (Section 5). To relate our work to existing theo-
retical models in VA, we incorporate the ﬁndings of the literature study
in a conceptual process for interactive DR [47]. We illustrate how such
models describe and support reasoning about dedicated systems, and
enumerate ﬁve open research opportunities derived from our analysis
(Section 6). Finally, we consider limitations of our work, and outline
topics we plan to address in the future (Section 7 and 8).

2 RELATED WORK
This study is related to previous work in several ways: it is concerned
with general theoretical models of VA and their relationship to machine
learning; it makes use of DR methods; it adopts basic ways of inter-
acting with data visualizations; and it is related to the general idea of
self-reﬂection in the visualization and VA community.

2.1 Theoretical Models
In the standard VA model [34], the discovery process is characterized
by interaction between data, models of the data, visualizations, and the
analyst. User interaction in this framework is aimed at model building
and parameter reﬁnement. Sacha et al. [48] extended it to describe the
human knowledge generation process. The extended model clariﬁes the
role of the analyst in knowledge generation, and highlights the impor-
tance of tight integration of human and machine by enabling interaction
with the system. The previous models apply to VA in a generic manner.
In contrast, the study presented here focuses speciﬁcally on interacting
with DR methods. Another framework describes the problem of DR as
a two-stage process [12]: it ﬁrst maps high-dimensional data to a lower-
dimensional space, then allows another stage to reduce it to 2D for
visualization. While this framework generalizes speciﬁc DR methods,
it focuses on a speciﬁc application to clustered data and is limited to the
two-stage process as described. The framework for observation-level
interaction with statistical models [20] focuses on interaction by direct
manipulation of visualization by different projection techniques. There-
fore, it yields a generic approach toward interacting with the output
of DR methods, which is one part of our human-in-the-loop process
model; i.e., observation-level interaction directly ﬁts in our proposed
process of interaction with DR methods. Another general model is
semantic interaction [18], taking acquired interaction data as a means
to build user models and guide the VA system.

2.2 Surveys of DR and Interaction Techniques
DR maps data into fewer dimensions aiming to preserve structure like
cluster gaps or local manifold continuity. In linear DR output axes are
linear combinations of original features, for example directions of
largest variation in principal component analysis (PCA), maximally
statistically independent directions in independent component analysis
(ICA) [26], directions of maximal between-class and minimal
within-class variation in linear discriminant analysis (LDA), or
directions of maximal correlation between feature subsets in canonical
correlation analysis (CCA). Nonlinear DR ﬁnds either a mapping
function or only output coordinates for the data set,
interpreted
through proximities or distances of output data; for example, mappings
are sought to preserve pairwise data distances in multidimensional
scaling (MDS), small distances in Sammon mapping, distances along
a neighborhood graph in Isomap, or neighborhood relationships in
neighbor embedding methods [54, 55]. Some methods seek mappings
onto a regular grid of units as in self-organizing maps (SOMs) or
generative topographic mapping (GTM). Details on PCA, MDS,
Sammon mapping, Isomap, SOM, and GTM are available in books
such as [36] and for LDA and CCA in [1].

Van der Maaten et al. [54] offer a comparative review of the state
of the art in DR techniques, focusing on the performance of nonlinear
techniques from the machine learning perspective. Similarly, Wism¨uller
et al. [57] survey nonlinear DR, manifold and topological learning tech-
niques. Bengio et al. [3] give an overview on representation learning in
the context of deep learning. However, all the aforementioned works do
not take into account VA or user interaction. A survey by Liu et al. [39]
covers visualization of high-dimensional data, including DR as one of
the main techniques. They include a short discussion of interaction,
and embed examples into the traditional visualization pipeline. How-
ever, they focus on general interaction techniques and not speciﬁcally
how users interact with DR. Furthermore, they enumerate interactive
model manipulation as a future research opportunity. Similarly, Buja
et al. [9] review interaction techniques in the general setting of high-
dimensional data visualization. Hoffman and Grinstein [24] and Bertini
and Lalanne [4] discuss visualization methods for high-dimensional

Fig. 2: Our four stage analysis process: 1. Automated ﬁltering, 2.
Manual ﬁltering, 3. Manual coding, 4. Manual sample validation.

data mining, including projection and interaction methods. Keim [33]
structures such visualization approaches according to the type of data
to be visualized, the actual visualization technique, and the interaction
and distortion method. However, none of these surveys performed a
systematic exploration of the existing literature, nor did they focus on
interaction techniques for DR.

2.3 Interaction Taxonomies
Our study addresses interaction in the context of DR. Therefore, re-
lated work includes general models of interaction for visualization. For
example, Yi et al. [58] identify seven interaction method categories:
select, explore, reconﬁgure, encode, abstract/elaborate, ﬁlter, and con-
nect. Brehmer and Munzner [7] provide a comprehensive description
of visualization tasks, leading to a multi-level typology of abstract tasks
(which includes the ones by Yi et al.). However, model interactions
only arise in tasks they call “aggregate” or “derive” tasks. Von Lan-
desberger et al. [56] deﬁne an interaction taxonomy that is suitable for
tracking and analyzing user actions in VA, and provides two types of
data processing interactions: data changes, such as editing or selecting
data, and processing changes, such as scheme or parameter changes. In
contrast, our work focuses less on a general description of user tasks,
but rather on the process of interacting with DR methods.

2.4 Self-Reﬂection in the Visualization and VA Community
Because our study is based on a systematic review, coding, and anal-
ysis of previous work in the visualization and VA community, it is
also related to previous work on self-reﬂection of empirical studies in
information visualization [35], evaluation in visualization research in
general [28], or affordance in human computation and human-computer
interaction [14]. While we adopt the methodology of systematic analy-
sis of previous work, our paper has a very different focus.

3 METHODS
To obtain a general understanding of visual interactive DR systems,
we systematically reviewed the IEEE InfoVis, IEEE VAST, TVCG,
and EuroVis literature. We ﬁrst automatically identiﬁed a relevant
subset of papers from these conferences and journals. Then, we carried
out a qualitative in-depth analysis of the relevant papers, iteratively
extracting and reﬁning visual DR interaction characteristics. Our over-
all approach to this analysis was inspired by Grounded Theory [11],
in which data is systematically analyzed until meaningful categories
emerge (see Section 4). This methodological approach is based on
identifying and reﬁning categories from a representative set of qualita-
tive data, here papers, which are then used to incrementally build up a
theoretical model (Section 6). This approach has been used in visual-
ization research [28, 35, 51] and related areas such as HCI before [25],
and its importance for building up the much needed theoretical foun-
dation in visualization has been recognized [45, 51]. We next describe
our method, followed by more detailed sub-sections on our analysis
procedure and ﬁndings.

3.1 Methodological Choices
We began our endeavor with a curated list of landmark publications in
interactive machine learning and visualization. Using these candidate
papers, we ﬁrst tried an open coding approach to identify “interesting”
aspects at the intersection of VA and machine learning in general.
This approach turned out to be very time consuming, and, ultimately,

impractical. While it led to a high level framework [47], our initial
goals of thoroughly and systematically depicting how the VA and
machine learning can be combined were largely unsatisﬁed. Hence
we decided to analyze a much larger set of sample papers, resulting in
three implications for our methodological choices. (1) We realized the
need to focus on a speciﬁc machine learning problem (in our case, DR)
to make the analysis more concrete, relevant, and actionable. (2) We
needed automated methods to reduce the set of potentially interesting
papers. (3) We opted for crisp, clear criteria for manual coding and
ﬁltering of papers. During this process, we reﬁned the process, ﬁltering
criteria, and coding options several times. Our ﬁnal workﬂow was
then composed of four major steps, shown in Figure 2: 1.) Automated
keyword-based paper ﬁltering, 2.) Manual paper ﬁltering, 3.) Manual
paper coding, and 4.) Manual sample validation.

3.2 Sample Set of Papers
Our overall goal was to identify which DR methods are used, and how
interaction is implemented in the VA and visualization communities.
We decided to take a representative sample of papers, constituted of all
IEEE VIS papers (1221) and EuroVis papers (629) from 2005 to 2015,
for a total of 1850 papers. From EuroVis we included all full and
short papers, as well as EuroVA publications. The IEEE VIS papers
included all InfoVis, VAST and TVCG papers. Our main focus was
abstract, multi-dimensional data; consequently we did not include IEEE
SciVis/Vis papers in the analysis, which generally focus on 3D spatial
data (e.g., ﬂow and volume rendering).

3.3 Automated Keyword-Based Filtering
We implemented a basic NLP pipeline to analyze the initial set of
papers. The pipeline parses the full text of each paper, applying a
tokenizer and a snowball stemmer implemented from StanfordNLP
components1. The same was done with keyword lists, one list for DR
and another for interaction keywords. From this, a feature vector of
all keyword occurrences was derived. Papers without any keyword
occurrences were deemed irrelevant and ﬁltered out, and the remaining
papers were listed in a csv ﬁle with associated keyword counts. This
ﬁle was the basis for the subsequent manual ﬁltering and coding steps.
For the keyword deﬁnition, we examined previously published sur-
veys and taxonomy papers in related ﬁelds, and formed a set of primary
papers in DR and interactive visualization. The keywords of these
papers were extracted and processed using the NLP pipeline. A manual
validation process then followed to reﬁne the keyword lists. For exam-
ple, ambiguous abbreviations (such as, LLC), or words that become
ambiguous after stemming (such as projection, which stems to project,
or some, which stems to SOM) were removed. The ﬁnal keyword
lists and statistics from the automated ﬁltering process are provided as
supplemental material.

After the automated process, the initial set of 1850 papers was ﬁl-
tered to 382 relevant papers based on DR keywords, then reduced to 377
papers (108 EuroVis, 247 VIS) based on interaction keywords. Figure 3
illustrates a histogram of the keyword frequencies in a logarithmic scale.
DR keywords are colored green and interaction keywords are shown
in light blue. Interact is the outlier with the maximum occurrence in
interaction keywords, while MDS and PCA are the most frequently
occurring DR methods.

3.4 Manual Expert Filtering
The remaining 377 papers were manually checked using the following
criteria. First, we checked if the paper is a visualization application
or technique paper, and if it handles “abstract data.” (We intended to
exclude theory and evaluation papers, as well as papers focused on
unrelated or tangential topics such as volume rendering or physical ﬂow
data). Second, we checked if the paper addresses the combination of
visualization, DR and interaction, and if the interaction feeds back to
the DR. For example Joia et al. [32] present an interesting technique for
sampling and feature selection. However, there is no interaction that
causes a recalculation of the DR. Given our focus on interactive DR,

1http://nlp.stanford.edu/software/

Fig. 3: The top keyword occurrences in the automatically identiﬁed
papers shown in a log-scale histogram. DR keywords are colored in
green and interaction keywords are colored in light blue.

we excluded interactions that do not feed back to the analysis pipeline,
such as exploration/navigation/DoD (Details on Demand) interactions.
Finally, we listed the DR techniques employed. Based on this, we
obtained a candidate set of 70 relevant papers.

3.5 Manual Paper Coding
We next analyzed these 70 papers in detail, by open coding the “inter-
esting” aspects of interaction described in each paper. For each paper,
we extracted a brief description of the proposed interaction, including
how interaction is performed and which parts of the DR pipeline are
affected. In addition, we iteratively identiﬁed and reﬁned a set of cri-
teria. A more general model [47] and the different components of the
DR pipeline (data, preprocessing, DR) served as initial set of criteria
to encode which parts are affected by the analysts feedback. However,
we had to adapt, split, and reﬁne these criteria several times. As a
result we arrived at seven scenarios for DR interaction, encoding “how
the DR pipeline is changed” (see Section 4), the interaction paradigm
(“how the interaction is performed”, see Section 5), the DR Method(s)
or Algorithm(s), and combined machine learning techniques such as
clustering or classiﬁcation. During our process we had to discard
several aspects that we initially were interested in. We started, for
example, to encode “who” is expected to perform the actual interaction
(e.g., DR expert or novice user), and “why” the human input is needed.
However, investigating these aspects turned out to be challenging as
the necessary information was not provided in many cases. A more
detailed description of the ﬁnal criteria and options is provided in the
following sections. 8 more papers were ﬁltered out in this iteration.

3.6 Manual Sample Validation
In a ﬁnal validation iteration, we aimed at more detailed analysis of
borderline cases and ended up removing 4 more papers. Our ﬁnal
corpus included 58 relevant papers, with the encoded information and
the corresponding feature vector of keyword occurrences. We “cleaned”
the encoded information and grouped the identiﬁed DR methods into
higher-level categories (see Section 5.2).

4 SEVEN GUIDING SCENARIOS FOR DR INTERACTION
We next describe the interaction scenarios that emerged from our
literature review. By examining the interactive machine learning
pipeline proposed in [47], we identiﬁed the main potential interactions
in data analysis, and classiﬁed them into seven guiding scenarios. This
categorization is based on the outcome of several iterations of the paper
ﬁltering and open coding process, and is one of the major ﬁndings of
our study. It enables us to evaluate various methods for “how the DR
pipeline is controlled through interaction”. In the following, we brieﬂy
describe these seven DR interaction scenarios “along the DR pipeline”
and illustrate them with examples:

S1 Data Selection & Emphasis: This group of interactions affects
the data records (or observations) that will be supplied to the actual
DR method. We found many examples in which a ﬁlter is applied
to the data, and the DR pipeline is re-run on the remaining subset.
In this scenario, we further identiﬁed several realizations. In some
situations, analysts select subsets directly in a two-dimensional visual
representation. In others, analysts specify conditions or ﬁlters through
control panels. Furthermore, we identiﬁed various preprocessing
conﬁgurations or parameters that can be adjusted by the analyst. An
example is J¨ackle et al.’s temporal MDS plot technique [29], where a
parameter sets the size of a sliding window. The resulting slices are
taken as input for subsequent DR by one-dimensional MDS. S1 Data
Selection & Emphasis was identiﬁed 26 times.

S2 Annotation & Labeling: A second group of operations enrich
data with annotations or labels on instances. In some systems, data may
be enriched with additional information. For example, StarSPIRE [6]
allows analysts to annotate documents with additional terms that will
be included in the similarity calculation. Other systems enable the
analyst to assign classiﬁcation or cluster labels if the DR is combined
with another form of machine learning (e.g. [23]). The cluster or classi-
ﬁcation labels, as well as data structures (such as a hierarchy obtained
from hierarchical clustering) are then translated into constraints for the
DR algorithm (e.g., cluster preservation).

In the classiﬁcation case, the analyst provides class labels within the
two-dimensional embedding to train a classiﬁer. Labels are provided
for data instances, or in some settings, for pairs of instances. The
classiﬁcation result inﬂuences subsequent DR (e.g., [22]).
In the
clustering case, the analyst deﬁnes cluster memberships, such as by
grouping elements into clusters, by adding or removing elements, or
by splitting or merging clusters. Resulting clusters are used by the
next iteration of DR. For example, the Bubble Cluster approach [23]
lets the analyst re-position points or draw cluster boundaries in a 2D
projection of the data, and use the new cluster assignments to update
the projection. S2 Annotation & Labeling was found in 15 papers.

S3 Data Manipulation: Some VA systems let the analyst explicitly
manipulate data values by moving points in a spatialization, or
by editing data in a table view. This interaction helps analysts to
investigate “what if” scenarios. For example the iPCA system [30]
allows the analyst to re-position a point in the 2D projection, and see
how other values change. Interestingly, Jeong et al. reported that
adjusting data values could be counter-intuitive to some of the subjects
in their study. However, they argued that these interactions are still
useful for revealing relationships in the data that might otherwise not
be recognized. S3 Data Manipulation was only rarely used (7 times).

S4 Feature Selection & Emphasis: We found many interaction
examples that feed back to the initial data space by adapting the
metric for calculating similarities or dissimilarities between data
instances. Many DR applications adopt a “default” metric such as
Euclidean distance. However, the default metric may not correspond
well with the analyst’s “notion” of dissimilarity, and the metric needs
to be adapted to the application. One way to do this is to associate
adjustable weights with each data dimension. Distances can be
calculated accordingly, giving more inﬂuence to relevant dimensions.
For example, iPCA [30] provides the analyst with weighting sliders for
each dimension. Another possibility is to infer the dimension loadings
from direct manipulation interactions of visual elements. An example
can be found in [43] where the analyst rearranges points serving as
control points for a subsequent optimization of the projection matrix.
Similarly in Dis-Function [8], an analyst drags and selects points on a
2D scatterplot, and a compatible distance function is learned by the
system. When the user is ﬁnished with manipulations, a button is
pressed to learn the distance function and re-render the result. Other
systems such as [44] provide analysts with drop-down menus to select
a distance metric. Further options are to let the analyst determine
interesting features in combination with subspace clustering (e.g., [41])
or quality metrics (e.g., [31]). S4 Feature Selection & Emphasis was

the most frequently implemented interaction scenario (37).

S5 DR Parameter Tuning: Some DR algorithms contain speciﬁc
parameters that can be tuned, such as LDA regularization in [13]. An
approach proposed by Schreck et al [49] allows the analyst to set
the grid dimensions of a self-organizing map (number of neurons,
DR structure). Some systems have parameters related to quality and
accuracy, such as thresholds or level-of-detail parameters. Garg et
al. [22] provide a similarity cutoff parameter that determines edges
with low similarity to be removed from a graph layout. Others
have parameters affecting visual appearance. For example, [16]
allows adjusting node padding or forcing strength in a force-directed
embedding. We also found examples where the analyst can deﬁne
algorithmic variants (by setting parameters), that animate or show
transitions between multiple DR results. In [43] a transition parameter
(slider) is set to compare and track changes. Finally, parameter sets
or conﬁgurations can be set indirectly, such as when the analyst is
offered several visualization recommendations or previously deﬁned
parameter sets, and may compare them to select the most appropriate
one. However, we did not identify any mature, ready-to-use system
incorporating this kind of parameter tuning. S5 DR Parameter Tuning
was found in 20 papers.

S6 Deﬁning Constraints: Interactions can be translated directly to
DR algorithm constraints. We identiﬁed several examples in which an
analyst directly arranges points in the visualization. These modiﬁed
points are interpreted as anchor points in the subsequent DR iteration,
in which their positions should remain ﬁxed to help the analyst track
other changes. For example, Endert et al. [20] introduced Guided
MDS, where user-deﬁned anchor points are used to ﬁx positions
and adjust similarities for maintaining consistency in visualization.
A similar example can be found in [6], where nodes representing
objects are marked as “ﬁxed” and subsequently not rearranged by a
force-directed algorithm. Constraints such as region or containment,
as well as visual constraints have also been proposed. For example
the technique introduced by [17] allows analysts to group points and
deﬁne regions that should not be split or overlap with others.
In
addition, constraints for the edges may be deﬁned, such as pointing
edge downward. Note, that in combination with another ML method,
the ML output can be thought of as a constraint for DR, e.g., items
that belong to the same cluster or classiﬁcation should be placed close
to each other, or a hierarchy obtained from hierarchical clustering
should be preserved. In some systems, these constraints can also be
interactively controlled (providing labels, setting parameters for the
clustering, etc.). S6 Deﬁning Constraints was described in 15 papers.

S7 DR Type Selection: Visual embeddings of high-dimensional
data can be generated by various DR algorithms and vary in terms
of layout and quality. For example, linear methods project data to
new axes, such as directions of maximal variance in PCA, whereas
methods such as MDS aim to preserve distances or neighborhoods
of data records. While some systems, such as iPCA and StarSPIRE,
focus on one DR technique, others implement multiple algorithms
so the analyst can select and compare their results while analyzing
data. A system by Rieck and Leitte [46] visualizes and ranks
embeddings from several DR algorithms according to quality
measures. Another system by Liu et al. [40] lets the analyst select
DR algorithms and compare them based on visualization of distortion
measures. We can even envision approaches for indirect S7 DR
Type Selection. Although we did not ﬁnd examples,
it seems
potentially useful to infer an appropriate DR Type from user inputs
automatically, on the ﬂy. We elaborate on this idea in Section 6. S7 DR
Type Selection had the lowest occurrence (4) among the seven scenarios.

Note that some of the seven guiding scenarios overlap. For example,
S1-S3 affect data items, and S5-S7 involve the choice of DR algorithm.
However, we identiﬁed these particular scenarios as useful descriptions
of the papers we studied. We found it useful to distinguish scenarios
based on the way interaction affects the DR pipeline. For example, to

Table 1: Result of the proposed coding process. Blue, orange, yellow
and green setups appear more frequently. Red points denote papers
implementing 4 different interaction scenarios. The three main column
groups specify interaction scenarios, combinations with other machine
learning methods, and interaction paradigms.

Fig. 4: Embedding of 58 papers based on interaction scenarios. The plot
shows a diverse set of interaction combinations. The main interaction
scenarios are S4 Feature Selection & Emphasis (blue cluster), S1 Data
Selection & Emphasis (orange cluster), the combination of S1 & S4
(yellow cluster), and S4 combined with S2 Data Manipulation (green
cluster). The red cluster contains papers that combine 4 different
interaction scenarios.

Observations: The ﬁnal result of our coding process is shown in
Table 1 and Figure 4. To provide an overview of the coded results, we
created a 2D projection of the papers using Multiscale Jensen-Shannon
Embedding [37], which aims to place papers with similar codes nearby
in the projection. Together with Table 1 we can investigate combi-
nations of interaction scenarios. In total, we identiﬁed 29 different
combinations. We found a maximum of 4 scenarios per paper (in 4
papers, colored in red). Papers colored blue only cover S4 Feature Se-
lection & Emphasis. This was the most frequent “setup” and appeared
in 9 papers. The ﬁve orange dots denote papers that only include S1
Data Selection & Emphasis, and the ﬁve yellow dots represent papers
with combinations of S1 and S4. Work applying S2 Annotation & La-
beling and S4 Feature Selection & Emphasis occurred 4 times (green
dots). We color the rest of the papers gray, as their combinations of
interactions occur less frequently. These gray dots generally appear
further away from the center of the view. For example, papers including
S5 DR Parameter Tuning are placed in the upper area, or S2. Annotation
& Labeling papers are placed near the upper left corner.

We further observe from Table 1 that some interaction scenarios
appear more frequently than others. This applies to S4, S1, and S5
maybe because they are more general or convenient than others. S3
Data Manipulation is used least. One reason might be that manipulat-
ing observations—often considered “ground truth”—is not common
practice in many domains (e.g., machine learning). Also note that S3
only appears in combination with other interaction scenarios.

In this respect, it would be interesting to investigate in more detail
why some interaction scenarios appear more or less frequently. This
naturally raises the question about the effectiveness of certain interac-
tion scenarios. In-depth investigation of effectiveness, however, goes
beyond the scope of this paper. Previous work has shown that assessing
effectiveness of interactive DR solutions depends heavily on context
factors, such as users, data, domain, and tasks at hand [50, 52]. A
generic comparison of the scenarios’ usefulness and effectiveness is
thus a non-trivial endeavor, and further work is needed. The study in
this paper is descriptive with the goal to characterize existing interaction
scenarios, and can be used as a starting point for such endeavors.

distinguish S2 Annotation & Labeling from S6 Deﬁning Constraints
interactions, we note that both add information to data items (e.g., a
class label vs. a “pinned” information), but S2 Annotation & Labeling
focuses on information about input data items, while S6 Deﬁning
Constraints involves information about desired results or outputs of
the DR. Note also that the role of the VA system is to translate these
similar inputs to different interaction scenarios (see Section 5).

Table 2: Identiﬁed DR techniques shown with interaction scenarios.

Fig. 5: Different interaction paradigms: Typical Direct Manipulation in-
teractions are shown in the upper half. On the bottom, control elements
are shown. DR-Interfaces are usually composed of both.

Table 3: Temporal statistics of interaction and DR Techniques.

5 FURTHER INSIGHTS
In this section, we analyze the interaction scenarios in different contexts,
such as the interaction paradigm, the combined DR algorithms or other
machine learning methods, as well as a temporal perspective.

5.1 Interaction Paradigm & Usability
Each interaction scenario can be realized in multiple ways. Therefore,
our analysis also encoded interaction paradigms, including Direct
Manipulation of visual elements, Controls (sliders, buttons, etc.),
Command Line Interface (CLI), Other (such as gestures or speech
input), or NA (if interaction was not described in the paper). The
results (see Table 1-right side columns) reveal balanced usage of Direct
Manipulation (36) and Controls (33). However, novel interaction
paradigms (Other) only appeared once (multi-touch in [59]) and
another set of papers omits details of how interaction is performed
(NA, 12). It is also worth mentioning that the amount of provided
information about the realization and implementation, as well as
discussions about usability of interactions strongly varies between the
analyzed papers.

Our results show that analysts interact with DR either directly in
the visualization, or using control elements. During our study we
noticed, especially in Direct Manipulation, similar actions may have
different meanings or implementations (see upper half of Figure 5 as
an example). An analyst can move points, select data records (followed
by an operation such as deletion), mark (label, or annotate) points, or
draw borders in a plot. However, the meaning of an action may vary.
Data movement can be “translated” to S2 Annotation & Labeling if a
point is moved outside a cluster, or to S3 Data Manipulation if the data
value is changed. Alternatively, the movement can be “translated” to
S4 Feature Selection & Emphasis by deriving (dis)similarities from
user deﬁned distances between data points, or S6 Deﬁning Constraints
if a data point being moved is interpreted as an anchor point. In such
cases visualization has to act as a “mediator” between human and
machine and translate the interactions to appropriate DR pipeline
components.
In contrast, control elements (Figure 5-bottom) are
usually directly coupled to speciﬁc DR pipeline components. The UI
provides, for example, sliders to directly control dimension loading
or DR parameters, drop-down menus to select metrics, or buttons to
trigger speciﬁc operations. There are also cases where natural language
text inputs are accepted. On the other hand, Command Line Interfaces
offer a powerful, well-speciﬁed language for programmers, but they
are not always convenient or even accessible to analysts.

The ﬁnal implementation determines the “complexity” of performing
an interaction scenario, which depends on user and task characteristics
though. DR experts, for instance, might require a large set of directly
steerable parameters, and accept a more complex interface. Other users,
however, might require less ﬂexibility, but simple ways to provide
feedback based on their domain knowledge.

5.2 DR & Machine Learning Algorithms
The interaction scenarios appeared with several different DR algo-
rithms. Each algorithm was assigned to a higher-level category of
Distance Based (DB), Linear Projection (LP), Graph/Force-Directed
(FD), Neural Network (NN), General, or Other (one approach did not
match any others – “data driven feature selection”). Table 2 lists these
DR algorithms as columns and interaction scenarios as rows. We see
that Distance Based methods (mainly MDS) were used alone most fre-
quently (17), followed by Linear Projections (mainly PCA) alone (12)
and Graph/Force-Directed methods alone (10). General approaches
appear in 5 papers, whereas Neural Networks were used 3 times (all
self organizing maps). The other columns show examples where var-
ious DR algorithms are used in combination. Note that only one of
the mixed approaches (other than General approaches) lets the analyst
switch between DR algorithms. Interestingly, S4 Feature Selection &
Emphasis was used in all of these DR algorithm combinations. We can
further derive from Table 1 that Clustering and Classiﬁcation appeared
in 31 papers. Clustering was used 28 and Classiﬁcation 12 times, while
in 9 papers both of them are used in combination.

5.3 Temporal Perspective
We did not ﬁnd any relevant papers on interactive DR in 2005. As
shown in Table 3, in the corpus we studied, published work on visual
interactive DR ﬁrst appeared in 2006, with one paper that reported
work in S1 Data Selection & Emphasis. This was followed by 5 related
papers published in 2007, where a wider range of interaction techniques
such as S4 Feature Selection & Emphasis, S2 Annotation & Labeling,
and S5 DR Parameter Tuning were included. These four interaction
scenarios appear to be more “established” than the others, as work was
continuously reported in these areas in the following years, whereas the
development of other interaction techniques have breaks in between.
For example S7 DR Type Selection ﬁrst appeared in 2007, but then there
is a gap until 2012, after which it appears consistently. We admit these
trends may not be fully representative due to the limited number of
papers and scenarios in our study. One obvious pattern is the number of
papers published by year. Years 2009 (9) and 2011 (10) are peaks. Of
course, a larger number of papers does not necessarily describe a richer
set of interaction scenarios (as shown in the “AVG Interaction/Paper”
row where large paper counts do not strongly correspond to large
average interaction counts).

Fig. 6: Proposed “human in the loop” process model for interactive DR. The analyst can iteratively reﬁne the analysis by interacting with the DR
pipeline. The visualization interface serves as a “lens” that interactively mediates between the DR pipeline and the analyst, presenting DR results
or updates and accepting feedback.

6 THE INTERACTIVE DR PROCESS

With the goal of making our study more broadly applicable, we sum-
marize our ﬁndings in a general process model for interactive DR in
VA. This model is shown in Figure 6. It depicts an expanded version
of the basic model in Figure 1 and is a specialized model of our gen-
eral pipeline model for visual interactive machine learning [47]. Note
that the general model is a superset of the model shown in Figure 6
and was needed to arrive at a more specialized version for interactive
DR, which contains speciﬁc steps, knowledge, and details tailored to
interactive DR, and is therefore much more actionable. At the top,
we add the seven scenarios of interacting with DR techniques, and
arrange them along the analysis pipeline. S1-S3 operate on the data,
such as by changing data values or annotating labels (blue); S4 operates
on the feature space, such as by changing distance functions or the
projection matrix (cyan); and S5-S7 directly affect the DR algorithms
(or additional ML models) (green). At the bottom, the results of the
DR process are propagated back to the analyst (yellow).

The core of our process model is the interactive visual interface
(red), which connects these two streams and serves as a lens for the
human analyst on the algorithmic building blocks. While our work
focused primarily on characterizing the forms of interaction shown by
the top arrows, it is also interesting to consider how DR results can be
visually presented to the user. We found dimensionally-reduced data is
typically presented in scatterplots or node-link diagrams, conﬁrming
previous empirical ﬁndings [50]. Yet, our model also draws attention
to the fact that other aspects of the process model can be visually repre-
sented. For instance, the dimensions (or eigenvector) can be mapped
to a parallel coordinate plot [30]. Furthermore, the quality of the DR
pipeline can potentially be visualized, either separately, or embedded in
the low dimensional representation. Some DR types calculate or iden-
tify errors, and in combination with other machine learning methods,
additional quality information might be obtained (e.g., the precision
of a classiﬁer [42]). Furthermore, different DR pipeline variants (e.g.,
pre-deﬁned DR conﬁgurations or automatically built recommendations)
can be visualized [27]. These different perspectives on the DR pipeline
support the analyst’s interpretation and validation process.

In many VA tools, the analyst has not only the ability to visually
inspect and validate the data, but also the ability to provide interactive
feedback to control the analysis through the interface. As discussed
previously, this feedback is usually in the form of controls and direct ma-
nipulation interactions, such as setting positions, selecting, or grouping
data items; other interaction paradigms such as command line scripts,
gestures and speech input are also possible. The VA system maps user
inputs to the speciﬁed interaction scenario(s), providing an instance
of a typical continuous and iterative process, as it is usually targeted
in VA [34, 47]. Note that the ability of the analyst to provide useful
feedback depends on the interpretability of visual observations but also

on the accessibility (implementation) of the interaction. These aspects
further depend on both, the technical competence (DR expertise) and
domain knowledge of the analyst, as well as the analysis task (e.g.,
analyzing data records vs. dimensions [50]). Especially novice analysts
with less mathematical skills face problems of interpreting different
DR concepts (e.g., linear vs. non-linear models) in a 2D-representation
where the actual meaning of the axis is lost.

We now demonstrate how the proposed process model can be used
for comparative, as well as generative purposes [2]. We ﬁrst use it to
describe and compare four existing examples. We then use it to identify
and reason about open research opportunities.

6.1 Descriptive Use of the Process Model – Examples
Figure 7b instantiates the DR process model on four examples. Their
representation in the proposed model provides a consistent way to
understand these systems and compare their capabilities for interaction.
iPCA (S1, S3, S4) The iPCA system [30] (Figure 7a-1) addresses
typical data and feature space interactions. Several aspects of PCA
are visualized in linked views, including projection, data, eigenvector,
and correlation views. Each view supports a wide range of interactions
including navigation, selection, and linking & brushing, however, the
authors focused on three interactions that require re-computation of
PCA. First, for S1 Data Selection & Emphasis an analyst can remove
data items (e.g., outliers) and observe the resulting changes in data- and
eigen-space. Second, the analyst can modify data values in some views
or spaces (S3 Data Manipulation). Finally, iPCA offers sliders for each
dimension for S4 Feature Selection & Emphasis, enabling the analyst to
modify each dimension’s contribution to the ﬁnal PCA calculation. This
lets the analyst test how the DR is affected by removing or “dimming”
the importance of certain dimensions.

Interactive Cluster Separation (S4, S6, other ML) Molchanov
and Linsen [43] present another way to infer feature weights from
interactions (Figure 7a-2). They invert the process of modifying the
projection matrix in a star coordinates widget by allowing the analyst
to specify the desired conﬁguration directly in the projection view (by
rearranging control points). They show an example where S4 Feature
Selection & Emphasis is inferred from direct manipulation of data
points. In addition, the control points serve as S6 Deﬁned Constraints
for the projection. To achieve an appropriate DR output, the projection
matrix is recalculated “based on an LS solution of an over determined
system of linear equations”. The control points can be selected by
the analyst, however, the authors recommend using cluster medians or
centroids for better cluster separation. This implies that the labels must
be contained in the data or determined by a classiﬁer beforehand.

StarSPIRE (S1, S2, S4, S6) Bradel et al. [6] extends the Force-
SPIRE system proposed by Endert et al. [19]. Their extension offers
a richer set of interaction scenarios. A modiﬁed force-directed layout

(a) Images for each example.

(b) Interactive DR process model instances for each example.

Fig. 7: Analyzed examples for DR interaction: 1.) iPCA, 2.) Interactive cluster separation, 3.) StarSPIRE, and 4.) Persistent Homology.

algorithm visualizes text documents under a computed similarity metric
(Figure 7a-3). They extend ForceSPIRE with an additional model for
relevance-based document retrieval that performs S1 Data Selection
& Emphasis inferred from user interactions. The analyst can also S2
Annotate text documents with further information (terms) that update
the similarity calculation and cause a change to the document layout. S4
Feature Selection & Emphasis is inferred from user interaction by ad-
justing the weightings of document terms. This is done in conjunction
with annotation, but also by re-sizing elements, searching, highlighting
and overlapping documents. In addition it is possible to rearrange and
pin document nodes in the spatialization. The pinned document serves
as a S6 Deﬁned Constraint for the force-directed layout.

Persistent Homology (S5, S7) Rieck and Leitte [46] describe
an approach to comparing DR parameter settings across various DR
types, such as PCA, t-SNE, HLLE and Isomap. Quality measures
are computed to validate and rank the DR setup conﬁgurations. The
proposed approach visualizes various DR embeddings together with
additional quality information (Figure 7a-4). Their study does not
explain in detail how an analyst would create the different combinations
of S5 Parameter Settings and S7 DR Type Selections (we encoded
this work as NA). However, several examples illustrate different DR
algorithms and parameterizations created by the authors (we assume
using CLI).

Comparison Figure 7b shows interactions supported by the
above-mentioned systems. For instance, while iPCA offers the ability
to S3 manipulate data items, StarSPIRE allows the analyst to S2
annotate documents with additional terms. iPCA, Cluster Separation,
and StarSPIRE allow S4 Feature Selection & Emphasis, however,
in different ways.
iPCA offers slider controls directly coupled to
dimension loading. Cluster Separation and StarSPIRE infer S4 Feature
Selection & Emphasis from direct manipulation, through optimization
and term weighting. Cluster Separation and StarSPIRE allow the
analyst to S6 Deﬁne Constraints for the DR process, by positioning
and pinning data items. The Persistent Homology approach focuses
on the validation and comparison of different DR setups by choosing
among several S7 DR Type Selections and S5 Parameter Settings.

The examples and their comparison illustrate the applicability of
the proposed interactive DR process model. It supports evaluating
systems with respect to the identiﬁed interaction scenarios and their
implementations, and can be used to derive further interaction scenarios
and implementations not present in current VA systems. We next detail
5 opportunities for research in visual interactive DR systems.

6.2 Generative Use of the Process Model – Opportunities
We can apply our study and process model to better understand and
reason about research opportunities. We recommend these directions:
Semantic Interaction Design One challenge in the design of
interactive DR systems is the semantic translation of front-end interac-
tions. Section 5.1 illustrates that the same front-end interaction can be
mapped to several different back-end computations. Ideally, intuitive
interactions would direct back-end computation and correctly express
the intention of the analyst. For example in StarSPIRE, by moving
points closer to each other in the visualization the analyst can have
the similarity measures and the layout updated accordingly. While
many systems provide good examples of semantic interaction design,
the translation only applies to a subset of interaction scenarios (e.g.,
feature weighting, similarity computation). Consistently mapping user
inputs to more complex actions covering the entire pipeline is an open
challenge. Especially in DR, interaction designers have to consider that
DR concepts and algorithms are often hard to understand and interpret.
Therefore, interaction needs to be accessible and interpretable for end
users, enabling them to work with distances and neighborhoods, clus-
ters and class memberships, or importance of dimensions. Scalability
of computation will play a crucial role in such interactive systems, as
delaying responses hinders usability [10].

Guidance on DR Type Selection Our study revealed that S7 DR
Type Selection has rarely been implemented. Furthermore, semantic
interactions derived from direct manipulation interactions are mostly
limited to DR pipeline adaptions of the feature space or DR parameters
and constraints. We envision future systems that can also infer an
appropriate DR algorithm from user inputs. Such VA systems would
probably need to implement, calculate and compare various DR types,
to identify the “best” results on the ﬂy. Work proposed by Rieck
and Leitte [46] shows a promising step in this direction. However,
realization and implementation of direct manipulation interactions and
translation to DR type selections is still missing. Such techniques, that
balance user ﬂexibility with system automation, have great potential for
guiding users through complex data analyses, so this is an important
area for further investigation. On the algorithmic side, the challenge
is to formulate speciﬁc DR algorithms as parametric instances that
allow smooth transitions between different DR types. For example,
continuous model spaces [37, 55] enable analysts to track and interpret
model switching and avoid abrupt and confusing transitions.

Evaluating DR Interactions As pointed out in Section 4 we are
not aware of studies evaluating the effectiveness of DR interactions
in a structured and general setup. It will be a challenge to design and

conduct a fair comparative assessment of different interaction scenar-
ios, as they depend on many factors, such as implementation, user
experience or tasks. However, it would be useful to gather insights
about effectiveness of the respective interaction scenarios under certain
conditions. This would guide researchers and developers in designing
interactive DR systems for their speciﬁc domains, tasks and data. Horn-
bæk provides a comprehensive overview of usability measures from
HCI [25] that could be applied to a comparative DR setup.

Fully Integrated Process As discussed in Section 4 and illus-
trated in Figure 7b, existing systems implement only a small subset
of possible interactions. While many previous systems have proven
useful for speciﬁc tasks and problems, more powerful, general-purpose
interactive DR tools are needed. An ideal system would provide ﬂexible
access to a range of DR algorithms, distance functions, optimization
algorithms or quality metrics, and offer many of the interaction types
we identiﬁed. It will be a challenge to conceptually integrate and steer
a wide range of algorithm speciﬁc parameters or different combinations
of computations. At the same time, the burden of choosing suitable
data, features, parameters and models could be mitigated by tightly
integrating the DR pipeline with interactive visualization.

Analytic Provenance Given the complex nature of many analysis
tasks, the analyst often has to go through many steps and even false
starts before reaching sound conclusions. Although analytic provenance
has been introduced as a research topic in VA, not much work has been
reported on recording interactions to support exploratory data analysis
for DR. A major task will be to compare and assess different DR
results in a sequence of interactions. For example, when switching
between different states, the resulting changes have to be observable
and measurable to automatically identify impactful actions within an
analysis session. Lehmann and Theisel provide a promising approach to
measure the (dis)similarity of projections [38]. However, more research
considering a larger set of DR types and interactions is needed.

as an add-on to our proposed scenarios. Considering quality measures
was a main concern when we began this study, but as the work matured
we decided to focus exclusively on interaction scenarios with DR.

8 CONCLUSION AND FUTURE WORK
Giving humans more interactive control over the DR process is a great
opportunity for improving exploratory data analysis.
It allows the
analyst to explore data, feature, parameter and model spaces, taking
advantage of their understanding of the data, application domain, and
experience in the analysis task at hand.

In this study, we systematically analyzed the visualization literature
with the goal of identifying common DR operations amendable to
interactive control. We summarized our ﬁndings in seven guiding
scenarios, which we contextualize in a conceptual process model for
visual interactive DR. Our analysis revealed several ways that DR can
be enriched by user interaction, how these strategies are supported
by current VA systems, and points to future research directions in
interactive DR. We hope that our contributions help other researchers
investigating, designing and evaluating interactive DR systems.

In future work, we plan to develop a system capable of inferring and
adapting its settings in a larger design space than current systems for
visual interactive DR. We plan to extend our analysis to papers from
related domains, such as machine learning and human-computer inter-
action. Beyond this, we would like to perform a literature analysis and
process modeling study focused on interactive clustering, classiﬁcation,
and regression analysis in VA.

ACKNOWLEDGMENTS
This paper is a result of Dagstuhl Seminar 15101, “Bridging Infor-
mation Visualization with Machine Learning”. The authors thank all
participants for inspiring and fruitful discussions. This work was par-
tially supported by the EU project VALCRI (FP7-SEC-2013-608142).

7 LIMITATIONS
Our work comes with certain limitations that result from the approach
we adopted. To keep the study focused and manageable, we had to
limit our literature analysis to a representative set of examples. After
many discussions, we decided to focus on the visualization literature.
Our goal was to identify papers that include DR, interaction and visu-
alization. We ﬁnd these mainly in the visualization community. We
primarily aimed at actionable and extensible results, and with that at
transparency and reproducibility by thoroughly describing our method.
Nevertheless, we are conﬁdent that we analyzed a representative sub-
set of the literature and that our derived model is stable regarding the
interaction scenarios. It would be interesting to evaluate the stabil-
ity of our results by performing an expanded “cross validation” study
that also includes/adds papers from machine learning (e.g., KDD) and
human-computer interaction (e.g., CHI). Note that we initially started
our analysis with landmark publications from all domains and had to
limit the set of papers to keep the work manageable.

In our analysis of the literature, we identiﬁed several contributions
that offer useful interactions to explore and validate DR results, without
directly feeding back to a DR calculation. We had long discussions
about including these interactions as another scenario, but ﬁnally de-
cided to exclude these papers to keep the work focused. An example is
the system proposed by Stahnke et al. [53] that provides interactions to
interpret and interrogate DR results. Their system allows an analyst to
investigate approximation errors, examine positions of data points, and
“overlay” the inﬂuence of speciﬁc data dimensions. However, these
interactions do not feed back to a subsequent DR calculation.

Similarly, other facets may be involved in interactive DR in speciﬁc,
and interactive machine learning in general. An important facet is
DR quality measures. A framework by Bertini et al. [5] describes an
enriched VA pipeline with quality-metric-driven automation. Quality
is measured at each stage of the pipeline, with the analyst steering the
entire process. Quality measures can augment user interaction at these
stages with automatic conﬁgurations or recommendations. However,
quality measures do not interact with the DR pipeline, and can be seen

2014.

[2] M. Beaudouin-Lafon. Designing interaction, not interfaces. In Conf. on

Advanced Visual Interfaces (AVI), pages 15–22. ACM, 2004.

REFERENCES
[1] E. Alpaydin. Introduction to Machine Learning, 3rd Edition. MIT Press,

[3] Y. Bengio, A. C. Courville, and P. Vincent. Representation learning: A
review and new perspectives. IEEE Trans. Pattern Anal. Mach. Intell.,
35(8):1798–1828, 2013.

[4] E. Bertini and D. Lalanne. Surveying the complementary role of automatic
data analysis and visualization in knowledge discovery. In Proceedings of
the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discov-
ery: Integrating Automated Analysis with Interactive Exploration, pages
12–20, 2009.

[5] E. Bertini, A. Tatu, and D. Keim. Quality metrics in high-dimensional
data visualization: An overview and systematization. IEEE Trans. on
Visualization and Computer Graphics, 17(12):2203–2212, Dec 2011.

[6] L. Bradel, C. North, L. House, and S. Leman. Multi-model semantic
interaction for text analytics. IEEE Conf. on Visual Analytics in Science
and Technology (VAST), pages 163–172, 2014.

[7] M. Brehmer and T. Munzner. A multi-level typology of abstract visu-
alization tasks. IEEE Trans. on Visualization and Computer Graphics,
19(12):2376–2385, 2013.

[8] E. T. Brown, J. Liu, C. E. Brodley, and R. Chang. Dis-function: Learning
distance functions interactively. IEEE Conf. on Visual Analytics in Science
and Technology (VAST), pages 83–92, 2012.

[9] A. Buja, D. Cook, and D. F. Swayne. Interactive high-dimensional data
visualization. Journal of Computational and Graphical Statistics, 5:78–99,
1996.

[10] S. K. Card, G. G. Robertson, and J. D. Mackinlay. The information
visualizer, an information workspace. ACM SIGCHI Conf. Human Factors
in Computing Systems (CHI), page 181186, 1991.

[11] K. Charmaz. Constructing grounded theory. Sage, 2014.
[12] J. Choo, S. Bohn, and H. Park. Two-stage framework for visualization
of clustered high dimensional data. IEEE Conf. on Visual Analytics in
Science and Technology (VAST), pages 67–74, 2009.

[13] J. Choo, H. Lee, J. Kihm, and H. Park.

iVisClassiﬁer: An interactive
visual analytics system for classiﬁcation based on supervised dimension

reduction. IEEE Conf. on Visual Analytics in Science and Technology
(VAST), pages 27–34, 2010.

[14] R. J. Crouser and R. Chang. An affordance-based framework for hu-
man computation and human-computer collaboration. IEEE Trans. on
Visualization and Computer Graphics, 18(12):2859–2868, 2012.

[15] D. Donoho. High-Dimensional Data Analysis: The Curse and Blessings
of Dimensionality. Aide-m´emoire for a lecture for the American Math.
Society “Math. Challenges of the 21st Century”, 2000.

[16] P. Drieger. Visual Text Analytics using Semantic Networks and Interactive
In K. Matkovic and G. Santucci, editors, EuroVA
3D Visualization.
2012: International Workshop on Visual Analytics. The Eurographics
Association, 2012.

[17] T. Dwyer, Y. Koren, and K. Marriott. Ipsep-cola: An incremental proce-
dure for separation constraint layout of graphs. IEEE Trans. on Visualiza-
tion and Computer Graphics, 12(5):821–828, 2006.

[18] A. Endert, R. Chang, C. North, and M. X. Zhou. Semantic interaction:
Coupling cognition and computation through usable interactive analytics.
IEEE Computer Graphics and Applications, 35(4):94–99, 2015.

[19] A. Endert, P. Fiaux, and C. North. Semantic interaction for visual text
analytics. ACM SIGCHI Conf. Human Factors in Computing Systems
(CHI), pages 473–482, 2012.

[20] A. Endert, C. Han, D. Maiti, L. House, S. Leman, and C. North.
Observation-level interaction with statistical models for visual analyt-
ics. IEEE Conf. on Visual Analytics in Science and Technology (VAST),
pages 121–130, 2011.

[21] A. Endert, M. S. Hossain, N. Ramakrishnan, C. North, P. Fiaux, and
C. Andrews. The human is the loop: new directions for visual analytics. J.
Intell. Inf. Syst., 43(3):411–435, 2014.

[22] S. Garg, I. V. Ramakrishnan, and K. Mueller. A visual analytics ap-
proach to model learning. IEEE Conf. on Visual Analytics in Science and
Technology (VAST), pages 67–74, 2010.

[23] C. Heine and G. Scheuermann. Manual clustering reﬁnement using inter-

action with blobs. Computer Graphics Forum, pages 59–66, 2007.

[24] P. E. Hoffman and G. G. Grinstein. Information visualization in data
mining and knowledge discovery. chapter A Survey of Visualizations for
High-dimensional Data Mining, pages 47–82. Morgan Kaufmann, San
Francisco, CA, USA, 2002.

[25] K. Hornbæk. Current practice in measuring usability: Challenges to
usability studies and research. International Journal of Man-Machine
Studies, 64(2):79–102, 2006.

[26] A. Hyv¨arinen, J. Karhunen, and E. Oja. Independent Component Analysis.

Wiley, 2001.

[27] S. Ingram, T. Munzner, V. Irvine, M. Tory, S. Bergner, and T. M¨oller.
Dimstiller: Workﬂows for dimensional analysis and reduction. IEEE Conf.
on Visual Analytics in Science and Technology (VAST), pages 3–10, 2010.
[28] T. Isenberg, P. Isenberg, J. Chen, M. Sedlmair, and T. M¨oller. A system-
atic review on the practice of evaluating visualization. IEEE Trans. on
Visualization and Computer Graphics, 19(12):2818–2827, 2013.

[29] D. J¨ackle, F. Fischer, T. Schreck, and D. A. Keim. Temporal MDS plots for
analysis of multivariate data. IEEE Trans. on Visualization and Computer
Graphics, 22(1):141–150, 2016.

[30] D. H. Jeong, C. Ziemkiewicz, B. D. Fisher, W. Ribarsky, and R. Chang.
iPCA: An interactive system for pca-based visual analytics. Computer
Graphics Forum, 28(3):767–774, 2009.

[31] S. Johansson and J. Johansson.

Interactive dimensionality reduction
through user-deﬁned combinations of quality metrics. IEEE Trans. on
Visualization and Computer Graphics, 15(6):993–1000, 2009.

[32] P. Joia, F. Petronetto, and L. G. Nonato. Uncovering representative groups
in multidimensional projections. Computer Graphics Forum, 34(3):281–
290, 2015.

[33] D. A. Keim. Information visualization and visual data mining. IEEE Trans.

on Visualization and Computer Graphics, 8(1):1–8, 2002.

[34] D. A. Keim, J. Kohlhammer, G. P. Ellis, and F. Mansmann. Mastering the
Information Age - Solving Problems with Visual Analytics. Eurographics
Association, 2010.

[35] H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and S. Carpendale. Empirical
studies in information visualization: Seven scenarios. IEEE Trans. on
Visualization and Computer Graphics, 18(9):1520–1536, 2012.

[36] J. Lee and M. Verleysen. Nonlinear dimensionality reduction. Springer,

2007.

[37] J. A. Lee, E. Renard, G. Bernard, P. Dupont, and M. Verleysen. Type 1
and 2 mixtures of Kullback-Leibler divergences as cost functions in di-
mensionality reduction based on similarity preservation. Neurocomputing,

112:92–108, 2013.

[38] D. J. Lehmann and H. Theisel. Optimal sets of projections of high-
dimensional data. IEEE Trans. on Visualization and Computer Graphics,
22(1):609–618, 2016.

[39] S. Liu, D. Maljovec, B. Wang, P.-T. Bremer, and V. Pascucci. Visualiz-
ing High-Dimensional Data: Advances in the Past Decade. Computer
Graphics Forum, 2015.

[40] S. Liu, B. Wang, P. Bremer, and V. Pascucci. Distortion-guided structure-
driven interactive exploration of high-dimensional data. Computer Graph-
ics Forum, 33(3):101–110, 2014.

[41] S. Liu, B. Wang, J. J. Thiagarajan, P. Bremer, and V. Pascucci. Visual ex-
ploration of high-dimensional data through subspace analysis and dynamic
projections. Computer Graphics Forum, 34(3):271–280, 2015.

[42] G. M. H. Mamani, F. M. Fatore, L. G. Nonato, and F. V. Paulovich.
User-driven feature space transformation. Computer Graphics Forum,
32(3):291–299, 2013.

[43] V. Molchanov and L. Linsen. Interactive Design of Multidimensional
Data Projection Layout. In N. Elmqvist, M. Hlawitschka, and J. Kennedy,
editors, EuroVis - Short Papers. The Eurographics Association, 2014.

[44] J. E. Nam and K. Mueller. Tripadvisorn-d: A tourism-inspired high-
dimensional space exploration framework with overview and detail. IEEE
Trans. on Visualization and Computer Graphics, 19(2):291–305, 2013.

[45] A. Oulasvirta and K. Hornbæk. Hci research as problem-solving. In ACM

SIGCHI Conf. Human Factors in Computing Systems (CHI).

[46] B. Rieck and H. Leitte. Persistent homology for the evaluation of dimen-
sionality reduction schemes. Computer Graphics Forum, 34(3):431–440,
2015.

[47] D. Sacha, M. Sedlmair, L. Zhang, J. A. Lee, D. Weiskopf, S. C. North,
and D. A. Keim. Human-Centered Machine Learning Through Interac-
tive Visualization: Review and Open Challenges. Proceedings of the
24th European Symposium on Artiﬁcial Neural Networks, Computational
Intelligence and Machine Learning, 2016.

[48] D. Sacha, A. Stoffel, F. Stoffel, B. C. Kwon, G. P. Ellis, and D. A. Keim.
Knowledge generation model for visual analytics. IEEE Trans. on Visual-
ization and Computer Graphics, 20(12):1604–1613, 2014.

[49] T. Schreck, J. Bernard, T. Tekusova, and J. Kohlhammer. Visual cluster
analysis of trajectory data with interactive kohonen maps. IEEE Conf. on
Visual Analytics in Science and Technology (VAST), pages 3–10, 2008.

[50] M. Sedlmair, M. Brehmer, S. Ingram, and T. Munzner. Dimensionality
reduction in the wild: Gaps and guidance. Technical Report TR-2012-03,
Dept. of Computer Science, University of British Columbia, 2012.

[51] M. Sedlmair, C. Heinzl, S. Bruckner, H. Piringer, and T. Moller. Visual
IEEE Trans. on

parameter space analysis: A conceptual framework.
Visualization and Computer Graphics, 20(12):2161–2170, 2014.

[52] M. Sedlmair, T. Munzner, and M. Tory. Empirical guidance on scatterplot
and dimension reduction technique choices. IEEE Trans. on Visualization
and Computer Graphics, 19(12):2634–2643, 2013.

[53] J. Stahnke, M. D¨ork, B. M¨uller, and A. Thom. Probing projections: Inter-
action techniques for interpreting arrangements and errors of dimension-
ality reductions. IEEE Trans. on Visualization and Computer Graphics,
22(1):629–638, 2016.

[54] L. van der Maaten, E. Postma, and H. van den Herik. Dimensionality
reduction: A comparative review. Technical report, Tilburg Centre for
Creative Computing, Tilburg University, 2009.

[55] J. Venna, J. Peltonen, K. Nybo, H. Aidos, and S. Kaski. Information
retrieval perspective to nonlinear dimensionality reduction for data visual-
ization. Journal of Machine Learning Research, 11:451–490, 2010.

[56] T. von Landesberger, S. Fiebig, S. Bremm, A. Kuijper, and D. W. Fellner.
Interaction taxonomy for tracking of user actions in visual analytics ap-
plications. In Handbook of Human Centric Visualization, pages 653–670.
Springer, New York, 2014.

[57] A. Wism¨uller, M. Verleysen, M. Aupetit, and J. A. Lee. Recent advances
in nonlinear dimensionality reduction, manifold and topological learn-
ing. In ESANN 2010, 18th European Symposium on Artiﬁcial Neural
Networks,Bruges, Belgium, April 28-30, 2010, Proceedings, 2010.

[58] J. S. Yi, Y.-a. Kang, J. T. Stasko, and J. A. Jacko. Toward a deeper
understanding of the role of interaction in information visualization. IEEE
Trans. on Visualization and Computer Graphics, 13(6):1224–1231, 2007.
[59] B. Yu, R. Liu, and X. Yuan. MLMD: Multi-Layered Visualization for
Multi-Dimensional Data. In M. Hlawitschka and T. Weinkauf, editors,
EuroVis - Short Papers. The Eurographics Association, 2013.

",False,2017.0,{},False,False,journalArticle,False,PN7N37WR,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7536217/,,Visual Interaction with Dimensionality Reduction: A Structured Literature Analysis,PN7N37WR,False,False
CJAT82Y9,IN4SMR4K,"See	discussions,	stats,	and	author	profiles	for	this	publication	at:	https://www.researchgate.net/publication/6451414

Value	and	Relation	Display:	Interactive	Visual
Exploration	of	Large	Data	Sets	with	Hundreds	of
Dimensions

Article		in		IEEE	Transactions	on	Visualization	and	Computer	Graphics	·	May	2007

DOI:	10.1109/TVCG.2007.1010	·	Source:	PubMed

CITATIONS
37

5	authors,	including:

READS
78

Matthew	Ward
Worcester	Polytechnic	Institute

133	PUBLICATIONS			3,082	CITATIONS			

Elke	Rundensteiner
Worcester	Polytechnic	Institute

567	PUBLICATIONS			7,531	CITATIONS			

SEE	PROFILE

SEE	PROFILE

All	content	following	this	page	was	uploaded	by	Matthew	Ward	on	12	March	2014.

The	user	has	requested	enhancement	of	the	downloaded	file.

Value and Relation Display: Interactive Visual
Exploration of Large Datasets with Hundreds of

Dimensions

1

Jing Yang

Dept of Computer Science

UNC Charlotte

jyang13@uncc.edu

Daniel Hubball
Dept of Computer Science
University of Wales Swansea

csdan@swansea.ac.uk

Matthew Ward
Dept of Computer Science

Worcester Polytechnic Institute

matt@cs.wpi.edu

Elke Rundensteiner
Dept of Computer Science

Worcester Polytechnic Institute

rundenst@cs.wpi.edu

William Ribarsky
Dept of Computer Science

UNC Charlotte

ribarsky@uncc.edu

Abstract— Few existing visualization systems can handle large
datasets with hundreds of dimensions, since high dimensional
datasets cause clutter on the display and large response time in
interactive exploration. In this paper, we present a signiﬁcantly
improved multi-dimensional visualization approach named Value
and Relation (VaR) display that allows users to effectively and
efﬁciently explore large datasets with several hundred dimen-
sions. In the VaR display, data values and dimension relationships
are explicitly visualized in the same display by using dimension
glyphs to explicitly represent values in dimensions and glyph
layout to explicitly convey dimension relationships. In particular,
pixel-oriented techniques and density-based scatterplots are used
to create dimension glyphs to convey values. Multi-dimensional
scaling, Jigsaw map hierarchy visualization techniques, and
an animation metaphor named Rainfall are used to convey
relationships among dimensions. A rich set of interaction tools
have been provided to allow users to interactively detect patterns
of interest in the VaR display. A prototype of the VaR display
has been fully implemented. The case studies presented in this
paper show how the prototype supports interactive exploration of
datasets of several hundred dimensions. A user study evaluating
the prototype is also reported in this paper.

Index Terms— Multi-dimensional visualization, high dimen-

sional datasets, visual analytics.

datasets in the Information Visualization ﬁeld. They include:
(cid:129) Using condensed displays to provide as much information
as possible to users. Typical approaches include pixel-
oriented techniques [12], [13] and density-based displays
[9], [24]. For example, in pixel-oriented techniques, infor-
mation is so condensed that each pixel presents a single
data value.

(cid:129) Examining relationships among dimensions to discover
lower dimensional spaces with signiﬁcant features. Ex-
ample approaches include ranking low dimensional pro-
jections by their features such as linear relationships [19],
and placing dimensions in a layout revealing their rela-
tionships to help users construct meaningful subspaces
[28].

(cid:129) Providing a rich set of interactions to allow users to
explore datasets from multiple coordinated views. In
these views, different subsets of dimensions and/or data
items can be examined at different levels of detail us-
ing different visualization techniques. Examples of such
approaches include the Hierarchical Parallel Coordinates
[10] and the VIS-5D system [11].

I. INTRODUCTION

Large datasets with hundreds of dimensions are common
in applications such as image analysis, ﬁnance, bioinformatics
and anti-terrorism. For example, in order to detect the semantic
contents of large image collections, it is common to analyze
hundreds of low level visual attributes of the images. It is
a challenge to make decisions based on these datasets, since
they are hard to analyze due to the dimensionality curse [5],
i.e., the lack of data separation in high dimensional space.
Using multi-dimensional visualization techniques to present
this data to analysts and allowing them to interactively explore
and understand the datasets are an important approach to
addressing this challenge. However, most traditional multi-
dimensional visualization techniques suffer from visual clutter
and only scale up to tens of dimensions. Up to now, few multi-
dimensional visualization systems have claimed to be scalable
to datasets with hundreds of dimensions. In this paper, we
present such a system, called the Value and Relation (VaR)
display, which is an improved version of a technique reported
in an earlier paper [27].

Our work is based on multiple concepts proposed and
explored in prior efforts toward visual exploration of large

The concepts above are signiﬁcant features of the VaR
display since its initial version [27]. In the ﬁrst version (see
ﬁgures 1a and b), pixel-oriented displays were used to show
data values and group them into dimension glyphs representing
individual dimensions. The dimension glyphs were then po-
sitioned on the screen using a fast Multi-dimensional scaling
(MDS) algorithm [4] according to dimension correlations to
reveal their inter-relationships (dimension correlation is used
since it is a typical measure of dimension relationships, but
other relationship measures can also be used). A rich set
of interactions were provided to facilitate navigation in the
display and generate lower dimensional spaces of interest. To
differentiate the ﬁrst version from the improved version, we
call it the Pixel MDS VaR display.

In the improved version of VaR presented in this paper,
these features are signiﬁcantly strengthened. A density-based
scatterplot [9], [24] has been added to the system as an alter-
nate approach to generating dimension glyphs. A Jigsaw map
layout [23] and the Rainfall metaphor have been added into the
system as alternate dimension glyph layout approaches. The
new version also supports a broader range of interaction tools
than the original version, including a new data item selection

2

Fig. 1.
(a) Illustration of the VaR display. On the left is the spreadsheet of a 4-dimensional dataset with each column representing a dimension. On the
bottom is a matrix that records the pair-wise relationships (such as correlations) among the dimensions. In the middle is the glyph of the fourth dimension.
On the right is the VaR display of the dataset. (b) The Pixel MDS VaR display of the Image-89 dataset (89 dimensions, 10,417 data itmes). (c) The X-Ray
scatterplot MDS VaR display of the same dataset.

and highlighting tool. The labeling issue, which was ignored in
the initial version, is addressed in this version. A case study is
included in this paper involving the visual analysis of a dataset
with 838 dimensions. A user study comparing the VaR display
with the rank-by-feature framework [19], [20] is also reported.
This paper is organized as follows. Section II reviews
related work. Section III brieﬂy introduces the original Pixel
MDS VaR display. Section IV presents the approach of us-
ing density-based scatterplots to generate dimension glyphs.
Section V describes the new Jigsaw and Rainfall dimension
glyph layout strategies. Section VI summarizes the correlation
calculation algorithm used in the VaR display. Section VII
presents the interaction tools. Section VIII addresses the
labeling issue. Section IX describes the implementation of
the VaR display and addresses the scalability issue.Section X
discusses visual exploration approaches with the VaR display.
Section XI presents a case study and Section XII presents
a user study for the VaR display. Section XIII presents our
conclusions and future work.

II. RELATED WORK

Many techniques exist for generating condensed displays
for large datasets. The work most related to our work is pixel-
oriented techniques and scatterplots. Pixel-oriented visualiza-
tion techniques [12], [13] are a family of multi-dimensional
display techniques that map each data value to a pixel on
the screen and arrange the pixels into subwindows to convey
relationships. The patterns of the subwindows may reveal
clusters, trends, and anomalies. Pixel-oriented techniques are
one among several options to create the dimension glyph in
the VaR display.

Scatterplots visualize 2-D datasets or 2-D projections of
multi-dimensional datasets. In a scatterplot, there is a hori-
zontal axis and a vertical axis, which are associated with two
dimensions (X and Y). The data items are plotted onto the
display according to their coordinates on X and Y. Scatterplots
are widely used since they provide rich information about the

relationship between two dimensions, such as strength, shape
(line, curve, etc), direction (positive or negative), and presence
of outliers [18]. Density-based scatterplots [24], [9] scale to
large datasets by using intensity of the spot in a scatterplot
to indicate the data density in that spot. We use the density-
based scatterplot as an option for generating the dimension
glyph and treat the areas with no data items in a scatterplot in
a different way from existing approaches due to the possible
overlaps among the scatterplots.

Scatterplots of multi-dimensional datasets are often orga-
nized together to show multiple 2D projections of the datasets.
Scatterplot matrices [7] organize the scatterplots of all N x (N-
1)/2 2-D projections of an N-dimensional dataset into a matrix.
Scatterplot matrices easily get cluttered when the number of
dimensions increases. Rather than displaying all 2D projec-
tions, we display N scatterplots between all dimensions and
a focus dimension and position them in a manner conveying
dimension relationships in our density-based scatterplot VaR
option.

There exist multiple visualization approaches to examining
relationships among dimensions to discover lower dimen-
sional spaces with signiﬁcant features. The rank-by-feature
framework [19] ranks 1D or 2D axis-parallel projections of
multi-dimensional datasets using statistical analysis to help
users detect 1D or 2D projections with desired features such
as linearly related dimensions. [16] visualizes correlations
between each pair of dimensions in a matrix and allows users
to interactively select dimensions from the matrix to con-
struct lower dimensional spaces. The interactive hierarchical
dimension reduction approach [28] visually conveys dimension
relationships using a dimension hierarchy to facilitate lower
dimensional space construction. The VaR display is different
from these approaches since it integrates data value visual-
ization with dimension relationship visualization in the same
display to use screen space more efﬁciently.

Multi-dimensional Scaling (MDS) [4], [15] is an itera-
tive non-linear optimization algorithm for projecting multi-

dimensional data down to a reduced number of dimensions.
It is often used to convey relationships among data items
of a multi-dimensional dataset. For example, IN-SPIRE [25]
uses MDS to map data items from a document dataset to
a 2D space. It generates a Galaxies display as a spatial
representation of relationships within the document collection.
In our approach, MDS is used in a different way, namely to
convey relationships among dimensions rather than data items.
The Jigsaw map [23] is a recent space ﬁlling hierarchy
layout method. By placing the leaf nodes of a hierarchy into
a 1D layout using a depth ﬁrst traversal and mapping the
1D layout
into a rectangular 2D mesh using space-ﬁlling
curves, this method creates hierarchy displays of nicely shaped
regions, good continuity and stability. When all leaf nodes are
of the same size, a Jigsaw map can draw all leaf nodes without
any distortion in shape, namely, they can be all equal-sized
squares. This property of the Jigsaw map makes it a perfect
option for us to lay out dimensions organized into a hierarchy
on a 2D mesh, with each dimension drawn as a square glyph.
The similarity based dimension arrangement proposed in
[1] also addressed the problem of arranging pixel oriented
subwindows (dimensions) on a 2D mesh. It aimed to place
similar dimensions close to each other on the 2D mesh. The
Jigsaw map dimension layout is different in that it aims to
use the dimension layout to convey the hierarchical structure
among the dimensions. As a consequence, not only similar
dimensions but also outlier dimensions are revealed.

[29] presents a multi-dimensional visualization technique
called Dust & Magnet. It represents dimensions as magnets
and data items as dust particles and attracts dust particles using
magnets to reveal data item values in the dimensions. The
Rainfall metaphor proposed in this paper was inspired by Dust
& Magnet. The difference is that the Rainfall metaphor attracts
dimensions using dimensions, while Dust & Magnet attracts
data items using dimensions.

III. PIXEL MDS VAR DISPLAY

Figure 1a illustrates the approach to generating a Pixel
MDS VaR display. First, a dimension glyph, called a glyph in
short, is generated to represent data values in each dimension,
i.e., values in the same column in the spreadsheet, using
pixel oriented techniques [13]. In particular, each value is
represented by a pixel whose color indicates a high or low
value, and pixels representing values from the same dimension
are grouped together to form a glyph. In a glyph, each pixel
occupies a unique position without overlap. In the original
version, a spiral pixel layout was used. Rows in the spreadsheet
are ordered according to their values in one dimension (Note:
actually any 1D order can be used). Data values in each
column are positioned into a spiral according to this order. In
all glyphs, pixels representing values in the same row occupy
the same position so that glyphs can be associated with each
other.

Second, the correlations among the dimensions are cal-
culated and recorded into an N x N matrix (where N is
the dimensionality of the dataset). In order to calculate the
correlations, different approaches can be used according to

3

different purposes. For example, if users are most interested
in linearly related dimensions, Pearson’s correlation coefﬁ-
cient can be used to capture the linear relationships among
dimensions. We proposed a scalable and ﬂexible correlation
calculation algorithm [27] and applied it in the VaR display.
We will brieﬂy introduce it in Section VI for the purpose of
completeness.

Third, the N x N relationship matrix is used to generate N
positions in a 2-D space, one position for each dimension. The
proximity among the positions reﬂects relationships among
the dimensions, i.e., closely related dimensions are spatially
close to each other, and unrelated dimensions are positioned
far way from each other. In particular, a multi-dimensional
scaling algorithm [4] is used to create the 2-D positions upon
the relationship matrix.

Finally, the dimension glyphs are placed in the 2-D space
in their corresponding positions to form the VaR display.
Figure 1b shows an example of the VaR display. It shows
the Image-89 dataset of 89 dimensions and 10,417 data items.
It is a real dataset containing 88 low level visual attributes
and classiﬁcation information for 10,417 image segments
generated by an image analysis approach [8]. In the ﬁgure,
each block is a dimension glyph and there are 89 glyphs.
In each glyph, data values of the dimension are mapped to
colors of pixels, and pixels are ordered in a spiral manner. The
closeness of the glyph positions reveals the correlations among
the dimensions calculated by the underlying algorithm. For
example, several clusters of closely correlated dimensions and
a few dimensions that are distinct from most other dimensions
can be detected from the glyph positions in Figure 1b.

The above approach can be summarized as dimension glyph
generation and layout. Glyphs explicitly convey data values
and their layout explicitly conveys dimension relationships.
Moreover, dimension relationships are also revealed by the
patterns of the glyphs. Similarity among glyph patterns in-
dicates dimension relationships, whether there is a linear or
non-linear relationship, or they are partially correlated (such
as dimensions for which a subset of the data items is closely
related). Since humans are good at pattern recognition, the
patterns of the glyphs provide straightforward and intuitive
comparison of the dimensions. On the one hand, the layout
approach brings related dimensions close to each other to make
the pattern comparison easier. On the other hand, the patterns
allow users to conﬁrm or refute the relationships suggested by
the layout using their eyes, and reveal how the dimensions are
related in detail.

Besides the techniques used in the original VaR display,
there are other approaches to creating glyphs and laying them
out, which will be introduced in the following sections. Since
glyph generation and layout are independent from each other,
they can be combined freely to form various VaR displays.

IV. DIMENSION GLYPH ALTERNATIVE: X-RAY

SCATTERPLOTS

The glyph generation approach used in the original VaR
display is not the only approach for creating dimension glyphs.
For example, different layouts of the pixels within a glyph

reveal different patterns. As an example, organizing pixels
into a calendar pattern according to the time stamps of the
data items can reveal time-dependant patterns among the data
items. Since these techniques have been widely studied in
pixel-oriented techniques [12] and they can be integrated
into the VaR display easily by replacing the original pixel-
oriented dimension glyph generation approach, they will not
be discussed in this paper. Instead, we present our work
on customizing a density-based scatterplot glyph (called an
X-Ray glyph) generation approach. This approach has been
introduced into the improved version (see Figure 1c for an
example VaR display using the scatterplot approach).

In the VaR display, a scatterplot

is generated for each
dimension. The Y dimension of a scatterplot dimension glyph
is the dimension it represents, while all of the glyphs have the
same X dimension. We choose to use the same X dimension
since it will be hard for users to associate different dimension
glyphs if both X and Y dimensions change from one glyph
to another. Although this causes information loss, users can
always interactively change the X dimension guided by the
semi-automatic selection tool (see Section VII) and their visual
exploration (see Section XI).

The VaR display is targeted at large datasets. It is time
consuming to draw the projection of each data item on each
of the N scatterplots. Also, the large number of projections
would clutter the glyph. In order to avoid clutter and increase
scalability, we store each glyph as an M X M pixel matrix,
where M is an adjustable integer, and divide the 2D space
within the value range of the dataset into M X M equal-
size bins. The number of projections falling into each bin
is recorded and translated into the color of its corresponding
pixel in the pixel matrix. In particular, the intensity of the pixel
is proportional to data density of the area it represents.

Fig. 2. X-Ray Scatterplots (a) The ﬁrst solution (b) The second solution (c)
The X-Ray scatterplot solution.

The ﬁrst image (Figure 2a) we generated is disappointing,
since it is hard to differentiate unoccupied area (areas with
zero data items) from areas with a few data items. In order
to solve this problem, we assign a different hue to the pixels
representing unoccupied areas. In the image generated (Figure
2b), there are no data items in the blue area. We then observed
that,
to glyphs generated using pixel-oriented
techniques where every pixel represents a data value, there
are often large contiguous unoccupied areas in a scatterplot
glyph, especially when the X and Y dimensions are closely
related. Recalling that some glyph layout approaches, such

in contrast

4

as MDS, could cause overlaps among different glyphs, we
made the unoccupied areas semi-transparent so that users can
see hidden glyphs through the unoccupied areas of the hiding
glyphs. Figure 2c shows this ﬁnal solution. Since in the ﬁgure
the glyphs look very much like X-Ray photos, we named
this VaR display the X-Ray scatterplot VaR display. To give
users more ﬂexibility, we allow them to interactively choose
the color and transparency of the unoccupied areas. If users
dislike the semi-transparent unoccupied areas, they are able to
set them to opaque.

V. DIMENSION LAYOUT ALTERNATIVES: JIGSAW MAP

LAYOUT AND RAINFALL

A. Jigsaw Map Glyph Layout

The MDS approach is effective in conveying dimension
relationships. However, using the MDS approach, the positions
of two glyphs could be very close to each other if they are
closely related. Glyphs might overlap in this case, which is
sometimes undesired by the users. Besides allowing the users
to reduce overlaps in the MDS layout using interactions (see
Section VII), we propose a Jigsaw Map dimension layout
based on the recently proposed Jigsaw map [23]. In this
approach, dimensions are grouped into a dimension hierarchy.
The Jigsaw map, which is a space-ﬁlling hierarchy visualiza-
tion method, is then used to lay the dimensions on a grid.
This approach not only prevents glyphs from overlapping, but
also conveys the hierarchical structure among the dimensions.
Figure 3 shows VaR displays with a Jigsaw layout.

The motivation of this approach is that grouping dimensions
of high dimensional datasets into dimension hierarchies makes
it easy to capture the relationships among the dimensions.
In a dimension hierarchy, dimensions are organized into a
hierarchy of clusters. Dimensions within a cluster have closer
relationships among each other than with dimensions outside
the cluster. Clusters in different levels of the hierarchy divide
the dimensions into groups of different granularity. With the
dimension relationship matrix, it is convenient to generate
a dimension hierarchy using existing hierarchical clustering
approaches. In the hierarchy, each leaf node is a dimension in
the high dimensional dataset.

In order to turn the dimension hierarchy into the dimension
layout, we examined existing hierarchy visualization tech-
niques. The basic requirements are 1) the layout should be
space efﬁcient since our target is high dimensional datasets
and 2) each dimension should be assigned a space of the
same size, shape and orientation since it is difﬁcult for users
to compare and associate glyphs with different sizes, shapes,
or orientations. Since node-linked diagrams do not use space
efﬁciently, we only considered the space-ﬁlling hierarchy
visualization techniques [3], [21], [23]. Among them, only
the Jigsaw map [23] and quantum treemaps [3] are capable
since all other techniques assign areas of different shapes or
orientations to leaf nodes. We chose the Jigsaw map since it
generates layouts of nicely shaped regions and is stable with
regards to changing tree structures and leaf nodes [23].

To generate the Jigsaw map layout, we ﬁrst hierarchically
cluster the N dimensions in a dataset based on their pair-
wise distances (a pair of more closely related dimensions

5

Fig. 3. The Image-838 dataset (838 dimensions, 11,413 data items). (a) The Pixel Jigsaw map VaR display with separated dimensions selected and labeled
(b) The X-Ray scatterplot Jigsaw map VaR display with dimension Coarseness as the X dimension. The X dimension is in a pink frame and labeled. (c)
The X-Ray scatterplot Jigsaw map VaR display with dimension angle 135 as the X dimension. The X dimension is at the left bottom corner of the map and
dimensions closely related to it are in red frames. (d) A zoomed in display of the selected dimensions with their labels shown.

has a smaller distance than a pair of less related dimensions)
using the minimum single linkage metrics [17]. Then, the N
dimensions are ordered into a 1-D sequence according to their
positions in the hierarchy using a depth-ﬁrst traversal of the
hierarchy, and then the sequence is mapped to a 2-D L x K (
L x K >= N) mesh by applying a space-ﬁlling curve called
an H curve (please refer to [23] for more details). Figure 3a
shows an example of the Jigsaw layout. In this ﬁgure, similar
dimensions are close to each other and signiﬁcant boundaries
of groups of closely related dimensions, such as the group of

dimensions in the left bottom part of the map, can be detected.
Outlier dimensions, such as the dimensions on the left top part
of the map, are also distinguishable since their textures look
different from their neighbors.

B. Rainfall Metaphor

When exploring a high dimensional dataset, users are often
interested in the relationships between a single dimension of
interest with all other dimensions. Beside the X-Ray scatter-
plot, which reveals the relationships using glyph textures, we

6

Fig. 4. The Rainfall Metaphor. (a) At the beginning of the rain. Dimensions more closely related to the dimension of interest in the bottom are falling in a
faster acceleration than less related dimensions. (b) The rain continues. The dimensions with different correlations to the dimension of interest are separated.
It can be seen that there are roughly three levels of association between the dimension of interest and other dimensions. (c) The Rain is close to its end.
Dimensions signiﬁcantly distinct from the dimension of interest are revealed. The dataset is the Image-89 dataset. The glyphs are pixel-oriented glyphs (pixels
are ordered in a line by line (horizontal lines) manner.

provide a simple animation approach to dynamically illustrate
the relationships by changing glyph positions. This approach is
named the Rainfall Metaphor since it imitates rain (see Figure
4 for an example).

In the beginning of the animation, the dimension of interest
is placed in the center bottom of the display (the ground) and
all other dimensions (raindrops) are placed in the top of the
display (the sky). The horizontal positions of the raindrops
are randomly generated. After the rain starts, a raindrop falls
toward the ground in an acceleration that is proportional to
its correlation with the dimension of interest. Thus, a raindrop
moves toward the ground faster than another raindrop if it has
a closer relationship to the dimension of interest. A raindrop
stops its movement after it hits the ground. There is a timer
that starts from the beginning of the rain and ends when
all raindrops hit the ground. Users can interactively play the
animation by moving the slider representing the timer. Users
can also interactively select the dimension of interest for the
animation.

Figure 4a-c shows some screen captures of the Rainfall
layout. Using this metaphor, users can focus on the relation-
ships between the dimension of interest and other dimensions,
without being distracted by relationships among the other
dimensions. In different moments of the rain, either similar
dimensions or distinct dimensions to the dimension on the
ground attract the users’ attention.

VI. CORRELATION CALCULATION

In the VaR display, a binning based correlation calculation
algorithm is used. We only brieﬂy introduce it here since
it has been presented in full detail in [27]. We claim that
any relationship calculation algorithm can be used in the VaR
display as long as it scales to large datasets. The layout of
the glyphs reﬂects the type of relationship calculated by the
underlying algorithm.

In our algorithm, distribution of the value differences (be-
tween the different dimensions for the same data item) is
recorded into bins. In particular, the possible range of value
differences between a pair of dimensions is divided into a
sequence of bins. The number of data items whose value
differences between these two dimensions fall into the bins is
recorded. For an N dimensional dataset, N x (N-1)/2 sequences
of bins (one sequence for each pair of dimensions) are created.
A pair of dimensions is considered to be closely related if
a large number of data items fall info a small number of
bins (K) in its sequence. With a given K, the correlations
can be calculated in this way: sort the bins in the sequence
according to their populations, and sum up the populations
of K bins with the highest populations. The sum divided by
the total population of the data items is proportional to the
correlation between the dimensions. K is selected to be the
number of bins that make the global variance of correlations
for all dimensions maximum. This algorithm scales to a large
number of data items. Except for the ﬁrst scan, which can
be done with minimal cost when inserting the dataset into
the database, its efﬁciency is only related to the number of
dimensions.

The above algorithm is a heuristic approach whose purpose
is to maximize the visibility of the structure of the MDS and
Jigsaw layout. There are many other optimization problems
in the VaR display, such as selecting a dimension ordering
the pixel-oriented display in the initial view to provide the
maximum information to users at a ﬁrst glance. A detailed
discussion of such problems is presented in [27] and not
repeated here.

VII. INTERACTIVE TOOLS IN THE VAR DISPLAY

A rich set of interaction tools has been developed for the
VaR display. Navigation tools help users reduce clutter in the
display and discover information about the dataset. Automatic
and manual dimension selection tools allow users to perform

human-driven dimension reduction by selecting subsets of
dimensions for further exploration in the VaR display as well
as other multi-dimensional visualizations. Data item selection
tools allow users to select subsets of data items for further
exploration. In addition, the data item masking tool allows
users to examine details of selected data items within the
context of unselected data items.

Most of the interactive tools make no special assumption
about the glyph positioning and generation strategies, i.e., they
can be applied to any realization of the VaR display. These
tools are called general tools. Unless speciﬁcally noted, an in-
teraction tool is a general tool in the following sections, where
details of each navigation and selection tool are presented.

A. Tools for Glyph Layout

The MDS dimension layout causes overlaps among the
glyphs. Overlaps emphasize close relationships among the
dimensions because glyphs overlap only if their dimensions
are closely related. However, overlaps can prevent a user
from seeing details of an overlapped glyph. We provide the
following operations to overcome this problem (see [27] for
more detail).

(cid:129) Showing Names: By putting the cursor on the VaR
display, the dimension names of all glyphs under the
cursor position are shown in a message bar. Thus a user
can be aware of the existence of glyphs hidden by other
glyphs.

(cid:129) Layer Reordering: With a mouse click, a user can force
a glyph to be displayed in front of the others. In this
way he/she can view details of a glyph that is originally
overlapped. Users can also randomly change the ordering
of all dimension glyphs by clicking a button in the control
frame. In addition, selected dimensions are automatically
brought to the front of the display.

(cid:129) Manual Relocation: By holding the control key, a user
can drag and drop a glyph to whatever position he/she
likes. In this way a user can separate overlapping glyphs.
(cid:129) Extent Scaling: Extent scaling allows a user to interac-
tively decrease the sizes of all the glyphs proportionally to
reduce overlaps, or to increase them to see larger glyphs.
(cid:129) Dynamic Masking: Dynamic masking allows users to
hide the glyphs of unselected dimensions from the VaR
display.

(cid:129) Automatic Shifting: This operation automatically re-
duces the overlaps among the glyphs by slightly shift-
ing the positions of the glyphs. There are many more
advanced overlap reducing algorithms that can be used,
such as those listed in [22].

(cid:129) Distortion: Users can interactively enlarge the size of
some glyphs while keeping the size of all other glyphs
ﬁxed. In this way users are allowed to examine details
of patterns in the enlarged glyphs within the context
provided by the other glyphs.

(cid:129) Zooming and Panning: Users can zoom in, zoom out
and pan the VaR display. For example, in order to reduce
overlaps, sometimes the size of the glyphs has to be set
very small when there are a large number of dimensions.

7

Zooming into the display will enlarge the glyphs so that
the user can have a clear view of the patterns in the
glyphs.

(cid:129) Reﬁning: A reﬁned VaR display can be generated for
a selected subset of dimensions and a selected subset
of data items. The selected dimensions and data items
are treated as a new dataset. The relationship calculation,
glyph generation and positioning are applied to the new
dataset.

B. Tools for Glyph Regeneration

In the Pixel-Oriented dimension glyphs, the dimension used
to sort the data items affects the glyph patterns signiﬁcantly.
Clusters in subspaces including this dimension can be easily
detected while clusters in other subspaces are not. Similarly, in
the X-Ray scatterplot dimension glyphs, relationships between
other dimensions and the X dimension are easier to detect
than relationships among other dimensions. We allow users to
interactively select the sorting dimension in the pixel-oriented
mode and the X dimension in the X-Ray scatterplot mode
by clicking the mouse button on the glyph of the desired
dimension or selecting from a combo-box.

In addition, a comparing mode can be used in the pixel-
oriented glyphs in order to compare the dimensions with a
dimension of interest. In this mode, except the glyph of the
base dimension, the pixels of all other glyphs will be colored
according to the differences between the values of the base
dimension and their dimensions. A ﬁgure of the comparing
mode can be found in [27].

C. Dimension Selection Tools

Dimension selection tools enable users to select dimen-
sions of interest for further exploration using other multi-
dimensional visualization techniques. They can also be used
as a ﬁlter to reduce the number of glyphs displayed in a VaR
display, since we allow users to hide glyphs of unselected
dimensions using dynamic masking (see Section VII-A). The
selection tools we provide to users include automatic selec-
tion tools for closely related dimensions and well separated
dimensions, in addition to manual selection.

The automatic selection tool for related dimensions
takes a user-assigned dimension and correlation threshold as
input. Here we assume that a pair of more closely related
dimensions has a larger correlation measure than a pair of
less related dimensions. Users pick the assigned dimension by
clicking its glyph and adjust the threshold through a slider. The
tool automatically selects all dimensions whose correlation
measures to the input dimension are larger than the threshold
by traversing the dimension relationship matrix. This tool
enables the users to select a set of closely related dimensions.
The automatic selection tool for separated dimensions
takes a user-assigned dimension and correlation threshold
as input and returns a set of dimensions that describe the
major features of the dataset. The assigned dimension will
be included in the returned set of dimensions. Between each
pair of dimensions in the result set, the correlation measure is
smaller than the threshold. For any dimension that is not in

8

Fig. 5. Masking of Unselected Data Items. Unselected data items are covered by a mask with adjustable color and transparency. (a) No mask or fully
transparent mask. (b) Opaque mask. (b) Semi-transparent mask. The dataset is the Image-89 dataset. The glyphs are pixel-oriented glyphs (pixels are ordered
in a line by line (vertical lines) manner.

the result set, there is at least one dimension in the result set
whose correlation measure with it is larger than the threshold.
Using this tool, a user is able to select a set of dimensions
to construct a lower dimensional subspace revealing the major
features of the dataset without much redundancy. In Figure 1b
separated dimensions selected automatically are labeled.

The following algorithm can be used for automatic selection

of separated dimensions:

1) Get the assigned dimension and the selection threshold.
2) Set the assigned dimension as “selected” and all other

dimensions as “unselected”.

3) Find all unselected dimensions whose correlation mea-
sures to all existing selected dimensions are smaller than
the threshold. Mark them as “candidates”.

4) If there is no candidate dimension, go to step 5. Else, set
one candidate dimension as “selected” and every other
candidates as “unselected”. Go back to step 3.

5) Return all dimensions marked as “selected”.
It is interesting that it is not deﬁned how to pick one dimen-
sion among the candidate dimensions in step 4. Thus it can
be customized according to the task of interest. For example,
in Section VIII, this approach is customized to reduce the
clutter among the labels of the selected dimensions for a good
labeling result. Here we present another customization.

When users start to explore an unknown dataset, it is often
desired to ﬁnd dimension groups containing large numbers of
closely related dimensions. Thus a heuristic approach can be
used in step 4: setting a threshold, for each candidate dimen-
sion counting the number of dimensions having correlation
measures to it that are larger than the threshold, and selecting
the dimension with the highest count. Using this approach
dimensions with a larger number of closely related dimensions
have higher priority to be selected.

Manual selection allows a user to manually select a dimen-
sion by clicking its corresponding glyph. The user can unselect
the dimension by clicking the glyph again. The combination of
manual and automatic selection makes the selection operation

both ﬂexible and easy to use.

D. Data Item Selection and Masking Tools

Rather than allowing a user to select data items directly from
the glyphs in the VaR display (which is hard when glyphs
are small), we allow the user to select data items from a
dialog. Firstly, the user selects a dimension name from a name
list in the dialog. Then a brief summary of the dimensions
will be provided to help the user set up the selection criteria
for the selected dimension. If the dimension is a categorical
dimension, the distinct values in that dimension as well as the
number of data items for each value will be provided. The user
can then select the desired distinct values. If the dimension is
a numeric dimension, a histogram of the dimension will be
provided. The user then set up a minimum value and maximum
value for the selection using two sliders. The user can set the
selection ranges for multiple dimensions.

After the user sets the selection criteria, he/she can click a
button in the dialog to trigger the selection. A problem here is
how to highlight the selected data items. In most visualization
systems, selected data items are highlighted using either a
special color, or a surrounding box around the selected items.
However, in the VaR display with pixel-oriented techniques,
color has been used to represent the values, and it is hard
to put a surrounding box in a condensed glyph, especially if
the selected data items are not adjacent to each other in the
glyphs.

A straightforward solution to this problem is to display only
the selected data items. This is a general solution suitable for
all realizations of the VaR display. However, a drawback of
this approach is that the context provided by unselected data
items is lost. Such a context is often useful. For example, the
users might want to compare the selected data items with the
unselected data items among the dimensions.

In order to overcome this drawback, we developed an
approach called data item masking. This approach is only
useful for VaR displays using pixel-oriented techniques. In

9

Fig. 6. Labeling Solutions (a) All dimensions are labeled with names (b) Dimensions selected by the labeling algorithm are labeled. Clutter is reduced. (c)
Angled text is used to label all dimensions in the Jigsaw map layout. The dataset is the Image-89 dataset.

according to the following two heuristic criteria: 1) they should
be distinct dimensions, i.e., two similar dimensions should
not be labeled at the same time. Dimensions distinct from all
other labeled dimensions should be labeled. 2). they should be
separated from each other as much as possible to avoid clutter
on the screen. In addition, we allow users to interactively
change the number of dimensions labeled to get a less cluttered
view or to see more labels.

Criterion 1 is exactly the criterion used for automatic se-
lection of separated dimensions (see Section VII-C). Criterion
2 adds more constraints to the selection. Recall that there is
some freedom in step 4 of the selection algorithm, i.e., any
dimensions in the candidate dimension set can be selected; we
modiﬁed the algorithm for labeling as follows:

1) Assign a dimension and a selection threshold.
2) Set the assigned dimension as “selected” and all other

dimensions as “unselected”.

3) Find all unselected dimensions whose correlations with
all existing selected dimensions are smaller than the
threshold. Mark them as “candidates”.

4) If there is no candidate dimension, go to step 5. Else, set
the candidate dimension which is the most far way on
the screen from its closest existing selected dimension
as “selected” and other candidates as “unselected”. Go
back to step 3.

5) Return all dimensions marked as “selected” and label

this approach, both selected and unselected data items are
drawn on the screen. Unselected data items are covered by
a mask. Users can interactively change the color of the mask,
and adjust the transparency of the mask though a slider. When
the mask is opaque, as shown in Figure 5b, unselected data
items are hidden. When the mask is fully transparent, as shown
in Figure 5a, the selected data items are not highlighted. When
the mask is semi-transparent, as show in Figure 5c, the selected
data items are highlighted within the context provided by
the unselected data items. Users can interactively change the
transparency of the mask to adjust the strength of the context.
The implementation of this masking operation is simple.
First, a mask is generated using an approach similar to the
generation of a normal dimension glyph. The only difference
is that the pixels are set to be transparent for selected data
items and with user assigned color and transparency for
unselected data items. Our mask generation mechanism has
no dependency on the order of the data items,
is
not necessary for the selected data items to be adjacent to
each other in the glyphs. Since the color and shape of the
masks are the same for all
the mask is only
generated once, stored as a texture object, and pasted in the
front of all the glyphs. Since the texture mapping operation is
efﬁcient in OpenGL, displaying masks has minimal effect on
the rendering of a VaR display.

the glyphs,

i.e.,

it

VIII. LABELING

them.

In the original version of the VaR display, dimension names
are labeled horizontally in the middle top region above the
dimension glyph for all dimensions shown on the screen (see
Figure 6a). The labels clutter the screen seriously for a high
dimensional dataset, thus we did not provide the labeling
option to users. Rather, when users moved the cursor over
a glyph, the glyph name showed in the message bar below
the display. However, users complained that ﬁnding dimension
names in this way was tiring. They argued that the VaR display
without dimension labels is much less meaningful than one
with names labeled. In order to solve this problem, we chose to
label a subset of dimensions on the screen for the MDS layout
(see Figure 6b). The dimensions to be labeled are selected

When calculating the screen distance between two dimen-
sions in step 4, we must consider the fact that horizontal labels
are used. Their lengths are much larger than their widths.
Assume that labels have 5 characters on average and the
characters have equal height and width, the screen distance
between two dimensions d1 and d2 D(d1, d2) = fabs((d1.x
- d1.x)) + 5 * fabs((d1.y - d2.y)), where x and y are the
screen coordinates of the dimensions. The equation means that
we prefer dimensions separated in the vertical direction than
the horizontal direction. Figure 6b shows the same display as
Figure 6a with selected dimensions labeled using the above
algorithm.

The same labeling approach can be applied to the Jigsaw

map layout. In addition, since the glyphs are placed in a regular
mesh in the Jigsaw map, applying an angle on all the labels
greatly reduces the clutter on the screen even when all labels
are shown. Figure6c shows the Image-89 datasets in the Jigsaw
map layout with all dimension names displayed at a 20 degree
angle. Almost all of the dimension names can be distinguished
from this display.

In our prototype we bind labeling with selections,

i.e.,
users have the option to show labels of selected dimensions
only. When a user chooses this option and uses the automatic
selection tool for separated dimensions, it is exactly the above
clutter-reducing labeling approach. When a user uses the
selection tool for related dimensions, the dimensions closely
related to the user-assigned dimensions are labeled (see Figure
3d for an example).

IX. IMPLEMENTATION AND SCALABILITY ISSUE

When there are several hundred dimensions, the datasets
can easily contain millions of data values even if they only
contain thousands of data items. Datasets often have a higher
number of data items. Such large datasets not only cause large
response time during interactions and problems in storing the
data structures in a visualization system, but also cause clutter
on the display. Scalability is a critical issue for visualization
systems aimed at high dimensional datasets.

We have implemented a fully working prototype of the VaR
display. The biggest dataset that has been successfully loaded
into the VaR display so far is an image classiﬁcation dataset
containing 838 dimensions and 11413 data items, which means
over 9 million data values (see Figure 3 for its VaR display).
Most interactions can be processed within a few seconds on a
typical PC for this dataset. This dataset is the biggest dataset
we currently have. In the future, we will test larger datasets
on the prototype.

The critical techniques we used in the prototype for increas-
ing scalability are texture mapping, binning, and sampling
techniques. Using the texture mapping techniques provided by
OpenGL, our prototype stores all dimension glyphs (including
the mask in the masking operation) as texture objects and
pastes them on the screen as needed. As long as the glyph
textures do not change,
the dataset does not need to be
rescanned, which is time consuming for large datasets. By
keeping the texture objects small (such as hundreds of pixels),
which is reasonable since each dimension glyph will not be
too big on the screen in order to reduce clutter, the system can
draw hundreds of dimension glyph textures on the screen in
almost real time. This approach greatly reduces the response
time for most interactions because, except for reordering for
pixel-oriented glyphs and resetting the X dimension for X-Ray
scatterplot glyphs, almost all other interactions do not change
the glyph textures. Rather, they refresh, resize, reposition, or
reorder the glyphs.

According to our experience, drawing fonts in OpenGL is
a time consuming task. Our prototype stores all dimension
name labels as texture objects. These texture labels are created
one time, and can be quickly pasted on the screen until users
change the contents or colors of the labels. The texture labels
can be scaled and rotated easily on the screen.

10

Binning, i.e., using buckets to stored statistic information
about groups of values rather than recording them individually,
is an approach widely used in data mining techniques for large
datasets. We use binning techniques to increase the speed of
the correlation calculation algorithm (see section VI) and the
X-Ray scatterplot glyph generation (see section IV).

The prototype stores datasets in an Oracle database server.
It dynamically requests data from the server when needed,
making use of the sorting and query functions provided by
the database server. When generating a VaR display for a
dataset containing a large number of data items, we use a
random sampling approach to reduce the response time for
fetching data items from the server, as well as the number
of values to be processed. In particular, the system keeps a
default maximum number. When the number of data items
contained in a dataset exceeds it, a uniform random sampling
is performed on the dataset to only fetch the maximum number
of data items. Users are allowed to interactively adjust the
maximum number in order to trade between the response time
and visualization accuracy.

Random sampling is easy to implement. However, it has
the big drawback that a large sampling rate is needed in order
to reduce small group loss in the samples [6]. In order to
overcome this problem, many solutions have been proposed,
such as biased sampling [14] or dynamic sample selection
[2]. It has been shown in the literatures that these approaches
successfully reduce small group loss. We will explore these
approaches in the future.

X. DISCUSSION

The VaR display can serve as an overview tool for a high
dimensional dataset. Starting from the VaR display, other
visualization techniques can be used for more detailed visual
analysis. For example, the VaR display is coordinated with
parallel coordinates, star glyphs, and scatterplot matrix views
in our prototype. Although these techniques could not handle
hundreds of dimensions, they work well in examining data
items and dimensions selected by the VaR display. Recently,
we completed an interesting project in coordinating the VaR
display with an image exploration interface. The VaR display
was used to show the high dimensional image content anno-
tations. Users were allowed to select images by contents from
the VaR display. The images were then examined in detail in
an image exploration interface. This work is described in [26].
The MDS and Jigsaw map glyph layout approaches have
their advantages and disadvantages. From its nature, MDS
is better in capturing high dimensional relationships than the
hierarchical approach. However, the non-overlap feature of the
Jigsaw map layout makes it a popular approach for users of
the VaR display thus far.

Although the pixel-oriented glyphs are mentioned less than
the X-Ray scatterplot glyphs in this paper, this is only because
the usage of the pixel-oriented techniques has been widely
studied and their effectiveness has been shown in many papers.
Compared to scatterplots, the pixel-oriented glyphs are more
effective in pixel usage since they make use of each pixel.
However, it is easier to compare the relationship between a

11

Fig. 7.
(a) The Pixel MDS VaR display of the Image-838 dataset with separated dimensions selected and labeled. (b)(c) The X-Ray scatterplot Jigsaw map
VaR display of the Image-89 dataset. The dimension in a yellow frame is non-linearly related to the X dimension. (c) The X-Ray scatterplot Jigsaw map VaR
display with another X dimension (the dimension highlighted by the yellow frame in (b)).

dimension of interest and all other dimensions using the scat-
terplot glyphs. Users ﬁnd it difﬁcult to compare the patterns
of pixel-oriented glyphs if they are far from each other.

Compared to scatterplot matrices, the X-Ray scatterplot VaR
display has its advantages and disadvantages. For datasets with
a small number of dimensions, scatterplot matrices might be
preferred since all possible axis-parallel 2-D projections are
provided in them. However, for datasets with tens, hundreds
or thousands of dimensions, the X-Ray scatterplot VaR display
might be preferred since it causes less clutter. Its disadvantage
that only part of possible 2-D projections are displayed is
leveraged by two facts: ﬁrst, dimension relationships conveyed
by the VaR display give strong hints on the shapes of the
undisplayed 2-D projections; second, users can interactively
access 2-D projections of interest through interactions.

Compared to approaches that rank the 1D or 2-D projections
according to their features and allow users to examine detail
of a projection by selecting it from diagrams or lists conveying
the ranking (such as the rank-by-feature framework [19]),
the VaR display also has its advantages and disadvantages.
Obviously for tasks such as ﬁnding the most linearly corre-
lated dimensions the ranking approaches are better choices.
However, the VaR display is better in helping users grasp the
global relationships among the dimensions.

XI. CASE STUDY

We have explored several real datasets using the VaR dis-
play, including the Image-838 dataset [8] with 838 dimensions
and 11,413 data items and the Image-89 dataset [8] with 89
dimensions and 10,471 data items. They all contain low level
visual attributes for image classiﬁcation. Image analysts are
interested in ﬁnding outlier dimensions that are uncorrelated to
most other dimensions, and dimensions representing a group of
correlated dimensions (a dimension cluster) in order to reduce
the number of low level visual attributes used in the image
classiﬁcation process.

For both datasets, we selected a Pixel MDS VaR display
with all dimensions displayed as the initial view, since the

pixel-oriented glyphs have a higher pixel usage efﬁciency
and the MDS display conveys dimension relationships more
accurately than the Jigsaw map layout. Figure 7a and Figure 1b
show the Pixel MDS VaR displays of the Image-838 dataset
and the Image-89 dataset respectively. From the ﬁgures, we
found that there are dimension outliers and clusters in both
datasets. We then applied automatic selections for separated
dimensions. Both outlier dimensions and dimensions repre-
senting dimension clusters were selected.

Then, we switched to the Jigsaw map layout. Figure 3a
shows the Pixel Jigsaw map VaR display of the Image-838
dataset. There are several distinguishable regions that can be
seen in the map where adjacent glyphs in the regions have
similar patterns. For example, there is a distinguishable region
composed of bright blue glyphs at the left bottom of the
map. If only one dimension is selected in such a region, it
means that the neighbors of the selected dimension are closely
related to it, since selection for separated dimensions was used.
Thus they are a dimension cluster and the selected dimension
can represent the cluster. The selected and labeled dimension
angle 135 at the left bottom corner is such a representative
dimension. Meanwhile, selected dimensions crowded together,
such as the selected dimensions in the left top of the map,
are potential outliers since they are distinct from their closest
neighbors. The selected and labeled dimension Coarseness at
the left top corner is such suspicious outlier.

In order to examine if dimension Coarseness is an outlier,
an X-Ray scatterplot VaR display was created using it as
the X dimension (see Figure 3b). From scatterplots in Figure
3b it can be seen that no other dimensions show strong
correlations with dimension Coarseness. Thus it is conﬁrmed
that dimension Coarseness is an outlier dimension.

Figure 3c examines if dimension angle 135 is a repre-
sentative dimension. The X dimension of the scatterplots is
dimension angle 135 and dimensions closely correlated to
dimension anagle 135 are selected and highlighted. It can be
seen that a large number of dimensions are selected and they
all contain a clear diagonal pattern which indicates a strong

linear correlation. Figure 3d shows a zoomed in display of the
selected dimensions in which their labels are shown.

A similar exploration approach was conducted for the
Image-89 dataset. An interesting pattern in this dataset was
found when we were examining dimension Channel Energy 5
using the X-Ray scatteplot Jigsaw map VaR display (Figure
7b): there was a glyph with a curved band (the glyph with a
yellow frame, the frame was manually added into the ﬁgure
for highlighting). It seemed that
this dimension was non-
linearly related to the target dimension. It raised our interest
and became our next target.

We clicked this dimension to set it as the X dimension in
the X-Ray scatterplots and got Figure 7c. It is labeled in
Figure 7c as Texture Brightness DC. Figure 7c shows that
dimension Texture Brightness DC is non-linearly related to
most dimensions in this dataset. The curved bands are fairly
thin in some dimensions, which means strong non-linear
relationships.

XII. USER STUDY

A user study has been conducted to evaluate the VaR display
by comparing it to the Rank-by-Feature feature of HCE [19].
To form a comparable study, we considered the X-Ray scat-
terplot glyph style of VaR and the scatterplot prism from the
HCE system, namely its 2D projection ranking, selection and
visualization feature. In HCE, 2D projections are ranked by
features such as strength of linear relationship or least square
error for curvilinear regression. The ranking is visualized in
both a matrix and a list. A window beside the ranking windows
shows the scatterplot of the 2D projection selected by the user.
Our assumption was that the VaR display would better help
users grasp global relationships among the dimensions in a
high dimensional dataset. The reason is that VaR provides a
detailed view of all dimensions at the same time while users
of HCE need to take efforts to associate multiple dimensions
since they can only examine a few detailed views at the same
time.

Eight subjects participated in the user study. The subjects
vary in educational backgrounds: one was a psychology grad-
uate student, two were computer science undergraduate stu-
dents, three were graduate students in the ﬁeld of visualization,
and two were researchers/post-doctorates in visualization. The
subjects completed the user study one by one on the same
computer with the same instructor. Each subject tested both
systems. The order of using VaR and HCE was alternated for
the subjects.

The study began with a 10 minute training session using
both VaR and HCE and a further 10 minutes to allow subjects
to explore the tools and ask the instructor questions. A set of
tasks were then completed by the subjects using both tools. A
post-test survey to ﬁnd user preferences and a discussion were
conducted immediately following the completion of the tasks.
We used the Image-89 dataset of 89 dimensions and 10,471
data items. As shown in the case study (Section XI), there
are some strong linearly related dimensions and some strong
non-linearly related dimensions in the Image-89 dataset.

The ﬁrst

task was to describe relationships between a
given dimension and each of the other dimensions using the

12

scatterplot displays by approximating the numbers of different
scatterplot shapes involved with the given dimension. Samples
of typical shapes, such as diagonal thin straight bands for lin-
ear relations, curved bands for non-linear relations, and evenly
distributed scatterplot indicating unrelated dimensions were
provided to users. The second task required users to describe
relationships among ﬁve randomly assigned dimensions using
their scatterplot shapes.

The majority of users performed the ﬁrst task quicker and
evaluated the task to be easier using the VaR display. The
average time was 3.2 minutes and the standard deviation was
0.5 minutes for VaR, and the average time was 4.7 minutes
and the standard deviation was 3.2 minutes for HCE. On a
scale of 0 (hard) to 5 (easy), the mean scores of 3.5 and 2.1
were given to VaR and HCE respectively. A similar trend was
identiﬁed in the second task: the average time was 3.5 minutes
and the standard deviation was 0.4 minutes for VaR, and the
average time was 8.5 minutes and the standard deviation was
2.9 minutes for HCE. The scores are 3.6 for VaR and 1.0 for
HCE. Results from these tasks highlighted the advantage of
the VaR display in providing a global view of the dimension
relationships.

Qualitative results and qualitative feedback from the post-
test survey were also encouraging. Users typically preferred
using VaR over HCE for the given tasks. The reasons given
by each user were generally similar and can be summarized by
the ability to examine details of multiple relations on a single
display. One user in the study preferred HCE over VaR due to
the more detailed and visible scatterplots in the HCE system.
Users were also asked if they agreed with the statement “this
tool is useful for exploring high dimensional data”. On a scale
of 0 (disagree) to 5 (agree), users responded with a mean score
of 4.3 and 3.5 for VaR and HCE respectively.

A number of comments and suggestions were made by the
users regarding both systems. Positive feedback from VaR
included an intuitive interface, the instantaneous global view
and ability to quickly select the X dimension of all scatterplots.
Improvements suggested by the users involved ranking the
dimensions by features, and using color and best-ﬁt-lines to
enhance the scatterplot displays which were considered too
dense. In addition, users suggested ordering the dimension
glyphs according to the shapes of the scatterplot using au-
tomatic image analysis techniques. For the HCE system, users
preferred the ranking features and the scatterplot display with
rich features and interactions. Users suggested that the global
view provided by the prism in HCE lacked details compared
to the VaR display. Future work may beneﬁt by combining the
best features of these two systems.

XIII. CONCLUSION

In this paper, the VaR display, which allows users to inter-
actively explore large datasets with hundreds of dimensions,
was presented. The essential
idea of the VaR display is
to represent each dimension in a high dimensional dataset
using an information-rich glyph, and arranging the glyphs to
reveal the relationships among the dimensions. By integrating
existing techniques such as MDS, Jigsaw map, pixel-oriented

13

[12] D.A. Keim. Designing pixel-oriented visualization techniques: Theory
IEEE Transactions on Visualization and Computer

and applications.
Graphics, 6(1):1–20, January-March 2000.

[13] D.A. Keim, H.-P. Kriegel, and M. Ankerst. Recursive pattern: a
technique for visualizing very large amounts of data. Proc. IEEE
Visualization ’95, pages 279–286, 1995.

[14] G. Kollios, D. Gunopulos, N. Koudas, and S. Berchtold. Efﬁcient
biased sampling for approximate clustering and outlier detection in large
IEEE Transactions on Knowledge and Data Engineering,
data sets.
15(5):1170–1187, 2003.

[15] J.B. Kruskal and M. Wish. Multidimensional Scaling. Sage Publications,

1978.

[16] A. MacEachren, X. Dai, F. Hardisty, D. Guo, and G. Lengerich. Explor-
ing high-d spaces with multiform matrices and small multiples. Proc.
IEEE Symposium on Information Visualization, pages 31–38, 2003.

[17] F. Murtagh. A survey of recent advances in hierarchical clustering

algorithms. Computer Journal, 26(4):354–359, 1983.

[18] NetMBA.

http://www.netmba.com/statistics/plot/scatter/.

[19] J. Seo and B. Shneiderman. A rank-by-feature framework for un-
supervised multidimensional data exploration using low dimensional
projections. Proc. IEEE Symposium on Information Visualization, pages
65–72, 2004.

[20] J. Seo and B. Shneiderman. A rank-by-feature framework for inter-
active exploration of multidimensional data. Information Visualization,
4(2):96–113, 2005.

[21] B. Shneiderman. Tree visualization with tree-maps: A 2d space-ﬁlling

approach. ACM Transactions on Graphics, 11(1):92–99, Jan. 1992.

[22] M.O. Ward. A taxonomy of glyph placement strategies for multidi-
mensional data visualization. Information Visualization, 1(3-4):194–210,
2002.

[23] M. Wattenberg. A note on space-ﬁlling visualizations and space-ﬁlling
curves. Proc. IEEE Symposium on Information Visualization, pages 181–
186, 2005.

[24] E.J. Wegman and Q. Luo. High dimensional clustering using parallel
coordinates and the grand tour. Computing Science and Statistics,
28:361–368, 1997.

[25] J.A. Wise, J.J. Thomas, K. Pennock, D. Lantrip, M. Pottier, A. Schur,
and V. Crow. Visualizing the non-visual: Spatial analysis and interaction
with information from text documents. Proc. IEEE Symposium on
Information Visualization, pages 51–58, 1995.

[26] J. Yang, J. Fan, D. Hubball, Y. Gao, H. Luo, W. Ribarsky, and
M. Ward. Semantic image browser: Bridging information visualization
with automated intelligent image analysis. Proc. IEEE Symposium on
Visual Analytics Science and Technology, pages 191–198, 2006.

[27] J. Yang, A. Patro, S. Huang, N. Mehta, M. Ward, and E. Rundensteiner.
Value and relation display for interactive exploration of high dimensional
datasets. Proc. IEEE Symposium on Information Visualization, pages
73–80, 2004.

[28] J. Yang, M.O. Ward, E.A. Rundensteiner, and S. Huang. Visual
hierarchical dimension reduction for exploration of high dimensional
datasets. Eurographics/IEEE TCVG Symposium on Visualization, pages
19–28, 2003.

[29] J. Yi, R. Melton, J. Stasko, and J. Jacko. Dust & magnet: Multivariate
information visualization using a magnet metaphor. Information Visual-
ization, 4:239–256, 2005.

techniques, and scatterplots, and allowing users to interactively
explore large datasets according to their interests, the VaR
display provides a rich metaphor for interactive exploration
of high dimensional datasets. The case studies and user study
conducted proved that the VaR display is an effective approach
with high scalability.

Although work presented in this paper has greatly extended
the functionality of the original VaR display [27], we believe
that
the VaR display still has much potential for further
development. Time-dependant dimension glyph generation or
layout, the ability to convey spatial information, and the ability
to visualize dynamically changing data streams, are future
directions we want to explore in the VaR display. In addition,
detecting features by analyzing and comparing textures of
dimension glyphs using automatic image analysis techniques
is also an appealing future work. Another important future
work is to conduct user studies to evaluate different options
provided by the VaR display.

ACKNOWLEDGMENT

We gratefully thank Dr. Daniel A. Keim for giving many
valuable suggestions for this work, Dr. Jianping Fan, Yuli Gao,
and Hangzai Luo for providing us the datasets, and the users
who participated in the user study.

This work was performed with partial support from NSF
grant IIS-0119276 and the National Visualization and Ana-
lytics Center (NVAC(tm)), a U.S. Department of Homeland
Security Program, under the auspices of the Southeastern Re-
gional Visualization and Analytics Center. NVAC is operated
by the Paciﬁc Northwest National Laboratory (PNNL), a U.S.
Department of Energy Ofﬁce of Science laboratory.

REFERENCES

[1] M. Ankerst, S. Berchtold, and D.A. Keim. Similarity clustering of
dimensions for an enhanced visualization of multidimensional data.
Proc. IEEE Symposium on Information Visualization, pages 52–60,
1998.

[2] B. Babcock, S. Chaudhuri, and G. Das. Dynamic sample selection
for approximate query processing. Proc. ACM SIGMOD International
Conference on Management of Data, pages 539–550, 2003.

[3] B. Bederson, B. Shneiderman, and M. Wattenberg. Ordered and quantum
treemaps: Making effective use of 2d space to display hierarchies. ACM
Transactions on Graphics, 21(4):833–854, 2002.

[4] C.L. Bentley and M.O. Ward. Animating multidimensional scaling
to visualize n- dimensional data sets. Proc. IEEE Symposium on
Information Visualization, pages 72–73, 1996.

[5] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft. When is “nearest
neighbor” meaningful? Lecture Notes in Computer Science, 1540:217–
235, 1999.

[6] S. Chaudhuri, R. Motwani, and V. Narasayya. Random sampling for
histogram construction: how much is enough? Proc. ACM SIGMOD
International Conference on Management of Data, pages 436–447, 1998.
[7] W.S. Cleveland and M.E. McGill. Dynamic Graphics for Statistics.

Wadsworth, Inc., 1988.

[8] J. Fan, Y. Gao, and H. Luo. Multi-level annotation of natural scenes
using dominant image components and semantic image concepts. Proc.
ACM international conference on Multimedia, pages 540 – 547, 2004.
Interactive information visualization of
a million items. Proc. IEEE Symposium on Information Visualization,
pages 117–124, 2002.

[9] J.-D. Fekete and C. Plaisant.

[10] Y. Fua, M.O. Ward, and E.A. Rundensteiner. Hierarchical parallel
coordinates for exploration of large datasets. Proc. IEEE Visualization,
pages 43–50, Oct. 1999.

[11] B. Hibbard and D. Santek. The vis-5d system for easy interactive

visualization. Proc. IEEE Visualization, pages 28–35, 1990.

View publication stats
View publication stats

",False,2007.0,{},False,False,journalArticle,False,CJAT82Y9,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4135655,,Value and Relation Display: Interactive Visual Exploration of Large Data Sets with Hundreds of Dimensions,CJAT82Y9,False,False
5UJ67Q46,LPICXL99,"Towards a Systematic Combination of Dimension Reduction

and Clustering in Visual Analytics

John Wenskovitch, Student Member, IEEE, Ian Crandell, Naren Ramakrishnan, Member, IEEE,

Leanna House, Scotland Leman, Chris North

Abstract— Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both
families of algorithms assist analysts in performing related tasks regarding the similarity of observations and ﬁnding groups in datasets.
Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems.
However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating
some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and
clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are
processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension
reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes
use of both families of algorithms.
Index Terms—Dimension reduction, clustering, algorithms, visual analytics.

1 INTRODUCTION
Visual metaphors for exploring high-dimensional datasets come in a
variety of forms, each with their own strengths and weaknesses in both
visualization and interaction [37, 69]. In particular, datasets with high
dimensionality present tractability challenges for computation, design,
and interaction [29]. One frequently used method of visual abstraction
is to reduce a high-dimensional dataset into a low-dimensional space
while preserving properties of the high-dimensional structure (e.g., re-
tain or respect pairwise relationships from the higher dimensions in
the lower dimensional projection). Such dimension reduction algo-
rithms are useful abstractions because some of the dimensions in the
dataset may not be essential to understanding the underlying patterns
in the dataset [38]. Instead, a subset of the dimensions can be selected
or learned (or new dimensions introduced) to deﬁne the important
characteristics of the dataset. The visualization tasks associated with
dimension reduction algorithms have been well studied [14, 15].
Many dimension reduction algorithms employ a “proximity ≈ simi-
larity” metaphor, in which a distance function measures the similarity
of pairs of observations1 at the high-dimensional level and attempts to
preserve those distance relationships in the low-dimensional projection
by minimizing a stress function. Due to this “proximity ≈ similarity”
relationship, observations with high similarity or an underlying rela-
tionship can form implicit clusters in the low-dimensional projection.
Indeed, clustering can even be thought of as extremely low-resolution
dimension reduction, where knowledge about the various attributes
of the observations leads to a one-dimensional bin assignment (or a
set of probabilities for bin assignments). This relationship between
dimension reduction and clustering is also supported mathematically
in speciﬁc instances. For example, Ding and He [27] proved that prin-
cipal components are the continuous solutions to the discrete cluster
membership indicators for k-means clustering, indicating that Principal

• John Wenskovitch, Naren Ramakrishnan, and Chris North are with the

Virginia Tech Department of Computer Science. E-mails: {jw87 | naren |
north}@cs.vt.edu.
Department of Statistics. E-mails: {ian85 | lhouse | leman}@vt.edu.

• Ian Crandell, Leanna House, and Scotland Leman are with the Virginia Tech

Component Analysis (PCA) dimension reduction implicitly performs
data clustering as well.

Indications from previous studies [8, 33] have shown that analysts
use a complex combination of both developing clusters and organizing
observations in space in the sensemaking process [76] as they explore
a dataset. These explorations generate clusters created by the analyst
during exploratory interactions to spatially organize information on
the display, as well as clusters that naturally develop due to expressive
interactions updating the underlying layout (these interaction types are
deﬁned by Endert et al [34]). Other studies have also linked dimension
reduction algorithms to clustered data; for example, Choo et al. dis-
cusses dimension reduction methods for two-dimensional visualization
of high-dimensional clustered data, proposing a two-stage framework
for visualizing such data based on dimension reduction methods [21].
While dimension reduction algorithms and clustering algorithms
have been implemented together in a number of visualization systems,
these algorithms often operate independently and in parallel. In other
words, each algorithm supports some analysis component in the system
without the inﬂuence of the other algorithm: perhaps a collection
of observations are clustered, but the output of that clustering has
limited or no effect on the dimension-reduced layout of the observations.
Alternatively, a change to the spatialization may perceptually imply
the need for a change to the cluster assignment, but no update to
the cluster assignment may occur. The second case can be seen in
the iVisClustering system [57]. This tool clusters documents into a
collection of topics and uses a force-directed layout in the Cluster
Relation View to present the documents spatially. However, making a
change to the layout of the projection (see Fig. 1) has no effect on the
clustering assignments of the documents.

Exploring the connections between dimension reduction and clus-

Manuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication
xx xxx. 201x; date of current version xx xxx. 201x. For information on
obtaining reprints of this article, please send e-mail to: reprints@ieee.org.
Digital Object Identiﬁer: xx.xxxx/TVCG.201x.xxxxxxx

1In this work, we employ the convention of referring to the features of a
dataset as dimensions, individual data items as observations, and the features of
those observations as attributes.

Fig. 1. The iVisClustering system [57] incorporates dimension reduction
and clustering algorithms in the same system; however, making a change
to the layout has no effect on the clustering assignment.

Table 1. A selection of dimension reduction algorithms, organized by
the complexity of manifold each can learn: linear manifolds, nonlinear
manifolds, and algorithms that have implementations of both types.

Linear

Both

Nonlinear

Selected Dimension Reduction Algorithms
Factor Analysis [43]
Principal Component Analysis (PCA) [74]
Probabilistic PCA (PPCA) [84]
Projection Pursuit [40]
Feature Selection [42]
Independent Component Analysis (ICA) [49]
Multidimensional Scaling (MDS) [85]
Weighted MDS (WMDS) [18]
Glimmer [50]
Isomap [82]
Latent Dirichlet Allocation (LDA) [11]
t-Distributed Stochastic Neighbor Embedding
(t-SNE) [65]

tering algorithms leads to several natural research questions. If the
data separates into implicit clusters, and the analyst sees advantages
in the creation of these implicit clusters, can we appropriately sup-
port explicit cluster deﬁnitions so that the dimension reduction and
clustering algorithms support each other rather than conﬂict with each
other (or simply do not interact with each other)? If so, how should we
deﬁne, visualize, and interact with both observations and clusters in
a dimension-reduced projection? And ﬁnally, is there a difference be-
tween how analysts interpret and interact with low-dimensional clusters
as opposed to high-dimensional clusters?

Our research explores initial steps to address these questions. In

particular, this work includes the following contributions:

1. An overview of combining dimension reduction and clustering
techniques into a visualization system, including a discussion of
algorithms, tasks, visualizations, and interactions.

2. A discussion of the design decisions that must be addressed when
creating a visualization system that combines dimension reduction
and clustering algorithms.

The remainder of this paper discusses these contributions through the
exploratory data analysis process. We begin by providing an overview
of existing dimension reduction and clustering algorithms in Sect. 2.
From there, we discuss common high-dimensional data analysis tasks
is Sect. 3, visualizations to support those tasks in Sect. 4, and interac-
tions on those visualizations in Sect. 5. We close with a discussion of
further challenges and lessons learned in Sect. 6 and conclude in Sect. 7
with a summary of design questions that should be considered when
developing a tool combining these algorithm families.

2 ALGORITHMS
In this section, we summarize the variety of algorithms that address
dimension reduction and clustering tasks in visualization systems.

2.1 Dimension Reduction Algorithms
The goal of dimension reduction algorithms is to represent high-
dimensional data in a low-dimensional space while preserving high-
dimensional structures, including outliers and clusters [58]. Dimension
reduction has a scalability advantage over other methods for visualizing
high-dimension data such as parallel coordinate plots and heatmaps, but
with the disadvantage of information loss when transforming the data
into the low-dimensional projection [37, 61, 69]. Here, we summarize
many of the common dimension reduction algorithms in the Visualiza-
tion ﬁeld; more detailed surveys of dimension reduction algorithms can
be found in the literature [38, 39, 58, 91]. In addition, several tools have
been implemented that allow analysts to switch between and compare
dimension reduction algorithms [62, 77].

Dimension reduction algorithms can be divided into linear and non-
linear classes, referring to the structure of the underlying manifolds or
topological spaces that each class can learn. Linear dimension reduction
algorithms are limited to learning linear manifolds, while nonlinear di-
mension reduction algorithms can learn more complex manifolds. Still
other dimension reduction algorithms have been implemented in both
linear and nonlinear variants. Table 1 provides a set of commonly used
dimension reduction algorithms in the visualization literature, divided
into whether they are linear, nonlinear, or have implementations of both
levels of manifold complexity. Principal Component Analysis (PCA) is
perhaps the most frequently-used linear dimension reduction algorithm,
which works by determining the axes of maximum variance in the col-
lection of observations [74]. Another often-used dimension reduction
technique is Multidimensional Scaling (MDS), which computes pair-
wise distances between observations in the high-dimensional space and
attempts to preserve those distances in a low-dimensional projection.
MDS implementations exist in both linear and nonlinear forms.

Many of these dimension reduction algorithms require a distance
function as input, which provides the method for calculating the simi-
larity of each pair of observations. Much like the breadth of algorithms
discussed, a number of distance functions are used in Visualization
systems. The most popular metrics are those derived from p-norms,
which give distance functions of the form

(cid:12)(cid:12)xi,k − x j,k

(cid:12)(cid:12)p(cid:19)1/p

.

(cid:18)

∑

k

dp(xi,x j) =

Such a distance is deﬁned for any positive p. The most familiar ex-
amples are p = 1, which is known as Manhattan distance (due to the
city’s regular grid structure), and p = 2, which is Euclidean distance.
Aggarwal et al. [2] showed that Manhattan distances are preferable
to Euclidean distances for high-dimensional data, as Euclidean dis-
tance (and p-norms of p > 1 in general) tends to compress the space
as more dimensions are added, resulting in high-dimensional distances
that are less distinguishable. In determining the appropriate distance
function, it is also worth considering that some distance functions are
computationally more difﬁcult when optimizing a stress function.

Large datasets present performance difﬁculties with some dimen-
sion reduction algorithms. For example, MDS requires a distance to
be computed between each pair of observations, resulting in ∼ n2/2
distances computed for n observations. However, tools do exist to
visualize such large datasets. ASK-GraphView supports the interactive
visualization of graphs with up to 200,000 nodes and 16,000,000 edges
by using clustering algorithms to construct a hierarchical graph, thus
visualizing only internal subsections of the graph at any time [1]. A
related solution that combines these algorithm families is to initially
cluster the data and then apply a dimension reduction algorithm such
as MDS on the cluster centroids, followed by subsequent dimension
reduction executions on each individual cluster. This minimizes the
amount of memory required to store pairwise distances at the expense
of no longer having a single global distance measure.

2.2 Clustering Algorithms
Hundreds of clustering algorithms have been implemented, each with
inherent strengths and weaknesses. The broad collection of approaches
in this class of algorithms stems from the notion that a “cluster” is in-
herently a subjective structure, and as such cannot be precisely deﬁned.
Therefore, new algorithms or improvements on existing algorithms are
often created to solve a single problem, though these new solutions
may be applied to future problems where appropriate. As a result, there
is no globally optimal clustering algorithm; the best clustering algo-
rithm is problem-speciﬁc and often determined experimentally [36].
Surveys of clustering algorithms exist in the literature, which include
clustering from the perspectives of machine learning, human-computer
interaction, visualization, and statistics [23, 92].

Clustering algorithms come in two primary forms: hierarchical and
partitioning. Hierarchical algorithms in turn can be divisive (top-down)
or agglomerative (bottom-up). The divisive strategy approaches the
identiﬁcation of clusters through iterative partitioning, beginning with a

Table 2. Sample exploratory data analysis tasks, organized by stage in the data analysis process (rows) and algorithm family (columns).

See the Result

Dimension Reduction
See distribution of observations

Both
See relative positions of observations

Understand the Result Measure distances between observations

Identify attribute values of observations

Affect the Result

Change distance metric
Select different dimensions

Reposition observations in the full space
Enhance an existing pattern in the projection

Clustering
Identify clusters of observations
Label clusters
Determine cluster structure
Change cluster membership of observations
Create/remove clusters

single group and breaking it down into smaller portions according to an
algorithm-speciﬁc differencing measure [4]. In contrast, agglomerative
algorithms approach clustering through iterating aggregation, beginning
with every item in its own group and joining groups together through
an algorithm-speciﬁc similarity measure [78].

Perhaps the most common clustering algorithm is k-means [64],
which partitions a dataset into k clusters according to a distance between
each observation and the nearest cluster centroid. Finding an optimal k-
means solution is an NP-Hard problem; therefore, heuristic algorithms
exist to converge quickly to a local solution. The k-means algorithm
has been extended to support a variety of tasks, including weighted
clustering [48], hierarchical clustering [75], textual data [26], and
constrained clustering [89]. A number of k-means variants are discussed
in detail by Cordeiro de Amorim and Mirkin [25]. A major limitation
of k-means is that it can only ﬁnd clusters with convex shapes. The
algorithm also requires input parameter k for the number of clusters to
create, presenting an additional complication in generating the best set
of clusters with its heuristic approach. Several solutions to determine
the most appropriate k value are used, such as the elbow method [83].
Many of the distance functions that are applied to dimension reduc-
tion algorithms are also useful when considering cluster computations.
Cosine distance, for example, can be used to measure cohesion within
clusters [81]. The Jaccard similarity coefﬁcient is used for measuring
diversity and dissimilarity between clusters or sets of observations [60].
In selecting a clustering algorithm, an additional consideration
should be whether an observation can be assigned to only one cluster
(“hard” clustering) or can belong to multiple clusters (“fuzzy” or “soft”
clustering). The Fuzzy C-means Clustering algorithm [9, 30] is a fuzzy
extension of the k-means algorithm, in which the centroid of a cluster
is now computed as the mean of all observations weighted by their
probability of belonging to the cluster. Fuzzy C-means has found use
in the ﬁelds of bioinformatics [87] and image analysis [3].

A common clustering tool used in statistics is the Dirichlet process
mixture model (DPMM) [10]. This is a probabilistic method, and rather
than return a hard clustering assignment, it gives each observation a
probability of belonging to any given cluster. Additionally, and unlike
k-means, DPMMs learn the number of clusters dynamically, creating
new clusters and closing old ones as the algorithm proceeds. It is not
without drawbacks, however. The DPMM requires speciﬁcation of a
probability model for the observations in each cluster, which in turn
introduces its own difﬁculties. The algorithm also scales more poorly
than k-means with additional data, especially if the model parameters
are estimated with Markov chain Monte Carlo.

Much like with dimension reduction algorithms, large datasets can
present performance issues with clustering algorithms. Consider again
the k-means algorithm, for which the common Lloyd’s algorithm heuris-
tic implementation has a running time of O(nkdi) for a dataset with n
observations of d dimensions each, k clusters, and i iterations before
convergence [64]. The runtime of this algorithm is thus linear in terms
of both the number of observations and the number of dimensions;
however, performance can be greatly improved by reducing the number
of dimensions. Assuming that n and k are ﬁxed, the execution time
of the k-means algorithm can hence be improved substantially with
dimension reduction, potentially dropping the value of d from hundreds
to two (which may simultaneously reduce i as well). Several clustering
algorithms are designed to use on large datasets. For one example,
the Bradley-Fayyad-Reina (B-F-R) clustering algorithm is a variant
of k-means that works by internally maintaining summaries of large
collections of observations [13].

3 TASKS
This section presents an overview of tasks commonly seen in visu-
alization systems that implement dimension reduction and clustering
algorithms for exploratory data analysis. We discuss the implications
of the order in which these algorithms are executed, along with related
design decisions and considerations.

3.1 Dimension Reduction and Clustering Tasks
Our discussion of tasks for dimension reduction and clustering algo-
rithms focuses on exploratory data analysis tasks. When exploring a
high-dimensional dataset with dimension-reduced projections, there
are an immense number of possible 2D- or 3D-projections that can be
generated from the dataset. An analyst should be afforded the ability to
explore these alternate projections, as well as the related clusterings in
those projections, in order to gain insight from the data.

One method for enabling this exploration is by applying weights to
the dimensions in the dataset. Biasing the algorithms towards combina-
tions of dimensions in the dataset enables the creation of projections
that are similarly biased towards those dimension combinations. Thus,
an analyst can explore clusters and patterns in a projection that is biased
towards dimensions X, Y , and Z, and contrast that result with clusters
and patterns in a projection biased towards only dimensions U and V ,
both from the same initial high-dimensional dataset.

When interactively exploring a dataset, dimension reduction tasks
(the left columns of Table 2) typically relate to position, while cluster-
ing tasks (the right columns of Table 2) typically relate to grouping. For
example, identifying a similarity relationship between two observations
based on their separation distance in a projection is a dimension reduc-
tion task, while positioning two similar observations close together is
a clustering task. However, there exists obvious ambiguity even with
such basic interactions. When positioning two objects close together to
form a cluster, the analyst is also communicating a distance relationship
between those observations. Thus, space is overloaded for both group-
ing and layout interactions, further suggesting a relationship between
the dimension reduction and clustering algorithm families. As seen in
the selected tasks breakdown in Table 2, tasks can often be addressed by
only using a dimension reduction algorithm or a clustering algorithm,
but there do exist many cases where the interplay between algorithms
affects both when a task is performed.

This relationship can be further seen in Brehmer et al. [15], in which
ten analysts from six application domains were interviewed with the
goal of understanding how analysts explore dimension-reduced data.
The end result of this study was a set of ﬁve task sequences. Although
the authors were focused on analyst interpretations of dimension-
reduced data, three of the ﬁve resulting task sequences were related
to clusters of items revealed in the low-dimensional data projection.
Indeed, the “Verify Clusters” task sequence was performed by all ten
of their analysts and the “Name Clusters” sequence was performed by
eight of the ten analysts. In contrast, the tasks sequences that were
not cluster-based were only performed by two (“Name Synthesized
Dimensions”) and four (“Map Synthesized to Original Dimensions”)
of the ten analysts. These ﬁndings suggest that analysts are discretizing
these clusters of observations in dimension-reduced projections. In
other words, the dimension reduction algorithm is creating a continuous
visual distribution that analysts interpret in discrete segments. More-
over, investigating these clusters within the projection are common
goals of user exploration and interaction with datasets.

In addition to investigating clusters in an existing projection, studies
have shown that analysts create their own clusters of observations. For

example, the “Space to Think” study by Andrews et al. [8] investigated
how analysts use large displays to navigate and lay out documents in
the sensemaking process [76], and that these clusters occasionally have
spatial relationships, both to develop a timeline and to keep similar clus-
ters of documents near to each other spatially. When interviewed about
their sensemaking process later, analysts spoke of their documents and
clusters both in terms of proximity and in terms of groups, implying
that these are similar cognitive processes. The ForceSPIRE [33] and
StarSPIRE [12] systems were designed in part from these ﬁndings.

Similar behavior was seen in the “Be the Data” system reported by
Chen et al. [20], which allows participants to explore a dataset by taking
on the role of the observations in a deﬁned physical space. By moving
about the space, participants update a dimension-reduced projection.
The system is thereby able to learn which dimensions of the dataset are
most important to the current “projection” of people. Presented with
a collection of animals and their attributes, a group of seventh grade
students were posed the question “What makes some animals good to
eat?” The students began their exploration of the data by clustering
animals into discrete Edible and Inedible clusters. However, the student
who embodied the Rat observation did not consider herself a part of
either group, noting that rats are normally not edible but are consumed
in some cultures. She then positioned herself between the two clusters.
This caused the rest of the students to reconsider their distribution,
turning the discrete clusters into a continuous distribution of Edibility.
Cluster investigation tasks (the right columns of Table 2) come in a
number of forms, each of which have some meaning in a dimension-
reduced projection. For example, analysts may wish to understand the
overall layout of clusters in a projection, explore the proximity of one
cluster to another, investigate clusters of clusters and similar structures,
the shape of a cluster, and describe outlying clusters versus central clus-
ters. In addition, analysts may be interested in the relationship between
clusters and the individual observations in the projection, exploring to
which cluster(s) an observation belongs, understanding the properties
of observations that are outliers to all clusters, and investigating the
properties of a set of observations that form a cluster. There is a mix
of distribution and group questions that can be addressed through the
combination of both dimension reduction and clustering algorithms.

Adding clusters and clustering interactions to dimension-reduced
data can also improve scalability as datasets continue to grow in
size [32]. Having the ability to abstract collections of observations
into a single cluster that acts as an interaction target enables the abil-
ity to place more objects into virtual spaces, useful both for standard
monitors and for large display systems.

While the outputs of dimension reduction and clustering algorithms
are useful to locate patterns in a dataset, we also beneﬁt from enabling
these algorithms to learn from user interactions [34, 46, 59]. By in-
terpreting the semantic meaning of user interactions, each of these
algorithms can better enable exploratory data analysis. For example,
an analyst may wish to know what model parameters are necessary to
create a cluster from observations A, B, and C. By manipulating the pro-
jection to form such a cluster and initiating a semi-supervised machine
learning routine, the dimension reduction and clustering algorithms
can be trained to learn such model parameters and to update the entire
projection in response to those new parameters. The new projection
may create a new cluster from observations D, E, and F in addition to
the analyst-created cluster, a new insight into the dataset. Therefore,
the dimension reduction and clustering algorithms can help both at
the beginning of the exploration process by providing a na¨ıve starting
point, as well as throughout the exploration process by responding to
the interactions of an analyst.

3.2 Coordinating the Algorithms
Another consideration in selecting dimension reduction and clustering
algorithms is determining what parameters should be learned and used
by each algorithm, as well as what information should be learned by the
analyst. Beginning with the analyst, we discussed in the previous sub-
section that dimension reduction algorithms and clustering algorithms
serve similar purposes. However, dimension reduction algorithms are
more suited to tasks for pairwise comparisons and similarities between

observations, while clustering algorithms are better suited for compar-
isons involving the recognition and description of groups.

For the algorithms, one obvious design decision is to determine
whether or not the dimension reduction algorithm and clustering algo-
rithm should be using the same distance function, or even if they should
be using the same set of weights on the dimensions. It is possible for
the dimension reduction algorithm and the clustering algorithm to store
separate sets of weights, or to use different distance functions entirely.
When considering the semantics of the order of dimension reduction
and clustering algorithms, using clustering in high-dimensional space as
the ﬁrst operation makes uncovering clusters the primary semantic role
of the system, and hence results in a system designed to support locating
and understanding groups in the input data. In contrast, clustering as
the second operation in the low-dimensional space after executing a
dimensional reduction algorithm results in a clustering algorithm that
is merely a secondary aid to the dimension reduction algorithm.

An open question is determining whether analysts are cognitively
clustering in high-dimensional or low-dimensional space. Given that
analysts typically form clusters of text documents directly from the text
instead of ﬁrst converting those documents into another form [8], it
appears that clustering is performed in the high-dimensional space, at
least for textual data. Understanding the clustering process of analysts
will lead to better semantic interactions in this dimension reduction and
clustering design space, leading to further system interactions such as
enabling humans to provide corrections to clustering assignments and
hence update dimension reduction algorithm weights and projections.
Naturally, it is not possible to coordinate all pairs of dimension
reduction and clustering algorithms. For example, some dimension
reduction algorithms such as PCA do not rely on distances between
observations. Therefore, using the same distance measure between
PCA and a clustering algorithm is not possible.

3.3 Dimension Reduction and Clustering Combinations
When developing a system that includes both dimension reduction and
clustering algorithms, it is important to consider the order in which
these algorithms are performed on the data, as the order of these al-
gorithms will generate projections with different semantic meanings.
Fig. 2 includes six different pipelines that display execution orders
and data ﬂows between these algorithms. Each of these pipelines is
discussed in the following paragraphs. As the analyst progressively
explores the dataset, they may select a different pipeline for each round
of exploration, continuing to explore new projections (Fig. 3).

Independent Algorithms: As discussed previously, many visual-
ization systems incorporate both dimension reduction and clustering
algorithms, but these algorithms often execute independently and in
parallel so that the output of one algorithm has no effect on the other.
This pipeline is highlighted ﬁrst in Fig. 2 and was discussed in the
iVisClustering [57] example in the Introduction. In this system, topics
are computed and assigned as clusters, and a force-directed compu-
tation performs the node layout in the spatialization. However, an
update to the layout has no effect on the clustering assignments. In
addition, computing both algorithms on the high-dimensional data
will be more computationally expensive than performing only a single
high-dimensional computation.

Dimension Reduction Preprocessing for Clustering: Another
possibility is to execute a dimension reduction algorithm on the high-
dimensional data, and then pass the low-dimensional projection to the
clustering algorithm to determine groups, clustering on the reduced
data rather than the source data. This decision may be advantageous
because the clustering algorithm can execute faster on a dataset with
fewer dimensions, but the outcome may be misleading because the
low-dimensional positions of each observation are an approximation of
the high-dimensional relationships. Rather than generating clusters of
the input data, we generate clusters using data with less information,
resulting in potentially misleading cluster assignments. This risk is dis-
cussed by Joia et al. [52], noting that distances in the low-dimensional
space may be misleading due to projection errors. As a result, what
appear to be distinct clusters must be conﬁrmed, as there is no guaran-
tee that these clusters do contain unique content. An example of this

Fig. 2. Six different options for pipelines depicting combinations of dimension reduction algorithms and clustering algorithms. In each of these
pipelines examples, it is implied that each algorithm could use an independent distance function, resulting in more than just these six pipelines.
Further, these pipelines represent a single analysis iteration.

pipeline can be seen in Zha et al. [94], in which a technique similar
to PCA is performed ﬁrst and followed by k-means on that output.
Likewise, Ng et al. [71] propose an algorithm in which the observations
are embedded in low-dimensional space such as the eigenspace of the
graph Laplacian, and then k-means is applied to that low-dimensional
projection. Be The Data also creates clusters dynamically based on the
current projection [20].

Clustering Preprocessing for Dimension Reduction: The reverse
of the previous behavior occurs when the clustering algorithm is the
ﬁrst to execute, and then some information from the clustering output
(the cluster assignments, or the locations of the centroids) is used by the
dimension reduction algorithm for layout. Now, the clusters represent
relationships that exist in the initial data in high-dimensional space.
However, the clustering algorithm will take longer to execute due to
the additional number of dimensions processed. While less common,
some systems do operate in this way. For example, Ding and Li ﬁrst
use k-means clustering to initially generate class labels, followed by
LDA dimension reduction for subspace selection [28]. Fuzzy clustering
introduces a new complexity to this pipeline, as cluster assignments
are now a probability distribution rather than a ﬁxed bin assignment.
A pipeline of this form can also improve scalability, as the time and
space complexity of many dimension reduction algorithms make them
infeasible to execute on very large datasets. Clustering observations
and then performing a dimension reduction algorithm on those clusters
is one solution to this challenge.

One Algorithm Implicitly Includes the Other: Another alterna-
tive is to only execute one of the algorithms, either dimension reduction
or clustering, and then convert or interpret the output of the executed
algorithm as the output for the other algorithm as well. In these cases,
the results from one algorithm are structured to ﬁt the objective of the
other algorithm, exploiting the mathematical equivalence between these
algorithm families discussed brieﬂy in the Introduction. For example,
we can codify soft k-means clustering as assigning n observations to
k features with some associated weight or probability. Likewise, we
can formulate dimension reduction as reducing m features to p features

Fig. 3.
Interactions from the analyst will drive additional executions
through the pipeline during the data exploration process. The analyst
does not need to select the same pipeline on every iteration of the
analysis.

with some associated weight or probability. Therefore, the outcome of
soft k-means clustering can be interpreted in terms of dimension reduc-
tion by making the k clustering features also represent the p dimension
reduction features. A similar argument exists to map the outcome
of a dimension reduction algorithm directly to a cluster encoding by
executing a dimension reduction algorithm like PCA and binning the
output along one of the axes. Perhaps a more straightforward example
of this pipeline is the self-organizing map [55], a dimension reduction
technique which can be directly interpreted as a set of clusters without
any feature transformation. Kriegal et al. [56] present a survey of clus-
tering techniques for high-dimensional data, and include a discussion
on subspace clustering algorithms. Such algorithms simultaneously
reduce both the number of observations and the number of dimensions
in a dataset, in contrast with having a dimension reduction algorithm
that reduces the number of dimensions computing separately from a
clustering algorithm that reduces the number of observations.

Global and Local Algorithm Combinations: Because dimension
reduction algorithms typically take a global view of the overall space
while clustering algorithms take a local view [26], another option is
to implement a pipeline in which the overall structure of the space
is informed by the dimension reduction algorithm while local struc-
tures are governed by the clustering algorithm. These algorithms can
communicate with each other to converge towards an optimal layout,
but each is responsible for its own aspect of the structure. To further
clarify the difference between this pipeline and some of those discussed
previously, consider organizing a large collection of documents in a
display. One possibility is to place related documents into folders, and
then organize the folders in the space. This example reﬂects the “Clus-
tering Preprocessing” pipeline, as we organize the clusters rather than
individual documents. In contrast, the analyst could organize groups of
documents in the space, and then select and move those groups with
respect to one another. This example affords some additional fuzzy
clustering capabilities, as a document that may belong to two or more
clusters can be placed between those clusters. Here, the overall layout
of the documents can be handled by dimension reduction, while some
local structures of similar documents are supported by clustering.

Iterative, Alternating Algorithms: The ﬁnal pipeline represents a
structure where both dimension reduction and clustering are working
together in the same overarching algorithm. As k-means is an algo-
rithm that alternates between updating cluster assignments and centroid
positions, a third stage can be added for dimension reduction. Ideally,
this iterative alternating process will enable dimension reduction and
clustering to work in harmony to converge towards a best layout, trying
to ﬁnd the right set of dimensions and a good set of clusters simultane-
ously while also communicating between the algorithms. This pipeline
differs from “One Algorithm Implicitly Includes the Other” in that both
algorithms process the data cooperatively, rather than only executing
one of the algorithms and using its outcome to present both a projection

Fig. 4. Three options for encoding group membership as studied by
Saket et al [79]. In (a), nodes are free-ﬂoating and colored based on
cluster membership. In (b), the cluster coloring remains, and links are
drawn as necessary between some of the nodes. In (c), the nodes are
replaced by colored space-ﬁlling regions to indicate cluster membership.

and a clustering. Since both the dimension reduction algorithm and
the clustering algorithm will begin on the high-dimensional data, this
pipeline will be among the slowest to converge. Niu et al. [72] provides
an example of this pipeline.

This collection of pipelines and examples demonstrates methods
for combining dimension reduction and clustering algorithms, but are
not without limitations. Even extending these pipelines with a looping
structure to iterate through the dimension reduction and clustering
stages is insufﬁcient. To better model this and other similar cognitive
processes, we must extend this discussion of algorithms into the realm
of visualization and interaction; algorithms alone are insufﬁcient for
complex cognition [35].

4 VISUAL REPRESENTATION
After the algorithms have been selected, the next step is determining
how to present the results of the computations to the analyst. In this
section, we ﬁrst discuss common visual representations for dimension-
reduced data and clustered data. This is followed by a discussion of
potential visual outcomes of the pipelines introduced in Sect. 3.3.

4.1 Known Visualization Issues
As the sample interfaces in the bottom row of Fig. 6 show, most dimen-
sion reduction algorithm outputs are shown in scatterplots or node-link
diagrams. These scatterplots come with inherent issues in some cases,
such as difﬁculties in displaying and interpreting the dimensions that
result from an MDS projection. When dealing with large datasets,
the scatterplot or node-link representation of the dimension reduction
output runs a high risk of overplotting, especially if the spatialization
exhibits clear clustering in the layout. One solution for overplotting is
to abstract a cluster of observations into a single glyph to represent a
collection of observations, such as suggested by the Splatterplots im-
plementation [67]. An alternative is to ﬁlter the number of observations
visible in an overdrawn region, keeping a representative ratio of each
cluster in the overdrawn region [19].

While the natural representation of the dimension reduction output
uses a spatial projection like a scatterplot or node-link diagram, the
possibilities for representing cluster membership are much more diverse.
In addition to demonstrating clusters using a collection of nodes in close
spatial proximity, cluster membership can be encoded with colors or
glyphs. Even then, a number of design decisions can be made for how
best to express these memberships by color and shape.

Saket et al. [79] evaluate several encodings of cluster information
(see Fig. 4 for a visual representation of each of these encodings),
relating each to node-based tasks (for example, “Given node X, what
is its background color?”) and group-based tasks (“Given nodes X
and Y, determine if they belong to the same group”). They found that
the addition of group encodings does not negatively impact time and
accuracy on node-based tasks. As would be expected, group-based
tasks were best solved by node-link-group encodings. This outcome
suggests that the visual representation used to encode the clusters in
the projection depends on the tasks that the system addresses.

Fig. 5. Four options for displaying cluster membership as studied by
Jianu et al [51]. In addition to a node-link representation similar to that
included by Saket et al., this study included Linesets [5], GMap [41], and
BubbleSets [24].

Jianu et al. [51] perform a similar evaluation on four visual represen-
tations, including a node-link diagram similar to that studied by Saket
et al. as well as three other visual representations that are shown in
Fig. 5. Linesets [5] include link colors that match the node colors rep-
resenting cluster membership, highlighting connections between nodes
that are in the same cluster or group (top-right of Fig. 5). GMap [41]
is a space-ﬁlling representation that renders a geographic-like map for
clusters, containing all of the nodes in a colored region similar to the
node-link-graph representation studied by Saket et al. (bottom-left
of Fig. 5). Finally, BubbleSets [24] draws isocontours around clus-
ters, effectively balancing the Linesets and GMap representations by
using the isocontours to highlight links connecting members of the
same cluster but becoming space-ﬁlling in regions with high node den-
sity (bottom-right of Fig. 5). This study found that BubbleSets was
the superior representation for group-based tasks, but that encoding
group information onto node-link diagrams adds a 25% time penalty
onto network-based tasks, a conﬂict with the conclusion of Saket et al.
Clearly, more research is needed in this area to resolve such conﬂicts.
In addition to the above, another method for visualizing clusters
in a scatterplot or node-link diagram is to enclose nodes from indi-
vidual clusters in a convex hull [90]. Because k-means solves for
convex clusters based on a distance from an observation to the nearest
cluster centroid, a convex hull visualization may be the most natural
visualization representation for a k-means clustering output.

Moving away from scatterplot and node-link representations, an
alternative representation for clusters is to encode topics into a stream-
graph. For example, Liu et al. use streamgraphs to encode related text
keywords into topical collections, using the streamgraph to show how
the importance of those topics and keywords changes over time [63].

4.2 Algorithm Order Visualizations
Designers have an additional choice regarding which features are em-
phasized in the visual representation. For example, should the spatial
layout of the dimension reduction be emphasized over the cluster as-
signments? Alternatively, should the cluster assignments inform the
layout of the observations? Should we attempt to balance the two
outputs? How much of an impact should the algorithm order play in the
ﬁnal layout? The order in which we execute the dimension reduction
and clustering algorithms should have some impact on the outcome
of the visualization, but the degree to which this execution order is

emphasized can vary by system goals. Here, we describe potential
visualization properties for each of the pipelines described in Sect. 3.3.
Independent Algorithms: Consider the ﬁrst pipeline from Fig. 2,
in which both algorithms execute independently and in parallel. One
potential outcome of this pipeline is to represent clusters using convex
hulls. Here, the dimension reduction algorithm operates to ﬁnd an ideal
layout, while the clustering algorithm separately ﬁnds an ideal cluster
set. When combining the outputs, a potential result is a cluttered visual-
ization that is somewhat ambiguous in the cluster assignments of some
observations due to intersections between the clusters. A potentially
better solution, used by iVisClustering [57], is to use nodes colored by
class in cases of cluster occlusion such as these. Another solution that
allows the convex hulls to remain is to implement layout constraints
(such as those in IPSep-CoLa [31]) so that objects that clearly belong
to different clusters are visibly separated in the spatialization. However,
this requires prior knowledge of key cluster-deﬁning objects, or an
initial clustering computation that precedes the main clustering process.
This also defeats the goal of the pipeline by removing the separation
between dimension reduction and clustering algorithm execution.

Dimension Reduction Preprocessing for Clustering:

In this
pipeline, the output of the dimension reduction algorithm is fed into
the clustering algorithm, enabling clustering on the low-dimensional
reduced data rather than on the initial high-dimensional data. Because
clusters are drawn based on the proximity of observations in the projec-
tion, it is unlikely that these clusters will intersect. As noted previously,
executing the clustering algorithm on the dimension-reduced data may
not produce an optimal clustering on the high-dimensional data, which
could affect the analyst’s comprehension of the projection.

Clustering Preprocessing for Dimension Reduction: In the re-
verse of this process, we now cluster in the initial high-dimensional
data, and use some of that information such as the cluster assignments
to inform the dimension reduction. Such a visualization will likely re-
sult in visibly separated clusters as in the previous case, though perhaps
even more separated because space can be artiﬁcially added between
the clusters. Again, because we execute the dimension reduction algo-
rithm on the cluster assignment information (or other cluster algorithm
output) rather than on the initial high-dimensional data, the dimension
reduction projection may not be optimal and could also affect the ana-
lyst’s comprehension of the projection. More clearly stated, two points
that the dimension reduction algorithm judges to be somewhat similar
(but not similar enough to belong to the same cluster) may have an
artiﬁcially large distance applied between them in this projection.

One Algorithm Implicitly Includes the Other: A pipeline in
which only one algorithm is executed to perform both the dimension
reduction and clustering functions has inherent limitations depending
on which algorithm is performed. For example, if the dimension reduc-
tion algorithm is executed and clustering is applied only on the result
of the dimension-reduced spatialization, the clustering will likely be
far from optimal but the dimension reduction will be ideal. This could
result in a visualization in which, for example, the clusters are simply
assigned based on x-position in the projection.

Global and Local Algorithm Combinations: The global and local
pipeline describes the dimension reduction algorithm as responsible for
the global layout, while the clustering algorithm is responsible for local
reﬁnements and layout. These algorithms work together to create an
overall layout in which the dimension reduction algorithm effectively
lays out the clusters in a meaningful manner while the internal structure
of each cluster is maintained by the clustering algorithm. As such,
the ﬁne details of the projection will not be as accurate spatially as
the dimension reduction outcomes in the Independent Algorithms and
Dimension Reduction Preprocessing for Clustering pipelines, and the
clustering is still executing in part on the low-dimensional projection.
However, the layout should be relatively clean and understandable, and
the overall structure of the projection (e.g., the relative positions of the
clusters) will be meaningful.

Iterative, Alternating Algorithms: The ﬁnal pipeline in Fig. 2
includes both the dimension reduction algorithm and the clustering
algorithm working simultaneously and collaboratively to structure a
projection that is near-optimal for both representations. As such, this

Fig. 6. A selection of interfaces and tools that support Parametric In-
teraction or Observation-Level Interaction. The upper row shows PI
interfaces that include slider bars from Andromeda (PI view) [80], Star
Coordinates [53], and SpinBox widgets from STREAMIT [6]. The lower
row shows OLI interfaces from StarSPIRE [12], Paulovich et al. [73], and
Mamani et al. [66].

structure may produce the best visualizations with respect to the mean-
ing of the data, albeit at the cost of runtime.

A number of further design decisions can be incorporated into the
visualization. We have the option to emphasize the relative distance
between clusters more than the relative distance between pairs of ob-
servations. The visualization space is thus clusters of observations that
are obviously separated from each other in the space, possibly with
another iteration of the dimension reduction algorithm performed on
each individual cluster to generate a local layout. As yet another alter-
native, if the analyst is most interested in the clusters in the projection,
the emphasis could also be placed on the distance between each obser-
vation and the centroid of the cluster that it belongs to. Clusters could
also be artiﬁcially separated by a secondary execution of the dimension
reduction algorithm, but the superior layout determination is dependent
on the distance between each observation and a centroid.

We noted in Sect. 3.2 that it is not possible to combine all pairs
of dimension reduction and clustering algorithms. Likewise, it is not
possible to include all visual representations of dimension reduction
and clustering in the same visualization. For example, dendrograms are
often used to show hierarchical clustering; however, dendrograms are
not a useful visual encoding for dimension reduction algorithms.

5 INTERACTING WITH PROJECTIONS AND CLUSTERS
After displaying a visualization of dimension-reduced and clustered
data, the next step is to provide interactions to afford user exploration
through the dataset. Many studies have been performed and taxonomies
generated for interacting with high-dimensional data in a data analytics
context [7, 17, 88, 93].

In the context of exploring dimension-reduced data projections, two
primary methods exist for modifying an underlying distance function:
Parametric Interaction and Observation-Level Interaction. Surface-
level interactions are also often incorporated into visualization systems,
though these do not modify the underlying model. We begin this sec-
tion by discussing these interaction techniques and some representative
tools, as well as discussing interaction techniques that address cluster-
ing challenges. We follow this with a discussion of potential interaction
techniques that can support interaction with both dimension reduction
and clustering algorithms simultaneously.

5.1 Current Interaction Techniques
Parametric Interaction (PI) refers to manipulating parameters directly
in order to create a new projection and/or clustering assignment. This
presents a difﬁculty to novice or non-mathematically-inclined ana-
lysts, who may not understand how to update a set of weights to cre-
ate the dimension-reduced projection that they desire.
In contrast,
Observation-Level Interaction (OLI) refers to direct manipulation of
the observations, which in turn triggers a backsolving routine to learn
new parameters [34, 46, 59]. In this way, OLI hides the manipulation

Table 3. Sample interactions, organized by type of interaction (rows) and by the type of algorithm affected by the interaction (columns).

PI

OLI

Dimension Reduction

Rotate the projection

Reposition an observation external to clusters
or within a single cluster

Both
Modify the weight on a dimension
Select a different distance function
Reposition an observation into a
different cluster

Surface Measure a distance between observations

Details-on-demand to obtain attribute values

Clustering
Modify the max/min radius of a cluster
Change the number of clusters sought
Change cluster membership
Merge several clusters or split a cluster
Count the size of a cluster
Annotate a cluster

of the model from the analyst, allowing the analyst to perform more
natural direct manipulation interactions with the observations them-
selves. In Andromeda [80], PI allows analysts to modify weights on the
dimensions to modify the distance function directly by interacting with
sliders, while OLI uses an inverse MDS computation to interpret the
semantic meaning of the interaction in order to solve for those weights.
The upper row of Fig. 6 shows sample examples of PI from recent
visualization systems, complemented by some representative interac-
tions in the upper row of Table 3. Horizontal and vertical slider bars are
frequently utilized to enable analysts to interact with model parameters,
despite the fact that these model parameters have a variety of contexts.
Some of these sliders, such as those in Andromeda [80], include addi-
tional glyphs on the sliders to show the values of selected observations
on each dimension. In addition to slider bars, other techniques have
been utilized to support the manipulation of model parameters, such as
the SpinBox widgets of STREAMIT [6] and the transforming axes of
Star Coordinates [53]. PI techniques can also be extended to interact
with dimensions as well as observations, as shown by Turkay et al [86].
As seen in the lower row of Fig. 6 and discussed in Sect. 4, scatter
plots and node-link diagrams are the overwhelming favorite for display-
ing dimension-reduced projections, including those that support OLI.
Despite the ubiquity of these visual representations, individual OLI sys-
tems do display unique features and properties, such as supplementing
the scatterplot with additional views for context [16], supporting PI
in addition to OLI on the scatterplot [80], including local transforma-
tions [66], and focusing exclusively on textual data [12].

An additional consideration for OLI is the “With Respect to What”
problem detailed by Self et al. [80], which is the fundamental challenge
of using rigid algorithms to interpret the ambiguous meaning of an
interaction that involves dragging a node from one part of the display
to another. Andromeda solves this challenge by deﬁning a radius at
both the starting and ending point of the interaction, implying that
the analyst is moving an observation away from all other observations
within x pixels of the source and towards all other observations within
x pixels of the destination of the interaction, though the analyst is
afforded the ability to deselect observations that do not apply to the
interaction [80]. Points contained within this radius are highlighted in
the visual representation, allowing analysts to clearly see the interaction
targets that they are expressing within the projection [47].

In addition to Parametric and Observation-Level Interactions, the
introduction of clusters affords a variety of cluster-based interactions
that can support sensemaking. To begin, OLI can be applied to clusters,
including such interactions as moving clusters together and further
apart to reﬂect similarities and differences between clusters, as well as
transferring that information either to the weights on the clusters or the
weights on the nodes. We can also apply parameter tuning to clusters
at a global level, changing the number of clusters or the radius of all
clusters, or we can tune the parameters of individual clusters, creating
a collection of clusters with a variety of radii. The Vizster system, for
example, includes a PI-style slider bar to change the number of clusters
displayed in the X-ray view [44].

Clusters also introduce new cluster-speciﬁc interactions, such as clus-
ter merging, splitting, and creation [22,45], cluster annotation [54], and
hierarchies of clusters [70]. Performing any of these interactions can
communicate semantic information back to the system, re-executing the
pipeline that may or may not also include re-executing the dimension
reduction algorithm as a result of this user interaction.

5.2 Combined Interaction Techniques
The pipelines discussed in Sect. 3.3 naturally support the Parametric,
Observation-Level, surface-level, and clustering interactions discussed
in the previous subsection. Interactions in general can be designed for
each of these pipelines individually, but it is also useful to consider
interactions that can have meaning to both the dimension reduction
algorithm and the clustering algorithm simultaneously. To do so means
facing similar ambiguity that is addressed by the “With Respect to
What” problem and the issue of overloaded space.

For example, consider an analyst who is interacting with the clus-
tering assignment in a projection. Regardless of whether the analyst is
interacting with high-dimensional or low-dimensional clusters, drag-
ging an observation from one cluster to another is a natural interaction
to correct a misclassiﬁcation. However, the cause of that misclassiﬁca-
tion may be unknown to the analyst. Perhaps the analyst is interacting
with a system that implements the dimension reduction preprocess-
ing pipeline. If that is the case, then the analyst may be correcting
a misclassiﬁcation that results from the clustering operating on the
projected low-dimensional data. Thus, the goal of the system should
be to learn from that interaction, with the goal of getting closer to the
ideal high-dimensional clustering.

Alternatively, if the analyst is interacting with a system that imple-
ments clustering on the high-dimensional data, then performing the
same interaction is correcting for a case where the heuristic clustering
algorithm did not ﬁnd the optimal solution. The system can still learn
from this interaction to correct future clusterings, but the different cause
of the misclassiﬁcation should result in a different model update. These
two misclassiﬁcation corrections may be semantically identical to the
analyst who seeks to correct an error, but the underlying mechanics that
caused and must correct the misclassiﬁcation are different.

The same is true of an analyst interacting with observations in a
dimension-reduced projection. If an analyst drags an observation, it
may simply be that the analyst wishes to adjust the strength of the
relationship between two observations. However, adjusting the strength
of a relationship calculated on the high-dimensional data is inherently
different than adjusting the strength of a relationship calculated on clus-
ter algorithm output. And does the semantic meaning of the interaction
change if that drag interaction crosses a cluster boundary?

The introduction of explicitly-deﬁned clusters allows for a formal
target against which to judge interactions. When explicit clusters are
deﬁned, the analyst has four clearly deﬁned “with respect to what”
operations: (1) moving an observation into a cluster, (2) moving an
observation out of a cluster, (3) moving an observation from one cluster
into another, and (4) moving an observation without changing cluster
membership [90]. Each of these interactions can be designed to have
an effect on both the dimension reduction algorithm and the clustering
algorithm. Keeping an observation within a cluster, or dragging it from
one cluster into another, provides information to the clustering algo-
rithm that the classiﬁcation is either correct or incorrect. At the same
time, relocating an observation to a different position communicates
suggested distance information between the moved observation and
one or more additional observations in the projection. Each of these
algorithms can thus work to update the weight vector that then leads to
a projection and clustering update with this new information.

When mapping interactions to the pipelines summarized in Fig. 2,
choosing the primary target of the interaction is important even when
an interaction affects both algorithms. In the previous example, the

pipeline is implemented with the interaction primarily occurring on the
clusters, changing the cluster assignment of observations in order to up-
date the dimension reduction projection [90]. In contrast, “Be the Data”
also implements the same pipeline but with an interaction primarily
on the observation layout, using the dimension reduction algorithm to
update the clusters [20]. These two systems are both implementations
of the same pipeline, but place the interaction on different algorithms
to answer different questions about the high-dimensional data. Thus,
interactions can be considered independent of the pipelines.

A further open question to be addressed regards interactions on the
clusters themselves. If an analyst drags a cluster or interacts with it in
another manner, what adjustments should be made to the observations
and relationships within that cluster, as well as the relationships that
cross that cluster boundary?

6 DISCUSSION
Combining dimension reduction and clustering algorithms into the
same visualization system provides a number of opportunities for visu-
alization and interaction design. A system in which the two algorithm
classes cooperate for exploratory data analysis results in a relationship
in which the projection space (the outcome of the dimension reduction
algorithm) helps to explain the meaning of the clusters in the space,
while the clusters themselves help to explain the meaning of the space.
Including a machine learning aspect into a visualization system to
permit the dimension reduction and clustering algorithms to learn from
the actions of the analyst presents a number of additional challenges
for interaction design. In particular, the overloaded space metaphor
discussed in Sect. 3.1 causes challenges, as interactions within the
system must be mapped to at least one algorithm and may ambiguously
be mapped to both. For example, if an analyst drags and drops a
datapoint to reposition it in space, but the new coordinates did not
result in a cluster reassignment, should the clustering algorithm learn
nothing, or did the analyst provide some “fuzzy” clustering feedback
to the algorithm? A notion of iterative reﬁnement, in which the analyst
gradually trains the algorithms and offers corrections to mistakes at
each iteration is necessary in these cases. Such an iterative reﬁnement
process mimics Pirolli and Card’s Sensemaking Process [76].

Maintaining an analyst’s mental map during layout adjustments is a
well-studied problem [68], and is another factor that should be consid-
ered in visualization and interaction design for dimension reduction and
clustering systems. ForceSPIRE and Andromeda approach this mental
map challenge in different ways. ForceSPIRE, using a force-directed
layout, maintains the positions of nearly all observations during an
interaction, only altering the positions of observations near the interac-
tion [33]. In Andromeda, on the other hand, it is possible that all ob-
servations could move the entire distance across the space. The system
cognitively aids the analyst to understand such broad changes with an
animation slider, affording the analyst with the ability to incrementally
follow the post-interaction transition, as well as a layout stabilization
module to suppress the rotation invariant property of the Weighted
Multidimensional Scaling dimension reduction algorithm [80].

This work focuses on exploring the breadth of design options avail-
able to visualization researchers when combining dimension reduction
and clustering algorithms. Our goal with this work is to highlight
many of the decisions that exist in this design space, spurring further
exploration of this space with new tools. While we present a number of
design questions that must be addressed in creating such a visualiza-
tion system, we do not claim to answer any of these questions, as the
answers to many of them depend on the tasks and goals of the system.

7 CONCLUSION
The combination of dimension reduction and clustering algorithms rep-
resents an immense design space, including considerations of algorithm
selection and order, tasks, visualization, and interaction. In this paper,
we have provided a survey of each of these considerations, describing
existing research and discussing relevant design decisions applicable to
current and future systems (summarized in Table 4).

Returning to our discussion of the “Be the Data” interaction ﬁrst
addressed in Sect 3, we saw a smooth transition from discrete to con-

Table 4. A summary of the design challenges and questions discussed
throughout the paper regarding the combination of dimension reduction
and clustering algorithms.

Section

2.1 & 2.2

3.2

3.3

4.1

4.2

5.2

Design Decision
In general, clustering places an emphasis on re-
lationships within and between clusters.
In con-
trast, dimension reduction emphasizes observation-
to-observation relationships. Which of these tasks is
the primary goal of the analyst?
What properties of the data is the visualization seek-
ing to highlight? Which properties of the data are
the system and analyst trying to discover? Should
the primary goal of the visualization system be em-
phasizing observation relationships, clusters of ob-
servations, or both? Should the dimension reduction
and clustering algorithms use the same distance func-
tion (if possible), or should each algorithm use an
independent method of measuring similarity?
Which order and interaction of dimension reduction
and clustering algorithms best models the task that
the visualization system is addressing?
How can we encode distances and cluster member-
ship information when both algorithms are present?
As the dimension reduction and clustering algo-
rithms are competing in the same visualization, what
features should be emphasized in the visualization
to best address the problem?
Should interactions be designed independently for
the dimension reduction and clustering algorithms,
or should a given interaction affect both algorithms?

tinuous thinking. The students initially formed the clusters of Edible
and Inedible animals and then positioned those clusters in space, ini-
tially mimicking the cluster preprocessing pipeline. The transition from
this projection into a spectrum of Edibility amounts to iterative and
interactive reﬁnement of those initial clusters into a broader projection.
Without the interaction component, the pipelines could not successfully
model this student behavior.

A useful future direction for research would be a cognitive study,
further attempting to understand how analysts cognitively combine
the ideas of dimension reduction and clustering in both virtual and
non-virtual spaces. Such a study can further inform the pipelines,
visualizations, and interactions presented in this work.

ACKNOWLEDGMENTS
This research was supported by NSF Grants IIS-1447416, IIS-1633363,
and DGE-1545362, as well as by a grant from General Dynamics
Mission Systems. The authors would like to recognize the role of
comments from reviewers and discussions with InfoVis Lab @ VT
research group members in improving this work.

REFERENCES
[1] J. Abello, F. V. Ham, and N. Krishnan. Ask-graphview: A large scale graph
visualization system. IEEE Transactions on Visualization and Computer
Graphics, 12(5):669–676, Sept 2006. doi: 10.1109/TVCG.2006.120

[2] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the Surprising
Behavior of Distance Metrics in High Dimensional Space, pp. 420–434.
Springer Berlin Heidelberg, Berlin, Heidelberg, 2001. doi: 10.1007/3-540
-44503-X 27

[3] M. N. Ahmed, S. M. Yamany, N. Mohamed, A. A. Farag, and T. Moriarty.
A modiﬁed fuzzy c-means algorithm for bias ﬁeld estimation and segmen-
tation of mri data. IEEE Transactions on Medical Imaging, 21(3):193–199,
March 2002. doi: 10.1109/42.996338

[4] M. S. Aldenderfer and R. K. Blashﬁeld. Cluster analysis. SAGE publica-

tions, Beverly Hills, USA, 1984.

[5] B. Alper, N. Riche, G. Ramos, and M. Czerwinski. Design study of linesets,
a novel set visualization technique. IEEE Transactions on Visualization

and Computer Graphics, 17(12):2259–2267, Dec 2011. doi: 10.1109/
TVCG.2011.186

[6] J. Alsakran, Y. Chen, Y. Zhao, J. Yang, and D. Luo. Streamit: Dynamic
visualization and interactive exploration of text streams. In 2011 IEEE
Paciﬁc Visualization Symposium, pp. 131–138, March 2011. doi: 10.1109/
PACIFICVIS.2011.5742382

[7] R. Amar, J. Eagan, and J. Stasko. Low-level components of analytic
activity in information visualization. In IEEE Symposium on Information
Visualization, 2005. INFOVIS 2005., pp. 111–117, Oct 2005. doi: 10.
1109/INFVIS.2005.1532136

[8] C. Andrews, A. Endert, and C. North. Space to think: Large high-
resolution displays for sensemaking. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems, CHI ’10, pp. 55–64.
ACM, New York, NY, USA, 2010. doi: 10.1145/1753326.1753336

[9] J. C. Bezdek. Pattern Recognition with Fuzzy Objective Function Algo-

rithms. Kluwer Academic Publishers, Norwell, MA, USA, 1981.

[10] D. M. Blei and M. I. Jordan. Variational inference for dirichlet process

mixtures. Bayesian Analysis, 1:121–144, 2005.

[11] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal

of machine Learning research, 3(Jan):993–1022, 2003.

[12] L. Bradel, C. North, L. House, and S. Leman. Multi-model semantic
interaction for text analytics. In 2014 IEEE Conference on Visual Analytics
Science and Technology (VAST), pp. 163–172, Oct 2014. doi: 10.1109/
VAST.2014.7042492

[13] P. S. Bradley, U. Fayyad, and C. Reina. Scaling clustering algorithms to
large databases. In Proceedings of the Fourth International Conference on
Knowledge Discovery and Data Mining, KDD’98, pp. 9–15. AAAI Press,
1998.

[14] M. Brehmer and T. Munzner. A multi-level typology of abstract visualiza-
tion tasks. IEEE Transactions on Visualization and Computer Graphics,
19(12):2376–2385, Dec 2013. doi: 10.1109/TVCG.2013.124

[15] M. Brehmer, M. Sedlmair, S. Ingram, and T. Munzner. Visualizing
dimensionally-reduced data: Interviews with analysts and a character-
ization of task sequences. In Proceedings of the Fifth Workshop on Beyond
Time and Errors: Novel Evaluation Methods for Visualization, BELIV
’14, pp. 1–8. ACM, New York, NY, USA, 2014. doi: 10.1145/2669557.
2669559

[16] E. T. Brown, J. Liu, C. E. Brodley, and R. Chang. Dis-function: Learning
In 2012 IEEE Conference on Visual
distance functions interactively.
Analytics Science and Technology (VAST), pp. 83–92, Oct 2012. doi: 10.
1109/VAST.2012.6400486

[17] A. Buja, D. Cook, and D. F. Swayne. Interactive high-dimensional data
visualization. Journal of Computational and Graphical Statistics, 5(1):78–
99, 1996. doi: 10.1080/10618600.1996.10474696

[18] J. D. Carroll and J.-J. Chang. Analysis of individual differences in mul-
tidimensional scaling via an n-way generalization of “eckart-young” de-
composition. Psychometrika, 35(3):283–319, 1970.

[19] H. Chen, W. Chen, H. Mei, Z. Liu, K. Zhou, W. Chen, W. Gu, and K. L.
Ma. Visual abstraction and exploration of multi-class scatterplots. IEEE
Transactions on Visualization and Computer Graphics, 20(12):1683–1692,
Dec 2014. doi: 10.1109/TVCG.2014.2346594

[20] X. Chen, J. Z. Self, L. House, and C. North. Be the data: A new approach
In IEEE Virtual Reality 2016 Workshop on

for immersive analytics.
Immersive Analytics, 03/2016.

[21] J. Choo, S. Bohn, and H. Park. Two-stage framework for visualization
of clustered high dimensional data. In 2009 IEEE Symposium on Visual
Analytics Science and Technology, pp. 67–74, Oct 2009. doi: 10.1109/
VAST.2009.5332629

[22] J. Choo, C. Lee, C. K. Reddy, and H. Park. Utopian: User-driven topic
modeling based on interactive nonnegative matrix factorization. IEEE
Transactions on Visualization and Computer Graphics, 19(12):1992–2001,
Dec 2013. doi: 10.1109/TVCG.2013.212

[23] J. Chuang and D. J. Hsu. Human-centered interactive clustering for data
analysis. Conference on Neural Information Processing Systems (NIPS).
Workshop on Human-Propelled Machine Learning, 2014.

[24] C. Collins, G. Penn, and S. Carpendale. Bubble sets: Revealing set
relations with isocontours over existing visualizations. IEEE Transactions
on Visualization and Computer Graphics, 15(6):1009–1016, Nov 2009.
doi: 10.1109/TVCG.2009.122

[25] R. Cordeiro de Amorim and P. Komisarczuk. On Initializations for the
Minkowski Weighted K-Means, pp. 45–55. Springer Berlin Heidelberg,
Berlin, Heidelberg, 2012. doi: 10.1007/978-3-642-34156-4 6

[26] I. S. Dhillon and D. S. Modha. Concept decompositions for large sparse

text data using clustering. Machine Learning, 42(1):143–175, 2001. doi:
10.1023/A:1007612920971

[27] C. Ding and X. He. K-means clustering via principal component analysis.
In Proceedings of the Twenty-ﬁrst International Conference on Machine
Learning, ICML ’04, pp. 29–. ACM, New York, NY, USA, 2004. doi: 10.
1145/1015330.1015408

[28] C. Ding and T. Li. Adaptive dimension reduction using discriminant
analysis and k-means clustering. In Proceedings of the 24th International
Conference on Machine Learning, ICML ’07, pp. 521–528. ACM, New
York, NY, USA, 2007. doi: 10.1145/1273496.1273562

[29] D. L. Donoho. High-dimensional data analysis: The curses and blessings
of dimensionality. In AMS Conference on Math Challenges of the 21st
Century, 2000.

[30] J. C. Dunn. A fuzzy relative of the isodata process and its use in detecting
compact well-separated clusters. Journal of Cybernetics, 3(3):32–57, 1973.
doi: 10.1080/01969727308546046

[31] T. Dwyer, Y. Koren, and K. Marriott. Ipsep-cola: An incremental pro-
cedure for separation constraint layout of graphs. IEEE Transactions on
Visualization and Computer Graphics, 12(5):821–828, Sept 2006. doi: 10.
1109/TVCG.2006.156

[32] C. F. Eick, N. Zeidat, and Z. Zhao. Supervised clustering - algorithms and
beneﬁts. In 16th IEEE International Conference on Tools with Artiﬁcial
Intelligence, pp. 774–776, Nov 2004. doi: 10.1109/ICTAI.2004.111

[33] A. Endert, S. Fox, D. Maiti, S. Leman, and C. North. The semantics of
clustering: Analysis of user-generated spatializations of text documents.
In Proceedings of the International Working Conference on Advanced
Visual Interfaces, AVI ’12, pp. 555–562. ACM, New York, NY, USA,
2012. doi: 10.1145/2254556.2254660

[34] A. Endert, C. Han, D. Maiti, L. House, S. Leman, and C. North.
Observation-level interaction with statistical models for visual analyt-
ics. In 2011 IEEE Conference on Visual Analytics Science and Technology
(VAST), pp. 121–130, Oct 2011. doi: 10.1109/VAST.2011.6102449

[35] A. Endert, M. S. Hossain, N. Ramakrishnan, C. North, P. Fiaux, and
C. Andrews. The human is the loop: new directions for visual analytics.
Journal of Intelligent Information Systems, 43(3):411–435, 2014. doi: 10.
1007/s10844-014-0304-9

[36] V. Estivill-Castro. Why so many clustering algorithms: A position paper.
SIGKDD Explor. Newsl., 4(1):65–75, June 2002. doi: 10.1145/568574.
568575

[37] U. M. Fayyad, A. Wierse, and G. G. Grinstein. Information visualization

in data mining and knowledge discovery. Morgan Kaufmann, 2002.

[38] I. K. Fodor. A Survey of Dimension Reduction Techniques. May 2002. doi:

10.2172/15002155

[39] S. L. France and J. D. Carroll. Two-way multidimensional scaling: A
review. IEEE Transactions on Systems, Man, and Cybernetics, Part C
(Applications and Reviews), 41(5):644–661, Sept 2011. doi: 10.1109/
TSMCC.2010.2078502

[40] J. H. Friedman and J. W. Tukey. A projection pursuit algorithm for
exploratory data analysis. IEEE Transactions on Computers, C-23(9):881–
890, Sept 1974. doi: 10.1109/T-C.1974.224051

[41] E. R. Gansner, Y. Hu, and S. Kobourov. Gmap: Visualizing graphs
and clusters as maps. In 2010 IEEE Paciﬁc Visualization Symposium
(PaciﬁcVis), pp. 201–208, March 2010. doi: 10.1109/PACIFICVIS.2010.
5429590

[42] I. Guyon and A. Elisseeff. An introduction to variable and feature selection.

J. Mach. Learn. Res., 3:1157–1182, Mar. 2003.
[43] H. H. Harman. Modern factor analysis. 1960.
[44] J. Heer and D. Boyd. Vizster: visualizing online social networks. In
IEEE Symposium on Information Visualization, 2005. INFOVIS 2005., pp.
32–39, Oct 2005. doi: 10.1109/INFVIS.2005.1532126

[45] C. Heine and G. Scheuermann. Manual clustering reﬁnement using inter-
action with blobs. In Proceedings of the 9th Joint Eurographics / IEEE
VGTC Conference on Visualization, EUROVIS’07, pp. 59–66. Eurograph-
ics Association, Aire-la-Ville, Switzerland, Switzerland, 2007. doi: 10.
2312/VisSym/EuroVis07/059-066

[46] L. House, S. Leman, and C. Han. Bayesian visual analytics: Bava. Sta-
tistical Analysis and Data Mining, 8(1):1–13, 2015. doi: 10.1002/sam.
11253

[47] X. Hu, L. Bradel, D. Maiti, L. House, C. North, and S. Leman. Semantics
of directly manipulating spatializations. IEEE Transactions on Visual-
ization and Computer Graphics, 19(12):2052–2059, Dec 2013. doi: 10.
1109/TVCG.2013.188

[48] J. Z. Huang, M. K. Ng, H. Rong, and Z. Li. Automated variable weighting

in k-means type clustering. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27(5):657–668, May 2005. doi: 10.1109/TPAMI.
2005.95

[49] A. Hyv¨arinen, J. Karhunen, and E. Oja. Independent component analysis,

vol. 46. John Wiley & Sons, 2004.

[50] S. Ingram, T. Munzner, and M. Olano. Glimmer: Multilevel mds on
the gpu. IEEE Transactions on Visualization and Computer Graphics,
15(2):249–261, March 2009. doi: 10.1109/TVCG.2008.85

[51] R. Jianu, A. Rusu, Y. Hu, and D. Taggart. How to display group infor-
mation on node-link diagrams: An evaluation. IEEE Transactions on
Visualization and Computer Graphics, 20(11):1530–1541, Nov 2014. doi:
10.1109/TVCG.2014.2315995

[52] P. Joia, F. Petronetto, and L. G. Nonato. Uncovering representative groups
in multidimensional projections. In Proceedings of the 2015 Eurographics
Conference on Visualization, EuroVis ’15, pp. 281–290. Eurographics
Association, Aire-la-Ville, Switzerland, Switzerland, 2015. doi: 10.1111/
cgf.12640

[53] E. Kandogan. Star coordinate: A multi-dimensional visualization tech-
nique with uniform treatment of dimensions. In Proceedings of the IEEE
Information Visualization Symposium, vol. 650, p. 22.

[54] E. Kandogan. Just-in-time annotation of clusters, outliers, and trends
in point-based data visualizations. In 2012 IEEE Conference on Visual
Analytics Science and Technology (VAST), pp. 73–82, Oct 2012. doi: 10.
1109/VAST.2012.6400487

[55] T. Kohonen. The self-organizing map. Proceedings of the IEEE,

78(9):1464–1480, Sep 1990. doi: 10.1109/5.58325

[56] H.-P. Kriegel, P. Kr¨oger, and A. Zimek. Clustering high-dimensional data:
A survey on subspace clustering, pattern-based clustering, and correlation
clustering. ACM Trans. Knowl. Discov. Data, 3(1):1:1–1:58, Mar. 2009.
doi: 10.1145/1497577.1497578

[57] H. Lee, J. Kihm, J. Choo, J. Stasko, and H. Park.

ivisclustering: An
interactive visual document clustering via topic modeling. Computer
Graphics Forum, 31(3pt3):1155–1164, 2012. doi: 10.1111/j.1467-8659.
2012.03108.x

[58] J. A. Lee and M. Verleysen. Nonlinear dimensionality reduction. Springer

Science & Business Media, 2007.

[59] S. C. Leman, L. House, D. Maiti, A. Endert, and C. North. Visual to

276, 1953. doi: 10.1007/BF02289263

[72] D. Niu, J. G. Dy, and M. I. Jordan. Dimensionality reduction for spectral
clustering. In Proceedings of the 14th International Conference Artiﬁcial
Intelligence and Statistics, AISTATS ’11, pp. 552–560. ACM, New York,
NY, USA, 2011.

[73] F. Paulovich, D. Eler, J. Poco, C. Botha, R. Minghim, and L. Nonato.
Piecewise laplacian-based projection for interactive data exploration and
organization. Computer Graphics Forum, 30(3):1091–1100, 2011. doi: 10
.1111/j.1467-8659.2011.01958.x

[74] K. Pearson. Principal components analysis. The London, Edinburgh and

Dublin Philosophical Magazine and Journal, 6(2):566, 1901.

[75] D. Pelleg, A. W. Moore, et al. X-means: Extending k-means with efﬁcient
estimation of the number of clusters. In ICML, vol. 1, pp. 727–734, 2000.
[76] P. Pirolli and S. Card. The sensemaking process and leverage points for an-
alyst technology as identiﬁed through cognitive task analysis. Proceedings
of International Conference on Intelligence Analysis, pp. 2–4, 2005.

[77] B. Rieck and H. Leitte. Persistent homology for the evaluation of dimen-
sionality reduction schemes. Computer Graphics Forum, 34(3):431–440,
2015. doi: 10.1111/cgf.12655

[78] D. Ro and H. Pe. Pattern classiﬁcation and scene analysis. John Wiley &

Sons, New York, USA, 1973.

[79] B. Saket, P. Simonetto, S. Kobourov, and K. Brner. Node, node-link,
and node-link-group diagrams: An evaluation. IEEE Transactions on
Visualization and Computer Graphics, 20(12):2231–2240, Dec 2014. doi:
10.1109/TVCG.2014.2346422

[80] J. Z. Self, R. K. Vinayagam, J. T. Fry, and C. North. Bridging the gap
between user intention and model parameters for human-in-the-loop data
analytics. In Proceedings of the Workshop on Human-In-the-Loop Data
Analytics, HILDA ’16, pp. 3:1–3:6. ACM, New York, NY, USA, 2016.
doi: 10.1145/2939502.2939505

[81] P.-N. Tan, M. Steinbach, and V. Kumar. Data mining cluster analysis:
basic concepts and algorithms. In Introduction to data mining, chap. 8.
Pearson Education India, 2013.

[82] J. B. Tenenbaum, V. d. Silva, and J. C. Langford. A global geometric frame-
work for nonlinear dimensionality reduction. Science, 290(5500):2319–
2323, 2000. doi: 10.1126/science.290.5500.2319

[83] R. L. Thorndike. Who belongs in the family? Psychometrika, 18(4):267–

[84] M. E. Tipping and C. M. Bishop. Probabilistic principal component
analysis. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 61(3):611–622, 1999.

[85] W. S. Torgerson. Theory and methods of scaling. 1958.
[86] C. Turkay, P. Filzmoser, and H. Hauser. Brushing dimensions - a dual
visual analysis model for high-dimensional data. IEEE Transactions on
Visualization and Computer Graphics, 17(12):2591–2599, Dec 2011. doi:
10.1109/TVCG.2011.178

[87] F. Valafar. Pattern recognition techniques in microarray data analysis.
Annals of the New York Academy of Sciences, 980(1):41–64, 2002. doi: 10
.1111/j.1749-6632.2002.tb04888.x

[88] T. von Landesberger, S. Fiebig, S. Bremm, A. Kuijper, and D. W. Fellner.
Interaction Taxonomy for Tracking of User Actions in Visual Analytics
Applications, pp. 653–670. Springer New York, New York, NY, 2014. doi:
10.1007/978-1-4614-7485-2 26

[89] K. Wagstaff, C. Cardie, S. Rogers, S. Schr¨odl, et al. Constrained k-means
clustering with background knowledge. In Proceedings of the Eighteenth
International Conference on Machine Learning, vol. 1, pp. 577–584, 2001.
[90] J. Wenskovitch and C. North. Observation-level interaction with clustering
and dimension reduction algorithms. In Proceedings of the 2nd Workshop
on Human-In-the-Loop Data Analytics, HILDA’17, pp. 14:1–14:6. ACM,
New York, NY, USA, 2017. doi: 10.1145/3077257.3077259

[91] A. Wism¨uller, M. Verleysen, M. Aupetit, and J. A. Lee. Recent advances
in nonlinear dimensionality reduction, manifold and topological learning.
In ESANN, 2010.

[92] R. Xu and D. Wunsch. Survey of clustering algorithms. IEEE Transactions
on Neural Networks, 16(3):645–678, May 2005. doi: 10.1109/TNN.2005.
845141

[93] J. S. Yi, Y. a. Kang, and J. Stasko. Toward a deeper understanding of the
role of interaction in information visualization. IEEE Transactions on
Visualization and Computer Graphics, 13(6):1224–1231, Nov 2007. doi:
10.1109/TVCG.2007.70515

[94] H. Zha, X. He, C. Ding, M. Gu, and H. D. Simon. Spectral relaxation
for k-means clustering. In Advances in Neural Information Processing
Systems, pp. 1057–1064, 2002.

parametric interaction (v2pi). PloS one, 8(3), 2013.

[60] M. Levandowsky and D. Winter. Distance between sets. Nature,

234(5323):34–35, 1971.

[61] S. Liu, D. Maljovec, B. Wang, P. T. Bremer, and V. Pascucci. Visualizing
high-dimensional data: Advances in the past decade. IEEE Transactions
on Visualization and Computer Graphics, 23(3):1249–1268, March 2017.
doi: 10.1109/TVCG.2016.2640960

[62] S. Liu, B. Wang, P.-T. Bremer, and V. Pascucci. Distortion-guided
structure-driven interactive exploration of high-dimensional data. Com-
puter Graphics Forum, 33(3):101–110, 2014. doi: 10.1111/cgf.12366

[63] S. Liu, M. X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian. Interactive,
topic-based visual text summarization and analysis. In Proceedings of
the 18th ACM Conference on Information and Knowledge Management,
CIKM ’09, pp. 543–552. ACM, New York, NY, USA, 2009. doi: 10.
1145/1645953.1646023

[64] S. Lloyd. Least squares quantization in pcm.

IEEE Transactions on
Information Theory, 28(2):129–137, Mar 1982. doi: 10.1109/TIT.1982.
1056489

[65] L. v. d. Maaten and G. Hinton. Visualizing data using t-sne. J. Mach.

Learn. Res., 9:2579–2605, Sept. 2008.

[66] G. M. H. Mamani, F. M. Fatore, L. G. Nonato, and F. V. Paulovich.
User-driven feature space transformation. Computer Graphics Forum,
32(3pt3):291–299, 2013. doi: 10.1111/cgf.12116

[67] A. Mayorga and M. Gleicher. Splatterplots: Overcoming overdraw in
scatter plots. IEEE Transactions on Visualization and Computer Graphics,
19(9):1526–1538, Sept 2013. doi: 10.1109/TVCG.2013.65

[68] K. Misue, P. Eades, W. Lai, and K. Sugiyama. Layout adjustment and the
mental map. Journal of Visual Languages & Computing, 6(2):183 – 210,
1995. doi: 10.1006/jvlc.1995.1010

[69] T. Munzner. Visualization Analysis and Design. CRC Press, 2014.
[70] E. J. Nam, Y. Han, K. Mueller, A. Zelenyuk, and D. Imre. Clustersculptor:
A visual analytics tool for high-dimensional data. In 2007 IEEE Sympo-
sium on Visual Analytics Science and Technology, pp. 75–82, Oct 2007.
doi: 10.1109/VAST.2007.4388999

[71] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis

and an algorithm. In NIPS, vol. 14, pp. 849–856, 2001.

",False,2018.0,{},False,False,journalArticle,False,5UJ67Q46,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/8019882/,,Towards a Systematic Combination of Dimension Reduction and Clustering in Visual Analytics,5UJ67Q46,False,False
8KJJXFM6,VV6MIDHB,"Semantic Interaction for Visual Text Analytics 
Chris North 
Alex Endert 
Virginia Tech 
Virginia Tech 

Patrick Fiaux 
Virginia Tech 

Blacksburg, VA USA 

aendert@vt.edu 

Blacksburg, VA USA 

pfiaux@vt.edu 

 

Blacksburg, VA USA 

north@vt.edu 

by 

For 

through 

ABSTRACT 
Visual analytics emphasizes sensemaking of large, complex 
datasets 
interactively  exploring  visualizations 
generated 
example, 
statistical  models. 
dimensionality  reduction  methods  use  various  similarity 
metrics to visualize textual document collections in a spatial 
metaphor,  where  similarities  between  documents  are 
approximately  represented  through  their  relative  spatial 
distances  to  each  other  in  a  2D  layout.  This  metaphor  is 
designed to mimic analysts’ mental models of the document 
collection  and  support  their  analytic  processes,  such  as 
clustering similar documents together. However, in current 
methods, users must interact with such visualizations using 
controls  external  to  the  visual  metaphor,  such  as  sliders, 
menus, or text fields, to directly control underlying model 
parameters  that  they  do  not  understand  and  that  do  not 
relate  to  their  analytic  process  occurring  within  the  visual 
metaphor.  In  this  paper,  we  present  the  opportunity  for  a 
new  design  space  for  visual  analytic  interaction,  called 
semantic  interaction,  which  seeks  to  enable  analysts  to 
spatially interact with such models directly within the visual 
metaphor using interactions that derive from their analytic 
process,  such  as  searching,  highlighting,  annotating,  and 
repositioning  documents.  Further,  we  demonstrate  how 
semantic  interactions  can  be  implemented  using  machine 
learning 
tool,  called 
ForceSPIRE, for interactive analysis of textual data within 
a  spatial  visualization.    Analysts  can  express  their  expert 
domain knowledge about the documents by simply moving 
them,  which  guides  the  underlying  model  to  improve  the 
overall layout, taking the user’s feedback into account. 
Author Keywords 
Visualization; visual analytics; interaction 
ACM Classification Keywords 
H5.m.  Information  interfaces  and  presentation  (e.g.,  HCI): 
Miscellaneous.  
General Terms 
Design; Human Factors; Theory 

in  a  visual  analytic 

techniques 

 
Permission to  make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CHI’12, May 5–10, 2012, Austin, Texas, USA. 
Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00. 
 

 

INTRODUCTION 
Visual analytics bases its success on combining the abilities 
of statistical models, visualization, and human intuition for 
users to gain insight into large, complex datasets [23]. This 
success often hinges on the ability for users to interact with 
the  information,  manipulating  the  visualization  based  on 
their  domain  expertise,  interactively  exploring  possible 
connections, and investigating hypotheses. It is through this 
interactive exploration that users are able to make sense of 
complex  datasets,  a  process  referred  to  as  sensemaking 
[19].  
The  two  primary  parts  of  sensemaking  are  foraging  and 
synthesis. Foraging refers to the stages of the process where 
users filter and gather collections of interesting or relevant 
information.  Then,  using  that  information,  users  advance 
through  the  synthesis  stages  of  the  process,  where  they 
construct  and  test  hypotheses  about  how  the  foraged 
information  may  relate  to  the  larger  plot.  Tools  exist  that 
support users for either foraging or synthesis – but not both. 
In  this  paper  we  present  semantic  interaction,  combining 
the  foraging  abilities  of  statistical  models  with  the  spatial 
synthesis abilities of analysts. Semantic interaction is based 
on the following principles: 
1. Visual  “near=similar”  metaphor  supports  analysts’ 
spatial  cognition,  and  is  generated  by  statistical  models 
and similarity metrics. [22] 

2. Use  semantic  interactions  within  the  visual  metaphor, 
based  on  common  interactions  occurring  in  spatial 
analytic  processes  [4]  such  as  searching,  highlighting, 
annotating, and repositioning documents.  

3. Interpret  and  map  the  semantic  interactions  to  the 
underlying parameters of the model, by updating weights 
and adding information. 

4. Shield  the  users  from  the  complexity  of  the  underlying 

mathematical models and parameters. 

5. Models  learn  incrementally  by  taking  into  account 
interaction during the entire analytic process, supporting 
analysts’ process of incremental formalism [10]. 

6. Provide  visual  feedback  of  the  updated  model  and 

learned parameters within the visual metaphor. 

7. Reuse  learned  model  parameters  in  future  or  streaming 

data within the visual metaphor. 

To  demonstrate  the  concept  of  semantic  interaction,  we 
present  a  prototype  visual  analytics  tool,  ForceSPIRE,  for 
spatial analysis of textual information. In ForceSPIRE, the 
user  interaction  takes on  a deeper,  more  integrated role in 

the  exploratory  spatial  analytic  process.  This  is  done 
through capturing the semantic interaction, interpreting the 
analytical  reasoning  associated  with  the  interaction,  and 
updating the statistical model, and ultimately updating the 
spatialization.  Hence,  users  are  able  to  leverage  semantic 
interaction  to  explore  and  analyze  the  data  interactively, 
while  the  system  is  responsible  for  properly  updating  the 
underlying statistical model.  
RELATED WORK 
Foraging Tools 

 

Figure  1.  A  model  of  interaction  with  foraging  tools.  Users 
interact  directly  with  the  statistical  model  (red),  then  gain 
insight  through  observing  the  change  in  the  visualization 
(blue). 
We  categorize  foraging  tools  by  their  ability  to  pass  data 
through  complex  statistical  models  and  visualize  the 
computed structure of the dataset for the user to gain insight 
(Figure  1).  Thus,  users  interact  with  these  tools  primarily 
through directly manipulating the parameters of the model 
used  for  computing  the  structure.  As  such,  users  are 
required  to  translate  their  domain  expertise  and  semantics 
about  the  information  to  determine  which  (and  by  how 
much) to adjust these parameters. The following examples 
further describe this category of tools. 
Visualizations such as IN-SPIRE’s “Galaxy View” (shown 
in  Figure  3)  present  users  with  a  spatial  layout  of  textual 
information where similar documents are proximally close 
to  one  another  [25].  An  algorithm  creates  the  layout  by 
mapping the high-dimensional collection of text documents 
down  to  a  two-dimensional  view.  In  these  spatializations, 
the  spatial  metaphor  is  one  from  which  users  can  infer 
meaning  of  the  documents  based  on  their  location.  The 
notion  of  distance  between  documents  represents  how 
similar the two documents are (i.e., more similar documents 
are  placed  closer  together).  For  instance,  a  cluster  of 
documents  represents  a  group  of  similar  documents,  and 
documents  placed  between  two  clusters  implies  those 
documents are connected to both clusters. These views are 
beneficial  as  they  allow  users  to  visually  gain  a  quick 
overview  of  the  information,  such  as  what  key  themes  or 
groups  exist  within  the  dataset.  The  complex  statistical 
models  that  compute  similarity  between  documents  are 
based on the structure within the data, such as term or entity 
frequency. In order to interactively change the view, users 
are  required  to  directly  adjust  keyword  weights,  add  or 
remove documents/keywords, or provide more information 
on how to parse the documents for keywords/entities upon 
import. 

 

to  a 

to  understand 

the 

[15].  Through  adjusting 

Similarly, an interactive visualization tool called iPCA uses 
Principal  Component  Analysis  (PCA)  to  reduce  high-
dimensional  data  down 
two-dimensional  plot, 
providing  users  with  sliders  and  other  visual  controls  for 
directly  adjusting  numerous  parameters  of  the  algorithm, 
such  as  individual  eigenvalues,  eigenvectors,  and  other 
components  of  PCA 
the 
parameters,  the  user  can  observe  how  the  visualization 
changes.  This  allows  users  to  gain  insight  into  a  dataset, 
given  they  have  a  thorough  understanding  of  PCA, 
necessary 
the 
changes they are making to the model parameters. 
Alsakran  et  al.  presented  a  visualization 
system, 
STREAMIT,  capable  of  spatially  arranging  text  streams 
based  on  keyword  similarity  [3].  Again,  users  can 
interactively  explore  and  adjust  the  spatial  layout  through 
directly  changing  the  weight  of  keywords  that  they  find 
important.  In  addition,  STREAMIT  allows  for  users  to 
conduct  a  temporal  investigation  of  how  clusters  change 
over time. 
Synthesis Tools 

implications  behind 

 

Figure  2.  A  model  of  interaction  with  synthesis  tools.  Users 
manually  create  a  spatial  layout  of  the  information  to 
maintain and organize their insights about the data. 
Synthesis  tools  focus  on  allowing  users  to  organize  and 
maintain their hypotheses and insight regarding the data in 
a  spatial  medium.  In  large  part,  this  is  done  through 
presenting users with a flexible spatial workspace in which 
they  can  organize  information  through  creating  spatial 
structures,  such  as  clusters,  timelines,  stories,  etc.  (Figure 
2). In doing so, users externalize their thought processes (as 
well  as  their  insights)  into  a  spatial  layout  of  the 
information. 
For example, Analyst’s Notebook [2] provides users with a 
spatial workspace where information can be organized, and 
connections  between  specific  pieces  of  information  (e.g., 
entities, documents, events, etc.) can be created. Similarly, 
The Sandbox [26] enables users to create a series of cases 
(collections  of 
information)  which  can  be  organized 
spatially within the workspace.  
From  previous  studies,  we  found  cognitive  advantages 
associated  with  the  manual  creation  of  a  spatial  layout  of 
the  information  [4].  By  providing  users  a  workspace  in 
which  to  manually  create  spatial  representations  of  the 
information, users were able to externalize their semantics 
of the information into the workspace. That is, they created 
spatial  structures  (e.g.,  clusters,  timelines,  etc.),  and  both 
the structures as well as the locations relative to remaining 
layout  carried  meaning  to  the  users  with  regards  to  their 
sensemaking process. Marshall et al. have pointed out that 

this 

interaction  (and 

From  the  sensemaking  loop  presented  by  Pirolli  and  Card 
[19],  we  learn  that  in  intelligence  analysis,  that  analytic 
process  consists  not  only  of  the  information  that  is 
explicitly  within  the  dataset  being  analyzed,  but  also  the 
domain knowledge of the analyst performing the analysis. It 
is through this domain knowledge that analysts interact and 
explore  the  dataset  to  “make  sense”  of  the  information. 
Thus,  we  believe 
the  domain 
knowledge  associated  with  it)  is  equally  important  as  the 
raw data, and must be incorporated into the visualization by 
tightly coupling the model with the interaction. 
From this body of work, we most notably come away with 
an understanding that 1) analysts fundamentally understand 
the spatial metaphor used in many spatial visualizations, 2) 
many  of  these  systems  are  constructed  using  complex 
mathematical  algorithms  to  transform  high-dimensional 
data  to  two  dimensions,  and  3)  in  most  cases  these 
algorithms  can  be  controlled  by  analysts  largely  through 
visual  controls  (e.g.,  sliders,  knobs,  etc.)  to  directly  adjust 
parameters of the algorithms, updating the spatial layout. 
SEMANTIC INTERACTION 

 

Figure 4. A model of semantic interaction. Users are able to 
interact directly in the spatial metaphor. The system updates 
the corresponding parameters of the statistical model based on 
the analytic reasoning of the users. Finally, the model updates 

the visualization based on the changes, thus unifying the 
synthesis and foraging stages of the sensemaking loop. 

In the purest sense, semantic interaction refers to interaction 
occurring  within  a  spatial  visualization,  with  the  added 
benefit that it is tightly coupled to the model calculating the 
spatial layout (Figure 4). Given the previous work of what 
interaction  in  visual  analytic  tools  is,  semantic  interaction 
occupies a new design space for interaction. It merges the 
ability to change the statistical model while maintaining the 
flexibility  and  familiar  methods  for  interacting  within  the 
metaphor  of  spatial  visualizations.  Users  can  benefit  from 
semantic  interactions  in  that  they  can  interact  within  a 
metaphor  which 
they  are  familiar  with,  performing 
interactions  which  are  part  of  the  spatial  analytic  process 
[4], without having to focus on formal updates to the model.  
Semantic  interaction  leverages  the  cognitive  connection 
formed  between  the  user  and  the  spatial  layout.  The 
following intelligence analysis scenario is representative of 
the strategies and interactions of analysts when performing 
an  intelligence  analysis  task  of  textual  documents  in  a 
spatial visualization, as previously found by Andrews et al. 
[4],  and  further  motivates  and  explains  the  concept  of 
semantic interaction: 

 
Figure  3.  The  IN-SPIRE  Galaxy  View  showing  a 
spatializtiation  of  documents  represented  as  dots.  Each 
cluster of dots represents a group of similar documents.  
 
allowing users to create such informal relationships within 
information  is  beneficial,  as  it  does  not  require  users  to 
formalize these relationships [17].  
From this related work, we believe a trend is emerging in 
how interaction is currently handled in many visual analytic 
systems where complex statistical models are used – users 
are  required  to  go  outside  of  the  metaphor.  That  is,  while 
the  visual  representation  given  to  users  is  spatial,  the 
methods of interaction require users to step outside of that 
metaphor  and  interact  directly  with  the  parameters  of  the 
statistical model using visual controls, toolbars, etc.  
There  has  been  some  work  in  providing  more  easy  to  use 
interactions  for  updating  statistical  models.  For  example, 
relevance feedback has been used for content-based image 
retrieval, where users are able to move images towards or 
away  from  a  single  image  in  order  to  portray  pair-wise 
similarity  or  dissimilarity  [24].  From  there,  an  image 
retrieval algorithm determines the features and dimensions 
shared between the images that the user has determined as 
being  similar.  We  view  this  as  one  example  where  the 
interaction stays in the spatial metaphor of the visualization.  
Also, spatializations of document sets exist that allow users 
to place “points of interest” into the spatial layout. In VIBE, 
users are allowed to define multiple points of interest in the 
spatial  layout  that  correspond  to  a  series  of  keywords 
describing  a  subject  matter  of  interest  to  the  user  [18]. 
Similarly,  Dust  &  Magnet  [27]  allows  users  to  place  a 
series  of  “magnets”  representing  keywords  into  the  space 
and observe how documents are attracted or repelled from 
the  locations  of  these  magnets.  Through  both  of  these 
systems, users can interact in the spatial metaphor through 
these  placements  of  “nodes”  representing  keywords. 
However, the focus of semantic interaction is on interacting 
with  data  (i.e.,  documents),  an 
important  distinction 
discussed in the following section. 

 

 

 
Figure  5.  (top)  The  basic  version  of  the  “visualization 
pipeline”.  Interaction  can  be  performed  on  directly  the 
Algorithm  (blue  arrow)  or  the  data  (red  arrow).  (bottom) 
Our  modified  version  of 
for  semantic 
interaction,  where  the  user  interacts  within  the  spatial 
metaphor (purple arrow). 

the  pipeline 

During her analysis, an intelligence analyst finds a 
suspicious  and 
interesting  phrase  within  a 
document. While reading through the document, she 
highlights  the  phrase  “suspicious  individuals  were 
spotted  at  the  airport”,  in  order  to  more  easily 
recall  this  information  later.  After  she  finishes 
reading the document, she moves the document into 
the  bottom  right  corner  of  her  workspace,  in  the 
proximity of other documents related to an event at 
an airport. To remind herself of her hypothesis, she 
annotates  the  document  with  “might  be  related  to 
Revolution  Now  terrorist  group”.  Now,  with  the 
goal  of 
the 
“airport”, she searches for the term, continuing her 
investigation. 

further  examining 

the  events  at 

investigating 

that  each  of 

instead  point  out 

the  analytic  process  of 

In addition to the three forms of semantic interaction in the 
scenario,  Table  1  provides  a  list  of  various  forms  of 
semantic  interaction,  including  how  each  can  be  used 
within 
textual 
information  spatially.  We  do  not  claim  that  this  list  is 
complete,  but 
these 
interactions  can  relate  to  a  user’s  reasoning  within  the 
analytic process.  
Designing for Semantic Interaction 
In order for analysts to interact with information in a spatial 
metaphor, it must first be created. Following the model of 
the visualization pipeline [13], this creation calls for a series 
of  mathematical  transformations,  turning  raw  data  into  a 
spatial  layout  –  much  the  way  many  of  the  visualizations 
mentioned  previously  are  constructed.  However,  these 
visualizations  fit  this  model,  as  their  user  interactions  are 
primarily  focused  on  directly  modifying  the  statistical 
model  (as  well  as  other  attributes  of  the  visualization  or 
data  transformation).  Designing  for  semantic  interaction 
requires  a  fundamentally  different  model  for  how  tools 
integrate  user  interaction  –  one  that  can  capture  the 
interaction,  interpret  the  associated  analytical  reasoning, 
and update the appropriate mathematical parameters.  
Figure  5  illustrates  this  model,  where  the  spatialization  is 
treated  a  medium  through  which  the  user  can  perceive 

 

Figure 6. Overview of how nodes and edges in ForceSPIRE’s 
force-directed layout are created from documents (Doc) and 
entities (Ent), respectively.  

 

 

it 

interaction, 

information  and  gain  insight,  as  well  as  interact  and 
perform  his  analysis.  Through  expanding  the  pipeline  to 
accommodate  for  semantic 
is  a  more 
appropriate match to the user’s sensemaking process. 
Capturing the Semantic Interaction 
A  non-trivial  first  step  in  the  model  is  capturing  the  user 
interaction.  Much  research  has  been  done  in  this  area, 
primarily  for  the  purpose  of  maintaining  process  history 
(e.g., [5], [21], [12], etc.). When considering how to capture 
interaction,  one  decision  to  be  made  is  at  what  “level”  to 
capture  it.  For  example,  GlassBox  [6]  captures  interaction 
at a rudimentary level (i.e. mouse clicks and key strokes), 
while  Graphical  History  [14]  keeps  track  of  a  series  of 
previous  visualizations  as  a  user  changes  the  visualization 
during the exploration of the data.  
Semantic  interaction  is  captured  at  a  data  level,  as  the 
interactions  occur  on  the  data,  and  within  the  spatial 
metaphor.  Using 
the 
interaction being captured would be: 

the  earlier  analytic  scenario, 

•  The highlighted phrase 
•  When the highlighting occurs (timestamp) 
•  The color chosen for the highlight 
•  The document in which the highlight occurs 
•  The new document location 
•  The text of the annotation 

By  capturing  (and  storing)  the  interaction  history,  we  can 
interpret the analytical reasoning of the user. Thus, we not 
only capture the interaction, but also use it. 
Interpreting the Associated Analytical Reasoning 
In interpreting the interaction, the goal is for the system to 
determine  the  analytical  reasoning  associated  with  the 
interactions  and  update  the  model  accordingly.  From 
previous findings [4], we can associate analytical reasoning 
with  forms  of  semantic  interaction  (see  Table  1).  It  is 
essentially the model’s task to determine  why, in terms of 
the data, the interaction occurred. To answer this question, 
we do not propose that this model can accurately gauge user 
intent.  Instead,  the  goal  is  to  calculate,  based  on  the  data, 

Figure 7. Using ForceSPIRE on a 32 megapixel large, 
high-resolution display. 

 

 
what information is consistent with the captured interaction. 
For  instance,  we  associate  text  highlighting  with  adding 
importance to the text being highlighted. We do not claim 
that we can associate the interaction of highlighting to the 
intuition that spurred the analyst to highlight the text, which 
is far more challenging, and arguably impossible. 
We refer to the captured and interpreted interactions as soft 
data, in comparison to the hard data that is extracted from 
the raw textual information (e.g., term or entity frequency, 
titles,  document  length,  etc.).  We  define  soft  data  as  the 
stored result of user interaction as interpreted by the system. 
In  representing  interaction  as  soft  data,  the  algorithm  can 
calculate  and  reconfigure  the  spatial  layout  accordingly. 
Figure  5  illustrates  how  our  approach  differs  from  the 
traditional visualization pipeline. 
There has been previous work in capturing and interpreting 
reasoning from user interaction. For instance, Dou et al. [7] 
performed  a  study  where  financial  analysts  were  asked 
analyze  a  dataset  using  WireVis,  an  interactive  financial 
transaction visualization. The tool developers then analyzed 
the captured interaction, and assumptions were made about 
the  reasoning  of  the  analysts  at  specific  points  in  the 
investigation. These results were compared to the analysts’ 
self-recorded  reasoning,  and  found  to  be  accurate  up  to 
82%. While our work has similar goals (i.e., interpreting the 
analytical reasoning associated with the analysts through an 
evaluation  of  the  interaction)  our  model  does  so  through 
tightly  integrating  the  interaction  with  the  underlying 
mathematical model. In doing so, the interpretation can be 
done algorithmically. 
Updating the Underlying Model 
Through  metric  learning  of  distance  weights,  the  layout 
uses  the  soft  data  to  update  the  underlying  model. 
Depending  on  the  algorithm  used  to  compute  the  spatial 
layout,  the  precise  parameters  being  updated  will  vary.  In 
general,  this  will  refer  to  weighting  of  a  combination  of 
dimensions  that  will  help  guide  the  model  as  to  which 
dimensions the user finds important.  
FORCESPIRE: SYSTEM OVERVIEW 
ForceSPIRE  is  a  visual  analytics  prototype  designed  for 
specific 
(document 
movement,  text  highlighting,  search,  and  annotation)  for 

forms  of 

interaction 

semantic 

 

Figure  8.  Moving  the  document  shown  by  the  arrow, 
ForceSPIRE  adapts  the  layout  accordingly.  Documents 
sharing entities with the document being moved follow. 

 

interactively exploring textual data. The system has a single 
spatial  view  (shown  in  Figure  12),  where  a  collection  of 
documents is represented spatially based on similarity (i.e., 
documents closer together are more similar).  
ForceSPIRE is designed for large, high-resolution displays 
(such  as  the  one  shown  in  Figure  7).  As  semantic 
interaction emphasizes the importance of context in which 
the  interaction  takes  place  (e.g.,  highlighting  text  in  the 
context  of  the  document),  having  the  full  detail  text 
available  in  the  context  of  the  spatial  layout  is  beneficial 
over having a single document viewer. Further, the physical 

Table  1.  Forms  of  semantic  interaction.  Each  interaction 
corresponds  to  reasoning  of  users  within  the  analytic 
process. 

Form of Semantic 

Interaction 

Document Movement 

Text Highlighting 

Pinning  Document 
Location 
Annotation, “Sticky Note” 

to 

Document Coloring 

Level of Visual Detail 

Query Terms 
 

Associated Analytic Reasoning 

• Similarity/Dissimilarity 
• Create 

spatial  construct 

timeline, list, story, etc) 

• Test 

hypothesis, 

see 
document “fits” in region 

(.e.g 

how 

• Mark 

importance  of  phrase 

(collection of entities) 

• Augment  visual  appearance  of 

document for reference 

to 

in 

• Give 

semantic  meaning 

space/layout 

• Put 

semantic 

information 

workspace, within context 
• Create visual group/cluster 
• Mark group membership 
• Change 

ease 

of 

visually 
referencing  information  (e.g.  full 
detail = more important = easy to 
reference) 

• Expressive search for entity 

(and 

to  match 

is  positioning 

Semantic Interaction in ForceSPIRE 
The  semantic  interactions  in  ForceSPIRE  are:  placing 
information  at  specific  locations,  highlighting,  searching, 
and annotating in order to incrementally change the spatial 
layout 
their  mental  model.  The  primary 
parameters  of  the  force-directed  model  that  are  being 
updated  through  this  learning  model  are  the  importance 
values of the entities.  
Document  Movement.  The  predominant  interaction  in  a 
spatial  workspace 
repositioning) 
documents.  In  previous  work,  we  have  demonstrated  how 
users can perform both exploratory and expressive forms of 
this type of interaction [9]. In ForceSPIRE, we allow for the 
following  exploratory  interaction  (i.e.,  interaction  that 
allows users to explore the structure of the current model, 
but  does  not  change  it).  Users  are  able  to  interactively 
explore the information by dragging a document within the 
workspace, pinning a document to a particular location (see 
Figure  8),  as  well  as  linking  two  documents.  When 
dragging a document, the force-directed system responds by 
finding the lowest energy state of the remaining documents 
given  the  current  location  of  the  dragged  document. 
Mathematically, this adds a constraint to the stress function 
being  optimized  (in  this  case  the  force-directed  model). 
This  allows  users  to  explore  the  relationship  of  that 
document in comparison to the remaining documents.  
In addition to the exploratory dragging of a document, users 
have the ability to pin a document. By pinning a document, 
users  are  able  to  incrementally  add  semantic  meaning  to 
locations in their workspace. By specifying key documents 
to  user-defined  locations,  the  layout  of  the  remaining 
documents will adapt to these constraints. Thus, users can 
explore  how  documents  are  positioned  based  on  their 
similarity  (or  dissimilarity)  to  the  pinned  documents.  For 
instance,  if  the  layout  places  a  document  between  two 
pinned  documents, 
the  particular 
document holds a link between the two pinned documents, 
sharing entities that occur in both. 
Finally,  users  can  perform  an  expressive  form  of  this 
interaction  by  linking  two  documents,  performed  by 
dragging  one  document  onto  another  pinned  document.  In 
doing so, ForceSPIRE calculates the similarity between the 
documents,  and  increases  the  importance  value  of  the 
entities  shared  between  both  documents.  As  a  result,  the 
layout will place more emphasis on the characteristics that 
make those two documents similar. 
Highlighting.  When  highlighting  a  term,  ForceSPIRE 
creates an entity from the term (if not already one), and the 
importance  value  of  that  term  is  increased.  Similarly, 
highlighting  a  phrase  results  in  the  phrase  being  first 
parsed for entities, then increasing the importance value of 
each  of  those  entities.  For  example,  Figure  11  shows  the 
effect of highlighting the terms “Colorado” and “missiles” 
in the document pointed to with the arrow. As a result, the 

it  may 

imply 

that 

 
Figure  9.  The  Effect  of  adding  an  annotation  (“these 
individuals  may  be  related  to  Revolution  Now”)  to  the 
document shown with an arrow. As  a result,  the document 
becomes 
linked  with  other  documents  mentioning  the 
terrorist organization “Revolution Now”.  

presence of these displays creates an environment in which 
the  virtual  information  (in  this  case  the  documents)  can 
occupy  persistent  physical  space.  As  a  result,  users  are 
further  immersed  into  the  spatial  metaphor,  as  they  can 
point and quickly refer to information based on the physical 
locations.  
Constructing the Spatial Metaphor 
The spatial layout of the text documents is determined by a 
modified  version  a  force-directed  graph  model  [11].  This 
model  functions  on  the  principle  of  nodes  with  a  mass 
connected  by  springs  with  varying  strengths.  Thus,  each 
node has attributes of attraction and repulsion: nodes repel 
other  nodes,  and  two  nodes  attract  each  other  only  when 
connected  by  a  spring  (edge).  The  optimal  layout  is  then 
computed  by  iteratively  calculating  these  forces  until  the 
lowest energy state of all the nodes is reached. A complete 
description of this algorithm can be found in [11].  
We  apply  this  model  to  textual  information  by  treating 
documents  as  nodes  (an  overview  is  shown  in  Figure  6). 
The entire textual content of each document is parsed into a 
collection  of  entities  (i.e.,  keywords).  The  number  of 
entities corresponds to the mass of each document (heavier 
nodes  do  not  move  as  fast  as  lighter  nodes).  A spring  (or 
edge) represents one or more matching entities between two 
nodes.  Therefore,  the  initial  distance  metric  is  a  based  on 
co-occurrence  of  terms  between  documents.  For  example, 
two  documents  containing  the  term  “airport”  will  be 
connected  by  a  spring.  The  strength  of  a  spring  (i.e.  how 
close together it tries to place two nodes) is based on two 
factors:  the  number  of  entities  two  documents  have  in 
common,  and  the  importance  value  associated  with  each 
shared entity (initially, importance values are created using 
a  standard  tfidf  method  [16]).  The  sum  of  all  importance 
values add up to 1. 
The resulting spatial layout is one where similarity between 
documents  is  represented  by  distance  relative  to  other 
documents.  Similarity  in  this  system  is  defined  by  the 
strength of the spring between two documents. A stronger 
spring  (and  therefore  a  larger  amount  of  shared  entities) 
will pull two documents closer together, and thus represent 
two similar documents. 

 

 
Figure  10.  Searching  for  the  term  ”Atlanta”,  documents 
containing the term highlight green within the context of the 
spatial  layout.  Additionally,  the  importance  value  of  entity 
“Atlanta” is increased. 

other  documents  containing  that  term  are  clustered  more 
tightly. 
Searching.  When  coming  across  a  term  of  particular 
interest, analysts usually search on that term in order to find 
other  occurrences.  In  a  spatial  workspace,  this  is  of 
particular  importance,  because  the  answer  to  “where  the 
term  is  also  found”  is  not  only  given  in  terms  of  what 
documents,  but  also  where  in  the  layout  those  documents 
occur. The positions of documents containing the term are 
shown in context of the entire dataset, from which users can 
infer the importance of that term (as shown in Figure 10).  
ForceSPIRE  first  creates  an  entity  from  the  search  term 
(unless  it  is  already  one),  then  increases  the  importance 
value  of  the  search  term.  Figure  10  gives  an  example  of 
how a search result appears in ForceSPIRE. Searching for 
the  term  “Atlanta”,  documents  that  contain  the  term  are 
highlighted  green,  and  links  are  drawn  to  show  where  the 
resulting documents are in relation to the current document.  
Annotation.  Annotations  (i.e.,  “sticky  notes”)  are  also 
viewed as a form of semantic interaction, occurring within 
the analytic process, from which analytic reasoning can be 
inferred. When a user creates a note regarding a document, 
that semantic information should be added to the document. 
For example, if Document A refers to “Revolution Now” (a 
suspicious  terrorist  group),  and  Document  B  refers  to  “a 
group of suspicious individuals”, and the user has reason to 
believe  these  individuals  are  related  to  Revolution  Now, 
adding a note to Document B stating “these individuals may 
be  related  to  Revolution  Now”  is  one  way  for  the  user  to 
add semantic meaning to the document.  
ForceSPIRE  handles  the  addition  of  the  note  (shown  in 
Figure 9) by 1) parsing the note for any currently existing 
entities,  then  2)  increasing  the  importance  value  of  each, 
and 3) creating any new springs between other documents 
sharing these entities. In the example in Figure 9, edges are 
created between Document B and Document A (as well as 
any  other  documents  that  mention  “Revolution  Now”). 
Additionally,  if  the  note  contains  any  new  entities  not 
currently in the model, they are created, with the intent that 

 

 
Figure 11. The effect of highlighting a phrase containing the 
entites  “Colorado”  and  “missiles”.  Documents  containing 
these  entities  move  closer,  as  the  increase  in  importance 
value increases the edge strength.  

the 

importance  values  of 

any future entities that may match to that note can be linked 
at that time. ForceSPIRE also handles cases where notes are 
edited,  with  text  added  or  removed  from  the  note,  by 
updating  the  entities  associated  with  the  document,  and 
adjusting 
these  entities 
accordingly. 
Model Updates 
Each  of  the  semantic  interactions  in  ForceSPIRE  impacts 
the  model  by  updating  the  importance  values  of  entities, 
and  the  mass  of  each  document.  The  calculation  for 
updating the importance value of an entity is the same for 
each interaction. If an entity was “hit” (i.e., it was included 
in  a  highlight,  it  was  searched,  it  was  in  a  note,  etc.), 
ForceSPIRE increases its importance value by 10%. As the 
sum  of  all  importance  values  of  entities  adds  up  to  1, 
ForceSPIRE  subtracts  an  equal  amount  from  all  other 
entities’ importance values. As a result, importance values 
decay over time, and entities that are rarely used during the 
analysis  have  less  impact  on  the  layout.  The  mass  of  a 
document  uses  a  similar  calculation,  in  that  each  time  a 
document  is  “hit”  (i.e.,  text  was  highlighted,  it  was  the 
result of a search hit, etc.), it increases by 10%.  
When  undoing  an 
standard 
the 
“Control+Z”  keyboard  shortcut,  a  linear  history  of  the 
interactions will be reversed, and the importance values of 
affected  entities  will  be  returned  to  their  prior  values  (as 
well  as  document  masses).  As  for  the  locations  of  the 
documents,  the  reverted  importance  values  and  document 
masses  will  be  responsible  for  updating 
layout. 
However, this does not guarantee that the layout will return 
to  the  exact  previous  view,  and  the  user  may  find  it 
necessary to perform small adjustments. 
The model updates used in ForceSPIRE serve as an initial 
approach at how to couple semantic interactions with model 
updates. Other, more complex methods may exist, and we 
encourage  further  research  in  this  area.  Sensemaking  is  a 
complex exploratory process. As such, semantic interaction 

interaction  using 

the 

through 

more  central  documents.  While  reading 
the 
documents, he highlighted phrases of interest. For example, 
he highlighted the phrase “Nizar A. is now known to have 
spent six months in Afghanistan”. In doing so, ForceSPIRE 
increased  the  importance  value  of  the  entities  within  the 
phrase,  particularly  “Afghanistan”  and  “Nizar  A”.  As  a 
result, the layout forms more tightly around those entities. 
Each change incrementally changes the layout. 
Continuing  with  his  investigation,  he  began  searching  for 
words  of  interest  (e.g.,  “weapons”,  “Colorado”,  “Atlanta”, 
etc.). ForceSPIRE provided him with quick visual feedback 
on where in the dataset each terms showed up (the search 
result  for  “Atlanta”  is  shown  in Figure  10).  In  addition  to 
gaining an overview of the distribution of the term within 
the  dataset  (by  highlighting  each  document  containing  the 
term  green),  ForceSPIRE  treats  performing  a  search  as 
either  creating  a  new  entity  from  the  search  term,  or 
increasing the importance value if an entity corresponding 
to the search term already exists. As a result of the multiple 
search terms and highlights corresponding to locations (e.g., 
“Atlanta”,  “Los  Angeles”,  “Missouri”,  etc.),  ForceSPIRE 
adapts  the  spatialization  by  creating  a  more  geographic-
oriented layout (shown in the “Mid Stage” layout in Figure 
12).  
During  further  investigation,  he  began  opening  more 
documents and adding annotations to documents where he 
found  information  missing  that  he  knew.  For  example, 
Figure  9  shows  how  he  opened  one  document  where 
“suspicious individuals” were mentioned. Earlier, he read a 
document  containing 
terrorist 
organization  named  “Revolution  Now”.  While  reading 
about  the  suspicious  individuals,  the  other  information  in 
the document triggered him to make a connection between 
these  individuals  and  Revolution  Now.  He  made  added  a 
note  to  the  document  about  the  suspicious  individuals 
stating  “these  individuals  may  be  related  to  Revolution 
Now”. As a result, ForceSPIRE parsed the note for entities, 
added  them  to  the  document,  and  pulled  the  document 
closer to other documents containing the entity “Revolution 
Now”.  
After  continuing  his  investigation  in  this  manner,  he 
ultimately  made  the  connections  within  the  dataset  to 
uncover  the  terrorist  plot.  The  progression  of  the  spatial 
layout,  shown  in Figure 12, shows the final layout, where 
he  was  able  to  pinpoint  regions  of  the  layout  as  being 
important  in  his  finding.  Some  of  the  spatial  locations  of 
clusters  are  a  result  of  him  pinning  documents  to  that 
region (e.g., “Atlanta”, “Los Angeles”, etc.). These pinned 
documents are shown in red. Perhaps more interestingly is 
not the regions that were created as a result of him pinning 
documents  to  that  location,  but  rather  how  the  remaining 
documents respond in the layout. For example, in the final 
state  shown  in  Figure  12,  a  group  of  documents  began  to 
emerge  in  the  middle  of  all  the  pinned  locations.  Upon 
examining  these  documents,  he  discovered  that  these 

information  about  a 

the 

layout 

 

interaction, 

instances  during 

 
Figure 12. The incremental change of the spatial layout (main 
view  of  ForceSPIRE)  from  the  initial  to  the  final  state. 
Through  semantic 
incrementally 
changed  based  on the  semantic  input of the user. We labeled 
the regions based on what the user told us the regions meant to 
him at each stage. 
can  enable  analysts  to  explore  their  hypothesis  in-situ, 
while  the  provenance  of  their  insights  is  captured  and 
stored. An open area of research is what analyzing the soft 
data might reveal about the analytic process. For instance, if 
the  importance  values  of  entities  converge  on  a  small 
number  of  entities,  specific  biases  might  be  revealed. 
Similarly, 
the  analysis  when  new 
hypotheses  are  being  explored  may  be  indicated  by 
diverging importance values. 
Use Case 
We  demonstrate  the  functionality  of  ForceSPIRE  through 
the  following  use  case.  In  this  scenario,  we  simulate  an 
intelligence  analysis  scenario  where  the  task  is  to  find  a 
hidden terrorist plot in a pre-constructed, ficticious textual 
dataset.  The  dataset  consists  of  50 
text  documents, 
containing  a  complex  terrorist  plot  (explosives  are  being 
transported to various cities in the U.S. using trucks). The 
combination of the task of finding the hidden terrorist plot 
and  the  textual  dataset  is  representative  of  daily  work 
performed  by  professional  intelligence  analysts  [8].  The 
analysis  described  below  lasted  70  minutes,  and  was 
performed  by  an  individual  computer  science  graduate 
student.  
The user began the investigation by loading the collection 
of  documents  into  ForceSPIRE.  The  documents  were 
automatically  parsed  for  entities  using 
the  LingPipe 
keyword  extraction  library  [1].  From  these  entities,  an 
initial layout was generated, shown in Figure 12(top). From 
this  layout,  he  began  investigation  by  reading  through  the 

 

interpreting 

leverage 

interactions 

DISCUSSION 
Unifying the Sensemaking Loop 
With the fundamentally different role occupied by semantic 
interaction, we explore a new design space for interaction in 
visual analytic tools. With the addition of soft data, and a 
model  capable  of 
the  user’s  analytical 
reasoning,  we 
that  are  already 
occurring in the spatial analytic process to further aid users 
in their sensemaking process.  
With  semantic  interaction,  the  amount  of  formalization 
between foraging and sensemaking (Figure 13) on the part 
of the user is reduced. For instance, in moving a document, 
users  can  formulate  a  hypothesis  based  on  that  document, 
expecting  similar  documents 
to  follow.  ForceSPIRE 
attempts to update the layout based on the interaction, and 
gives the user feedback. Thus, the foraging stage occurs as 
a  result  of  the  hypothesis  being  formed  through  semantic 
interaction.  By  not  forcing  users  to  over-formalize  their 
analytic  reasoning  too  early  in  order  to  forage  for  the 
relevant  information,  semantic  interaction  creates  a  more 
seamless 
transition  between 
foraging  and  synthesis, 
unifying the sensemaking loop.  
Future Work 
Semantic 
interaction,  as  a  concept,  opens  up  many 
possibilities for further research, such as: what interactions 
to  capture  and  store,  which  parameters  of  the  model  to 
update,  how  to  store  the  soft  data,  and  which  models 
present a metaphor that can be extended upon.  
In  order  to  make  more  concrete  claims  regarding  the 
usability  and  effectiveness  of  ForceSPIRE  (and  thus,  of 
semantic  interaction),  a  formal  user  study  is  needed.  Our 
plan is to introduce ForceSPIRE to professional intelligence 
analysts  and  have  them  solve  scenarios  that  model  their 
daily  task,  such  as  one  of  the  VAST  datasets  [2020].  The 
observations  and  feedback  from  these  users  will  provide 
ecological validity for semantic interaction. 
CONCLUSION 
In  this  paper  we  have  discussed  how  the  concept  of 
semantic  interaction  leads  to  a  new  design  space  for 
interaction 
information. 
Semantic  interactions  occur  directly  within  the  spatial 
metaphor,  support  spatial  cognition,  and  exploit  spatial 
analytic  interactions.  We  describe  semantic  interaction, 
discussing  the  three  components  required  –  capturing  the 
interaction, 
the  analytical  reasoning,  and 
updating  the  mathematical  model.  Further,  we  present 
ForceSPIRE, designed for semantic interaction with textual 
information, discussing its functionality and demonstrating 
how it can be used through a use case. Lastly, we discuss 
how  semantic  interaction  has  the  opportunity  to  unify  the 
sensemaking  loop,  creating  a  more  seamless  analytic 
process.  In  allowing  users  to  interact  within  the  spatial 
metaphor, they can remain more focused on their analysis 
of  the  data,  without  having  to  become  experts  in  the 
underlying mathematical models of the system.  

in  spatializations  of 

interpreting 

textual 

 

Figure  13.  The  sensemaking  loop,  illustrating  the  complex 
sequence  of  steps  used  by  intelligence  analysts  in  order  to 
gain insight into data.  
 
documents  are  about  the  terrorist  organization  using  “U-
Haul”  or  “Ryder”  trucks  for  transportation  between  these 
locations. ForceSPIRE placing these documents in between 
these  cities  in  the  layout  was  helpful,  as  these  documents 
contain  information  “connecting”  the  events  in  these 
locations.  Immediately  after  noticing  this  event,  he  also 
made use of the expressive form of interaction, performed 
by dragging two of these documents together to determine 
what  made  them  similar.  After  seeing  that  it  was  indeed 
terms  such  as  “Ryder”  and  “U-Haul”,  the  layout  formed 
more tightly around these terms. 
ForceSPIRE interpreted the analytical reasoning of the user 
through the creation of new entities that were not found by 
the  initial  keyword  extraction,  as  well  as  the  increase  of 
importance values of existing entities. This is evidenced by 
the  creation  of  39  new  entities  during  the  course  of  the 
analysis.  LingPipe  extracted  89  initial  entities  from  this 
dataset,  and  at  the  time  of  completing  our  investigation 
ForceSPIRE  included  128.  Examples  of  newly  created 
entities  are  “big  event”,  “grenades”,  “Fisher  Island”, 
“weapons”,  and  others.  The  ability  for  new  entities  to  be 
created  via  semantic  interaction  did  not  interfere  with  the 
fluid sensemaking process of the user. Instead, it aided the 
process  by  creating  new  entities,  which  in  turn  created 
semantically relevant connections within the dataset. 
In  addition  to  creating  new  entities,  existing  entities 
dynamically  changed  their  importance  value  based  on  the 
semantic 
interpreted 
reasoning 
interactions.  Examples  of  entities 
their 
importance  values  are  “Atlanta”,  “Revolution  Now”, 
“Colorado”,  “L.A.”,  and  others.  As  a 
the 
ForceSPIRE incrementally adapted the layout based on the 
user  input.  This  shows  that  adjusting  importance  values, 
creating entities, and changing locations of key documents 
helped  the  user  discover  the  structure  of  the  dataset,  and 
ultimately make out the hidden terrorist plot.  

of 
that  changed 

analytical 

result, 

the 

 

ACKNOWLEDGEMENTS 
This research was funded by the NSF grant CCF-0937071 
and the DHS center of excellence. 
REFERENCES 
1.  Alias-i. 2008. LingPipe 4.0.1. City, 2008. 
2.  i2 Analyst's Notebook. City. 
3.  Alsakran, J., Chen, Y., Zhao, Y., Yang, J. and Luo, D. 

STREAMIT: Dynamic visualization and interactive 
exploration of text streams. In Proceedings of the IEEE 
Pacific Visualization Symposium, 2011.  

4.  Andrews, C., Endert, A. and North, C. Space to Think: 
Large, High-Resolution Displays for Sensemaking. In 
Proceedings of the CHI '10, 2010.  

5.  Callahan, S. P., Freire, J., Santos, E., Scheidegger, C. E., 

C, Silva, u. T. and Vo, H. T. VisTrails: visualization 
meets data management. In Proceedings of the 
SIGMOD international conference on Management of 
data (Chicago, IL, USA, 2006). ACM.  

6.  Cowley, P., Haack, J., Littlefield, R. and Hampson, E. 

Glass box: capturing, archiving, and retrieving 
workstation activities. In Proceedings of the workshop 
on Continuous archival and retrival of personal 
experences (Santa Barbara, California, USA, 2006). 
ACM.  

7.  Dou, W., Jeong, D. H., Stukes, F., Ribarsky, W., 

Lipford, H. R. and Chang, R. Recovering Reasoning 
Processes from User Interactions. IEEE Computer 
Graphics and Applications, 2009. 

8.  Endert, A., Andrews, C., Fink, G. A. and North, C. 

Professional Analysts using a Large, High-Resolution 
Display. In Proceedings of the IEEE VAST Extended 
Abstract (2009).  

9.  Endert, A., Han, C., Maiti, D., House, L., Leman, S. C. 

and North, C. Observation-level Interaction with 
Statistical Models for Visual Analytics. IEEE VAST, 
2011. 

10. Frank M. Shipman, I. and Marshall, C. C. Formality 

Considered Harmful: Experiences, Emerging Themes, 
and Directions on the Use of Formal Representations 
inInteractive Systems. ACM CSCW, 8, 4, 1999, 333-352. 

11. Fruchterman, T. M. J. and Reingold, E. M. Graph 

drawing by force-directed placement. Software: Practice 
and Experience, 21, 11 1991, 1129-1164. 

12. Gotz, D. Interactive Visual Synthesis of Analytic 

Knowledge. IEEE VAST, 2006. 
13. Heer, J. prefuse manual, 2006. 
14. Heer, J., Mackinlay, J., Stolte, C. and Agrawala, M. 

Graphical Histories for Visualization: Supporting 
Analysis, Communication, and Evaluation. IEEE 
Transactions on Visualization and Computer Graphics, 
14, 6 , 2008, 1189-1196. 

 

15. Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. 

and Chang, R. iPCA: An Interactive System for PCA-
based Visual Analytics. Computer Graphics Forum, 28, 
2009, 767-774. 

16. Karen A Statistical Interpretation of Term Specificity 

and its Application in Retrieval. Journal of 
Documentation, 28, 1972, 11-21. 

17. Marshall, C. C., Frank M. Shipman, I. and Coombs, J. 

H. VIKI: spatial hypertext supporting emergent 
structure. In Proceedings of the European conference on 
Hypermedia technology (Edinburgh, Scotland, 1994). 
ACM.  

18. Olsen, K. A., Korfhage, R. R., Sochats, K. M., Spring, 
M. B. and Williams, J. G. Visualization of a document 
collection: the vibe system. Information Process 
Management, 29, 1 1993, 69-81. 

19. Pirolli, P. and Card, S. Sensemaking Processes of 

Intelligence Analysts and Possible Leverage Points as 
Identified Though Cognitive Task Analysis Proceedings 
of the International Conference on Intelligence 
Analysis,2005, 6. 

20. Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., 

O'Connell, T., Laskowski, S., Chien, L., Tat, A., Wright, 
W., Gorg, C., Zhicheng, L., Parekh, N., Singhal, K. and 
Stasko, J. Evaluating Visual Analytics at the 2007 
VAST Symposium Contest. Computer Graphics and 
Applications, IEEE, 28, 2 2008, 12-21. 

21. Shrinivasan, Y. B. and Wijk, J. J. v. Supporting the 

analytical reasoning process in information 
visualization. In Proceedings of the CHI '08 (Florence, 
Italy, 2008). ACM.  

22. Skupin, A. A Cartographic Approach to Visualizing 
Conference Abstracts. IEEE Computer Graphics and 
Applications, pp. 50-58, January/February, 2002. 

23. Thomas, J. J., Cook, K. A., National, V. and Analytics, 
C. Illuminating the path. IEEE Computer Society, 2005. 
24. Torres, R. S., Silva, C. G., Medeiros, C. B. and Rocha, 

H. V. Visual structures for image browsing. In 
Proceedings of the conference on Information and 
knowledge management (New Orleans, LA, USA, 
2003). ACM.  

25. Wise, J. A., Thomas, J. J., Pennock, K., Lantrip, D., 

Pottier, M., Schur, A. and Crow, V. Visualizing the non-
visual: spatial analysis and interaction with information 
for text documents. Morgan Kaufmann Publishers, 1999. 

26. Wright, W., Schroh, D., Proulx, P., Skaburskis, A. and 

Cort, B. The Sandbox for analysis: concepts and 
methods. In Proceedings of the CHI '06 (New York, 
NY, 2006). ACM.  

27. Yi, J. S., Melton, R., Stasko, J. and Jacko, J. A. Dust & 
magnet: multivariate information visualization using a 
magnet metaphor. Information Visualization, 4, 4, 2005, 
239-256. 

",False,2012.0,{},False,False,conferencePaper,False,8KJJXFM6,[],self.user,False,False,False,False,http://dl.acm.org/citation.cfm?doid=2207676.2207741,,Semantic interaction for visual text analytics,8KJJXFM6,False,False
ZMGBDR3L,T4TKQ4MM,"This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

The Subspace Voyager: Exploring High-Dimensional Data along 

a Continuum of Salient 3D Subspace 

 Bing Wang and Klaus Mueller, Senior Member, IEEE 

Abstract— Analyzing high-dimensional data and finding hidden patterns is a difficult problem and has attracted numerous research 
efforts. Automated methods can be useful to some extent but bringing the data analyst into the loop via interactive visual tools can 
help  the  discovery  process  tremendously.  An  inherent  problem  in  this  effort  is  that  humans  lack  the  mental  capacity  to  truly 
understand spaces exceeding three spatial dimensions. To keep within this limitation, we describe a framework that decomposes a 
high-dimensional  data  space  into  a  continuum  of  generalized  3D  subspaces.  Analysts  can  then  explore  these  3D  subspaces 
individually  via  the  familiar  trackball  interface  while  using  additional  facilities  to  smoothly  transition  to  adjacent  subspaces  for 
expanded  space  comprehension. Since the  number  of such  subspaces suffers  from  combinatorial  explosion,  we  provide  a  set  of 
data-driven  subspace  selection  and  navigation  tools  which  can  guide  users  to  interesting  subspaces  and  views.  A  subspace  trail 
map  allows  users  to  manage  the  explored  subspaces,  keep  their  bearings,  and  return  to  interesting  subspaces  and  views.  Both 
trackball and trail map are each embedded into a word cloud of attribute labels which aid in navigation. We demonstrate our system 
via several use cases in a diverse set of application areas – cluster analysis and refinement, information discovery, and supervised 
training of classifiers. We also report on a user study that evaluates the usability of the various interactions our system provides.  
Index Terms— High-dimensional data, subspace navigation, trackball, PCA, ant colony optimization

1.   IN T R OD UC T I ON 

to 

recognize 

tools 

D 

ATA  with  many  attributes  have  become  commonplace  in  a 
wide range of domains, such as science, business, medicine, 
etc.  In  these  data,  the  most  interesting  relations  are  often 
multivariate,  and  gaining  proper 
these 
relationships  reliably  is  still  an  active  area  of  research.  While 
automated  analysis  can  be  useful  in  finding  some  of  the  high-
dimensional patterns, adding the human into the loop can break ties 
and help discern patterns in confounding and noisy data settings that 
benefit  from  the  intricate  reasoning  faculties  of  human  domain 
experts.  However,  we  are  still  far  off  from  having  effective  visual 
tools  for  high-D  data  analytics  that  make  the  best  use  of  the  inborn 
capabilities  of  the  human  visual  system  and  at  the  same  time  also 
observe its limitations.   

High-D  space  is  generally  confusing  to  most  people  since 
humans  do  not  possess  the  innate  neural  network  to  recognize  and 
reason  with  high-D  objects.  Spatial  reasoning  skills  are  acquired  in 
early  childhood  where  often  haptic  and  visual  experiences  are 
combined to build 3D mental models of the real world. Since high-D 
objects are largely mathematical and do not occur in a tangible form, 
the associated cognitive reasoning chains are not developed in these 
critical  early  years.  This  lack  of  reasoning  faculties  represents  a 
barrier  for  most  people  when  dealing  with  high-D  data  later  in  life 
and so deprives them of the chance to find more insight in these data.  
We describe a framework and interface that eases this barrier by 
design, called the Subspace Voyager. It serializes the exploration of 
high-D  space  into  a  continuous  travel  along  a  string  of  generalized, 
but not necessarily dimension axis-aligned 3D subspaces, visualized 
as scatterplot projections of the data points. This serialization allows 
us  to  abolish  the  complex  interactions  and  representations  that  are 
often  typical  to  high-D  space  exploration  tools.  We  replace  them 
with  paradigms  familiar  to  most  people,  such  as  trackballs,  maps, 
and  word  clouds.  Our  interface  uses  these  to  help  users  explore  the 
generalized 3D subspaces, navigate the continuum of 3D subspaces, 
and assess the relevance of individual attributes for a given subspace.  
The  simplicity  gained  through  the  3D  subspace  decomposition 
comes at a price – the extent of the transformations defined on such a 
restricted  subspace  is  limited  and  may  not  reach  far  enough  to 
generate  a  projection  in  which  a  pattern  of  current  interest  is  well 

                               ———————————————— 

  Bing Wang and Klaus Mueller are with the Visual Analytics and Imaging 
Lab at the Computer Science Department, Stony Brook University, Stony 
Brook, NY. Email: {wang12, mueller}@cs.sunysb.edu. 

Manuscript received February 07. 2017. 

 

1 
 

expressed. To enable a reach beyond these limits we have augmented 
the 3D navigation interface with extra capabilities that allow users to 
“chase” the discovered patterns by moving to adjacent 3D subspaces 
via simple mouse interactions. In this way, patterns can be observed 
that are truly multivariate and not restricted to a single 3D subspace.  
In some sense, our approach is akin to that taken in an upcoming 
Indie  video  game,  Miegakure  [46]  (itself  inspired  by  the  classic 
novel Flatland [2]) which enables 4D space travel by swapping one 
of the three current dimensions. We go significantly further than this 
game:  (1)  our  spaces  are  much  greater  than  4D,  and  (2)  we  allow 
transitions  in  all  dimensions  simultaneously.  Yet,  it  is  encouraging 
that the entertainment industry sees fun in this type of space travel. It 
suggests that our interface might be fun and engaging as well, which 
will immensely benefit the analytics that is performed with it. 

The  3D  subspaces  our  system  supports  are  general  in  the  sense 
that they do not need to be constrained to three specific data axes but 
can be spanned by a basis of three arbitrary orthogonal vectors. This 
affords a better alignment with the high-D phenomenon under study 
and  effectively  allows  its  exploration  in  relation to  all  relevant  data 
dimensions. It, however, also brings about a huge number of possible 
subspaces.  To  manage  this  complexity  we  provide  a  variety  of 
objective-driven  search  and  clustering  facilities  that  assist  users  in 
locating subspaces with interesting structures. 

When  designing  our  interface  we  placed  great  emphasis  on 
making  the  interactions  direct,  intuitive,  and  responsive  [34].  Most 
exploration goals can be achieved by expressing them directly in the 
visualization,  via  simple  mouse  selections  and  transitions.  At  the 
same  time,  our  framework  is  quite  general  and  is  readily  applicable 
for  many  tasks  and  application  areas  that  involve  multivariate  data, 
such  as  cluster  sculpting  [30]  and  analysis,  information  discovery, 
and the supervised training of classifiers, just to name a few.  

In summary, the specific contributions of our work are: 

  A  serialization  of  high-D  space  exploration  into  a  journey 
within and across a string of adjacent generalized 3D subspaces 
  An  interactive  trackball  interface  for  3D  subspace  exploration 
augmented with direct controls for goal-directed transitioning to 
adjacent 3D subspaces – an activity we call cluster chasing 

  An illustrative, non-obtrusive labeling scheme that allows users 

to appreciate the influence of different variables on the display 

  Various  goal-directed  view  optimization  and  view  selection 
facilities  that  lower  the  subspace  navigation  overhead  and 
expand the search for interesting high-D phenomena 

  A  map-like  interface  organized  by  view  similarity  where  users 
can  store  interesting  scatterplot  views  and  construct  a  tour  for  
presentation within an animated scatter plot display 
Our  paper  is  organized  as  follows.  Section  2  reports  on  related 
research  motivating  our  work.  Section  3  focuses  specifically  on  the 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

TripAdvisorND  system  –  a  precursor  of  the  Subspace  Voyager. 
Section  4  provides  a  system  overview.  Section  5  describes  the 
trackball  based  subspace  explorer.  Section  6  presents  the  subspace 
trail map. Section 7 outlines three use scenarios. Section 8 describes 
our user study and its outcomes, and Section 9 offers conclusions. 

2.  RE L AT ED  W O RK   

Our principal visualization modality is  the scatterplot – a projection 
of the data into an orthogonal 2D basis.  In scatterplots, clusters and 
their  shapes  are  relatively  easy  to  recognize,  but  points  distant  in 
high-D space may project into similar  locations and this can lead to 
ambiguities. Helping users deal with these ambiguities is one of the 
major  aims  of  our  work.  Another  aim  is  to  aid  users  in  producing 
informative  projective  views  into  interesting  subspaces  of  the  data. 
In the following, we divide work related to ours into four aspects.      

Dealing with projection ambiguities  

One way to resolve projection ambiguities is to decompose the space 
into  a  matrix  of  axis-aligned  bivariate  scatterplots,  called  SPLOM 
[17]. While SPLOMs can help with disambiguation, users might find 
it  difficult  to  integrate  information  from  such  a  mosaic  of  plots, 
especially when the number of dimensions is even moderately large.  
Another approach is to use layout optimization schemes, such as 
Multidimensional Scaling (MDS) [24], Linear Discriminant Analysis 
(LDA)  [28],  and  Stochastic  Neighbor  Embedding  (t-SNE)  [42]. 
MDS,  for  example,  seeks  to  generate  a  layout  where  the  pairwise 
distances  of  points  in  2D  are  relatively  similar  to  those  in  high-D 
space.  But  even  with  layout  optimization,  trying  to  warp  high-D 
space  onto  a  2D  plane  is  inherently  ill-posed  since  it  cannot  fully 
capture multivariate data variations. Distortions are the consequence, 
making  it  difficult  to  correctly  recognize  the  true  shape  and 
appearance of clusters,  and also hampering the  assessment of point-
wise distances, both far and near. Hence, while ambiguities might be 
resolved, the risk of distortions has taken their place.   

A  third  alternative  is  to  enable  users  to  change  the  projection 
basis  in  a  continuous  fashion,  effectively  using  motion  parallax  to 
resolve  depth  and  relative  distance.  Several  systems  have  followed 
this  paradigm.  One  of  these  is  ScatterDice  [11].  It  restricts  the 
transitions  to  motions  between  two  bivariate  projections  at  a  time, 
giving  rise  to  a  dynamic  3D  point-cloud  projection  display.  More 
general  is  the  GGobi  system  [39],  itself  derived  from  the  seminal 
concept  of  the  ‘Grand  Tour’  [5],  as  well  as  the  TripAdvisorND 
framework  devised  by  one  of  the  co-authors  [31].  Both  allow  users 
to  transition  between  arbitrary  multivariate  projections.  Our  current 
framework  also  follows  this  general  paradigm  but  offers  interactive 
exploration  capabilities  that  greatly  exceed  those  provided  by  these 
earlier  systems.  For  example,  while  GGobi  also  uses  a  trackball,  it 
does  not  offer  the  advanced  subspace  exploration  facilities  our 
trackball interface provides.  

Defining a multivariate projection basis  

𝑁−1
𝑖=0

𝒗𝒊

Our layout is a generalized projection display where the 2D location 
p of a projected N-D data point x with coordinates xi, 0 ≤ i ≤ N-1, is 
given  by 𝒑 = ∑
𝑥𝑖.  Here,  the  vi  are  a  set  of  2D  basis  vectors 
with common origin O. We can use this formulation to compare our 
display  with  several  others  that  are  in  common  use.  In  Star 
Coordinates  [21]  all  basis  vectors  have  unit  length  and  by  ways  of 
changing  their  orientations,  users  can  interactively  increase  the 
spread  of  the  projected  data  points.  RadViz  [20]  is  similar  but 
includes a normalization by ∑
. Conversely, in biplots [14] the 
vector  basis  is  a  projection  of  the  axis  vectors  into  the  2D  frame 
spanned by the two major principal component (PC) axes. As a result, 
the vector v are typically not (all) unit length and their orientation is 
clearly defined. Projecting the data points into the PC-basis naturally 
maximizes their spread in the 2D display which removes the need for 
interaction. However, the projection ambiguity problems still remain.  
Our  display  is  similar  to  biplots  but  distinct  in  two  ways.  First, 
we  allow  users  to  change  the  biplot  projection  basis  interactively 

𝑁−1
𝑖=0

𝑥𝑖

which  helps  overcome  the  ambiguity  problems  via  motion  parallax. 
The transitions can affect many dimensions at once, and not just one 
at  a  time  like  in  Star  Coordinates  and  RadViz.  Second,  we  plot  the 
dimension  labels  at  the  display  periphery.  We  use  the  sizes  and 
opacities  of  the  dimension  labels  to  indicate  the  influences  of  the 
attributes on the projection. Conversely, biplots project the data axes 
as arrow-headed lines directly into the display leading to clutter.   

Selecting informative views 

The problem of projective view overload is not unique to SPLOMs. 
In many cases, it can be helpful to include proper quality criteria by 
which  the  most  informative  views  can  be  selected.  Research  in  this 
area has mainly addressed the selection of axis-aligned views in the 
presence of clustered or classified data. Sips et al. [37] define a class 
consistency measure which favors views based on the distance to the 
class center of gravity or on the entropies of the spatial distributions. 
Tatu et al. [40] assess quality by measures on density, histogram, and 
class  separation.  The  rank-by-feature  system  [36]  allows  users  to 
specify  certain  statistical  criteria,  such  as  correlation,  scatterplot 
uniformity,  etc.  Schäfer  et  al.  [35]  describe  a  quality  metric  that 
focuses  on  structural  preservation  and  visual  clutter  avoidance. 
GGobi  uses  projection  pursuit  [9][13]  to  generate  interesting 
multivariate projections. We use a popular evolutionary  algorithm – 
ant  colony  optimization  (ACO)  [10]  –  in  conjunction  with  view 
quality  metrics  such  as  stress,  class  density,  class  separation,  holes, 
and central mass.  

Finally,  a  problem  with  having many  projections  is  also how  to 
manage  and  organize  them.  Several  map-based  diagrams  have  been 
proposed [31][45].  We  provide  a  novel  map  that  is  dedicated  to  the 
management  of  generalized  subspaces.  In  addition,  our  map  also 
allows users to construct animated tours for presentation purposes. 

Managing interesting subspaces 

Subspace  clustering  has  been  an  active  research  area  in  the  data 
mining  community  [23]  but  the  focus  was  mostly  on  automated 
algorithms.  In  the  field  of  visualization,  one  may  distinguish  the 
contributions by how much they rely on automated subspace analysis 
methods. On one end are the works of Yuan et al. [44] and Kim et al. 
[22]  where  users  are  in  full  control.  The  former  proposes  a  visual 
subspace  exploration  approach  that  focuses  mainly  on  interactive 
dimension set selection and refinement. The latter suggests a system 
where  users  can  drop  data  points  into  two  different  groups  and  the 
projection  basis  vectors  are  updated  automatically.  Lehmann  et  al. 
[26] find minimal sets of projections, allowing  users to draw a path 
to  traverse  between  them.  In  our  system,  users  can  also  modify  the 
projection basis to favor certain dimensions, namely by emphasizing 
the influence of these dimensions directly in the interface.   

Other approaches first perform an automated subspace clustering 
step  and  then  visualize  the  results  as  small  multiples  of  scatterplot 
projections  [4],  as  MDS  layouts  [41],  or  use  animated  transitioning 
between them [27] akin to our map. We also first perform clustering 
but then use the results to provide guidance in the subsequent visual 
exploration  of  the  actual  subspaces,  focusing  on  cluster  appearance 
and relations. This can be helpful in the visual reasoning process.  

Related  in  some  respect  is  also  the  LineUp  system  by  Gratzl  et 
al.  [16].  LineUp  requires  users  to  manually  set  a  weight  for  each 
attribute to determine its influence on the rankings of the data items. 
However,  setting  weights  explicitly  might  not  be  intuitive  to 
mainstream  users  with  limited  quantitative  reasoning  abilities.  They 
may  simply  not  know  their  preferences  at  this  level  of  detail  but 
rather  discover  them  implicitly  during  data  exploration.  Our  system 
supports this type of exploratory discovery process.  

3.  REC AP: THE  T RI PADV IS O R N D F R AM EW O RK 

The  approach  we  have  taken  is  largely  motivated  by  our  earlier 
TripAdvisorND  framework  [31]  and  the  shortcomings  we  have 
observed in its use. One major improvement is the new trackball  

2 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

Shortcomings of TripAdvisorND motivating our work 

While the pad interface allows unprecedented control in the dynamic 
manipulation  of  the  view  onto  the  N-D  point  cloud,  the  need  to 
separately manipulate two pointers in sequence suffers from a certain 
lack of ergonomics. A further shortcoming is that users are required 
to keep track of two interfaces at the same time: (1) the visualization 
window  that  shows  the  moving  point  cloud  along  with  a  projected 
coordinate system, and (2) the pad that controls the orientation of the 
projection  plane.  In  practice,  a  user  may  observe  one  or  more 
dimensions  that  should  be  emphasized  in  the  display  as  they  might 
offer the potential to break up a cluster into two or more components. 
To do this, the user would need to looks at the pad to identify which 
pointer should be moved and in what direction, and then observe the 
effect in the display. In the present work,  we aimed for an interface 
that  makes  this  operation  more  straightforward  by  embedding  the 
navigation  controls  directly  into  the  display.  Enhancing  the  well-
known  trackball  interface  with  N-D  navigation  capabilities  seemed 
to be good choice toward this goal. We also added view optimization 
and  other  navigation  aids  to  support  the  manual  exploration, 
allowing users to arrive at meaningful projections faster. 

4.  S YS TEM  OV ERV IE W 

Fig.  2  shows  the  Subspace  Voyager  interface.  It  has  three  main 
components:  the  Subspace  Explorer  (SE),  the  Subspace  Trail  Map 
(STM),  and  the  control  panel.  The  latter  allows  users  to  set  the 
various parameters and modes in the system.  

The  exploration  pipeline  of  the  Subspace  Voyager  is  illustrated 
in Fig. 3. After loading the data, our system performs either Random 
Projection  or  Subspace  Clustering  and  Principal  Component 
Analysis  (PCA)  to  identify  an  initial  promising  3D  subspace.  More 
3D subspaces can be generated via the control panel at any time. 

The  data  is  then  projected  into  this  generated  subspace  and  is 
displayed  in  the  SE-embedded  trackball.  There  are  different 
interaction modes users can perform on the trackball. The first mode 

 

Fig.  1.  Pad-based  navigation  interface  of  TripAdvisorND.  In  the 
setting  shown,  the  PPA-x  vector  is  dominantly  a  combination  of 
dimension axis DA 4 and DA 5, while PPA-y is a combination of 
DA 6, DA 1, and DA 2. 

interface,  which  is  much  more  direct  than  the  spatially  disjoint 
navigation  pad  of  TripAdvisorND  (see  Fig.  1).  This  navigation  pad 
consists of a polygon with S vertices, where S is the cardinality of the 
subspaces.  Each  vertex  corresponds  to  a  native  dimension  –  hence 
the  subspaces  are  axis-aligned  (and  not  generalized).  It  should  also 
be noted that for S>3 different orderings of the vertices are required 
to allow users to access the full projection coverage of the subspace. 

The  interior  of  the  polygon  shows  two  disk-shaped  pointers. 
They represent the two (N-D) basis vectors into which the N-D point 
cloud  is  projected  for  display  using  the  vector  dot  product.  In  [31] 
these  two  vectors  are  called  Projection  Plane  Axis  (PPA)  vectors  – 
the  x-axis  is  PPA-x  and  the  y-axis  is  PPA-y.  The  vectors  are 
computed  from  their  positions  in  the  pad  polygon  via  generalized 
barycentric coordinate interpolation [29]. 

In  the  pad-based  interface,  users  can  control  the  influence  a 
dimension has on the display by moving either the PPA-x or PPA-y 
pointer  toward  that  dimension.  This  essentially  spreads  out  the 
projected  point  cloud  along  that  dimension  and  so  reveals  the 
dimension’s  ability  to  separate  the  data  points  into  different 
populations/clusters.  Then,  by  moving  the  other  pointer  toward 
another dimension, bivariate relationships can be visualized. Finally, 
when  moving  either  or  both  pointers  midway  between  a  set  of 
dimensions  users  can  appreciate  the  combined  effects  stemming 
from the multivariate relationships of these dimensions. 

Fig. 2. Subspace Voyager interface. It has three main components: the Subspace Explorer (SE), the Subspace Trail Map (STM) and the 
control panel. The SE is coupled with the trackball interface. It not only displays the data as a  scatterplot, but it also allows users to 
visualize the current directions of the projected dimension axis vectors as labels placed outside its circular boundary. The  labels are 
properly sized in terms of the corresponding attribute’s influence on the display. The SE offers various interactions for users to examine 
the data. The STM holds a set of views (and their parameters) that users may have found interesting during the exploration, embedding 
them into a word cloud of attributes. Finally, the control panel allows users to set the various parameters and modes in the system. 

3 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

 

Fig. 4. 3D trackball. Given the current and previous mouse clicks, 
both the axis of rotation and the rotation angle can be computed. 

indicate  to  what  extent  its  associated  attribute  is  expressed  in  the 
projection.  A  larger  and  bolder  font  means  that  the  scatterplot 
exhibits  more  of  the  attribute’s  variability.  The  label  placement,  on 
the  other  hand,  reveals  the  radial  direction  along  which  the 
variability  is  mostly  exposed.  The  simplest  form  of  trackball 
interaction  generates  scatterplot  projections  confined  to  the  current 
generalized  3D  subspace  projected  into  the  SE.  This  projected  3D 
subspace can be modified by:  
  Mouse-initiated  trackball  interaction:  users  can  transition  to 

adjacent 3D subspaces by augmented trackball interaction   

  Randomized  projections:  this  discovers  new  3D  subspaces  ready 

for trackball-based exploration    

  3D  Subspace  interpolation:  moving  a  slider  in  the  control  panel 
generates  a  continuous  set  of  3D  subspaces,  intermediate  to  two 
subspaces in the STM, which can be explored via the trackball 

  View  optimization:  the  3D  subspace  (as  well  as  the  current 
projection view within the current 3D subspace) can be optimized 
via projection pursuit driven by a user-defined set of criteria  

     The control panel provides several options for trackball  use. The 
checkbox  ‘TurnOff’  specifies  if  all  data  points  are  to  be  shown  or 
only those that are well described in the current subspace, i.e., belong 
to  that  subspace.  The  color  bar  on  the  bottom  right  is  the  brushing 
tool. It allows users to tag individual points or groups of points in a 
dedicated color to cluster them or mark them as inactive in gray.  

4.3 

The Subspace Trail Map (STM)  

The STM holds a set of views (and their parameters) that users may 
have  found  interesting  during  the  trackball  exploration.  The  view 
images  are  embedded  into  a  word  cloud  of  attributes.  Their 
placement  with  respect  to  each  word  indicates  the  influence  of  the 
corresponding  attribute  to  the  view.  We  treat  each  view  as  a  point 
and use PCA on all of them to spread them out. The circular shape of 
the  images  mimics  the  shape  of  the  trackballs.  A  smaller  diameter 
reduces overlap of similar views in the STM while a larger diameter 
provides  magnification.  Users  can  drag  any  view  back  into  the 
trackball  for  further  exploration,  or  they  can  connect  interesting 
views by lines to produce animated transitions for presentations.  

5.  THE  S UB SP ACE  E XP L OR ER   AND  TR AC K B AL L  I N TE R F ACE 

Users can tilt the trackball and watch the resulting scatterplot react to 
the motion. Fig. 4 sketches how a trackball works. Imagine a virtual 
sphere that encapsulates the current generalized 3D subspace. When 
clicked, the screen coordinate of the mouse is mapped to this sphere. 
Given  the  current  and  previous  mouse  clicks,  we  can  compute  the 
axis of rotation n and the rotation angle . From those two quantities, 
a 3×3 rotation matrix is derived, as described in [3].  

5.1 

Creating the Trackball Space Projection Matrix 

The trackball system only  works in 3D but our data points are N-D 
and so we need to project the ND points into 3D before rotating. We 
achieve this by post-multiplying the trackball rotation matrix T with 
the 3N projection matrix P. We have two options for the first two of 
the  vectors  in  P:  (1)  the  orthogonal  PPA  x-axis  and  y-axis  pair  we 
obtained  from  the  randomized  projection  procedure,  or  (2)  the  two 
most significant PCs we obtained when performing PCA for the 

Fig.  3.    Subspace  Voyager  workflow.  See  Section  4  for  a 
narration.  
is to rotate the trackball while pressing down  the left  mouse button. 
This enables an exploration of the current 3D subspace. The second 
mode allows users to transition to  adjacent subspaces where certain 
attributes  of  interest  have  a  higher  emphasis  than  in  the  current  3D 
subspace.  It  yields  data  projections  that  better  capture  the  cluster 
distributions  in  these  attributes.  In  this  cluster  chasing,  users  move 
the mouse – now with the right mouse button depressed – toward the 
respective attribute labels displayed on the trackball periphery. This 
increases the weight of these dimensions in the PPA vectors.  

As  mentioned,  in  our  system  there  is  no  need  for  manually 
optimizing  views  which  can  be  tedious.  Our  system  provides  Ant 
Colony  Optimization  (ACO)  [10]  to  generate  the  best  trackball 
configuration  automatically  according to  a  set  of  user-selected  view 
quality  criteria.  Users  can  also  tag  points  by  brushing  them  into 
different colors. This is helpful for cluster analysis or for editing out 
unwanted  structures.  Finally,  at  any  time  users  can  save  the  current 
trackball view to the STM to keep track of interesting findings. Any 
of these STM views can then be dragged back  into the trackball for 
further  exploration.  Multiple  small  views  can  also  be  linked  and 
traversed in order, providing a smooth transition between views.    

4.1  Generating a Set of Subspaces  

Choosing meaningful subspaces for exploration is a key challenge in 
multivariate data analysis and much work has been dedicated toward 
this goal (see Section 2). We have implemented two such strategies: 
(1)  random  view  generation  and  (2)  subspace  clustering.  Users  can 
generate new subspaces at any time via the control panel.   

For the former (1), we use the technique proposed by Anand et al. 
[1]  and  then  further  optimize  the  subspace  using  ACO  powered 
projection  pursuit  (see  Section  5.4).  For  the  latter  (2)  we  assume  – 
similar  to  Liu  et  al.  [27]  and  our  own  work  [43] –  that  each  cluster 
forms a subspace on its own. We characterize each such subspace by 
the three principal components obtained with PCA. Finally, for both 
of these methods, we use ACO view optimization to generate a high 
quality  (given  the  chosen  metric)  scatterplot  projection  in  the 
trackball display. 

We should also note that in a view that has the PC vectors as its 
basis  if  two  (or  more)  dimension  vectors  are  very  close,  it  means 
they are to some extent correlated. This is especially true when these 
dimensions  have  large  weightings  in  one  significant  PC  (i.e.  these 
dimensions  are  strongly  correlated  [47]).  We  will  make  use  of  this 
relationship in the use case described in Section 7.1. 

4.2 

The Subspace Explorer (SE) 

The  SE  is  coupled  with  the  trackball  interface.  It  not  only  displays 
the  data  as  a  scatterplot,  but  it  also  allows  users  to  visualize  the 
current  directions  of  the  projected  dimension  axis  vectors  as  labels 
placed outside its circular boundary. The size and opacity of a label 

4 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

 

Fig. 5. Updating the PPA x-axis and PPA y-axis vectors by moving 
the mouse towards one or more dimensions. The influence of each 
dimension is weighted by a Gaussian function. 
selected cluster. In both cases we require a third orthogonal axis, call 
it  the  PPA  z-axis.  Since  this  is  N-D  space  we  have  a  number  of 
choices. We can either (1) randomly generate an N-D vector, or (2) if 
the PPA x-and PPA y-axes are generated via PCA, use the third most 
significant axis for the PPA z-axis.  

Note that the resulting vector is not necessarily orthogonal to the 
PPA  x-axis  and  the  PPA  y-axis.  To  make  it  orthogonal  we  use  the 
Gram-Schmidt  orthonormalization  process  [7]  to  find  orthogonal 
basis  vectors.  The  Gram-Schmidt  process 
linearly 
independent  vectors  and  produces  N  orthonormal  vectors  spanning 
the same N-D space. In practice, we keep the PPA x-axis and PPA y-
axis  which  are  already  orthonormal  and  run  Gram-Schmidt  to 
orthonormalize the PPA z-axis from the initially chosen vector. Once 
P is configured in this way, T is reset to the identity matrix, ready to 
be manipulated in the 3D trackball interaction.  

takes  N 

5.2 

Processing the Points within the Trackball Space 

With P in place, the following sequence of operations is executed for 
every  trackball  move:  (1)  compute  the  3N  compound  projection 
matrix  M=S∙T∙P,  where  S  is  an  optional  scaling  matrix  that  allows 
zooming  into  the  display,  and  (2)  multiply  each  N-D  point  vector 
VND  by  M  to  obtain  the  3D  points  V3D=M∙VND.  But  ultimately  we 
are only interested in the projection of the points into the coordinate 
system  spanned  by  the  PPA-x  and PPA-y  vectors  manipulated  with 
the trackball. This yields a set of 2D points, V2D, which are the first 
two components of V3D since the projection is orthogonal.  

We have not observed a significant delay in the direct projection 
of  N-D  points  in  the  operation  of  the  trackball.  But  first  pre-
computing  a  3D  point  cloud  right  after  construction  of  the  3D 
coordinate system and rotating them directly for the lifetime of P can 
reduce  the  number  of  computations  to  roughly  N/3  of  the  original 
computations.  We  have  not  chosen  this  intermediate  step  because  it 
requires extra storage which can be significant for large point clouds.  

5.3  Mouse Interactions within the Trackball Interface 

We  provide  three  modes  of  mouse  interactions  within  the  trackball. 
All  are  controlled  with  different  mouse  buttons  depressed.  The  first 
is the basic mouse interaction when the trackball is rotated within the 
current  3D  subspace.  It  is  performed  when  the  left  mouse  button  is 
depressed  (see  Section  5.2).  The  other two  operations  are  described 
in more detail in the following.  

Chase clusters in adjacent 3D subspaces  

5.3.1 
When  using  the  basic  3D  subspace  exploration  mode  (Section  5.2) 
we  frequently  observed  that  interesting  patterns  were  starting  to 
evolve but their full exposure was out of reach since it occurred in a 
different,  albeit  nearby,  subspace  (i.e.  a  subspace  that  could  not  be 
reached simply by 3D rotation). In these situations, we often felt the 
need to “break out” of the current 3D subspace in the direction of the 
trackball  movement  such  that  these  patterns  could  be  reached.  To 
solve  this  shortcoming  we  added  the  capability  to  smoothly 
transition  from  one  subspace  to  an  adjacent  one.  It  allows  users  to 
interactively  change  the  influence  of  the  data  dimensions  whose 

5 
 

projections  align  with  the  current  trackball  motion,  progressively 
increasing  their  bias  in  the  projection  matrix  P.  This  gives  the 
exploring user access to the adjacent 3D subspace where the patterns 
of interest are better expressed. It lets him/her explore the data with a 
higher emphasis on one or more attributes of interest.  

To engage into this mode of exploration users would release the 
left mouse button and instead press the right button while moving the 
mouse  in  the  direction  of  the  desired  dimension’s  projection,  as 
indicated  by  the  corresponding  attribute’s  label  on  the  trackball’s 
periphery.  The  further  the  mouse  is  moved  the  more  the  projection 
plane is tilted into the dimension’s axis  vector. Conversely,  moving 
backward  along  that  direction,  towards  the  center  of  the  trackball, 
decreases the influence of this dimension.   

As  Fig.  5  illustrates,  ideally  we  would  accomplish  this  task  by 
adding  (or  subtracting) 
increments  x=ka∙d∙sin()  and  y= 
ka∙d∙cos() to the PPA-x and PPA-y vectors, respectively, where   
is the angle between the mouse movement vector and the trackball x-
axis (the PPA-x vector). Here d is the distance the mouse moved in 
the  direction  of  the  projected  dimension  axis  vector  (positive  when 
moving towards the periphery, negative otherwise), and ka is a user-
adjustable  speed  constant  (we  use  the  dot  products  instead  of  the 
trigonometric functions). Subsequently, Gram-Schmidt is used to re-
orthonormalize  P  (see  Section  5.1),  using  the  original  PPA  z-axis 
vector.  One  problem  here  is  that,  after  Gram-Schmidt,  the  direction 
of  this  data  dimension  would  change  and  thus  there  might  be  other 
dimensions taking the selected one’s direction. We overcome this by 
fixing the selected dimension until the user releases the mouse.  

This basic approach generalizes to more than one dimension. Fig. 
5  illustrates  the  practical  case  in  which  there  are  two  or  more 
projected  dimension  axis  vectors  in  close  range  of  the  exploration 
direction.  This  might  be  an  indication  of  multivariate  relationships. 
To properly scale the axes vector influences geometrically, we apply 
a Gaussian weighting in terms of their direction misalignment.  This 
is done via the following equation: wd=exp(-kd∙dot(vm, vd)) where wd 
is  the  weight  applied  to  this  axis  vector,  vm and vd are the  direction 
vectors  of  the  mouse  and  the  axis  vector,  respectively,  and  kd 
determines  the  reach  of  the  Gaussian.  The  remaining  steps  are 
similar to the single-vector case described in the previous paragraphs. 
Our  system  also  supports  the  case  in  which  a  user  would  first 
select  an  attribute  via  a  mouse  click  on  the  trackball  boundary  but 
then move the mouse in a direction not necessarily aligned with the 
attribute’s dimension vector. This will gradually align the dimension 
vector  with  the  mouse  motion  and  move  the  attribute  label 
accordingly.  Again,  the  selected  dimension’s  weighting  changes 
according to the direction and length of the mouse movement. 

5.3.2  Go “deeper” into high-dimensional space 
By clicking the middle mouse button, our system generates a PPA-z 
vector  according  to  the  two  options  described  in  Section  5.1  and  a 
new orthogonal vector is computed using Gram-Schmidt. Then with 
a  trackball  up  (down)  motion,  the  emphasis  of  the  dimensions 
projecting on the PPA z-axis is increased (decreased). The effect  of 
this  operation  will  only  be  visible  once  the  trackball  is  rotated 
regularly  and  the  new  3D  subspace  is  exposed.  We  call  this 
functionality “deep” since the axis that is changed is the PPA z-axis 
(i.e. the axis pointing into the depth of the display). 

5.4 

Display of Attribute Labels on the SE Boundary 

As  mentioned,  in  order  to  better  comprehend  the  relationships 
between a scatterplot projection and the data dimensions (attributes), 
we  display  the  attribute  names  as  labels  along  the  SE  trackball 
periphery (see Fig. 6). The extent of  which  a dimension contributes 
to  the  projected  point  cloud  is  indicated  by  label  size  and  opacity. 
The  larger  and bolder  the  label’s  font is,  the  stronger  the  attribute’s 
contribution  to  the  plot.  The  location  of  each  label  is  computed  by 
the attribute’s weighting in the PPA-x and PPA-y vectors. Let wx be 
the  PPA-x  weighting,  and  wy  be  the  PPA-y  weighting.  Then  the 
angle  between  this  dimension  vector  and  the  positive  x-axis  is 
computed as α = atan(wy/wx). 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

                            (a)                                                               (b)                                                              (c)                                                                  

Fig. 6. Dimension label overlap prevention. (a) Naïve implementation causing label overlap; (b) Using our angular spacing scheme to 
prevent  label overlap; (c) Illustration of our label overlap prevention scheme. 

Preventing Overlapping Attribute Labels 

5.4.1 
In practice, attribute labels may come to print on top of one another 
(Fig.  6(a)).  This  occurs  because  several  dimension  vectors  overlap. 
We solved this problem by forcing labels to locate at least β degrees 
apart  from  their  neighbors.  Fig.  6(c)  shows  this  for  the  upper  left 
quadrant.  Here, 𝑑1 is  the  location  of  label1  located  γ  degrees  away 
′ is  the  location  of  neighboring  label2,  spaced 
from  PPA-y  and 𝑑2
𝛽′ degrees  away.  We  see  that 𝛽′ is  too  small  causing  the  two  labels 
to  overlap.  Therefore  we  introduce  a  small  displacement  which 
places  label2  at 𝑑2 .  Now  label1  and  label2  are  spaced 𝛽  degrees 
apart and no longer overlap. 

In experiments, we found that the best choice for β is dependent 
on  the  orientation  of  the  dimension  vector.  The  more  vertical  it  is, 
the  larger  β  should  be,  while  for  a  more  horizontal  alignment,  a 
smaller β will suffice. The following equation relates β to the angle γ 
between the vertical axis and the dimension vector (for the upper left 
quadrant only – the other three quadrants are related by symmetry): 

𝜃𝑣 − (𝜃𝑣 − 𝜃ℎ) ∗

            0 ≤ 𝛾 <  45°

γ

        β =   { 

 
          𝜃ℎ                                      45° < 𝛾 ≤  90°    

45°

Here, 𝜃𝑣 and 𝜃ℎ  are  constants  we  determined  for  the  maximal  font 
size  of  the  labels  which  occur  when  the  corresponding  dimension 
vectors  are  fully  projected.  The  angle   𝜃ℎ =  4°  is  the  displacement 
needed  when  γ  is  greater  than  45°,  while  an  angle  of 𝜃𝑣 = 24° is 
needed  when  γ=0°.  When  γ  is  between  0°  and  45°  we  determine  β 
via linear interpolation. Fig. 6(b) shows the configuration of Fig. 6(a) 
with our label displacement scheme enabled. 

We  also  found  that  while  displacing  the  labels  provided  for 
better  readability,  it  was  distracting  in  interactive  mode  when  users 
were rotating the trackball since it could lead to sudden jumps of the 
labels.  Hence  we  only  apply  the  overlap  removal  method  when  the 
projection  is  fixed  (after  releasing  the  mouse).  Conversely,  when  a 
dataset  has  many  dimensions,  the  label  overlap  can  never  be 
prevented. For this reason, we added a slider to the control panel by 
which  users  can  set  the  maximum  number  of  displayed  attribute 
labels.  These  can  be  the  most  significant  attributes  or  attributes 
manually selected by clicking on their labels with <ctrl> depressed.    

5.5 

Point Brushing, Tagging, and De-Activation 

Our interface also provides the ability to label a point (or a group of 
points)  with  a  color  chosen  from  a  palette.  This  is  useful  when 
monitoring  a  certain  point’s  (or  point  group’s)  behavior  when  the 
trackball  rotates.  It  greatly  helps  in  distinguishing  different  clusters 
or seeing sub-clusters emerge during motion.  

Conversely,  by  painting  a  selected  group  of  points  in  gray  they 
will become invisible and will be excluded from all further analysis. 
This  helps,  for  example,  in  recognizing  other  structures  that  were 
hidden or ambiguous before this removal. 

6 
 

6.  THE  S UB SP ACE  T R AI L  M AP   AN D VI EW  GEN ER AT I O N 

The  subspace  trail  map  (STM)  is  a  spatial  layout  of  thumbnail 
representations  of  views.  It  serves  three  purposes.  First,  it  enables 
users to keep track of the subspaces explored so far. These subspaces 
can  be  revisited  for  further  exploration.  Second,  it  serves  as  a 
presentation  platform  for  the  system  to  suggest  new  subspaces  not 
yet  explored.  Third,  it  permits  users  to  define  routes  along  which 
they  can  transition  between  two  or  more  of  these  subspaces, 
essentially  using  them  as  keyframes.  In  the  STM,  users  can  double 
click any view thumbnail and add  it back into the SE. For clustered 
data, all subspaces can be inserted into the STM at once by clicking 
the ‘AllSubspace’ button in the control panel.  

6.1 

Populating the Subspace Trail Map (STM) 

Each  view  thumbnail  in  the  STM  holds  the  view’s  2D  scatterplot 
embedded  into  a  circle  to  mimic  its  appearance  in  the  SE.  PCA 
analysis  is  used  to  ensure  a  well-spread  layout  of  the  view 
thumbnails  with  a  minimum  of  overlap.  If  overlap  occurs  the 
‘SmallViewSize’  slider  can  be  employed  to  lower  the  circle  sizes 
uniformly  (see  Fig.  11(i)).  Alternatively,  clicking  on  a  partially 
hidden view will bring it to the foreground.  

To  illustrate  how  the  STM  layout  works,  suppose  there  are 𝑝 
subspace views stored in the STM and the dimensionality of the data 
set is 𝑁. The three orthogonal PPA vectors (the PPA x, y, and z-axes) 
spanning a subspace j can then be formally expressed as: 

𝑁−1

𝑃𝑃𝐴𝑖𝑗 = ∑ 𝑤𝑖𝑗𝑘𝑑𝑘

 

𝑘=0

where 𝑖 is either x, y, or z, 0 ≤ j ≤ 𝑝 − 1, 𝑤𝑖𝑗𝑘 is the weighting of the 
𝑘𝑡ℎ  data  dimension  on  𝑃𝑃𝐴𝑖𝑗  and  𝑑𝑘  is  the  𝑘𝑡ℎ  dimension  axis 
vector.  We then use the L2 norm to define the overall  weighting of 
the 𝑘𝑡ℎ data dimension for the 𝑗𝑡ℎ  subspace: 

𝑊𝑗𝑘 =   √𝑤𝑥𝑗𝑘

2 + 𝑤𝑦𝑗𝑘

2 + 𝑤𝑧𝑗𝑘

2    

These weights define an N-D vector for each subspace: 

𝑆𝑗 = [𝑊0𝑗, 𝑊1𝑗 , 𝑊2𝑗 … 𝑊𝑝−1,𝑗] 

This  allows  us  to  treat  each  subspace  as  an 𝑁-D  point.  We  perform 
PCA on this space of points. We keep the first two PCs and project 
all points (subspaces) into this basis. Since PCA automatically seeks 
to  find  the  directions  that  maximize  the  variance  of  the  data  points, 
the view thumbnails will be organized in a way that reduces overlaps.  

Finally,  the  view  thumbnails  are  embedded  in  a  word  cloud  of 
dimension labels (see Fig. 2). These labels are likewise placed based 
on  this  PC-basis,  using  the  projection  strength  of  their  dimension 
vectors to define their sizes and opacities. To prevent clutter we only 
keep the labels of the ten most significant dimensions.   

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

6.2 

 Subspace and View Optimization  

We  perform  view  optimization  for  several  tasks.  One  is  to  produce 
an  optimized  3D  subspace  from  a  higher  dimensional  subspace 
generated  via  subspace  clustering.  Users  may  also  use  it  on  the  fly 
when  interacting  with  the  SE:  (1)  during  exploration  of  a  3D 
subspace, and (2) for chasing clusters into neighboring subspaces. In 
the latter case, the view optimization can be set to perform the search 
within  a  narrow  range  of  dimension  increments,  or  across  an 
expanded range. Both of these applications aid users in the trackball-
based  exploration.  They  help  accelerate 
tedious  manual 
exploration  needed  to  find  a  view  that  fits  a  certain  view  quality 
criterion, such as a cluster or a class separation.  

the 

View optimization via ant-colony optimization   

6.2.1 
A  popular  view  optimization  method  in  the  context  of  high-D  data 
visualization  is  projection  pursuit.  Starting  from  any  projection, 
projection  pursuit  returns  the  PPA  x-axis  and  PPA  y-axis  that 
optimizes  a  targeted  projection  pursuit  index  (PPI).  A  number  of 
methodologies  have  been  proposed  for  this  task,  such  as  hill 
climbing  [8],  random  search  [32],  or  simulated  annealing  [9].  We 
have  strived  for  a  sophisticated  yet  comparably  easy-to-implement 
algorithm – Ant Colony Optimization (ACO) [10]. To the best of our 
knowledge, ACO has not been used for projection pursuit so far.  

General description of ant colony optimization (ACO) 

ACO  simulates  the  behavior  of  ants  in  nature.  When  looking  for 
food, ants initially travel randomly until they find food. On their way 
back  they  leave  a  pheromone  trace  along  the  route.  Instinct 
prescribes  that  other  ants  most  likely  follow  this  pheromone  trace 
instead  of  wandering  randomly.  But  pheromone  also  evaporates 
gradually,  and  so  over  time,  shorter  (lower  cost)  paths  will  be 
traveled  more  frequently  and  become  more  attractive,  leading  to  a 
convergence  on  the  optimal  path.  Based  on  this  intuition,  the 
simplest  ACO  algorithm  consists  of  the  following  three  steps 
executed  iteratively:  (1)  construct  solutions,  (2)  evaluate  solutions, 
and  (3)  update  pheromone,  increasing  it  on  low-cost  paths  and 
evaporating  it  on  others.  It  has  been  shown  that  the  solution  so 
generated is typically quite close to the optimal solution.  

The  ACO  algorithm  requires  a  discrete  search  space.  Projection 
pursuit,  however,  is  typically  performed  in  the  continuous  domain. 
General  solutions  that  address  this  problem  have  been  proposed 
[6][38] – we opted for a grid-based approach. In addition, ACO also 
requires an objective function to judge the quality of the solutions. In 
our case, this can be any view quality metric, no matter how complex. 
This  freedom  of  choice  is  enabled  because  ACO  does  not  require  a 
mathematical  derivation  of  a  gradient  measure  which  would  be 
needed for an analytical optimization scheme.  

Specific application of ACO for subspace and view optimization  

In our case, the search space is the set of all possible PPA x-axes and 
PPA  y-axes  and  the  objective  function  is  a  chosen  view  quality 
metric  –  low  stress  [24],  high  class-consistency  [37],  or  others.  To 
explain  how  ACO  works  for  this  application,  suppose  (with no  loss 
of generality) the simple case of a 2D data set with two data axes, d1 
and  d2,  where  the  PPA  x-axis  and  y-axis  can  be  represented  as 
𝑃𝑃𝐴𝑥 =   𝛼1𝑑1 + 𝛽1𝑑2 ,  and 𝑃𝑃𝐴𝑦 =   𝛼2𝑑1 + 𝛽2𝑑2.  There  are  four 
unknowns – 𝛼1, 𝛽1, 𝛼2 and 𝛽2 (for an N-D dataset there would be 2N 
unknowns).  As  an  illustration,  these  unknown  parameters  are 
represented as the four vertical gridded bars in Fig. 7. 

Our  ACO  algorithm  differs  from  the  traditional  one  in  the 
selection  of  the  initial pheromone  distribution.  While  the  traditional 
ACO  typically  begins  with  an unbiased  distribution,  ours  cannot do 
this  since  we  begin  from  an  initial  PPA  x-axis  and  y-axis 
configuration,  e.g.,  a  randomized  view  or  the  PC-basis  of  a  cluster. 
To  account  for  this,  we  increase  the  pheromone  of  this  view’s 
parameter levels, giving rise to  the red path in Fig. 7, which sets its 
levels to the discretized 𝛼1, 𝛽1, 𝛼2 and 𝛽2 values of this initial view.    

7 
 

 
 
 
 
 
 
 

 

2

             𝛼2                   𝛽

                      𝛼1               𝛽
Fig.  7.  Illustration  of  the  ACO  algorithm  in  the  discrete  domain. 
Each  vertical  bar  grid  point  stands  for  a  level  of  the  parameter 
represented  by  the  bar.  The  red,  piecewise  linear  polyline  is  a 
possible solution with the levels indicated by the bar intersections. 

1

Next,  a  generation  of  ants  is  set  free,  moving  across  the 
parameter  space  (from  left  to  right  in  Fig,  7)  selecting  levels  via 
pheromone-weighted  randomization.  While  the  levels  of  the  initial 
view  are  more  likely,  the  randomization  ensures  a  more  diverse  set 
of choices. After the whole set of parameters has been traversed, the 
generated  views  are  evaluated  by  the  chosen  view  quality  metric. 
The pheromone of each parameter level is then updated according to 
the  quality  of  the  views  it  was  part  of.  The  algorithm  stops  after  a 
fixed  number  of  iterations  and  for  each  view  parameter, 𝛼1, 𝛽1, 𝛼2 
and 𝛽2, the level with the highest amount of pheromone is chosen. 

Fig. 7 resembles a parallel coordinate display. We observed that 
after  the  single  initial  polyline,  ACO  tends  to  generate  many 
polylines  which  eventually  narrow  down  to  a  single  slim  cluster  – 
the optimized view.  

 ACO  can  also  be  constrained  to  produce  views  in  a  preferred 
interval.  For  example,  one  can  constrain  the  search  range  on  each 
parameter to be close to the initial path. This can be done  by fixing 
the  two  ends  of  the  vertical  bars  to  be  close  to  the  initial  values. 
Likewise, one can also loosen this condition and do a global search. 
In this case, the resulting view would be a global optimum according 
to different criteria. Finally, we should also take into account that the 
ACO  needs  to  return PPA  vectors,  which  are  required  to  be  of  unit 
length  and  orthogonal.  We  therefore  always  normalize  the  returned 
PPA  x-axis  and  then  use  Gram-Schmidt  orthonormalization  to  find 
the corresponding PPA y-axis. 

6.2.2  Other optimization capabilities  
Our  system  also  allows  users  to  select  several  dimensions  and 
produce  a  view  in  which  those  dimensions  are  equally  expressed.  
This produces plots similar to Star Coordinates or RadViz and can be 
useful in cases where  one wishes to see the influence of a subset of 
attributes  on  the  data.  It  is  achieved  by  clicking  on  the  respective 
labels  along  the trackball  while  depressing  the  ctrl-  and  space  keys. 
Then,  when  releasing  the  mouse,  the  weightings  for  the  selected 
dimensions are set to the maximum. A Gram-Schmidt step follows to 
orthogonalizes the transformation matrix. Fig. 8 shows an example.  

Illustrative use case 

6.2.3 
Fig.  9  shows  results  that  can  be  obtained  with  our  ACO-based 
subspace and view optimization framework using the sales campaign 
dataset  described  in  Section  7.1.  We  first  apply  simple  k-means 
clustering  using  the  Structure-Based  Distance  Metric  of  Lee  et  al. 
[25] and obtain three subspace clusters. A subsequent PCA analysis 
for each cluster establishes three separate 3D subspaces. Clicking the 
‘AllSubspace’  button  adds  all three  subspaces  to  the  STM  (see  Fig. 
9(a)). We color the three subspace clusters blue, magenta, and green, 
and color the circumference of each thumbnail view by the subspace 
it  represents.  We  observe  that  for  the  magenta  and  green  subspace 
views, 
the  focus  cluster  (magenta  or  green, 
respectively)  still  overlap  with  points  of  other  (co-)  clusters  – 
especially  for  the  magenta  subspace.  Next,  we  optimize  the  three 
subspace  views  using  distribution  consistent  criteria  [37],  shown  in 
Fig. 9(b-d). We observe that the blue cluster’s subspace and  

the  points  of 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

(a) 

(b) 

 (c) 

(d) 

Fig.  9.  Using  the  ACO-powered  subspace  and  view  optimizer  to 
optimize  the  visual  separation  of  three  subspace  clusters,  colored 
blue,  magenta,  and  green.  (a)  The  STM  with  the  thumbnail  views  of 
each  subspace.  The  color  of  each  thumbnail  circle  indicates  the 
subspace  cluster  it  shares  its  basis  with.  We  observe  that  the 
subspace PCs alone cannot isolate the subspaces well  – there is still 
a significant amount of cluster overlap. (b-d) Optimized subspaces for 
the  blue,  magenta  and  green  cluster,  respectively,  using 
the 
distribution  consistent  view  quality  criteria.  All  subspace  clusters  are 
now well separated from the others in their respective subspaces. 

instead  of  using  the  slider.  The  animation  provides  a  smooth 
transition  between  findings  when  presenting  the  results,  as  opposed 
to abruptly changing the views or simply cross dissolving them.   

7.  APP L IC AT I O N  EX AM P L E S 

In the following, we demonstrate the versatility of our framework by 
ways of applying it to a diverse set of use scenarios involving high-D 
data.  We  show  our  framework’s  application  in  (1)  visual  cluster 
analysis,  (2)  visual  item  discovery  and  selection,  helping  users  to 
recognize and negotiate tradeoffs among items, and (3) visual cluster 
refinement,  allowing  users  to  partition  feature-driven  clusters  based 
on  the  visual  expression  of  the  aggregation  of  these  features.  A 
fourth  use  case  –  the  visual  setup  of  a  classifier  in  the  presence  of 
intermixing outliers – is presented in the paper’s supplement.    

7.1 

Use Scenario #1:  Visual Cluster Analysis  

To  illustrate  the  trackball  interactions,  we  chose  a  multivariate 
cluster  analysis  task  involving  an  interactive  study  of  a  sales  force 
working for a large company. The dataset consists of 900 points (one 
per salesperson) and 10 attributes parameterizing the basic corporate 
sales pipeline. Briefly, a sales campaign begins with a lead  

                             (a)                                            (b) 
Fig.  8.  Equally  expressing  several  dimensions.  (a)  The  original 
projection. 
(b)  The  optimized  projection  where  %Complete, 
#Opportunity, and #Leads are equally expressed.  

projection  are  almost  unchanged.  This  is  because  the  three  clusters 
are already well separated here. Since we only run optimization in a 
close range of the original PC projection this view might already be 
the best compared to its neighbors. (Better views could possibly be 
obtained by expanding this range.) Conversely, the subspaces of the 
magenta  and  green  clusters  have  significantly  improved.  In  each 
panel,  the  respective  subspace  clusters  are  now  clearly  separated 
from the others.  

6.3 

Transitioning Between Subspaces  

Self-initiated  and  controlled  animation  can  be  a  helpful  paradigm 
for humans to understand how two or more different representations 
of  the  same  information  relate  to  one  another  [18][33].  We  have 
employed  animation  to  help  users  understand  how  two  subspaces 
relate to one another, with the added aim that this might also instill a 
better  understanding  of  the  high-D  data  space  in  a  larger  context. 
Users  can  select  multiple  thumbnail  views  in  the  STM  and  connect 
them with a path. Moving the ‘TraverseBtw’ slider then changes the 
PPA axis vectors from one subspace to another.  

Simply  linearly  interpolating  between  bases  of  PPA  axes, 
however,  would  lead  to  nonlinear  intermediate  projections.  We, 
therefore,  adopted  the  algorithm  by  Cook  et  al.  [9]  to  transition 
between the two subspaces using singular value decomposition. Fig. 
10  shows  three  snapshots  of  a  sequence  of  frames  from  such  an 
animation,  along  with  the  path  connecting  the  two  corresponding 
nodes  in  the  STM.  All  keyframes  and  the  path  connecting  them  are 
shown in panel (d). Panels (a) to (c) show intermediate views along 
the path, and the yellow dot in panel (d) indicates the view’s location 
in  panel  (b).  Since  these  still  frames  can  only  provide  a  limited 
illustration,  the  reader  is  encouraged  to  view  the  provided  video  to 
appreciate the insightful visual effect of this animation. 

Alternatively,  we  also  include  a  ‘presentation  mode’  where  a 
narrator  would  click  the  ‘Next’  button  to  go  to  the  next  keyframe 

                 (a)                                                     (b)                                                      (c)                                                              (d) 

Fig. 10. Transitioning between two subspaces marked in the STM using the animation slider. (a)(b) and (c) are three intermediate views. (d) 
is the animation path in the STM. The yellow dot indicates the location of the view in (b). The provided video has a complete animation. 

8 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

(a) 

(b) 

(d) 

(e) 

(g) 

(h) 

(c) 

(f) 

(i) 

Fig. 11. Analyzing the sales force dataset. (a) The dataset projected onto the first two PCs. There are three visually separable clusters -- the three 
sales teams under study. (b) STM with view thumbnails of  the overall space and the extracted subspaces for each of the three teams  -- each 
optimized such that its focus cluster is maximally separated from the others. (c)  Subspace of the blue team (d) green team subspace, and (e) 
magenta team subspace. (f) Increasing the weighting of PipelineRevenue and ExpectedROI by moving the mouse towards the respective labels 
(with right mouse button depressed). Both the green and magenta team generates more revenue than the blue team. (g) Increasing the weighting 
of #Opportunity along the PPA-y axis. The green team generates the fewest opportunities. (h) Increasing the weighting of Cost/WonLead. The 
green team is the most frugal, but has the most revenue, while the blue team is the most wasteful with not much revenue. (h)  STM setup for the 
animated presentation of these findings.    

generator  who  produces  prospective  customers  that  a  salesperson 
might  be  able  to  close  a  deal  with.  If  these  leads  receive  positive 
responses, they become won leads and receive a sales pitch at a cost 
per  won  lead.  Upon  further  positive  response,  they  become 
opportunities  or  potential  customers.  Cost  is  involved  in  every  step 
and high pipeline revenue is the ultimate goal. Three are three sales 
teams in our dataset.  

Step 1: explore the PCA view 

7.1.1 
Let us assume a sales team analyst, Pat, is about to analyze the data. 
He begins with treating the entire dataset as one cluster and performs 
PCA  –  shown  in  Fig.  11(a).  He  immediately  notices  that  there  are 
three  visually  separable  clusters  representing  the  three  sales  teams. 
These distinct clusters suggest that the three sales teams indeed seem 
to  apply  different  strategies  for  possibly  different  outcomes.  Pat 
clicks  the  ‘Apply’  button  to  load  the  cluster  information  from  the 
original data. The result (the thumbnail view on the bottom right side 
of panel (a)) confirms that the three clusters are indeed real clusters.  

Next,  Pat  examines  the  SE  boundary  in  Fig.  11(a).  He  notices 
that there are two groups of closely mapped  attributes with strongly 
printed  labels:  (1)  Expected  ROI  and  Pipeline  Revenue,  and  (2) 
LeadsWon and #Leads. As explained in Section 4.1, this means that 
the  attributes  in  each  of  these  groups  are  strongly  correlated.  Pat 
finds this view informative and saves it to the STM. 

Step 2: explore the salient subspaces 

7.1.2 
Next,  Pat  wishes  to  examine  the  subspaces  of  each  cluster.  He 
performs  PCA  on  all  of  them  and  adds  them  to  the  STM.  Pat  then 
optimizes each subspace  such that its focus cluster is best separated 
from the others. In Fig. 11(b), the view thumbnails outlined in blue, 
magenta,  and  green  are  the  subspaces  for  the  correspondingly 
colored clusters. The neutral view is the subspace for the entire data. 
Pat  first  brings  the  blue  cluster’s  subspace  back  to  the  SE  for 
closer  examination  (Fig.  11(c)).  He  notices  that  Cost  has  the  most 
prominent  label  and  that  the  blue  cluster  varies  significantly  in  this 
direction  –  more  than  the  two  others.  This  suggests  that  there  is  a 
wide diversity in the cost incurred by members of the blue sales team.  

9 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

Next,  Pat  brings  the  subspace  of  the  green  cluster  into  the  SE 
(Fig.  11(d)).  He  notices  that  in  this  subspace  #Leads  and  Cost  are 
most widely expressed (i.e. these attributes best distinguish the green 
sales team  from the others). From the plot, Pat learns that the green 
team,  with  its  cluster  being  most  closely  located  to  the  #Leads 
attribute,  seems  to  generate  the  most  leads,  while  the  blue  team 
generates the fewest. He also confirms the finding from the last view 
that the blue team seems to incur the highest cost.  

Lastly, Pat brings the magenta subspace into the SE (Fig. 11(e)). 
He  confirms  some  of  the  findings  of  the  previous  plots  and  also 
learns that PlannedRev, Cost and ExpectedROI are the attributes that 
have  the  highest  variance  for  this  group  of  data.  Finally,  he  also 
learns that the magenta sales team is separated from the other two by 
a combination of PlannedRev, #Opportunity and ExpectedROI. 

Step 3: look for differences in sales strategy 

7.1.3 
Pat  knows  that  high  Pipeline  Revenue  and  Expected  ROI  are 
important  targets  for  any  business.  He  decides  that  it  would  be  a 
beneficial  undertaking  to  explore  how  the  company’s  sales  force 
relates to these two revenue parameters.  

He  uses  the  STM  to  bring  the  initial  PCA  view  (small  panel  in 
Fig.  11(a))  back  to  the  SE.  He  presses  the  right  mouse  button  and 
moves the mouse in the direction of the two revenue parameters. Fig. 
11(f) shows the outcome. Note that the font of the two revenue labels 
gets  stronger  which  means  that  the  corresponding  two  attributes 
receive more weight in the viewed 3D subspace. The plot shows that 
both the green and magenta sales teams generate more revenue than 
the  blue  one  and  that  the  green  team  is  slighter  better  than  the 
magenta one. Pat also notices the #Opportunity attribute near the top 
of the plot and that it seems to separate the clusters well. He figures 
that revenue probably has a lot to do with the generated opportunities 
and he decides to give this attribute more emphasis.  

He  uses  cluster  chasing  to  emphasize  #Opportunity,  clicking  on 
its  label  and  moving  the  mouse  upwards  with  the  right  button 
depressed. He similarly emphasizes pipeline revenue and arrives at a 
traditional bivariate scatterplot that has pipeline revenue along PPA-
x  and  #opportunity  along  PPA-y  (see  Fig.  11(g)).  He  observes  that 
while  the  green  and  magenta  teams  vary  in  the  number  of 
opportunities – magenta creates more – both groups have somewhat 
similar revenue but green has a slight advantage. On the other hand, 
the blue team also has high #Opportunity but its revenue is low. 

So  why  does  the  blue  team  lack  revenue  despite  its  similar 
amount of opportunities? Pat knows that sales teams typically spend 
money to turn won leads into opportunities. He decides to make Cost 
per  won  lead  the  new  PPA-x  axis  by  selecting  it  from  the  attribute 
list  in  the  control  panel  (since  it is  not  visible  currently).  Fig. 11(h) 
shows  the  outcome.  He  quickly  notices  that  the  blue  team  incurs 
much  higher  cost  than  the  other  teams  and  that  the  green  team  has 
the  highest  pipeline  revenue.  In  fact,  the  green  team  is  the  most 
frugal having the narrowest cluster. 

Based  on these  discoveries, Pat concludes  that  while  generating 
many  opportunities  sounds  like  a  winning  strategy,  it  is  associated 
with  high  cost  and  therefore  the generated  revenue  tends  to  be  low. 
This  is  the  lesson  taught  by  the  blue  team.  It  thus  seems  better  to 
replicate  the  green  team’s  strategy  –  spend  little  cost  on  each  won 
lead and, despite gaining fewer opportunities, obtain higher revenue.       

Step 3: use the STM for sharing the findings 

7.1.4 
Pat is excited about his findings and plans a presentation to his group. 
He  notices  that  the  STM  is  too  cluttered  and  so  he  uses  the 
“SmallViewSize”  slider  to  reduce  the  size  of  the  view  thumbnails. 
Then  he  connects  them  by  simple  mouse  clicks  and  builds  a  path 
(bottom  panel  in  Fig.  11(i)).  Clicking  the  ‘Next’  button,  all  his 
findings can now be displayed sequentially, in an animated fashion. 

Conclusions from this use case 

7.1.5 
We believe that this example convincingly demonstrates how our SE 
interface  enables  users  to  playfully  arrive  at  different  multivariate 
scatterplot projections, quickly respond to new explorations ideas on 

10 
 

a  whim,  make  casual  observations  in  the  process,  and  just  as  easily 
return  back  to  a  traditional  bivariate  scatterplot  visualization.  The 
interested reader may watch the video to see the complete process. 

7.2 

Use Scenario #2: Visual Item Discovery & Selection    

Selecting  the  best  college,  given  the  many  personal  constraints  and 
preferences  one  might  have,  is  arguably  one  of  the  most  difficult 
choices a person will make in life. It involves the task of discovering 
the  set  of  schools  that  best  meet  one’s  personal  requirements, 
comparing them by weighing their trade-offs, and then selecting the 
college that fits best. Here we use the mixed dataset initially created 
by Nam and Mueller [31]. It has multi-faceted data on 50 of the top 
US colleges, enabling the college-seeking student to look at schools 
not only  through  the  lens  of  academics  but  also  through  the  lens  of 
social  life  and  the  general  environment  the  school  resides  in. 
Academic  ranking  and  tuition  information  were  extracted  from  a 
leading source of such information – the US News & World Report 
[49].  The  College  Prowler  website  [48],  on  the  other  hand,  ranks 
colleges  on  a  multitude  of  social  and  environmental  factors.  We 
picked  8  of  the  20  the  site  offers:  athletics,  campus  housing,  local 
atmosphere,  nightlife,  safety,  transportation,  academic  environment, 
and  weather.  Each  score  is  available  letter-graded  ranging  from  A+ 
to D-. We mapped these equidistantly to values in the range 0 to 1.  

The  College Prowler  website  allows  users  to  navigate  the  space 
of  college  attributes  by  filtering,  using  slider  bars  and  menu 
selections for each parameter to narrow down the search. This can be 
rather tedious and it also makes it difficult to recognize tradeoffs. We 
believe that our SE provides a more playful and targeted experience, 
while the STM is a better platform to save any intermediate findings.  
In  the  following,  we  shall  follow  17-year  old  Tina  who  is  just 
about  to  finish  high  school  and  see  how  she  uses  our  subspace 
voyager to find the university she feels best about. 

Checking out the relationships of attributes 

7.2.1 
Tina starts out with a view onto the dataset as a single cluster using 
the primary PC axes as a basis (Fig. 12(a)). As mentioned in Section 
4.1,  in  such  a  view  the  dimension  vectors  of  strongly  positively 
correlated attributes tend to coincide and as a result, their labels map 
to  similar  locations  along  the  trackball  boundary.  Conversely, 
negatively  correlated  attributes  will  map  to  opposite  sides  of  the 
trackball  boundary.  The  only  condition  for  both  is  that  their 
projection into the PC-axes basis is sufficiently significant, which is 
visually expressed in our system  by a large and heavy label font. In 
the  initial  view  of  Fig.  12(a)  Tina  observes  two  sets  of  positively 
correlated  attributes: 
(2) 
LocalAtmosphere,  NightLife,  and  Transportation.  She  also  observes 
a  few  negatively  correlated  attributes,  among  them:  (1)  Academic 
with  Weather  and  Athletics,  and  (2)  LocalAtomosphere  and 
NightLife,  with  Safety.  From  these  constellations,  Tina  quickly 
recognizes  that  top  academic  universities  tend  to  charge  higher 
tuition, but at the same  time, their athletic teams are not necessarily 
among the best. She also learns that universities built in nice town or 
city  areas  usually  have  better  nightlife  and  transportation  systems, 
but  they  also  tend  to  be  less  safe.  All  this  is  good  to  know  before 
engaging in the actual selection process described next.    

(1)  Academics  and  Tuition,  and 

Finding the set of schools that fit the best 

7.2.2 
Tina  does  not  come  from  a  wealthy  family  and  so  her  immediate 
focus is tuition cost. Her first step is, therefore, to select Tuition and 
move  the  mouse  towards  that  label  (to  the  left).  Next,  she  wants  to 
see which of the schools have a good academic ranking. She selects 
USNewsScore  and  moves  the  mouse  downward  to  maximize  the 
spread.  This  leaves  her  with  the  axis-aligned  scatterplot  shown  in 
Fig.  12(b).  In  this  plot,  all  points  on  the  lower  right  side  are  the 
universities  with  high  rankings  but  low  tuition  –  these  are  the  ones 
Tina is interested in the most. She colors them in magenta and asks 
the system to label them – in this case with the university names. 

Tina  likes  the  outdoors  a  lot  which  requires  the  weather  to  be 

generally good. So she adds Weather as another requirement to 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

(a) 

(c) 

(c) 

axis 

(b).  Make  Tuition  the 
PPA-x 
and 
USnewsScore  the  PPA-y 
The  magenta-
axis. 
colored, 
labeled  points 
are  the  schools  with  high 
US  News  Score  and  low 
Tuition.  

(b) 

(a).  The  PCA  view  of  the  entire 
dataset.  It  reveals  a  strong  positive 
correlation  between  Academic  and 
between 
Tuition, 
LocalAtmosphere,  NightLife, 
and 
Transportation.  

as  well 

as 

(c). Increase  Weather along the 
PPA-y  axis.    Purdue  moves  up 
revealing 
weather, 
TexaxA&M  and  UMaryland  do  not 
move  much  but  are  low  in  both 
scores. 

bad 

its 

(f) 

(d) 

(e). Increase Nightlife along the 
PPA-y  axis.  USCViterbi  has  the 
best  overall  score, 
followed  by 
UCLA, Georgia Tech, UCSanDiego 
and UC Berkeley.  

(e) 

(f). The final setup. The dimension 
four  dimension 

reveals 

projection 
groups.  

(d). 

Increase  Athletic  along 

the 
PPA-x  axis.    UCSantaBabara  has  a 
very low athletic score. 

Fig. 12. Finding the best college in the college dataset. 

                       Table 1 
Rankings of the final five candidates 
 

College 

Acad. 

Athletics  House  Atmos.  Night Life 

Safety 

Trans.  Weather  US News 

Tuition 

10 

10 

10 
10 
9 

10 

2 

11 
9 
8 

5 

8 

5 
8 
6 

12 

11 

10 
9 
11 

10 

12 

9 
8 
9 

9 

7 

7 
7 
11 

4 

8 

7 
10 
6 

11 

11 

8 
11 
12 

69 

77 

86 
89 
72 

22428 

22734 

22188 
14998 
14694 

UCLA 

USC-Viterbi 

Georgia Tech 

UC Berkeley 

UC San Diego 

11 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

USNewsScore by selecting the Weather label and moving the mouse 
in  the  downward  direction  (Fig.  12(c)).  This  enables  her  to 
appreciate any tradeoffs that may exist in these two variables.  

After  making  this  choice,  she  sees  ‘Purdue’  moving  away 
significantly. This means that even though its USNewsScore is quite 
good, it is not good enough to make up for the unfavorable weather.  
Likewise,  ‘GeorgiaTech’  also  moves  away  but  not  as  much  and  so 
Tina  keeps  it  marked  and  labeled.  ‘UMaryland’  and  ‘TexasA&M’ 
did not  move  too  much  either,  but  both  of  their  scores  are  not  high 
from  the  onset.  It  means  that  none  of  these  two  schools  does  well 
enough  in  either  UsnewScore  or  Weather  to  make  up  for  the 
moderate performance in the other attribute. Tina removes these two 
schools as well, repainting them to neutral blue.  

Tina  enjoys  the  excitement  of  college  team  sports.  She  is  also 
quite  athletic  and  she  thinks  she  might  be  able  to  secure  an  athletic 
scholarship to pay for her tuition. So she puts the Athletics attribute 
near  the  tuition  using  the  aforementioned  mouse  interactions  (Fig. 
12(d)). She notices that ‘UCSantaBarbara’ has a rather poor athletic 
score  and  henceforth  she  eliminates  that  school.  On  the other  hand, 
‘USC-Viterbi’  has 
followed  by 
‘GeorgiaTech’,  ‘UCLA’,  ‘UCBerkely’  and  ‘UCSanDiego’.  She 
keeps all them magenta colored and labeled. 

the  highest  athletic  score, 

Of course, Tina wants to have some fun in college. She focuses 
on  NightLife  and  moves  it  to  the  bottom  (Fig.  12(e)).  ‘USCViterbi’ 
moves  down  confirming  that  it  has  a  good  nightlife.  ‘UCBerkeley’ 
and  ‘UCSanDiego’  move  up,  indicating  that  they  may  have  good 
weather  but  the  nightlife  is  limited.  Conversely,  ‘GeorgiaTech’  and 
‘UCLA’ stay put – they are more balanced in those two factors. 

The decision – selecting the #1 school 

7.2.3 
Looking at the plot shown in Fig. 12(e) Tina sees that ‘USC-Viterbi’ 
might  be  the  best  candidate.  It  is  somewhat  in  the  middle  between 
Athletics  and  Nightlife  and  it  is  closest  to  the  circle  boundary 
indicating  that  it  has  the  highest  values  there.  Yet,  ‘GeorgiaTech’ 
and ‘UCLA’ are both not far behind and could be close contenders. 
In  order  to  gain  an  overall  impression,  Tina  puts  all  attributes  of 
interest  into  one  view.  She  tilts  the  trackball  and  creates  the 
configuration shown in Fig. 12(f). Four dimension groups emerge (1) 
Athletic and faintly Academic, (2) Tuition, (3) LocalAtmosphere and 
Transportation  (both  with  small  weighting),  (4)  NightLife,  Weather 
and  faintly  USNewsScore,  Safety,  and  CampusHousing.  Among  all 
those groups of factors, Tina values Athletic and Academic the most, 
and  so  she  chooses  ‘GeorgiaTech’  as  her  #1  top  choice  school  to 
apply for. 

Comparison with TripAdvisorND  

7.2.4 
We  purposely  conducted  a  similar  selection  task  than  Nam  and 
Mueller  in  [31],  and  a  partial  goal  of  this  use  case  was  to  compare 
the two systems. We obtained rather similar, almost identical results 
than  these  authors,  except  that  their  final  candidate  list  did  not 
contain ‘UCLA’. We compared UCLA’s scores with that of the other 
candidates (see Table 1) and found that except for a lower rating in 
transportation  and  a  slightly  lower  rating  in  USNewsScore,  it  is  not 
worse  in  other  aspects  and  hence  should  be  included  in  the  final 
candidate set. Especially in the final dimension group Academic and 
Athletic, it has a better combination of values than the other schools 
in the set, except for ‘Georgia Tech’.  

We 

think 

that  the  omission  of  ‘UCLA’  occurs  because 
TripAdvisorND’s  motion  trail  makes  it  sometimes  difficult  to 
precisely  follow  each  point.  But  the  motion  trail  is  needed  there  to 
convey  the  dynamic  movement.  Conversely,  with  our  trackball, 
motion  trails  are  not  required  since  the  perception  of  the  motion  is 
much  more  tightly  linked  to  the interaction  that  is  causing  it – both 
occur in the same interface.  

Another  advantage  of  our  new  system  is  that  users  no  longer 
need to take their eyes off the visualization while they are interacting 
to  change  the  view.  TripAdvisorND’s  touchpad  required  this.  It  also 
required  that  two  points  are  moved  separately  –  the  one  due  to  the 
PPA-x  and  the  one  due  to  the  PPA-y  vector.  With  our  trackball 

12 
 

interface, users can express these goals much more directly. In fact, 
they do not even need to be aware of the existence of these axes and 
vectors which we believe makes our interface much more appealing 
to general users.  

7.3 

Use Scenario #3 – Visual Cluster Refinement 

Often  high-D  data  are  derived  from  feature  analysis  where  the 
features themselves are not overly meaningful in isolation. Rather, it 
is the synthesis of all features that allow users to describe a grouping 
of  the  data  points.  In  this  process,  the  feature-based  clustering 
provides  the  organization  in  which  the  boundaries  of  the  individual 
groups can be delineated. We now demonstrate how our system can 
be used to allow humans to assist in creating and refining these kinds 
of  groupings  in  data,  using  visualization  as  a  gateway.  We  call  this 
process visual cluster refinement.  

images.  It  provides  a 

  We have selected an image classification tasks for this use case. 
The  CLEF  Cross-Language  Image  Retrieval  Track  (ImageCLEF) 
[50], launched in 2003, is an evaluation forum for the cross-language 
annotation  and  retrieval  of 
language 
independent platform where visual information retrieval systems can 
be  evaluated  for  analysis,  classification  and  retrieval  tasks.  The 
ImageCLEF  data  [15][51]  entails  three  sets  of  images  –   training, 
testing, and development. Each set uses different feature descriptors 
to describe the images, such as SIFT, color histogram, and GIST. We 
use  the  GETLF  feature  vector  from  the  development  set  of 
ImageCLEF 2013 [52], which is a 256-dimensional histogram based 
feature.  For  the  exact  information  on  how  to  generate  these 
descriptors, the reader is referred to [15]. 

In the following, we employ our Subspace Voyager as a medium 
to bring users into the loop of assessing and assisting the process of 
top-down  clustering  of  this  dataset.  Since  the  cognitive  processes 
driving  image  recognition  and  assessment  are  still  much  better 
developed in humans (as opposed to machines) a visual interface that 
allows humans to participate in this task can be very valuable.  

We begin by setting the initial number of clusters to a value of 2 
and  run  k-means  clustering  with  the  structure-based  distance  metric 
[25]  on  the  collection  of  points.  Fig  13  (a)(b)  shows  the  two 
subspaces in which the two clusters reside. Since the attribute labels 
on  the  trackball  boundary  are  rather  cryptic,  a  visual  assessment  of 
cluster  quality  is  difficult  and  even  more  so  is  their  interactive 
refinement. This can only be done by visualizing the semantics of the 
data points themselves – in this case, the underlying images.  

To  facilitate  this,  we  examine  the  two  clusters  separately  inside 
their own subspaces by turning off the other cluster. Similar to Liu et 
al. [27] we randomly select a subset of the data points in each cluster 
and  display  the  corresponding  images  next  to  them  (Fig.  13(c)(d)). 
We  notice  that  the  images  in  the  magenta  cluster  (Fig.  13(c))  are 
overall more saturated than those in the blue cluster (Fig. 13(d)). For 
the magenta cluster in Fig. 13(c) a clear change from yellow to black 
can  be  observed  along  the  PPA-y  axis,  with  yellow  sunsets,  yellow 
flames,  and  yellow-leaved  trees  on  the  bottom  of  the  distribution, 
mixed  with  increasingly  more  black  towards  the  top.  For  the  blue 
cluster in Fig. 13(d), the images on the bottom left are a paler green 
mixed  with  gray  while  those  on  the  top  are  mostly  blue  toned.  The 
images on the right half have a background that is mostly white, with 
the  main  objects  being  low  saturated.  The  differences  between  the 
two  clusters  in  Fig.  13  (c,  d)  are  obvious  and  this  confirms  that  k-
means can separate the dataset well at this level of the hierarchy. 

We  now  continue  this  process  and  further  partition  the  clusters 
using  k-means.  We  choose  the  blue  cluster  (Fig.  13(d))  since  its 
diversity  is  much  greater  than  that  of  the  magenta  cluster.  As  there 
seem to be three main categories of images we pick k=3. Figs. 13(e-
g)  show  the  results.  We  observe  that  the  classification  has  become 
more  specific.  The  sub-cluster  in  Fig.  13(e)  has  the  blue  images, 
fading out towards the bottom left. The sub-cluster of Fig. 13(f) has 
the  green  images,  again  fading  out  towards  the bottom  left.  Finally, 
the  sub-cluster  in  Fig. 13(g)  has the  saturated  images  or  the  images 
with white background. While this 2-level tree could already suffice 
for some applications, one more level of sub-clustering might  

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

Fig.13.  Exploring  the  ImageCLEF  dataset.  (a)  The  subspace  of  the  blue  cluster.  (b)  The  subspace  of  the  dark  magenta  cluster.  (c)  The 
images in the dark magenta cluster. (d) The images in the blue cluster. (e-g) The three sub-clusters of the cluster in (d). 
 

separate  the  fully  saturated  images  from  the  less  saturated  ones  in 
each of the clusters obtained so far. Here the visualizations can help 
to determine where and whether such a step is required or desirable. 

8.  USER  STUDY 

To evaluate the features of our interface, we conducted a user study 
with 10 graduate students – 3 females and 7 males of diverse cultural 
background (4 North Americans and 6 Asians). None had experience 
in visual analytics. We sought to test whether the participants could 
fulfill certain data analysis tasks using our system. 

8.1 

Setup 

We invited all participants to sit down with us individually. We first 
showed  them  a  3-minute  intro  video  which  covered  all  basic 
interactions 
trackball 
interactions, saving views to the STM and bringing them back to the 
SE, and traversing between STM views. We used the Iris dataset [53] 
as a walk-through example in this video. 

the  Subspace  Voyager  supports  – 

the 

After  the  video,  each  participant  could  ask  questions  to  resolve 
any doubts. This was the only time  we entertained questions. Three 
participants  were  unclear  on  how  to  interpret  the  dimension  labels 
along the trackball -- a brief explanation resolved these doubts. Next, 
each  participant  was  asked  to  perform  three  tasks.  With  their 
consent,  we  filmed  the  computer  screen  to  record  their  operations. 
We  asked  them  to  speak  out  their  thoughts  during  the  entire  time, 
which we also recorded. We used both recordings in our study. 

8.2 

Tasks 

Our  tasks  were  designed  to  measure  three  levels  of  understanding 
[19]  gained  from  the  visual  interactions  –  shape,  organization,  and 
relations.  For  the  first  task,  we  used  a  somewhat  contrived  3D  data 
set: a hollow tube with a stick in the middle that was not aligned with 
any of the data axes. We asked the participants to describe the shape 

13 
 

of  the  data,  first  using  the  SPLOM  (Fig.  14(a))  and  then  using  the 
Subspace Voyager (Fig. 14(b)). 

The  second  task  measured  visual  understanding.  We  did  not 
inform the participants about the nature of the dataset. We initialized 
the  STM  with  two  scatterplots  of  the  sales  force  data  (Fig.  15(a-b)) 
and  asked  the  participants  to  describe  what  they  saw  in  these  two 
plots.  Then  we  encouraged  them  to  transition  between  the  two 
scatterplots and again asked them to describe their impressions. 

For  the  data  understanding  task,  we  also  used  the  sales  force 
dataset  but  now  we  first  showed  the  participants  a  1-minute  video 
that  introduced  the  attributes  of  the  data.  The  initial  trackball 
configuration is shown in Fig. 16(a). In this view, the clusters for the 
three teams overlap. The participants were told that there were three 
sales teams, who used different sales strategies and that the task was 
to determine which of the strategies was best to reach high revenue. 

8.3 

Result   

The  following  results  also  include  a  comparative  study  with  the 
TripAdvisorND interface, using the same data and participants.  

Task 1 – Shape understanding 

8.3.1 
Not a single participant found the hidden stick in the SPLOM. This 
was to be expected since this structural feature can only be observed 
from  a  non-axis  aligned  angle.  Eight  participants  asked  for  pen  and 
paper  to  reconstruct  the  distribution  of  the  data  but  none  got  it 
completely right. Descriptions were ‘tilted cylinder’ or ‘oval prism’.   
On the other hand, using our trackball’s rotation functionality, all 
participants managed to find the hidden stick. They spent 48 seconds 
on  average  for  this  task.  Some  of  the  descriptions  given  were  ‘pipe 
with something in the middle’, ‘cylinder with some coaxial cable’, or 
‘two  concentric  cylinders’.  Fig.  14(c)  and  (d)  shows  two  typical 
views  the  participants  generated  and  which  helped  them  draw  their 
conclusions. This high success rate demonstrates that our trackball is 
able to help users understand the structure of data.  

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

                      (a)                                             (b)                                                       (c)                                                   (d) 

Fig. 14. User study 1: shape understanding. We asked the participants to use both (a) SPLOM and (b-d) our system to examine the shape 
of the data. (b) The initial view in the SE. (c, d) Two typical views the participants generated to help them draw their conclusions. 

                 (a)                                                       (b)                                                  (c)                                                  (d) 

Fig. 15. User study 2: visual understanding. We showed the participants an STM composed of two views (a) and (b). We then asked them 
to traverse between these two views and describe what they saw along the path. This path included view (c) which was the most revealing 
– the motion parallax clarified that there were indeed three clusters. (d) The three clusters marked in different colors as a confirmation.     

                  (a)                                                     (b)                                                  (c)                                                   (d) 

Fig. 16. User study 3: data understanding. (a-d) Various views our participants generated. See Section 8.3.3 for a narration of these plots.  

Task 2 – Visual understanding 

8.3.2 
Starting out with the two view thumbnails (shown expanded in Fig. 
15(a-b)) eight of the participants stated that there were two clusters. 
On the other hand, two of the participants suspected that there might 
be  a  third  cluster,  mostly  based  on  the  view  in  Fig.  15(a).  Next,  all 
used the ‘TraverseBtw’ slider to go from one view to the other (nine 
users went  from the top view to the bottom one while one went the 
other  way).  Fig.  15(c)  has  the  most  revealing  view  they  generated 
(colored  in  Fig.  15(d)).  Everyone  spotted  the  third  cluster  while 
traveling. The average time for completing the task was 83s. Most of 
this time was spent on describing the observations while only 10–20s 
was spent on traversing between the two subspaces. 

The  transition  interface  seemed  very  effective  in  helping  the 
participants understand the high-D structures. Some of the comments 
were  ‘The  bigger  cluster  separates  into  two,  one  of  them  remains  a 
separate cluster, while the other one merges with the smaller cluster’ 
and  ‘The  upper  left  cluster  seems  to  be  moving  forward.  Another 
cluster  is  moving  upward,  and  the  third  one  is  moving  downward’. 
One person saved a couple of views in the STM and later mentioned 
that  ‘if  I  look  at  those  still  frames,  I  probably  still  cannot  tell  that 

there are three clusters, it’s really the motion by which you can tell.’ 
This  comment  nicely  verbalizes  the  power  of  motion  parallax.  It 
makes it easier to spot patterns than relying purely on still frames.   

8.3.3     Task 3 – Data understanding 
For  this  task,  we  colored  the  points  according  to  sales  team.  We 
observed  that  the  participants  often  used  the  ‘chase  cluster’ 
functionality but as a part of different exploration strategies. One of 
these  strategies  consisted  of  making  individual  data  dimensions  the 
dominant  factor  on  either  the  PPA-x  or  the  PPA-y  axis  and  then 
observing  the  distribution  of  the  three  sales  teams  along  these 
attributes.  Fig.  16(b)  shows  a  view  generated  by  a  participant  who 
wanted  to  examine  the  influence  of  #opportunities.  Later  on,  when 
using  the  STM  to  return  to  this  view  to  present  her  findings,  she 
stated that ‘the blue team has the lowest number of opportunities’.  

Another strategy was to keep PipelineRevenue as either the PPA-
x  or  PPA-y  axis  and  assign  another  attribute  to  the  other  axis.  By 
doing  this,  these  participants  managed  to  create  traditional  axis-
aligned  scatterplots  where  they  could  examine  the  relationships 
between two variables. One typical view of this method is shown in 
Fig.  16(c).  Here  the  participant  concluded:  ‘The  blue  and  magenta 

14 
 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

teams all have high revenue. The magenta and green teams all have a 
high number of opportunities.’ 

Yet another analysis strategy was to try to come up with certain 
views where some attributes, or all of them, were well expressed. A 
typical  view  for  this  strategy  is  shown  in  Fig.  16(d).  Based  on  this 
view, the participant generating it described his findings as ‘The blue 
and  magenta  teams  have  higher  revenues  than  the  green  one.  They 
both generate more leads and have a lower cost per won lead. Those 
two factors seem to be important.’ 

All participants used the STM to save important findings. When 
they reported these finding to us, six participants simply dragged and 
dropped  their  saved  views  back  to  the  trackball,  while  four 
participants  used  the  STM  to  traverse  between  these  key  frames 
when  telling  us  the  story  because  they  ‘liked  the  animation’.  When 
their  findings,  all  participants  arrived  at  similar 
presenting 
conclusions, viz. ‘The blue and the magenta teams have the highest 
revenue’, ‘The blue team has a high number of leads and this might 
lead to their high revenue’ and ‘Cost per won lead needs to be low’.  
The participants spent on average 17 minutes and 48 seconds on 
this task. There was a wide time spread. One of the participants spent 
almost  33  minutes  because  he  was  ‘having  fun’  and  just  wanted  to 
explore  the  dataset  more.  We  believe  this  proves  that  our  system  is 
very playful and nicely engages the users into the data analytics.  

8.3.4     Further finds from the voice recordings 
Analyzing  the  recordings  produced  further  interesting  comments. 
One participant said our system was ‘very intuitive’ and ‘helped me 
not  only  understand  the  distribution  of  the  data  on  one  specific 
dimension  but  multiple  dimensions  at  the  same  time’.  Other 
participants stated that our system made ‘exploring data fun’, that the 
trail map made ‘switching back and forth very fast’, and that this was 
the ‘first time seeing data exploration could be done in this way’.  

8.3.5     Comparative study with TripAdvisorND 
We  also  conducted  a  comparative  study  with  the  TripAdvisorND 
interface  to  test  if  our  system  could  outperform  its  ancestor.  Since 
TripAdvisorND does not have a trail map for the navigation between 
different subspaces, we only repeated the first and the third tasks. We 
engaged the same participants as in the Subspace Voyager study and 
we  used  the  same  data.  There  were,  however,  six  months  between 
the two studies. This greatly diminished learning effects with respect 
to  the data.  Our  goal  was  primarily  to  learn  about pros  and  cons of 
the two systems, and only get a rough estimate of the time needed to 
accomplish views comparable to those obtained before. 

that 

the  participants  were  mostly  moving 

For  the  first  task,  the  participants  expressed  that  the  subspace 
voyager  was  more  ‘direct’.  They  thought  that  the  pad  navigation 
interface  in  TripAdvisorND  ‘made  the  control  of  the  shape  difficult. 
We  observed 
the 
navigation  points  ‘arbitrarily’  until  they  found  out  what  was  going 
on. One participant mentioned that because of the ‘extra layer of the 
interface, how to control the points is less obvious and less intuitive’.  
We  asked  him  what  he  meant  by  ‘extra  layer’.  He  replied  ‘it’s  the 
separate navigation interface and also the switching between x and y 
control  points’.  On  average,  the  participants  spent  91  seconds  to 
finish  this  task  using  TripAdvisorND.  This  is  about  twice  the  time 
they needed with the Subspace Voyager.  

For  the  third  task,  the  one  participant  who  chose  to  make 
individual data dimensions the dominant factor on either the PPA-x 
or  PPA-y  axis  preferred  the  navigation  polygon  in  TripAdvisorND. 
He  thought  moving  the  red  and  blue  dots  onto  any  two  dimensions 
was  very  straightforward  in  TripAdvisorND  while  ‘it  required  a  bit 
more  adjustment  to  single  out  the  two  dimensions  I  want’  in 
Subspace  Voyager.  In  contrast,  the  other  participants  who  tried  to 
come  up  with  certain  views  where  some  attributes,  or  all  of  them, 
were well expressed all preferred Subspace Voyager. They all agreed 
that  it  was  much  easier  to  come  up  with  a  meaningful  multivariate 
projection  using  the  new  system.  One  participant  said  that  for 
TripAdvisorND,  ‘to  get  the  superposition  of  dimensions,  I  had  to 
move  the  vertices  of  the  dimensions  I  am  interested  in  next  to  each 

15 
 

other  in  the  polygon  and  it’s  tedious’  while  ‘In  the  Subspace 
Voyager,  moving  dimensions  to  desired  locations  and  controlling 
their weights was very straightforward’.  

We  believe 

these  (mostly  qualitative)  findings  and 
assessments make a conclusive argument for the Subspace Voyager.   

that 

9.  CO NC L US I ON S 

We  demonstrated  a  system  for  high-D  data  exploration  in  form  of 
scatterplot projections that decomposes the high-D data space into a 
continuum  of  generalized  3D  subspaces.  Using  3D  space  as  the 
immediate visual context affords a natural user interface  well suited 
for  mainstream  users.  The  interactive  tools  we  designed  do  not 
require  users  to  ever  think  of  data  in  their  native  high-D  context. 
Rather, users fluidly transition from one generalized 3D subspace to 
the next in a goal-directed manner, emphasizing and de-emphasizing 
the  weights  of  the  various  attributes  on  the  fly  during  the  visual 
interactions. Key elements of our system are an augmented trackball 
with  a  peripheral  weight-adaptive  attribute  label  display,  a  metric-
driven view and subspace optimizer, and a map that allows users to 
organize  the  scatterplots  of  key  findings  and  transition  between 
them.  We  also  provided  several  measures  that  help  scalability  for 
both  attributes  and  data  items.  Users  can  control  the  number  of 
attribute  labels  shown  and  they  can  hide  data  points  temporality  to 
improve  the  visibility  of  the  points  currently  deemed  relevant. 
Several  user  studies  confirmed  that  our  system  supports both  visual 
and data understanding. 

In  future  work,  we  would  like  to  add  depth  information  when 
displaying the data points in the trackball. This could be achieved by 
introducing  depth  of  field  effects  and  fog  and  converting  the  point 
clouds into shaded 3D shapes with drop and self-shadows, combined 
with occlusion and semi-transparent effects. All of these could allow 
users  to  better  appreciate  the  structure  of  the  data  and  discover 
patterns  that  would  otherwise  be  hard  to  notice  in  a  conventional 
scatter  plot  display.  The  end  goal  of  all  of  these  efforts  is  to  create 
intuitive  displays,  which  invite  discussion  of  personal  findings  with 
colleagues  in  business  planning  and  policy-making  scenarios,  etc. 
We also plan to refine our system via percept-oriented studies [12]. 

10.  AC KN OW L ED G EM EN T S 

This  research  was  partially  supported  by  NSF  grants  IIS  1527200 
and IIS 1117132, as well as the MSIP (Ministry of Science, ICT and 
Future  Planning),  Korea,  under  the  ""ITCCP  Program""  directed  by 
NIPA. We thank Eric Papenhausen for proofreading the manuscript.  

RE FE RE NC ES 

[1]  A.  Anand,  L.  Wilkinson,  T.  Dang.  ""Visual  pattern  discovery  using 

random projections."" Proc. VAST, pp. 43-52, 2012. 

[2]  E.  Abbot,  Flatland:  A  Romance  of  Many  Dimensions,  Dover  Thrift 

Edition, 1984. 

[3]  E. Angel, D. Shreiner, Interactive Computer Graphic with WebGL (7th 

[4] 

Edition), Addison-Wesley, 2014. 
I.  Assent,  R.  Krieger,  E.  Müller,  T.  Seidl,  “VISA:  visual  subspace 
clustering analysis,” ACM SIGKDD Explorations Newsletter, 9(2), 5-12, 
2007. 

[5]  D.  Asimov,  ""The  Grand  Tour:  A  tool  for  viewing  multidimensional 
data,"" SIAM J. Scientific and Statistical Computing, 6(1):128-143, 1985. 
[6]  G. Chen, J. Wang. C. Li, ""Solving the optimization of projection pursuit 
model  using  improved  ant  colony  algorithm,""  Conference  on  Natural 
Computation, pp. 521 – 525, 2008. 

[7]  W. Cheney, D. Kincaid, Linear Algebra: Theory and Applications, 2009. 
[8]  D.  Cook,  A.  Buja,  J.  Cabrera,  C.  Hurley,  ""Grand  Tour  and  Projection 

Pursuit,"" Computational and Graphical Statistics, 4(3): 155-172, 1995. 

[9]  D.  Cook,  E.  Lee,  A.  Buja,  H.  Wickham,  ""Grand  Tours,  Projection 
Pursuit  Guided  Tours  and  Manual  Controls,""  Handbook  of  Data 
Visualization, New York, Springer, 2006. 

[10]  M.  Dorigo,  ""Optimization,  Learning  and  Natural  Algorithms,""  Ph.D. 

Thesis, Politecnico di Milano, 1992. 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

This is the author's version of an article that has been published in this journal. Changes were made to this version by the publisher prior to publication.

The final version of record is available at

 http://dx.doi.org/10.1109/TVCG.2017.2672987

[11]  N.  Elmqvist,  P.  Dragicevic,  J.-D.  Fekete,  ""Rolling  the  dice:  a  multi-
dimensional  visual  exploration  using  scatterplot  matrix  navigation,"" 
IEEE  Trans.  Visualization  and  Computer  Graphics,  14(6):1539-1148, 
2008. 

[12]  R.  Etemadpour,  R.  Motta,  J.  de  Paiva,  R.  Minghim  M.  de  Oliveira,  L. 
Linsen.  “Perception-based  evaluation  of  projection  methods  for 
multidimensional data visualization,” IEEE Trans. on Visualization and 
Computer Graphics, 21(1):81-94. 

[13]  J.  Friedman,  J.  Tukey,  ""A  projection  pursuit  algorithm  for  exploratory 

data analysis,"" IEEE Trans. on Computers, 23(9): 881-890, 1974. 

[14]  K.  Gabriel,  ""The  biplot  graphic display  of  matrices  with application  to 

reduction:  A  structured 
Visualization and Computer Graphics, 23(1), pp 241-250, 2017 

literature  analysis,” 

IEEE  Trans.  on 

[35]  M.  Schäfer,  L.  Zhang,  T.  Schreck,  A.  Tatu,  J.  Lee,  M.  Verleysen,  D. 
Keim,  ""Improving  projection-based  data  analysis  by  feature  space 
transformations,"" SPIE Electronic Imaging, pp. 86540H-86540H, 2013. 
[36]  J. Seo, B. Shneiderman, ""A rank-by-feature framework for unsupervised 
multidimensional  data  exploration  using  low  dimensional  projections,"" 
Proc. IEEE Info Vis, pp. 65-72, 2004. 

[37]  M.  Sips,  B.  Neubert,  J.  Lewis,  P.  Hanrahan,  ""Selecting  good  views  of 
high-dimensional  data  using  class  consistency,""  Computer  Graphics 
Forum, 28(3): 831-838, 2009. 

principal component analysis"". Biometrika. 58 (3): 453–467, 1971. 

[38]  K.  Socha,  M.  Dorigo,  ""Ant  colony  optimization  for  continuous 

[15]  A.  Gilbert,  et  al.  “Overview  of  the  ImageCLEF  2015  Scalable  Image 
Annotation,  Localization  and  Sentence  Generation  task,”  Proc.  CEUR 
Workshop, CEUR-WS.org, Toulouse, France, September 2015 

[16]  S. Gratzl, A. Lex, N. Gehlenborg, H. Pfister, M. Streit, ""Lineup: Visual 
analysis of multi-attribute rankings"", IEEE Trans. on Visualization and 
Computer Graphics,  19(12): 2277-2286, 2013  

[17]  J.  Hartigan,  ""Printer  graphics  for  clustering,""  Journal  of  Statistical 

Computation and Simulation, 4(3): 187-213, 1975. 

domains,"" European J. Operational Research 185: 1155-1173, 2008. 

[39]  D. Swayne, D. Lang, A. Buja, D. Cook, ""GGobi: evolving from XGobi 
into an extensible framework for interactive data visualization,"" Comp. 
Statistics & Data Analysis, 43(4):423-444, 2003. 

[40]  A. Tatu, G. Albuquerque, M. Eisemann, P. Bak, H. Theisel, M. Magnor, 
D.  Keim,  ""Automated  analytical  methods to  support  visual  exploration 
of high-dimensional data,"" IEEE Trans. on Visualization and Computer 
Graphics, 17(5): 584-597, 2011. 

[18]  J.  Heer,  G.  Robertson.  ""Animated  transitions  in  statistical  data 
graphics."" IEEE Trans. on Visualization and Computer Graphics, 13(6): 
1240-1247, 2007. 

[41]  A.  Tatu,  L.  Zhang,  E.  Bertini.,  T.  Schreck.  Keim,  S.  Bremm,  T.  von 
Landesberger,  “Clustnails:  Visual  analysis  of  subspace  clusters,” 
Tsinghua Science and Technology, 17(4), 419-428, 2012. 

[19]  N. Henry, J.-D. Fekete, “Evaluating Visual Table Data Understanding,” 

[42]  L. van der Maaten, G. Hinton, ""Visualizing data using t-SNE,"" Journal 

Proc. BELIV, p. 1, 2006. 

[20]  P.  Hoffman,  G.  Grinstein,  “A  survey  of  visualizations  for  high-
dimensional  data  mining.  Information  visualization  in  data  mining  and 
knowledge  discovery,”  Information  Visualization  in  Data  Mining  and 
Knowledge Discovery, pp. 47-82, 2012. 

[21]  E.  Kandogan,  ""Visualizing  multi-dimensional  clusters,  trends,  and 

outliers using star coordinates."" ACM SIGKDD, pp. 107-116, 2001 

[22]  H.  Kim,  J.  Choo,  H.  Park,  A.  Endert,  “InterAxis:  Steering  scatterplot 
axes  via  observation-level  interaction,”  IEEE  Trans.  Visualization  and 
Computer Graphics, 22(1):131-140, 2015. 

[23]  H. Kriegel, P. Kröger, A. Zimek, ""Clustering high-dimensional data: A 
survey on subspace clustering, pattern-based clustering, and correlation 
clustering,"" ACM Trans. Knowledge Discovery from Data, 3(1):1, 2009. 
[24]  J.  Kruskal,  M.  Wish,  ""Multidimensional  Scaling,""  Sage  Publications, 

1977. 

[25]  J.  Lee,  K.  T.  McDonnell,  A.  Zelenyuk,  D.  Imre  K.  Mueller,  “A 
structure-based  distance  metric  for  high-dimensional  space  exploration 
with  multi-dimensional  scaling”,  IEEE  Trans  on  Visualization  and 
Computer Graphics, 20(3): 351-364, 2014. 

[26]  DJ  Lehmann,  H  Theisel,  “Optimal  sets  of  projections  of  high-
dimensional  data,”  IEEE  Trans  on  Visualization  and  Computer 
Graphics, 2016, 22(1): 609-618. 

[27]  S.  Liu,  B.  Wang,  J.J.  Thiagarajan,  P.-T.  Bremer,  V.  Pascucci.  ""Visual 
exploration  of  high-dimensional  data  through  subspace  analysis  and 
dynamic projections."" Computer Graphics Forum. 34(3): 271-280, 2015. 
[28]  G.  McLachlan,  Discriminant  Analysis  and  Statistical  Pattern 

Recognition, Wiley, 2004. 

[29]  M.  Meyer,  H.  Lee,  A.  Barr,  M.  Desbrun,  ""Generalized  barycentric 
coordinates  on  irregular  polygons,""  Graphics  Tools,  7(1):1086-7651, 
2002. 

[30]  E. Nam, Y. Han, K. Mueller, A. Zelenyuk, D. Imre, ""ClusterSculptor: A 
visual analytics tool for high-dimensional data,"" IEEE VAST, pp. 75-82, 
2007. 

[31]  J.  Nam,  K.  Mueller,  ""TripAdvisorND:  A  tourism-inspired  high-
dimensional  space  exploration  framework  with  overview  and  detail,"" 
IEEE Trans. Visualization & Computer Graphics, 19(2):291-305, 2013. 
[32]  C.  Posse,  ""An  effective  two-dimensional  projection pursuit algorithm,"" 
Communications  in  Statistics:  Simulation  and  Computation  19:  1142-
1164, 1990. 

[33]  P.  Ruchikachorn,  K.  Mueller,  “Learning  visualizations  by  analogy: 
promoting visual literacy through visualization morphing,” IEEE Trans. 
on Visualization and Computer Graphics, 21(9):1028-1044, 2015. 

[34]  D. Sacha, L. Zhang, M. Sedlmair, J. A. Lee, J. Peltonen, D. Weiskopf, S. 
North,  and  D.  A.  Keim.  “Visual  interaction  with  dimensionality 

16 
 

of Machine Learning Research, 9(11): 2579-2605, 2008. 

[43]  B.  Wang.  K.  Mueller,  ""Does  3D  really  make  sense  for  visual  cluster 
analysis?  Yes!""  International  Workshop  on  3DVis:  Does  3D  Really 
Make Sense for Data Visualization?  Paris, France, November 2014. 

[44]  X.  Yuan,  D.  Ren,  Z.  Wang,  and  C.  Guo,  “Dimension  projection 
matrix/tree:  interactive  subspace  visual  exploration  and  analysis  of 
high-dimensional  data”,  IEEE  Trans.  on  Visualization  and  Computer 
Graphics, 19(12): 2625 – 2633, 2013. 

[45]  Z.  Zhang,  K.  McDonnell,  K.  Mueller,  ""A  network‐based  interface  for 
the  exploration  of  high‐dimensional  data  spaces,""  Proc.  IEEE  Pacific 
Vis, pp. 17-24, Songdo, Korea, March 2012. 

[46]  http://miegakure.com/ 
[47]  https://onlinecourses.science.psu.edu/stat505/node/54 
[48]  College Prowler (Accessed 9/09), http://collegeprowler.com, 2012. 
[49]  US  News  Best  Colleges  (Accessed  9/09),  http://colleges.usnews. 

rankingsandreviews.com, 2012. 

[50]  http://www.imageclef.org/ 
[51]  http://risenet.prhlt.upv.es/webupv-datasets/dwld/v2013/feats_visual/ 
[52]  http://risenet.prhlt.upv.es/webupv/datasets/agreement?dwld=dwld/v201

3/feats_visual/webupv13_devel_visual_getlf.feat.gz 

[53]  https://archive.ics.uci.edu/ml/datasets/Iris 

Bing  Wang  received  her  PhD  from  the 
Computer  Science  Department  at  Stony 
Brook University. Her research focus is high 
dimensional  data  visualization  and  visual 
analytics.  
 
 
 
 
 
Klaus  Mueller  received  a  PhD  in  computer 
 
science  from  The  Ohio  State  University  and 
 
is currently a professor of computer science 
at  Stony  Brook  University.  His  research 
interests  are  visual  analytics,  HCI,  and 
medical imaging. He won the NSF CAREER 
award  in  2001  and  the  SUNY  Chancellor 
Award  for  Excellence  in  Scholarship  and 
Creative  Activity  in  2011.  For more  info  see 
http://www.cs.sunysb.edu/~mueller/ 

Copyright (c) 2017 IEEE. Personal use is permitted. For any other purposes, permission must be obtained from the IEEE by emailing pubs-permissions@ieee.org.

",False,2018.0,{},False,False,journalArticle,False,ZMGBDR3L,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7862917/,,The Subspace Voyager: Exploring High-Dimensional Data along a Continuum of Salient 3D Subspaces,ZMGBDR3L,False,False
VA72NXDN,WCW4X466,"Voyagers and Voyeurs:  
Supporting Asynchronous 
Collaborative Visualization

Doi:10.1145/1435417.1435439

By Jeffrey Heer, Fernanda B. Viégas, and Martin Wattenberg

abstract
This article describes mechanisms for asynchronous collab-
oration in the context of information visualization, recasting 
visualizations  as  not  just  analytic  tools,  but  social  spaces. 
We contribute the design and implementation of sense.us, 
a Web site supporting asynchronous collaboration across a 
variety of visualization types. The site supports view sharing, 
discussion, graphical annotation, and social navigation and 
includes novel interaction elements. We report the results 
of user studies of the system, observing emergent patterns 
of social data analysis, including cycles of observation and 
hypothesis, and the complementary roles of social naviga-
tion and data-driven exploration.

1. intRoDuction
Visual  representations  of  information  often  lead  to  new 
insights by enabling viewers to see data in context, observe 
patterns, and make comparisons. In this way, visualizations 
leverage the human visual system to improve our ability to 
process  large  amounts  of  data.  Card  et  al.6  describe  how 
visualization supports the process of sensemaking, in which 
information  is  collected,  organized,  and  analyzed  to  form 
new knowledge and inform further action. They emphasize 
the ways visualization exploits an individual’s visual percep-
tion to facilitate cognition.

In practice, however, sensemaking is often also a social 
process. People may disagree on how to interpret the data 
and  may  contribute  contextual  knowledge  that  deepens 
understanding.  As  participants  build  consensus  or  make 
decisions  they  learn  from  their  peers.  Furthermore,  some 
data sets are so large that thorough exploration by a single 
person  is  unlikely.  This  suggests  that  to  fully  support  sen-
semaking,  visualizations  should  also  support  social  inter-
action. In this spirit, a recent report23 names the design of 
collaborative  visualization  tools  as  a  grand  challenge  for 
visualization research.

These  considerations  are  not  just  hypothetical.  For 
example, the manager of a business group in our company 
described  to  us  how  quarterly  reports  are  disseminated 
within  his  organization  via  e-mail.  Heated  discussion 
takes place around charts and graphs as the group debates 
the  causes  of  sales  trends  and  considers  possible  future 
actions. However, writing about particular trends or views 
is  difficult,  involving  awkward  references  to  attached 
spreadsheets from the e-mail text. Furthermore, the discus-
sion is scattered and disconnected from the visualizations, 

making it difficult for newcomers to catch up or others to 
review and summarize the discussion thus far. According to 
the manager of the group, the analysis process could ben-
efit from a system for sharing, annotating, and discussing 
the visualized data.

Similar  scenarios  appear  in  other  domains.  Moreover, 
experiences  with  deployments  of  visualizations  hint  at 
ways  that  social  phenomena  already  occur  around  visual-
izations.  Wattenberg  and  Kriss27  describe  the  response  to 
NameVoyager,  an  online  visualization  of  historical  baby 
name  trends.  Playful  yet  often  surprisingly  deep  analysis 
appeared on numerous blogs as participants discussed their 
insights  and  hypotheses.  Observing  the  use  of  a  physical 
installation of the Vizster social network visualization, Heer18 
noted that groups of users, spurred by storytelling of shared 
memories,  spent  more  time  exploring  and  asked  deeper 
analysis questions than individuals. Similarly, Viégas et al.24 
found that users of the PostHistory e-mail archive visualiza-
tion  immediately  wanted  to  share  views  with  friends  and 
family and engage in storytelling.

While  suggestive,  these  observations  provide  only  a  cir-
cumstantial  understanding  of  the  social  aspects  of  asyn-
chronous analysis around visualizations. In the case of the 
NameVoyager and PostHistory, the findings were essentially 
accidental. Vizster was designed for playful interaction, but 
in a synchronous and less analytic context. It would there-
fore  be  valuable  to  replicate  these  findings  to  deepen  our 
understanding of this type of interaction.

Furthermore, if social interaction is an important accom-
paniment to data visualization, it is natural to look for ways 
to  support  and  encourage  it.  To  address  both  these  goals, 
we designed and implemented a Web site, sense.us, aimed 
at group exploration of demographic data. The site provides 
a suite of interactive visualizations and facilitates collabora-
tion through view bookmarking, doubly linked discussions, 
graphical  annotation,  saved  bookmark  trails,  and  social 
navigation through comment listings and user profiles. We 
then conducted user studies to observe closely how people 
engage in social data analysis. The studies also allowed us 
to evaluate the new design elements in the site and suggest 
directions for future work.

A  previous  version  of  this  paper  was  published  in  the  
Proceedings of the SIGCHI Conference on Human Factors in 
Computing Systems, April 2007.

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     87

research highlights 

2. PRioR WoRK
Collaboration has been well studied in contexts that are not 
directly  related  to  information  visualization.  The  study  of 
how computer systems can enable collaboration is referred 
to as computer-supported cooperative work, or CSCW. Because 
collaboration occurs in a variety of situations, CSCW schol-
ars often use a “time-space” matrix21 to outline the concep-
tual landscape. The time dimension represents whether or 
not  participants  interact  at  the  same  time  (synchronously 
or  asynchronously)—for  example,  instant  messaging  is  a 
largely synchronous communication medium, while e-mail 
is  asynchronous.  The  space  dimension  describes  whether 
users are collocated or geographically distributed.

Most work on collaborative visualization has been done 
in  the  context  of  synchronous  scenarios:  users  interact-
ing at the same time to analyze scientific results or discuss 
the  state  of  a  battlefield.  Collocated  collaboration  usually 
involves shared displays, including wall-sized, table-top, or 
virtual  reality  displays  (e.g.,  Dietz,14  General  Dynamics16). 
Systems  supporting  remote  collaboration  have  primar-
ily  focused  on  synchronous  interaction,1,4  such  as  shared 
virtual  workspaces8  and  augmented  reality  systems  that 
enable multiple users to interact concurrently with visual-
ized  data.3,9  In  addition,  the  availability  of  public  displays 
has  prompted  researchers  to  experiment  with  asynchro-
nous, collocated visualization (same place, different time), 
for  example,  in  the  form  of  ambient  displays  that  share 
activity information about collocated users.7

In  this  article,  we  focus  on  remote  asynchronous 
 collaboration—the kind of collaboration that is most com-
mon over the Web. One reason for our interest is that parti-
tioning work across both time and space holds the potential 
of greater scalability in group-oriented analysis. For exam-
ple,  one  decision-making  study  found  that  asynchronous 
collaboration resulted in higher-quality outcomes—broader 
discussions, more complete reports, and longer solutions—
than  face-to-face  collaboration.2  However,  as  noted  by 
Viégas and Wattenberg,25 little research attention has been 
dedicated  to  asynchronous  collaboration  around  interac-
tive visualization. Instead, users often rely on static imag-
ery when communicating about these interactive systems. 
Images of the visualization are transferred as printouts or 
screenshots,  or  included  in  word-processing  or  presenta-
tion documents.

A few commercial visualization systems introduced prior 
to  our  work  provide  asynchronous  collaboration  features. 
Online mapping systems (e.g., Google Maps) provide book-
marks (URLs) that users can send to others to share views. 
The  visualization  company  Spotfire  provides  DecisionSite 
Posters,  a  Web-based  system  that  allows  a  user  to  post  an 
interactive  visualization  view  that  other  users  can  explore 
and  comment  on.  The  Posters  apply  only  to  a  subset  of 
Spotfire’s full functionality and do not allow graphical anno-
tations, limiting their adoption.25

One  common  feature  of  these  systems  is  application 
bookmarks: URLs or URL-like objects that point back into a 
particular  state  of  the  application,  for  example,  a  location 
and zoom level in the case of Google Maps. This pattern is 
not surprising; for users to collaborate, they must be able to 

88    communications of thE acm   |   January 2009  |   V ol. 52  |   no. 1

 

share  what  they  are  seeing  to  establish  a  common  ground 
for conversation.12

One  of  the  primary  uses  of  bookmarks  is  in  discussion 
forums  surrounding  a  visualization.  Some  systems  use 
what  we  term  independent  discussion,  where  conversations 
are decoupled from the visualization. For example, Google 
Earth provides threaded discussion forums with messages 
that  include  bookmarks  into  the  visualized  globe.  In  such 
systems there are unidirectional links from the discussion 
to  the  visualization,  but  no  way  to  discover  related  com-
ments while navigating the visualization itself.

Another  stream  of  related  work  comes  from  wholly 
or  partly  visual  annotation  systems,  such  as  the  regional 
annotations in sites such as Flickr.com and Wikimapia.org 
and  in  Churchill  et  al.’s  anchored  conversations.10  Such 
systems enable embedded discussion that places conversa-
tional markers directly within a visualization or document. 
Discussion  of  a  specific  item  may  be  accessed  through  a 
linked  annotation  shown  within  the  visualization.  These 
systems may be seen as the converse of independent dis-
cussions, allowing unidirectional links from an artifact to 
commentary.

In this article, we extend the past work with a comprehen-
sive design for asynchronous collaboration around interac-
tive  data  visualizations,  addressing  issues  of  view  sharing, 
discussion, graphical annotation, and social navigation.

3. thE DEsiGn of sEnsE.us
To  explore  the  possibilities  for  asynchronous  collabora-
tive visualization, we designed and implemented sense.us, 
a prototype Web application for social visual data analysis. 
The  site  provides  a  suite  of  visualizations  of  United  States 
census  data  over  the  last  150  years  (see  Figures  1  and  2) 
and  was  designed  for  use  by  a  general  audience.  We  built 
sense. us to put our design hypotheses into a concrete form 
which we could then deploy and use to study collaborative 
data exploration.

The  primary  interface  for  sense.us  is  shown  in  Figure 
1. In the left panel is a Java applet containing a visualiza-
tion. The right panel provides a discussion area, display-
ing commentary associated with the current visualization 
view,  and  a  graphical  bookmark  trail,  providing  access 
to views bookmarked by the user. With a straightforward 
bookmarking  mechanism,  sense.us  supports  collabora-
tion with features described in detail below: doubly linked 
discussions,  graphical  annotations,  saved  bookmark 
trails, and social navigation via comment listings and user 
activity profiles.

3.1. View sharing
When  collaborating  around  visualizations,  participants 
must be able to see the same visual environment in order to 
ground12 each others’ actions and comments. To this aim, 
the  sense.us  site  provides  a  mechanism  for  bookmark-
ing  views.  The  system  makes  application  bookmarking 
transparent  by  tying  it  to  conventional  Web  bookmark-
ing. The browser’s location bar always displays a URL that 
links  to  the  current  state  of  the  visualization,  defined  by 
the  settings  of  filtering,  navigation,  and  visual  encoding 

figure 1: the sense.us collaborative visualization system. (a) an interactive visualization applet, with a graphical annotation for the  
currently selected comment. the visualization is a stacked time-series visualization of the u.s. labor force, broken down by gender. here  
the percentage of the work force in military jobs is shown. (b) a set of graphical annotation tools. (c) a bookmark trail of saved views.  
(d) text-entry field for adding comments. Bookmarks can be dragged onto the text field to add a link to that view in the comment.  
(e) threaded comments attached to the current view. (f) uRl for the current state of the application. the uRl is updated automatically  
as the visualization state changes.

 

figure 2: sample visualizations from sense.us. (a) interactive state map. the image shows the male/female ratio of the states in 2005.  
(b) stacked time series of immigration data, showing the birthplace of u.s. residents over the last 150 years. the image shows the number 
of u.s. residents born in European countries. (c) Population pyramid, showing population variation across gender and age groups. additional 
variables are encoded using stacked, colored bands. the image visualizes school attendance in 2000; an annotation highlights the  
prevalence of adult education.

parameters.  As  the  visualization  view  changes,  the  URL 
updates  to  reflect  the  current  state  (Figure  1f),  simplify-
ing  the  process  of  sharing  a  view  through  e-mail,  blogs, 
or instant messaging by enabling users to cut-and-paste a 

link  to  the  current  view  at  any  time.  To  conform  to  user 
expectations, the browser’s back and forward buttons are 
tied to the visualization state, allowing easy navigation to 
previously seen views.

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     89

research highlights 

3.2. Doubly linked discussion
To situate conversation around the visualization, we created 
a  technique  called  doubly  linked  discussion.  The  method 
begins with an “independent” discussion interface in which 
users can attach comments to particular states (or views) of 
a  visualization.  Comments  are  shown  on  the  right  side  of 
the  Web  page  and  grouped  into  linear  discussion  threads 
(Figure  1e).  Each  comment  shows  the  thread  topic,  com-
ment text, the author’s full name, and the time at which the 
comment was authored. Clicking on a comment takes the 
visualization  to  a  bookmarked  state  representing  the  view 
seen by the comment’s author.

Users can add comments either by starting a new thread 
or  posting  a  reply  to  an  existing  thread.  When  a  “New 
Comment” or “Reply” link is clicked, a text editor appears at 
the site where the comment will be inserted and the graphi-
cal annotation tools (discussed next) become active. Upon 
submission, the comment text and any annotations are sent 
to the server and the comment listing is updated.

The interface described above is based on links from the 
commentary into the visualization. Our system also provides 
links in the other direction: from the visualization into the 
discussion. As a user changes parameters and views in the 
visualization, they may serendipitously happen upon a view 
that another person has already commented on. When this 
occurs,  the  relevant  comments  will  automatically  appear 
in the right-hand pane. Our intuition was that this “doubly 
linked”  discussion  interface,  which  combines  aspects  of 
independent  and  embedded  discussion,  would  facilitate 
grounding  and  enable  the  visualization  itself  to  become  a 
social place.

3.3. Pointing via graphical annotation
In  real-time  collocated  collaboration,  participants  com-
monly  use  both  speech  and  gesture,  particularly  point-
ing,11,20  to  refer  to  objects  and  direct  conversation.  For 
asynchronous  collaboration,  graphical  annotations  can 
play  a  similar  communicative  role.  We  hypothesized  that 
graphical annotations would be important both for point-
ing  behavior  and  playful  commentary.  To  add  a  pictorial 
element  to  a  comment  or  point  to  a  feature  of  interest, 
authors can use drawing tools (Figure 1b) to annotate the 
commented  view.  These  tools  allow  free-form  ink,  lines, 
arrows, shapes, and text to be drawn over the visualization 
view.  The  tools  are  similar  to  presentation  tools  such  as 
Microsoft  PowerPoint  and  are  intended  to  leverage  users’ 
familiarity with such systems.

Comments  with  annotations  are  indicated  by  the  pres-
ence of a small shape logo to the left of the author’s name in 
the comment listing (see Figure 1e). When the mouse hov-
ers over an annotated comment, the comment region high-
lights  in  yellow  and  a  hand  cursor  appears.  Subsequently 
clicking the region causes the annotation to be shown and 
the highlighting to darken and become permanent. Clicking 
the  comment  again  (or  clicking  a  different  comment)  will 
remove the current annotation and highlighting.

We refer to this approach as geometric annotation, which 
operates like an “acetate layer” over the visualization, in con-
trast to data-aware annotations directly associated with the 

90    communications of thE acm   |   January 2009  |   V ol. 52  |   no. 1

 

underlying data. We chose to implement a free-form annota-
tion mechanism so that we could first study pointing behav-
iors in an unconstrained medium. Aside from the freedom 
of  expression  it  affords,  geometric  annotation  also  has  a 
technical advantage: it allows reuse of the identical annota-
tion  system  across  visualizations,  easing  implementation 
and preserving a consistent user experience.

3.4. collecting and linking views
In  data  analysis  it  is  common  to  make  comparisons 
between  different  ways  of  looking  at  data.  Furthermore, 
storytelling has been suggested to play an important role 
in social usage of visualizations, as discussed by Viégas et 
al.24 Drawing comparisons and telling stories both require 
the ability to embed multiple view bookmarks into a single 
comment.

To support such multiview comments and narratives, we 
created a “bookmark trail” widget. The bookmark trail func-
tions  something  like  a  shopping  cart:  as  a  user  navigates 
through  the  site,  he  or  she  can  click  a  special  “Add  View” 
link to add the current view to a graphical list of bookmarks 
(Figure 1c). Bookmarks from any number of visualizations 
can be added to a trail. A trail may be named and saved, mak-
ing it accessible to others.

The bookmark trail widget also functions as a short-term 
storage mechanism when making a comment that includes 
links  to  multiple  views.  Dragging  a  thumbnail  from  the 
bookmark trail and dropping it onto the text area create a 
hyperlink to the bookmarked view; users can then directly 
edit or delete the link text within the text editor. When the 
mouse hovers over the link text, a tooltip thumbnail of the 
linked view is shown.

3.5. awareness and social navigation
Social navigation15 leverages usage history to provide addi-
tional navigation options within an information space. Our 
initial system supports social navigation through comment 
listings  and  user  profile  pages  that  display  recent  activity. 
Comment  listings  provide  a  searchable  and  sortable  col-
lection  of  all  comments  made  within  the  system,  and  can 
be filtered to focus on a single visualization (see Figure 3). 
Comment  listing  pages  include  the  text  and  a  thumbnail 
image of the visualization state for each comment. Hovering 
over  the  thumbnail  yields  a  tooltip  with  a  larger  image. 
Clicking a comment link takes the user to the state of the 
visualization  where  the  comment  was  made,  displaying 
any annotations included with the comment. The author’s 
name links to the author’s profile page, which includes their 
five  most  recent  comment  threads  and  five  most  recently 
saved bookmark trails. The view also notes the number of 
comments made on a thread since the user’s last comment, 
allowing users to monitor the activity of discussions to which 
they contribute.

Although more elaborate social navigation mechanisms 
are  possible,  we  wanted  to  observe  system  usage  with 
just  these  basic  options.  We  were  particularly  interested 
in  observing  the  potential  interplay  between  data-driven 
exploration and social navigation. By allowing discussions 
to be retrieved unobtrusively while a user explores the data, 

figure 3: the sense.us comment listing page. comment listings 
display all commentary on visualizations and provide links to the 
commented visualization views.

potentially relevant conversation can be introduced into the 
exploration process. Meanwhile, comment listings and indi-
cations of recent posts may help users find views of interest, 
making social activity a catalyst for data exploration.

3.6. unobtrusive collaboration
Finally,  while  designing  sense.us  we  also  wished  to  follow 
a  common  CSCW  design  guideline:  collaborative  features 
should  not  impede  individual  usage.17  Hence  we  did  not 
litter views with prior annotations or commentary. Rather, 
commentary  on  a  visualization  is  retrieved  and  displayed 
unobtrusively on the right side of the screen and graphical 
annotations are displayed “on demand” by the user.

4. imPlEmEntation notEs
While  many  aspects  of  sense.us  rely  on  well-known  tech-
niques, this section provides implementation details for the 
more  complex  features:  application  bookmarking,  doubly 
linked discussions, and graphical annotations.

4.1. application bookmarking
Bookmarks of visualization state are implemented as a set of 
name–value  pairs  of  visualization  parameters,  listed  using 
standard URL query syntax. Normally, changing the brows-
er’s URL will force a reload of the page to prevent security 
attacks.  Because  a  reload  would  cause  a  disruptive  restart 
of the visualization applet, the bookmark URL encodes the 
query  string  as  a  page  anchor—using  the  URL  ‘#’  delim-
iter  instead  of  the  standard  ‘?’  delimiter—so  that  the  URL 
updates in place. Furthermore, updated URLs are put into 
the browser’s history stack, so that the browser’s back and 
forward buttons have their usual behavior. When a visualiza-
tion URL is updated due to use of the back or forward but-
tons or manual typing, scripts send the updated URL to the 

 

applet, which is parsed and used to update the current visu-
alization state.

4.2. Doubly linked discussions
The  bookmarking  mechanisms  alone  are  not  sufficient  to 
support  doubly  linked  discussions.  To  see  the  challenge 
in  linking  from  a  view  state  back  to  all  comments  on  that 
view,  consider  the  visualization  in  Figure  1.  When  a  user 
types  “military”  into  the  top  search  box,  they  see  all  jobs 
whose  titles  begin  with  the  string  “military.”  On  the  other 
hand,  if  they  type  only  “mili,”  they  see  all  titles  beginning 
with  “mili”—but  this  turns  out  to  be  the  identical  set  of 
jobs. These different parameter settings result in different 
URLs,  and  yet  provide  exactly  the  same  visualization  view. 
More generally, parameter settings may not have a one-to-
one mapping to visualization states. To attach discussions 
to views we therefore need an indexing mechanism which 
identifies  visualization  states  that  are  equivalent  despite 
having different parametric representations.

We  solve  this  indexing  problem  by  distinguishing 
between  two  types  of  parameters:  filter  parameters  and 
view  parameters.  Filter  parameters  determine  which  data 
elements are visible in the display. Rather than index filter 
parameters directly, we instead index the filtered state of 
the application by noting which items are currently visible, 
thereby  capturing  the  case  when  different  filter  param-
eters give rise to the same filtered state. View parameters, 
on the other hand, adjust visual mappings, such as select-
ing a normalized or absolute axis scale. Our current system 
indexes  the  view  parameters  directly.  The  bookmarking 
mechanism  implements  this  two-part  index  by  comput-
ing a probabilistically unique hash value based on both the 
filtered state and view parameters. These hash values are 
used  as  keys  for  retrieving  the  comments  for  the  current 
visualization state.

4.3. annotation
The  graphical  annotations  take  the  form  of  vector  graph-
ics drawn above the visualization. When a new comment is 
submitted, the browser requests the current annotation (if 
any) from the visualization applet. The annotation is saved 
to an XML format, which is then compressed using gzip and 
encoded  in  a  base  64  string  representation  before  being 
passed to the browser. When comments are later retrieved 
from the server, the encoded annotations are stored in the 
browser as JavaScript variables. When the user requests that 
an  annotation  be  displayed,  the  encoded  annotations  are 
passed to the applet, decoded, and drawn.

5. EValuation
To gain a preliminary understanding of asynchronous col-
laboration  practices  around  visualizations,  we  ran  explor-
atory  user  studies  of  the  sense.us  system.  The  studies  had 
two specific goals: first, to better understand emergent usage 
patterns  in  social  data  analysis;  second,  to  learn  how  well 
the  various  features  of  the  sense.us  system  supported  this 
analysis. We ran the studies in two different parts: a pair of 
controlled lab studies and a 3-week live deployment on the 
IBM  corporate  intranet.  To  analyze  the  data,  we  employed 

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     91

 

were the visualizations of Figures 1 and 2 and a scatterplot of 
demographic metrics (see Figure 4). We also introduced two 
visualizations specific to the company: stacked time series 
of  keyword  tagging  activity  and  individual  user  activity  on 
dogear, an internal social bookmarking service. The site was 
publicized through an e-mail newsletter, an intranet article, 
and individual e-mails.

5.3. findings
In the rest of this section, we report observations from these 
studies,  organized  by  commentary,  graphical  annotations, 
navigation patterns, and use of doubly linked discussion. As 
variation in content and tone differed little across studies, 
the discussion incorporates data aggregated from each. The 
data analyzed were drawn from 12.5 h of qualitative observa-
tion and from usage logs including 258 comments: 41 from 
the pilot, 85 from the first study, 60 from the second, and 72 
from the deployment.

5.4. comments
We  first  wanted  to  learn  how  comments  were  being  used 
to  conduct  social  data  analysis—was  there  a  recognizable 
structure  to  the  discussions?  To  find  out,  we  performed  a 
formal  content  analysis  on  the  collected  comments.  Each 
paper author independently devised a coding rubric based 
upon  a  reading  of  the  comments.  We  then  compared  our 
separate rubrics to synthesize a final rubric that each author 
used to independently code the comments. The final coding 
rubric categorized comments as including zero or more of 
the following: observations, questions, hypotheses, links or 
references to other views, usage tips, socializing or joking, 
affirmations of other comments, to-dos for future actions, 
and tests of system functionality. We also coded whether or 
not comments made reference to data naming or collection 
issues,  or  to  concerns  about  the  Web  site  or  visualization 
design.  The  coded  results  were  compared  using  Cohen’s 

figure 4: scatterplot of u.s. states showing median household 
 income (x-axis) vs. retail sales per capita (y-axis). new hampshire 
and Delaware have the highest retail sales.

research highlights 

a mixed-methods analysis approach combining qualitative 
and quantitative observations.

5.1. lab study
We first ran a pilot study with 6 subjects (2 females, 4 males), 
all of whom were members of our immediate research team. 
Comments  from  the  pilot  were  visible  in  a  subsequent  12 
subject (3 females, 9 males) study, with subjects drawn from 
our greater research lab. Subjects were at least peripherally 
familiar  with  each  other  and  many  were  coworkers.  Ages 
ranged  from  the  early-twenties  to  mid-fifties  and  educa-
tion  varied  from  the  undergraduate  to  the  doctoral  level, 
spanning backgrounds in computer science, design, social 
science, and psychology. Concerned that our lab’s focus in 
collaborative software might bias results, we replicated the 
lab  study  in  a  university  environment  with  additional  12 
subjects (5 females, 7 males). Subject variation in age, edu-
cation, and social familiarity remained similar.

Subjects  conducted  a  25 min  usage  session  of  the  
sense.us system. A single visualization was available in the 
study: a stacked time series of the U.S. labor force over time, 
divided by gender (Figure 1). Users could navigate the visu-
alization by typing in text queries (matched to job title pre-
fixes), filtering by gender, and setting the axis scale, either to 
total people count or percentage values.

This  data  set  was  chosen  for  several  reasons.  First,  job 
choice is a topic that most of our users should have no dif-
ficulty relating to. Second, like many other real-world data 
sets, there are data collection issues, including missing data 
and  unclear  or  antiquated  labels.  Third,  we  suspected  the 
data would be an interesting boundary case for annotations, 
as for many visualization views, text seemed sufficient when 
referencing spikes or valleys in the data.

After a brief tutorial of system features, participants were 
instructed to use the system however they liked—no specific 
tasks were given. However, users were told that if they felt at 
a loss for action, they could browse the data for trends they 
found interesting and share their findings. An observer was 
present taking notes and a think-aloud protocol was used. 
User actions were also logged by the software. Subjects were 
run  in  sequential  order,  such  that  later  participants  could 
view  the  contributions  of  previous  subjects  but  not  vice 
versa. The system was seeded with five comments, each with 
an observation of a particular data trend.

After the study, subjects completed a short exit question-
naire  about  their  experiences.  Participants  were  asked  to 
rate on a 5-point Likert scale to what degree (1) they enjoyed 
using the system, (2) they learned something interesting, (3) 
 others’ comments were helpful in exploring the data, and if 
they found annotations useful for (4) making their own com-
ments,  or  (5)  understanding  others’  comments.  Subjects 
were  also  asked  free  response  questions  about  what  they 
liked, disliked, and would change about the system.

5.2. live deployment
We also conducted a live deployment of the system on the 
IBM corporate intranet for 3 weeks. Any employee could log 
in to the system using their existing intranet account. Eight 
visualizations  were  available  in  the  system,  among  them 

92    communications of thE acm   |   January 2009  |   V ol. 52  |   no. 1

kappa  statistic.  The  lowest  pairwise  kappa  value  was  0.74, 
indicating a satisfactory inter-rater reliability.

Most  commentary  on  sense.us  involved  data  analysis. 
A  typical comment made note of an observed trend or outlier, 
often coupled with questions, explanatory hypotheses, or both. 
A  typical  reply  involved  discussing  hypotheses  or  answering 
questions. The results of coding the comments are shown in 
Figure 5. In total, 80.6% of comments involved an observation 
of visualized data, 35.5% provided an explanatory hypothesis, 
and 38.1% included a question about the data or a hypothesis. 
Most  questions  and  hypotheses  accompanied  an  observa-
tion (91.6% and 92.2%, respectively) and half the hypotheses 
were either phrased as or accompanied by a question (49.0%).
For example, participants in both lab studies discovered 
a  large  drop  in  bartenders  around  the  1930s  and  posted 
comments  attributing  the  drop  to  alcohol  prohibition.  In 
the live deployment, one user commented on a scatterplot 
view,  asking  why  New  Hampshire  has  such  a  high  level  of 
retail  sales  per  capita  (Figure  4).  Another  user  noted  that 
New  Hampshire  does  not  have  sales  tax,  and  neither  does 
Delaware, the second highest in retail sales. In this fashion, 
discussion regularly involved the introduction of contextual 
information not present in the visualization. For instance, 
Figure  1  includes  a  timeline  of  events  that  was  iteratively 
constructed by multiple users, while the graph of teachers in 
Figure 6 notes the introduction of compulsory education.

One  instance  of  social  data  analysis  occurred  around  a 
rise, fall, and slight resurgence in the percentage of dentists 
in the labor force. The first comment (one of the five seed 
comments)  noted  the  trends  and  asked  what  was  happen-
ing.  One  subject  responded  in  a  separate  thread,  “Maybe 
this has to do with fluoridation? But there’s a bump . . . but kids 
got spoiled and had a lot of candy??” To this another subject 
responded  “As  preventative dentistry  has  become  more  effec-
tive, dentists have continued to look for ways to continue working 
(e.g., most people see the dentist twice a year now v. once a year 
just a few decades ago).” Perhaps the most telling comment, 
however, included a link to a different view, showing both 
dentists and dental technicians. As dentists had declined in 
percentage, technicians had grown substantially, indicating 
specialization within the field. To this, another user asked 
“I wonder if school has become too expensive for people to think 
about dentistry, or at least their own practice when they can go 
to  technical  school  for  less?”  Visual  data  analysis,  historical 
knowledge,  and  personal  anecdote  all  played  a  role  in  the 
sensemaking  process,  explicating  various  factors  shaping 
the data.

Another  role  of  comments  was  to  aid  data  interpreta-
tion, especially in cases of unclear meaning or anomalies in 
data collection. Overall, 15.7% of comments referenced data 
naming,  categorization,  or  collection  issues.  One  promi-
nent  occupation  was  labeled  “Operative,”  a  general  cate-
gory consisting largely of skilled labor. This term had little 
meaning to subjects, one of whom asked “what the hell is an 
operative?” Others responded to reinforce the question or to 
suggest an explanation, e.g., “I bet they mean factory worker.” 
Another subject agreed, noting that the years of the rise and 
fall  of  operatives  seemed  consistent  with  factory  workers. 
Other examples include views missing data for a single year 

figure 5: content analysis categorization of sense.us comments. 
categories are not mutually exclusive.

 

38.1%

35.5%

80.6%

Observation
Question
Hypothesis
Data Integrity
Linking
Socializing
System Design
Testing
Tips
To-do
Affirmation

15.7%
14.2%

9.0%
9.0%

5.6%
4.1%
2.6%
1.5%

0%

20%

40%

60%

80%

100%

figure 6: Visualization of the number of teachers. annotations 
 indicate the start of compulsory education and the rise of teachers 
in the post-World War ii era.

(1940 was a common culprit), leading users to comment on 
the probable case of missing data.

Some  users  were  less  interested  in  specific  views  than 
in recurring patterns. One user was interested in exploring 
careers  that  were  historically  male-dominated,  but  have 
seen increasing numbers of females in the last half-century. 
The user systematically explored the data, saving views in a 
bookmark trail later shared in a comment named “Women’s 
Rise.” Similarly, a more mathematically minded participant 
was  interested  in  patterns  of  job  fluctuations,  creating  a 
trail showcasing recurring distributions. Another searched 
for jobs that had been usurped by technology, such as bank 
tellers and telephone operators. In each of these cases, the 
result was a tour or story winding through multiple views.

Overall,  14.2%  of  comments  referenced  an  additional 
view, either implicitly in the text or explicitly through drag-
and-drop bookmark links. Although 22 of the 24 lab study 
subjects  (87.5%)  saved  at  least  one  view  to  the  bookmark 
trail,  only  14  (58.3%)  created  one  or  more  drag-and-drop 
bookmark links. The amount of view linking varied by user, 
ranging from 0 to 19 links with an average of 2.17.

Comments served other purposes as well. A number were 
simple tests of system functionality (5.6%), often deleted by 

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     93

research highlights 

the user. Some included tips for using the system (4.1%), not-
ing how to take advantage of specific features. Overall, 9.0% 
of comments referenced the site design, either in the form 
of usage tips or feature requests. A few comments included 
to-dos for future work (2.6%), such as later adding a link to 
a relevant wikipedia article. Others served solely as affirma-
tions to another comment (1.5%). For example, people stat-
ing “I agree with that” to support a hypothesis. In many cases, 
study participants would note out loud “that is interesting!” 
without posting a comment to the system.

Finally,  some  comments  were  social  in  nature  (9.0%). 
Most pointed out trends in the data, but did so in a joking 
manner.  One  user  built  a  view  comparing  female  lawyers 
and  bartenders,  writing  “Women  at  the  bar  and  behind  the 
bar.” In the pilot study, one of our lab members annotated a 
drop in stock brokers after 1930 with a picture of a person’s 
trajectory  off  a  skyscraper  (Figure  7).  This  elicited  smiles 
and laughter from subjects in the subsequent study, one of 
whom replied with an affirmation simply saying “Whoa!”

We  also  analyzed  the  structural  aspect  of  comments. 
Excluding  comments  from  the  pilot  study,  deleted  test 
comments,  and  those  written  by  the  paper  authors,  195 
 comments were collected. Of those, 140 (71.8%) started new 
discussion  threads  while  55  (28.2%)  were  replies  to  exist-
ing threads. The average thread length was 1.35 comments 
(SD 0.82), with a maximum of 5 comments. In some cases, 
discussion spanned multiple threads.

5.5. Graphical annotation
Next, we wanted to understand how graphical annotations 
were used and to what degree they contributed to social data 
analysis.  Of  the  195  nonpilot,  nondeleted  comments,  68 
(35.9%) included annotations. The vast majority (88.6%) of 
annotations involved pointing to items or trends of interest. 
The others (11.4%) involved more playful expression, such as 
drawn smiley faces and the visual commentary of Figure 7.

Across  these  annotations,  a  total  of  179  “shapes”  were 
drawn, with the options being free-form ink, lines, arrows, 

figure 7: annotated view of stock brokers. the attached comment 
reads “Great depression ‘killed’ a lot of brokers.”

94    communications of thE acm   |   January 2009  |   V ol. 52  |   no. 1

 

rectangles,  ovals,  and  text.  Arrows  were  the  most  popular 
shape  (25.1%  of  shapes),  and  were  used  to  point  to  items 
as well as to situate information provided by text captions 
(24.6%). Ovals (17.9%) were primarily used to enclose regions 
of interest. Free-form ink drawn with the pencil tool (16.2%) 
was used for pointing, enclosing irregularly shaped regions, 
and free-form drawing. Of the rest, lines made up 14.5% of 
all shapes and rectangles only 1.7% (Figure 8).

A few users, particularly those with experience in graphic 
design, noted that graphical annotations were their favorite 
feature.  Other  users  noted  that  the  annotations  were  often 
unnecessary  for  comments  where  text  could  describe  the 
trend(s) of interest. A few of these users added annotations to 
such views anyway, saying the annotations were “surprisingly 
satisfying,” enabling “personal expression.” Exit survey results 
somewhat reflected these views, as users ranked annotations 
more useful for writing their own comments (M = 3.5/5.0, SD 
= 0.85) than understanding others’ comments (M = 3.2/5.0, 
SD = 0.90). This difference, however, did not reach statistical 
significance (t(23) = −1.67, p < 0.108, two-tailed).

5.6. Visitation and navigation
Our next questions concerned how users navigated the visual-
izations. Most users began exploring the data directly, starting 
from  the  default  overview  and  drilling  down.  A  few  imme-
diately went to the comments listing to see what others had 
done. Many participants searched for their own occupations 
and  those  of  friends  and  family.  Other  strategies  included 
browsing for items of interest found in the overview (“Wow, 
look how the poor farmers died out”) and formulating queries 
based on an over-arching interest, such as gender balance.

Looking to the usage logs, navigation by interaction with 
the  visualization  or  attached  commentary  was  by  far  the 
most common navigation technique, accounting for 70.5% 
of state views. The second most popular was the back and 
forward buttons at 17.5%, validating our integration of the 
visualization  with  browser  history  mechanisms.  Following 
a link from the comment listings accounted for 8.7% of all 
views, while the final 3.3% were due to clicking a bookmark 
in the bookmark trail (Figure 9).

figure 8: usage of sense.us graphical annotation tools.

Arrows
Text
Ovals
Pencil
Lines
Rectangles

1.7%

25.1%
24.6%

17.9%

16.2%

14.5%

0%

5%

10%

15%

20%

25%

figure 9: usage of sense.us navigation mechanisms.

Visualization
Back/Forward
Comment Listings
Bookmark Trail

17.5%

8.7%

3.3%

70.5%

0%

10%

20%

30%

40%

50%

60%

70%

80%

At some point, every subject explored the comment list-
ings.  Some  felt  they  would  find  interesting  views  more 
quickly.  Remarks  to  this  effect  included  “I  bet  others  have 
found even more interesting things” and “You get to stand on 
the  shoulders  of  others.”  Other  subjects  were  interested  in 
specific people they knew or discovering what other people 
had investigated. Said one participant, “I feel like a data voy-
eur. I really like seeing what other people were searching for.” 
Switching between data-driven exploration and social navi-
gation was common. Views discovered via comment listings 
often sparked new interests and catalyzed more data-driven 
exploration. After some exploration, participants routinely 
returned to the listings for more inspiration. In the survey, 
the  question  “Did  you  find  other  people’s  comments  use-
ful for exploring the data?” received the highest marks (M = 
4.46/5.0, SD = 0.63).

5.7. Doubly linked discussions
We  also  wanted  to  investigate  participant  reaction  to  the 
doubly linked model of comments. All users understood the 
model  readily  and  no  problems  were  reported  when  users 
wanted to comment on a specific view. The model became 
more problematic when users wanted to comment on mul-
tiple views. In this case, the user had to choose one view as 
primary,  comment  on  that,  and  then  reference  the  other 
views, either indirectly in the text or by creating a link from 
the bookmark trail. Some users expressed the opinion that 
creating links was a workable solution, while others wanted 
to  be  able  to  simultaneously  compare  multiple  views  for 
purposes of both analysis and commentary. One important 
aspect of doubly linked discussions is the problem of deter-
mining identical views, despite potentially differing visual-
ization parameters. In this respect, we found our indexing 
scheme improved the odds of discovering existing commen-
tary while navigating the visualization. Across both lab stud-
ies, 28.2% of all unique visits to a visualization state were to 
a view that had been reached through two or more differing 
parameter settings. Without the view indexing, there would 
be a much higher potential for “cross talk,” where users post 
comments concerning similar observations on related views, 
unaware of each other. Nonetheless, cross talk was observed 
in a total of six cases, typically when both normalized and 
absolute axis scales led to similar views. In two cases, par-
ticipants added linking comments that bridged the related 
discussions.

5.8. user experience
Overall, users found using sense.us both enjoyable and infor-
mative. In the exit survey, the question “Did you enjoy using 
the system?” received a mean rating of 4.0/5.0 (SD = 0.52). 
The  question  “Did  you  learn  something  interesting  using 
the system?” received a mean rating of 4.2/5.0 (SD = 0.65). 
Users also provided usability remarks and suggested addi-
tional collaboration features. The next section addresses a 
number of these requests (Figure 10).

6. Discussion
The usage we observed echoed some of the earlier findings 
about social data analysis.27 In particular, we saw cascading 

 

conversation threads in which users asked questions, stated 
hypotheses, and proposed explanations, all in a social con-
text.  A  significant  number  of  comments  were  playful  or 
joking,  as  were  a  few  graphical  annotations.  It  has  been 
hypothesized that one of the spurs to social data analysis is 
a situation in which each user brought a unique perspective 
to bear.27 In the case of job data, this unique perspective was 
the set of professions of friends and family of the user. We 
did indeed see people exploring in this fashion, covering a 
broad set of the data.

On the other hand, we observed a somewhat more busi-
nesslike tone to analysis than was seen previously. This was 
likely in part due to the corporate and laboratory settings of 
use. The presence of an observer in the lab studies undoubt-
edly  also  influenced  results,  though  many  users  reported 
they had fun conducting social data analysis.

Further  research  is  clearly  needed  to  understand  the 
broad  principles  of  analytical  behavior  in  the  context  of 
visualizations. Since the original publication of this article, 
some  of  that  research  has  occurred.  In  the  next  sections, 
as we describe research directions suggested by reactions 
to  sense.us,  we  also  provide  brief  notes  on  how  recent 
work  has  shed  light  on  issues  of  collaboration  around 
visualizations.

6.1. mechanisms for social data analysis
The doubly linked discussion model was probably the most 
effective  and  well-liked  novel  feature  of  sense.us.  If  there 
was any frustration with this feature, it was that users had to 
navigate to a precise location to see related comments. This 
shortcoming,  coupled  with  the  high  rate  of  within-applet 
navigation (Figure 9), raises an intriguing question for future 
research: would it be helpful to embed social navigation cues 
in the visualization or interface widgets themselves?

For  example,  a  dynamic  query  widget  used  to  filter  the 
visualization might include visual cues of how many people 
have visited or commented on the views reachable using the 
widget, providing information scent by which the user can 
purposefully  navigate  toward  either  popular  or  unpopular 
views.  Such  widgets  could  aid  the  discovery  of  interesting 
trends  that  simply  had  not  yet  been  seen.  In  our  context, 
one might imagine a slider—controlling a view parameter—
with marks indicating the presence of comments at specific 
parameter  values.  Similar  techniques  can  be  devised  for 
other interface widgets. A recent system for such “scented 
widgets”28  provides  evidence  that  such  cues  can  result  in 
increased revisitation to popular views while also directing 
users’ attention to under-explored data regions.

figure 10: Results of poststudy survey. mean values are shown,  
error bars indicate standard deviation.

1 Enjoyed using system
2 Learned interesting things
3 Other’s comments useful
4 Own annotations useful
5 Other’s annotations useful

0

1

2

3

4

5

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     95

research highlights 

A second approach, suggested by many users, would be 
to show commentary related, though not directly attached 
to,  the  current  view.  Requested  features  include  show-
ing comments from other views that contain links to the 
current  view  (“trackbacks”),  and  related  commentary  on 
“nearby”  or  “similar”  views.  The  latter  could  help  allevi-
ate cross talk. Along these lines, there are appealing pos-
sibilities  for  generalizing  the  notion  of  view  indexing, 
for  example,  suggesting  conversations  on  views  deemed 
semantically  similar  to  the  current  view.  This  would 
require  an  index  of  visualization  state  providing  not  just 
equality  comparisons,  but  distance  measures.  Such  a 
retrieval model might be used to provide additional ben-
efits,  such  as  general  searchability  and  data-aware  auto-
complete mechanisms.

Users  have  also  suggested  using  visitation  data  or 
explicit  ratings  of  “interestingness”  to  suggest  views  of 
potential  interest.  Others  suggested  supporting  keyword 
tagging of comments22 and mining usage data. For exam-
ple,  both  manual  and  automated  tagging  of  questions  or 
other action items could be used to help direct collabora-
tive effort.

The  scope  of  comment  visibility  is  a  larger  issue  that 
affects  all  discussion  models.  What  happens  when  the 
amount  of  discussion  becomes  untenably  large,  or  users 
don’t want their activity exposed to everyone? The ability to 
form  groups  and  limit  comment  visibility  to  group  mem-
bers  is  one  means  requested  by  users  to  support  privacy 
and  make  discussion-following  both  more  relevant  and 
tractable.

Although  individual  usage  varied  substantially,  most 
lab study users (87.5%) did use the bookmark trails, which 
proved  essential  for  comments  that  included  multiple 
views.  Multiple  users  remarked  on  the  usefulness  of  the 
bookmark trails and wanted to more easily share trails as 
first  class  objects.  At  times,  users  were  frustrated  when 
following  multiple  links  in  a  comment,  as  the  original 
comment  would  disappear  when  a  new  view  was  loaded, 
requiring  use  of  the  back  button  to  perform  “hub-and-
spoke”  browsing.  In  response,  users  suggested  adding 
a  dedicated  “presentation”  mode  to  facilitate  tours  and 
storytelling.

Finally, the graphical annotations saw significant usage, 
despite mixed reactions from users. Though they were used 
for pointing, many users did not find them necessary for dis-
ambiguation. We expect that the value of annotations varies 
significantly  depending  on  the  type  of  visualization  being 
referenced. Regardless, annotations were used regularly for 
pointing and sometimes for socializing.

If the free-form annotations prove helpful, a second chal-
lenge would be to extend them to cover dynamic or evolving 
data  sets.  The  decoupled  nature  of  geometric  annotations 
can  prove  problematic  when  the  underlying  data  changes. 
Similar  problems  have  been  investigated  in  the  context  of 
document  annotation.5  More  recent  work19  has  explored 
“data-aware”  annotations  that  translate  user  selections 
into declarative queries over the underlying data, allowing 
annotations to be applied to time-varying data and different 
visual encodings.

96    communications of thE acm   |   January 2009  |   V ol. 52  |   no. 1

 

6.2. communities and data
Since  the  original  sense.us  experiment,  there  have  been 
several new examples of systems that support conversation 
around  data.  Web  sites  such  as  Swivel.com  have  provided 
social-network-style  platforms  for  conversation  around 
data,  along  with  basic  charting  capabilities.  Tableau 
Software launched its Tableau Server product, which (much 
like Spotfire’s DecisionSite Posters) allows users to collabo-
rate asynchronously around intranet-based visualizations. 
Little  has  been  published  about  usage  of  these  systems, 
however.

One new system where results have been reported is the 
Many Eyes Web site.26 In contrast to sense.us, or tools like 
Tableau or Spotfire, Many Eyes is freely available on the pub-
lic internet and allows users to upload their own data. Unlike 
data-oriented  sites  like  Swivel,  Many  Eyes  lets  users  apply 
more than a dozen interactive visualization techniques. They 
may  then  have  discussions  around  visualizations,  though 
annotation capabilities are more basic than in sense.us. The 
experiences on the site26 lend support to the idea that visu-
alization  can  catalyze  discussion.  While  these  discussions 
can be analytical, they also can be purely social, partisan, or 
game-like. In addition, the move from a closed setting to the 
public internet has made clear that these discussions can be 
highly distributed,13 with a significant proportion of collabo-
ration occurring (via hyperlinks) off the site. Designing for 
this type of multisite conversation suggests a whole new set 
of challenges.

7. conclusion
In  this  article,  we  investigated  mechanisms  supporting 
asynchronous collaboration around interactive information 
visualization, seeking to more tightly tie the perceptual and 
cognitive benefits of visualization to social processes of sen-
semaking.  To  do  so,  we  implemented  a  collaborative  data 
visualization site, sense.us. We then observed usage of the 
site in order to better understand the social dynamics sur-
rounding collective use of visualizations as well as the effi-
cacy of the particular features.

The  features  of  the  site—doubly  linked  discussions, 
bookmark  trails,  geometric  annotations,  and  comment 
listings—were  all  exploited  by  users.  The  doubly  linked 
discussions  successfully  enabled  users  to  fluidly  transfer 
attention  between  visualization  and  commentary  and  we 
suggested  ways  to  further  improve  this  type  of  discussion. 
Bookmark trails and geometric annotations were also well 
used, enabling tours through multiple views and pointing to 
items of interest, respectively. Finally, users played the roles 
of both voyager and voyeur, alternating between data-driven 
exploration directly within the visualization and social navi-
gation  through  comment  listings  and  user  profiles  to  dis-
cover new views of interest.

Overall, we believe these results show the value of focus-
ing  on  the  social  aspects  of  visual  analysis.  Our  user  stud-
ies  indicate  that  combining  conversation  and  visual  data 
analysis can help people explore a data set both broadly and 
deeply.  From  a  design  perspective,  there  lies  a  promising 
opportunity for exploring new widgets and modes of inter-
action aimed at enhancing collaboration.

References

  1.  anupam, V., bajaj, c.l., schikore, D., 
and shikore, m. representations in 
distributed cognitive tasks. IEEE 
Computer 27, 7 (1994), 37–43.
  2.  benbunan-fich, r., hiltz, s.r., 
and turoff, m. a comparative 
content analysis of face-to-face vs. 
asynchronous group decision making. 
Decision Support Systems 34, 4 
(2003), 457–469.

  3.  benko, h., ishak, e.w., and feiner, s.  

collaborative mixed reality 
visualization of an archaeological 
excavation. in IEEE International 
Symposium on Mixed and Augmented 
Reality (ISMAR 2004) (arlington, Va, 
2004), 132–140.

  4.  brodlie, K.w., Duce, D.a., gallop, J.r.,  

walton, J.P.r.b., and wood, J.D. 
Distributed and collaborative 
visualization. Computer Graphics 
Forum 23, 2 (2004), 223–251.

  5.  brush, a., bargeron, D., gupta, a., and 
cadiz, J. robust annotation positioning 
in digital documents. in Proceedings 
of ACM Human Factors in Computing 
Systems (CHI 2001), 2001.

  6.  card, s.K., mackinlay, J.D., and 

shneiderman, b. Readings in 
Information Visualization: Using Vision 
to Think. morgan-Kaufmann, 1999.

Journal of Computer-Supported 
Cooperative Work 13, 3 (2004), 
305–327.

  8.  chuah, m.c. and roth, s.f. Visualizing 

common ground. in Proceedings of the 
Conference on Information Visualization 
(IV) (los alamitos, usa, 2003), ieee 
computer society, 365–372.

  9.  chui, y.-P. and heng, P.-a. enhancing 

view consistency in collaborative 
medical visualization systems using 
predictive-based attitude estimation. 
in First IEEE International Workshop 
on Medical Imaging and Augmented 
Reality (MIAR’01) (hong Kong, 
china), 2001.

 10.  churchill, e.f., trevor, J., bly, s., 

nelson, l., and cubranic, D. anchored 
conversations: chatting in the context of 
a document. in ACM CHI 2000, 2000.

 11.  clark, h.h. Pointing and placing. in 
Pointing. Where Language, Culture, 
and Cognition Meet. s. Kita, ed. 
erlbaum, 2003, 243–268.

 12.  clark, h.h. and brennan, s.e. 

grounding in communication. in 
Perspectives on Socially Shared 
Cognition (1991), american 
Psychological association, 127–149.

 13.  Danis, c.m., Viégas, f.b., 

website. accessed: november, 2007.
 17.  grudin, J. groupware and social 
dynamics: eight challenges for 
developers. Communications of the 
ACM 37, 1 (1994), 92–105.

 18.  heer, J. socializing visualization. in 
ACM CHI 2006 Workshop on Social 
Visualization, 2006.

 19.  heer, J., agrawala, m., and willett, w. 

generalized selection via interactive query 
relaxation. in ACM CHI 2008, 959–969.
 20.  hill, w.c. and hollan, J.D. Deixis and 
the future of visualization excellence. 
in Proceedings of IEEE Visualization 
(1991), 314–319.

 21.  Johansen, r. Groupware: Computer 

Support for Business Teams. the free 
Press, new york, 1988.

 22.  millen, D.r., feinberg, J., and Keer, b. 

Dogear: social bookmarking in the 
enterprise. in Proceedings of ACM 
CHI 2006, 111–120.

 23.  thomas, J. and cook, K. Illuminating the 

 
Jeffrey Heer (jheer@stanford.edu), 
computer science Department, stanford 
university, stanford, ca.

communication-minded visualization: 
a call to action. IBM Systems Journal 
45, 4 (2006), 801–812.

 26.  Viégas, f.b., wattenberg, m., van ham, 

f., Kriss, J., and mcKeon,  
m. many eyes: a site for visualization 
at internet scale. IEEE Transactions 
on Visualization and Computer 
Graphics 12, 5 (nov./Dec. 2007), 
1121–1128.

 27.  wattenberg, m. and Kriss, J. 

Designing for social data analysis. 
IEEE Transactions on Visualization 
and Computer Graphics 12, 4 
(July–august 2006), 549–557.

 28.  willett, w., heer, J., and agrawala, m. 

scented widgets: improving 
navigation cues with embedded 
visualizations. IEEE Transactions on 
Visualization and Computer Graphics 
13, 6 (nov./Dec. 2007), 1129–1136.

Martin Wattenberg (mwatten@us.ibm.
com), Visual communication lab,  
ibm t.J. watson research center, 
cambridge, ma.

© acm 0001-0782/09/0001 $5.00

acknowledgments
We  thank  Jesse  Kriss,  Frank  van  Ham,  Doug  Fritz,  Kate 
Hollenbach,  Steve  Whittaker,  David  Millen,  and  Maneesh 
Agrawala  for  insightful  discussions.  We  also  would  like  to 
thank all the study participants and users of our system. 

 

(CHI 2008), 2008.

 14.  Dietz, P.h. and leigh, D.l. 

Diamondtouch: a multi-user touch 
technology. in Proceedings of ACM 
Symposium on User Interface Software 
and Technology, 2001, 219–226.

 15.  Dourish, P. and chalmers, m. running 

out of space: models of information 
navigation. in Human Computer 
Interaction (HCI’94), 1994.

Path: The Research and Development 
Agenda for Visual Analytics. ieee 
computer society, 2005.

 24.  Viégas, f.b., danah boyd, nguyen, D.h., 

Potter, J., and Donath, J. Digital artifacts 
for remembering and storytelling: 
Posthistory and social network 
fragments. in Proceedings of Hawaii 
International Conference on System 
Sciences (HICCSS) (2004), 105–111.

 16.  Dynamics, g. command post of the future. 

 25.  Viégas, f.b. and wattenberg, m. 

CACM lifetime mem half page ad:Layout 1  9/4/08  4:04 PM  Page 1
  7.  carter, s., mankoff, J., and goddi, P. 
building connections among loosely 
coupled groups: hebb’s rule at work. 

wattenberg, m., and Kriss, J. your place 
or mine? Visualization as a community 
component. in Proceedings of ACM 
Human Factors in Computing Systems 

Fernanda B. Viégas (viegasf@us.ibm.
com),Visual communication lab  
ibm t.J. watson research center, 
cambridge, ma.

Take Advantage of 
ACM’s Lifetime Membership Plan!

◆ ACM Professional Members can enjoy the convenience of making a single payment for their

entire tenure as an ACM Member, and also be protected from future price increases by
taking advantage of ACM's Lifetime Membership option.

◆ ACM Lifetime Membership dues may be tax deductible under certain circumstances, so

becoming a Lifetime Member can have additional advantages if you act before the end of
2008. (Please consult with your tax advisor.)

◆ Lifetime Members receive a certiﬁcate of recognition suitable for framing, and enjoy all of

the beneﬁts of ACM Professional Membership.

Learn more and apply at:

http://www.acm.org/life

January 2009  |   V ol. 52  |   no. 1  |   communications of thE acm     97

",False,2009.0,{},False,False,journalArticle,False,VA72NXDN,[],self.user,False,False,False,False,http://portal.acm.org/citation.cfm?doid=1435417.1435439,,Voyagers and voyeurs: Supporting asynchronous collaborative visualization,VA72NXDN,False,False
7DLBYPHS,QF8T8QRU,"Visual Analytics: Definition, Process and Challenges
Daniel Keim, Gennady Andrienko, Jean-Daniel Fekete, Carsten Görg, Jörn

Kohlhammer, Guy Melançon

To cite this version:
Daniel Keim, Gennady Andrienko, Jean-Daniel Fekete, Carsten Görg, Jörn Kohlhammer, et al.. Vi-
sual Analytics: Definition, Process and Challenges. Andreas Kerren and John T. Stasko and Jean-
Daniel Fekete and Chris North. Information Visualization - Human-Centered Issues and Perspectives,
Springer, pp.154-175, 2008, LNCS. <lirmm-00272779>

HAL Id: lirmm-00272779

https://hal-lirmm.ccsd.cnrs.fr/lirmm-00272779

Submitted on 11 Apr 2008

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Visual Analytics:

Deﬁnition, Process, and Challenges

Daniel Keim1, Gennady Andrienko2, Jean-Daniel Fekete3, Carsten G¨org4,

J¨orn Kohlhammer5, and Guy Melan¸con6

1 Department of Computer and Information Science, University of Konstanz,

78457 Konstanz, Germany,

2 Fraunhofer Institute for Intelligent Analysis and Information Systems(IAIS),

keim@informatik.uni-konstanz.de

Schloss Birlinghoven 53754 Sankt Augustin, Germany,

gennady.andrienko@iais.fraunhofer.de
3 Universit´e Paris-Sud, INRIA, Bˆat 490,

4 School of Interactive Computing & GVU Center, Georgia Institute of Technology,

F-91405 Orsay Cedex, France,
Jean-Daniel.Fekete@inria.fr

85 5th St., NW, Atlanta, GA 30332-0760, USA,

5 Fraunhofer Institute for Computer Graphics Research,

goerg@cc.gatech.edu

Fraunhoferstraße 5, D-64283 Darmstadt, Germany,

joern.kohlhammer@igd.fraunhofer.de

6 INRIA Bordeaux – Sud-Ouest, CNRS UMR 5800 LaBRI,

Campus Universit´e Bordeaux I,

351 Cours de la lib´eration, 33405 Talence Cedex, France,

Guy.Melancon@labri.fr

1 Introduction and Motivation

We are living in a world which faces a rapidly increasing amount of data to be
dealt with on a daily basis. In the last decade, the steady improvement of data
storage devices and means to create and collect data along the way inﬂuenced our
way of dealing with information: Most of the time, data is stored without ﬁltering
and reﬁnement for later use. Virtually every branch of industry or business,
and any political or personal activity nowadays generate vast amounts of data.
Making matters worse, the possibilities to collect and store data increase at a
faster rate than our ability to use it for making decisions. However, in most
applications, raw data has no value in itself; instead we want to extract the
information contained in it.

The information overload problem refers to the danger of getting lost in

data which may be

– irrelevant to the current task at hand
– processed in an inappropriate way
– presented in an inappropriate way

A. Kerren et al. (Eds.): Information Visualization, LNCS 4950, pp. 154–175, 2008.
c(cid:1) Springer-Verlag Berlin Heidelberg 2008

Visual Analytics: Deﬁnition, Process, and Challenges

155

Due to information overload, time and money are wasted, scientiﬁc and in-
dustrial opportunities are lost because we still lack the ability to deal with the
enormous data volumes properly. People in both their business and private lives,
decision-makers, analysts, engineers, emergency response teams alike, are often
confronted with massive amounts of disparate, conﬂicting and dynamic infor-
mation, which are available from multiple heterogeneous sources. We want to
simply and eﬀectively exploit and use the hidden opportunities and knowledge
resting in unexplored data sources.

In many application areas success depends on the right information being
available at the right time. Nowadays, the acquisition of raw data is no longer
the driving problem: It is the ability to identify methods and models, which can
turn the data into reliable and provable knowledge. Any technology, that claims
to overcome the information overload problem, has to provide answers for the
following problems:

– Who or what deﬁnes the “relevance of information” for a given task?
– How can appropriate procedures in a complex decision making process be

identiﬁed?

way?

ing?

– How can the resulting information be presented in a decision- or task-oriented

– What kinds of interaction can facilitate problem solving and decision mak-

With every new “real-life” application, procedures are put to the test possibly
under circumstances completely diﬀerent from the ones under which they have
been established. The awareness of the problem how to understand and analyse
our data has been greatly increased in the last decade. Even as we implement
more powerful tools for automated data analysis, we still face the problem of un-
derstanding and “analysing our analyses” in the future: Fully-automated search,
ﬁlter and analysis only work reliably for well-deﬁned and well-understood prob-
lems. The path from data to decision is typically quite complex. Even as fully-
automated data processing methods represent the knowledge of their creators,
they lack the ability to communicate their knowledge. This ability is crucial: If
decisions that emerge from the results of these methods turn out to be wrong,
it is especially important to examine the procedures.

The overarching driving vision of visual analytics is to turn the information
overload into an opportunity: Just as information visualization has changed our
view on databases, the goal of Visual Analytics is to make our way of processing
data and information transparent for an analytic discourse. The visualization of
these processes will provide the means of communicating about them, instead
of being left with the results. Visual Analytics will foster the constructive eval-
uation, correction and rapid improvement of our processes and models and -
ultimately - the improvement of our knowledge and our decisions (see Figure 1).
On a grand scale, visual analytics solutions provide technology that combines
the strengths of human and electronic data processing. Visualization becomes
the medium of a semi-automated analytical process, where humans and machines
cooperate using their respective distinct capabilities for the most eﬀective results.

156

D. Keim et al.

Fig. 1. Tight integration of visual and automatic data analysis methods with database
technology for a scalable interactive decision support.

The user has to be the ultimate authority in giving the direction of the analysis
along his or her speciﬁc task. At the same time, the system has to provide
eﬀective means of interaction to concentrate on this speciﬁc task. On top of
that, in many applications diﬀerent people work along the path from data to
decision. A visual representation will sketch this path and provide a reference
for their collaboration across diﬀerent tasks and abstraction levels.

The diversity of these tasks can not be tackled with a single theory. Visual
analytics research is highly interdisciplinary and combines various related re-
search areas such as visualization, data mining, data management, data fusion,
statistics and cognition science (among others). Visualization has to continuously
challenge the perception by many of the applying sciences that visualization is
not a scientiﬁc discipline in its own right. Even if the awareness exists, that
scientiﬁc analysis and results must be visualized in one way or the other, this
often results in ad hoc solutions by application scientists, which rarely match
the state of the art in interactive visualization science, much less the full com-
plexity of the problems. In fact, all related research areas in the context of visual
analytics research conduct rigorous, serious science each in a vibrant research
community. To increase the awareness of their work and their implications for
visual analytics research clearly emerges as one main goal of the international
visual analytics community (see Figure 2).

Because visual analytics research can be regarded as an integrating discipline,
application speciﬁc research areas should contribute with their existing proce-
dures and models. Emerging from highly application-oriented research, dispersed
research communities worked on speciﬁc solutions using the repertoire and stan-
dards of their speciﬁc ﬁelds. The requirements of visual analytics introduce new
dependencies between these ﬁelds.

Visual Analytics: Deﬁnition, Process, and Challenges

157

Fig. 2. Visual analytics integrates scientiﬁc disciplines to improve the division of labor
between human and machine.

2 Deﬁnition of Visual Analytics

In “Illuminating the Path” [39], Thomas and Cook deﬁne visual analytics as the
science of analytical reasoning facilitated by interactive visual interfaces. In this
chapter, however, we would like to give a more speciﬁc deﬁnition:

Visual analytics combines automated analysis techniques with interactive
visualizations for an eﬀective understanding, reasoning and decision making on
the basis of very large and complex data sets.

The goal of visual analytics is the creation of tools and techniques to enable

people to:

– Synthesize information and derive insight from massive, dynamic, ambigu-

ous, and often conﬂicting data.

– Detect the expected and discover the unexpected.
– Provide timely, defensible, and understandable assessments.
– Communicate assessment eﬀectively for action.

By integrating selected science and technology from the above discussed disci-
plines and as illustrated in Figure 2, there is the promising opportunity to form
the unique and productive ﬁeld of visual analytics. Work in each of the partici-
pating areas focuses on diﬀerent theoretical and practical aspects of users solving
real-world problems using Information Technology in an eﬀective and eﬃcient
way. These areas have in common similar scientiﬁc challenges and signiﬁcant sci-
entiﬁc added-value from establishing close collaboration can be identiﬁed. Beneﬁt
of collaboration between the ﬁelds is identiﬁed to be two-fold:

– Jointly tackling common problems will arrive at better results on the local

level of each discipline, in a more eﬃcient way.

– Integrating appropriate results from each of the disciplines will lay the fun-
dament for signiﬁcantly improved solutions in many important data analysis
applications.

158

D. Keim et al.

Visual Analytics versus Information Visualization

Many people are confused by the new term visual analytics and do not see a dif-
ference between the two areas. While there is certainly some overlay and some of
the information visualization work is certainly highly related to visual analytics,
traditional visualization work does not necessarily deal with an analysis tasks
nor does it always also use advanced data analysis algorithms.

Visual analytics is more than just visualization. It can rather be seen as an
integral approach to decision-making, combining visualization, human factors
and data analysis. The challenge is to identify the best automated algorithm for
the analysis task at hand, identify its limits which can not be further automated,
and then develop a tightly integrated solution with adequately integrates the best
automated analysis algorithms with appropriate visualization and interaction
techniques.

While some of such research has been done within the visualization commu-
nity in the past, the degree to which advanced knowledge discovery algorithms
have been employed is quite limited. The idea of visual analytics is to funda-
mentally change that. This will help to focus on the right part of the problem,
i.e. the parts that can not be solved automatically, and will provide solutions to
problems that we were not able to solve before.

One important remark should be made here. Most research eﬀorts in Infor-
mation Visualization have concentrated on the process of producing views and
creating valuable interaction techniques for a given class of data (social network,
multi-dimensional data, etc.). However, much less has been suggested as to how
user interactions on the data can be turned into intelligence to tune underlying
analytical processes. A system might for instance observe that most of the user’s
attention concern only a subpart of an ontology (through queries or by repeated
direct manipulations of the same graphical elements, for instance). Why not then
use this knowledge about the user’s interest and update various parameters by
the system (trying to systematically place elements or components of interest in
center view, even taking this fact into account when driving a clustering algo-
rithm with a modularity quality criteria, for instance).

This is one place where Visual Analytics maybe diﬀers most from Information
Visualization, giving higher priority to data analytics from the start and through
all iterations of the sense making loop. Creativity is then needed to understand
how perception issues can help bring more intelligence into the analytical process
by “learning” from users’ behavior and eﬀective use of the visualization.

3 Areas Related to Visual Analytics

Visual analytics builds on a variety of related scientiﬁc ﬁelds. At its heart, Visual
Analytics integrates Information and Scientiﬁc Visualization with Data Manage-
ment and Data Analysis Technology, as well as Human Perception and Cognition
research. For eﬀective research, Visual Analytics also requires an appropriate In-
frastructure in terms of software and data sets and related analytical problems
repositories, and to develop reliable Evaluation methodology (see Figure 3).

Visual Analytics: Deﬁnition, Process, and Challenges

159

Fig. 3. Visual Analytics integrates Scientiﬁc and Information Visualization with core
adjacent disciplines: Data management and analysis, spatio-temporal data, and human
perception and cognition. Successful Visual Analytics research also depends on the
availability of appropriate infrastructure and evaluation facilities.

An example for a common problem in several of the disciplines is that of scalabil-
ity with data size. The larger the data set to be handled gets, the more diﬃcult
it gets to manage, analyze, and visualize these data eﬀectively. Researching ap-
propriate forms to represent large data volumes by smaller volumes containing
the most relevant information beneﬁts each of the data management, analy-
sis, and visualization ﬁelds. On top of these individual progresses, a synergetic
collaboration of all these ﬁelds may lead to signiﬁcantly improved processing
results. Consider a very large data stream. Appropriate data management tech-
nology gives eﬃcient access to the stream, which is intelligently processed and
abstracted by an automatic analysis algorithm which has an interface to the
data management layer. On top of the analysis output, an interactive visual-
ization which is optimized for eﬃcient human perception of the relevant infor-
mation allows the analyst to consume the analysis results, and adapt relevant
parameters of the data aggregation an analysis engines as appropriate. The com-
bination of the individual data handling steps into a Visual Analytics pipeline
leads to improved results and makes data domains accessible which are not ef-
fectively accessible by any of the individual data handling disciplines. Similar
argumentations apply to other related ﬁelds and disciplines. In many ﬁelds, vi-
sualization is already used and developed independently as a means for analyzing
the problems at hand. However, a uniﬁed, interdisciplinary perspective on using
visualization for analytical problem-solving will show beneﬁcial for all involved
disciplines. As common principles, best practices, and theories will be developed,
these will become usable in the individual disciplines and application domains,
providing economies of scale, avoiding replication of work or application of only
sub-optimal techniques.

160

D. Keim et al.

3.1 Visualization

Visualization has emerged as a new research discipline during the last two dec-
ades. It can be broadly classiﬁed into Scientiﬁc and Information Visualization.
In Scientiﬁc Visualization, the data entities to be visualized are typically 3D
geometries or can be understood as scalar, vectorial, or tensorial ﬁelds with ex-
plicit references to time and space. A survey of current visualization techniques
can be found in [22,35,23]. Often, 3D scalar ﬁelds are visualized by isosurfaces or
semi-transparent point clouds (direct volume rendering) [15]. To this end, meth-
ods based on optical emission- or absorption models are used which visualize the
volume by ray-tracing or projection. Also, in the recent years signiﬁcant work
focused on the visualization of complex 3-dimensional ﬂow data relevant e.g.,
in aerospace engineering [40]. While current research has focused mainly on eﬃ-
ciency of the visualization techniques to enable interactive exploration, more and
more methods to automatically derive relevant visualization parameters come
into focus of research. Also, interaction techniques such as focus&context [28]
gain importance in scientiﬁc visualization.

Information Visualization during the last decade has developed methods
for the visualization of abstract data where no explicit spatial references are
given [38,8,24,41]. Typical examples include business data, demographics data,
network graphs and scientiﬁc data from e.g., molecular biology. The data con-
sidered often comprises hundreds of dimensions and does not have a natural
mapping to display space, and renders standard visualization techniques such as
(x, y) plots, line- and bar-charts ineﬀective. Therefore, novel visualization tech-
niques are being developed by employing e.g., Parallel Coordinates and their
numerous extensions [20], Treemaps [36], and Glyph [17]- and Pixel-based [25]
visual data representations. Data with inherent network structure may be visual-
ized using graph-based approaches. In many Visualization application areas, the
typically huge volumes of data require the appropriate usage of automatic data
analysis techniques such as clustering or classiﬁcation as preprocessing prior to
visualization. Research in this direction is just emerging.

3.2 Data Management

An eﬃcient management of data of various types and qualities is a key com-
ponent of Visual Analytics as this technology typically provides the input of
the data which are to be analyzed. Generally, a necessary precondition to per-
form any kind of data analysis is an integrated and consistent data basis [18,19].
Database research has until the last decade focused mainly on aspects of eﬃ-
ciency and scalability of exact queries on homogeneous, structured data. With
the advent of the Internet and the easy access it provides to all kinds of hetero-
geneous data sources, the database research focus has shifted toward integration
of heterogeneous data. Finding integrated representation of diﬀerent data types
such as numeric data, graphs, text, audio and video signals, semi-structured
data, semantic representations and so on is a key problem of modern database

Visual Analytics: Deﬁnition, Process, and Challenges

161

technology. But the availability of heterogeneous data not only requires the map-
ping of database schemata but includes also the cleaning and harmonization of
uncertainty and missing data in the volumes of heterogeneous data. Modern ap-
plications require such intelligent data fusion to be feasible in near real-time and
as automatically as possible [32]. New forms of information sources such as data
streams [11], sensor networks [30] or automatic extraction of information from
large document collections (e.g., text, HTML) result in a diﬃcult data analysis
problem which to support is currently in the focus of database research [43].
The relationship between Data Management, Data Analysis and Visualization
is characterized such that Data Management techniques developed increasingly
rely on intelligent data analysis techniques, and also interaction and visualiza-
tion to arrive at optimal results. On the other hand, modern database systems
provide the input data sources which are to be visually analyzed.

3.3 Data Analysis
Data Analysis (also known as Data Mining or Knowledge Discovery) researches
methods to automatically extract valuable information from raw data by means
of automatic analysis algorithms [29,16,31]. Approaches developed in this area
can be best described by the addressed analysis tasks. A prominent such task
is supervised learning from examples: Based on a set of training samples, deter-
ministic or probabilistic algorithms are used to learn models for the classiﬁcation
(or prediction) of previously unseen data samples [13]. A huge number of algo-
rithms have been developed to this end such as Decision Trees, Support Vector
Machines, Neuronal Networks, and so on. A second prominent analysis task is
that of cluster analysis [18,19], which aims to extract structure from data with-
out prior knowledge being available. Solutions in this class are employed to au-
tomatically group data instances into classes based on mutual similarity, and to
identify outliers in noisy data during data preprocessing for subsequent analysis
steps. Further data analysis tasks include tasks such as association rule mining
(analysis of co-occurrence of data items) and dimensionality reduction. While
data analysis initially was developed for structured data, recent research aims at
analyzing also semi-structured and complex data types such as web documents
or multimedia data [34].

It has recently been recognized that visualization and interaction are highly
beneﬁcial in arriving at optimal analysis results [9]. In almost all data analysis
algorithms a variety of parameters needs to be speciﬁed, a problem which is
usually not trivial and often needs supervision by a human expert. Visualization
is also a suitable means for appropriately communicating the results of the au-
tomatic analysis, which often is given in abstract representation, e.g., a decision
tree. Visual Data Mining methods [24] try to achieve exactly this.

3.4 Perception and Cognition
Eﬀective utilization of the powerful human perception system for visual analysis
tasks requires the careful design of appropriate human-computer interfaces. Psy-
chology, Sociology, Neurosciences and Design each contribute valuable results to

162

D. Keim et al.

the implementation of eﬀective visual information systems. Research in this area
focuses on user-centered analysis and modeling (Requirement Engineering), the
development of principles, methods and tools for design of perception-driven,
multimodal interaction techniques for visualization and exploration of large in-
formation spaces, as well as usability evaluation of such systems [21,12]. On the
technical side, research in this area is inﬂuenced by two main factors: (1.) The
availability of improved display resources (hardware), and (2.) Development of
novel interaction algorithms incorporating machine recognition of the actual user
intent and appropriate adaptation of main display parameters such as the level
of detail, data selection and aggregation, etc. by which the data is presented[44].
Important problems addressed in this area include the research of perceptual,
cognitive and graphical principles which in combination lead to improved visual
communication of data and analysis results; The development of perception-
theory-based solutions for the graphical representation of static and dynamic
structures; And development of visual representation of information at several
levels of abstraction, and optimization of existing focus-and-context techniques.

3.5 Human-Computer Interaction

Human-computer interaction is the research area that studies the interaction
between people and computers. It involves the design, implementation and eval-
uation of interactive systems in the context of the user’s task and work [12].
Like visual analytics itself, human-computer interaction is a multi-disciplinary
research area that draws on many other disciplines: computer science, system
design, and behavioral science are some of them. The basic underlying research
goal is to improve the interaction between users and computers: how to make
computers more receptive to the users’ intentions and needs. Thus, the research
areas discussed in the previous section about perception and cognition are also
much related to human-computer interaction [21].

As pointed out in the introduction, visual analytics aims to combine and
integrate the strengths of computers and humans into an interactive process to
extract knowledge from data. To eﬀectively switch back and forth between tasks
for the computer and tasks for the human it is crucial to develop an eﬀective
user interface that minimizes the barrier between the human’s cognitive model
of what they want to accomplish and the computer’s understanding of the hu-
man’s task. The design of user interfaces focuses on human factors of interactive
software, methods to develop and assess interfaces, interaction styles, and design
considerations such as eﬀective messages and appropriate color choice [37].

3.6 Infrastructure and Evaluation

The above described research disciplines require cross-discipline support regard-
ing the evaluation of the found solutions, and need certain infrastructure and
standardization grounding to build on eﬀectively. In the ﬁeld of information vi-
sualization, standardization and evaluation came into the focus of research only
recently. It has been realized that a general understanding of the taxonomies

Visual Analytics: Deﬁnition, Process, and Challenges

163

regarding the main data types and user tasks [2] to be supported are highly de-
sirable for shaping visual analytics research. A common understanding of data
and problem dimensions and structure, and acceptance of evaluation standards
will make research results better comparable, optimizing research productivity.
Also, there is an obvious need to build repositories of available analysis and vi-
sualization algorithms, which researchers can build upon in their work, without
having to re-implement already proven solutions.

How to assess the value of visualization is a topic of lively debate [42,33]. A
common ground that can be used to position and compare future developments
in the ﬁeld of data analysis is needed. The current diversiﬁcation and dispersion
of visual analytics research and development resulted from its focus onto speciﬁc
application areas. While this approach may suit the requirements of each of
these applications, a more rigorous and overall scientiﬁc perspective will lead to
a better understanding of the ﬁeld and a more eﬀective and eﬃcient development
of innovative methods and techniques.

3.7 Sub-communities

Spatio-Temporal Data: While many diﬀerent data types exist, one of the
most prominent and ubiquitous data types is data with references to time and
space. The importance of this data type has been recognized by a research
community which formed around spatio-temporal data management and anal-
ysis [14]. In geospatial data research, data with references in the real world
coming from e.g., geographic measurements, GPS position data, remote sensing
applications, and so on is considered. Finding spatial relationships and patterns
among this data is of special interest, requiring the development of appropriate
management, representation and analysis functions. E.g., developing eﬃcient
data structures or deﬁning distance and similarity functions is in the focus of re-
search. Visualization often plays a key role in the successful analysis of geospatial
data [6,26].

In temporal data, the data elements can be regarded as a function of time.
Important analysis tasks here include the identiﬁcation of patterns (either lin-
ear or periodical), trends and correlations of the data elements over time, and
application-dependent analysis functions and similarity metrics have been pro-
posed in ﬁelds such as ﬁnance, science, engineering, etc. Again, visualization of
time-related data is important to arrive at good analysis results [1].

The analysis of data with references both in space and in time is a chal-
lenging research topic. Major research challenges include [4]: scale, as it is often
necessary to consider spatio-temporal data at diﬀerent spatio-temporal scales;
the uncertainty of the data as data are often incomplete, interpolated, collected
at diﬀerent times, or based upon diﬀerent assumptions; complexity of geograph-
ical space and time, since in addition to metric properties of space and time
and topological/temporal relations between objects, it is necessary to take into
account the heterogeneity of the space and structure of time; and complexity of
spatial decision making processes, because a decision process may involve hetero-

164

D. Keim et al.

geneous actors with diﬀerent roles, interests, levels of knowledge of the problem
domain and the territory.

Network and Graph Data: Graphs appear as ﬂexible and powerful math-
ematical tools to model real-life situations. They naturally map to transporta-
tion networks, electric power grids, and they are also used as artifacts to study
complex data such as observed interactions between people, or induced interac-
tions between various biological entities. Graphs are successful at turning seman-
tic proximity into topological connectivity, making it possible to address issues
based on algorithmics and combinatorial analysis.

Graphs appear as essential modeling and analytical objects, and as eﬀective
visual analytics paradigms. Major research challenges are to produce scalable
analytical methods to identify key components both structurally and visually.
Eﬀorts are needed to design process capable of dealing with large datasets while
producing readable and usable graphical representations, allowing proper user
interaction. Special eﬀorts are required to deal with dynamically changing net-
works, in order to assess of structural changes at various scales.

4 The Visual Analytics Process

A number of systems for information visualization, as well as speciﬁc visual-
ization techniques, motivate their design choice from Shneiderman’s celebrated
mantra “Overview ﬁrst, Filter and zoom, Details on demand”. As is, the mantra
clearly emphasizes the role of visualization in the knowledge discovery process.
Recently, Keim adjusted the mantra to bring its focus toward Visual Analytics:
“Analyze ﬁrst, Show the Important, Zoom, ﬁlter and analyze further, Details
on demand”. In other words, this mantra is calling for astute combinations of
analytical approaches together with advanced visualization techniques.

The computation of any visual representation and/or geometrical embedding
of large and complex datasets requires some analysis to start with. Many scalable
graph drawing algorithms try to take advantage of any knowledge on topology
to optimize the drawing in terms of readability. Other approaches oﬀer repre-
sentations composed of visual abstractions of clusters to improve readability.
The challenge then is to try to come up with a representation that is as faithful
as possible to avoid introducing uncertainty. We must not fall into the na¨ıve
assumption that visualization can oﬀer a virgin view on the data: any represen-
tation will inevitably favor an interpretation over all possible ones. The solution
oﬀered by Visual Analytics is then to let the user enter into a loop where data
can be interactively manipulated to help gain insight both on the data and the
representation itself.

The sense-making loop structures the whole knowledge discovery process
supported through Visual Analytics. A generic scenario can be given following a
schema developed by van Wijk [42], which furthermore admits to be evaluated
and measured in terms of eﬃciency or knowledge gained. A choice for an initial
representation and adequate interactions can be made after applying diﬀerent

Visual Analytics: Deﬁnition, Process, and Challenges

165

statistical and mathematical techniques, such as spatio-temporal data analysis or
link mining depending on the nature of the dataset under study. The process then
enters a loop where the user can gain knowledge on the data, ideally driving the
system toward more focused and more adequate analytical techniques. Dually,
interacting on the visual representation, the user will gain a better understanding
of the visualization itself commanding for diﬀerent views helping him or her to
go beyond the visual and ultimately conﬁrm hypotheses built from previous
iterations (see Figure 4).

Fig. 4. The sense-making loop for Visual Analytics based on the simple model of
visualization by Wijk [42].

5 Application Challenges

Visual Analytics is a highly application oriented discipline driven by practical
requirements in important domains. Without attempting a complete survey over
all possible application areas, we sketch the potential applicability of Visual
Analytics technology in a few key domains.

In the Engineering domain, Visual Analytics can contribute to speed-up de-
velopment time for products, materials, tools and production methods by oﬀering
more eﬀective, intelligent access to the wealth of complex information resulting
from prototype development, experimental test series, customers’ feedback, and
many other performance metrics. One key goal of applied Visual Analytics in
the engineering domain will be the analysis of the complexity of the production
systems in correlation with the achieved output, for an eﬃcient and eﬀective
improvement of the production environments.

Financial Analysis is a prototypical promising application area for Visual
Analytics. Analysts in this domain are confronted with streams of heterogeneous
information from diﬀerent sources available at high update rates, and of varying

166

D. Keim et al.

reliability. Arriving at a unifying, task-centered view on diverse streams of data
is a central goal in ﬁnancial information systems. Integrated analysis and visu-
alization of heterogeneous data types such as news feeds, real-time trading data,
and fundamental economic indicators poses a challenge for developing advanced
analysis solutions in this area. Research based on results from Information Vi-
sualization is regarded as promising in this case.

Socio-economic considerations often form the basis of political decision
processes. A modern society can be regarded as a complex system of interre-
lationships between political decisions and economic, cultural and demographic
eﬀects. Analysis and Visualization of these interrelationships is promising in de-
veloping a better understanding of these phenomena, and to arrive at better
decisions. Successful Visual Analytics applications in this domain could start
being developed based on currently existing Geo-Spatial analysis frameworks.

Public Safety & Security is another important application area where Vi-
sual Analytics may contribute with advanced solutions. Analysts need to con-
stantly monitor huge amounts of heterogeneous information streams, correlating
information of varying degrees of abstraction and reliability, assessing the cur-
rent level of public safety, triggering alert in case of alarming situations being
detected. Data integration and correlation combined with appropriate analysis
and interactive visualization is promising to develop more eﬃcient tools for the
analysis in this area.

The study of Environment and Climate change often requires the ex-
amination of long term weather records and logs of various sensors, in a search
for patterns that can be related to observations such as changes in animal pop-
ulations, or in meteorological and climatic processes for instance. These require-
ments call for the development of systems allowing visual and graphical access
to historical monitoring data and predictions from various models in search for
or in order to validate patterns building over time.

These diverse ﬁelds of applications share many problems on an abstract level,
most of which are addressed by Visual Analytics. The actual (software) solution
must be adapted to the speciﬁc needs and terminologies of the application area
and consequently, many researchers currently focus on a speciﬁc customer seg-
ment. Much can be achieved, if the European research infrastructure in this ﬁeld
becomes strong enough to encourage the exchange of ideas on a broad scale, to
foster development of solutions applicable to multiple domains, achieving syn-
ergy eﬀects.

6 Technical Challenges

The primary goal of Visual Analytics is the analysis of vast amounts of data to
identify and visually distill the most valuable and relevant information content.
The visual representation should reveal structural patterns and relevant data
properties for easy perception by the analyst. A number of key requirements
need to be addressed by advanced Visual Analytics solutions. We next outline
important scientiﬁc challenges in this context.

Visual Analytics: Deﬁnition, Process, and Challenges

167

Scalability with Data Volumes and Data Dimensionality: Visual Ana-
lytics techniques need to be able to scale with the size and dimensionality of
the input data space. Techniques need to accommodate and graphically repre-
sent high-resolution input data as well as continuous input data streams of high
bandwidth. In many applications, data from multiple, heterogeneous sources
need to be integrated and processed jointly. In these cases, the methods need
to be able to scale with a range of diﬀerent data types, data sources, and levels
of quality. The visual representation algorithms need to be eﬃcient enough for
implementation in interactive systems.

Quality of Data and Graphical Representation: A central issue in Visual
Analytics is the avoidance of misinterpretations by the analyst. This may result
due to uncertainty and errors in the input data, or limitations of the chosen
analysis algorithm, and may produce misleading analysis results. To face this
problem, the notion of data quality, and the conﬁdence of the analysis algorithm
needs to be appropriately represented in the Visual Analytics solutions. The user
needs to be aware of these data and analysis quality properties at any stage in
the data analysis process.

Visual Representation and Level of Detail: To accommodate vast streams
of data, appropriate solutions need to intelligently combine visualizations of
selected analysis details on the one hand, and a global overview on the other
hand. The relevant data patterns and relationships need to be visualized on
several levels of detail, and with appropriate levels of data and visual abstraction.

User Interfaces, and Interaction Styles and Metaphors: Visual Analytics
systems need to be easily used and interacted with by the analyst. The analyst
needs to be able to fully focus on the task at hand, not on overly technical or
complex user interfaces, which potentially distract. To this end, novel interaction
techniques need to be developed which fully support the seamless, intuitive visual
communication with the system. User feedback should be taken as intelligently
as possible, requiring as little manual user input as possible, which guarantees
the full support of the user in navigating and analyzing the data, memorizing
insights and making informed decisions.

Display Devices: In addition to high-resolution desktop displays, advanced
display devices such as large-scale power walls and small portable personal assis-
tant, graphically-enabled devices need to be supported. Visual Analytics systems
should adapt to the characteristics of the available output devices, supporting
the Visual Analytics workﬂow on all levels of operation.

Evaluation: Due to the complex and heterogeneous problem domains addressed
by Visual Analytics, so far it has been diﬃcult to perform encompassing evalua-
tion work. A theoretically founded evaluation framework needs to be developed
which allows assessing the contribution of any Visual Analytics system toward
the level of eﬀectiveness and eﬃciency achieved regarding their requirements.

168

D. Keim et al.

Infrastructure: Managing large amounts of data for visualization or analysis
requires special data structures and mechanisms, both in memory and disks.
Achieving interactivity means refreshing the display in 100ms at worst whereas
analyzing data with standard techniques such as clustering can take hours to
complete. Achieving the smooth interaction required by the analysts to perform
their tasks while providing high-quality analytical algorithms need the combi-
nation of asynchronous computation with hybrid analytical algorithms that can
trade time with quality. Moreover, to fully support the analytical process, the
history of the analysis should also be recorded and interactively edited and an-
notated. Altogether, these requirements call for a novel software infrastructure,
built upon well understood technologies such as databases, software components
and visualization but augmented with asynchronous processing, history manage-
ments and annotations.

7 Examples for Visual Analytics Applications

7.1 Visual Analytics Tools for Analysis of Movement Data

With widespread availability of low cost GPS devices, it is becoming possible to
record data about the movement of people and objects at a large scale. While
these data hide important knowledge for the optimization of location and mobil-
ity oriented infrastructures and services, by themselves they lack the necessary
semantic embedding which would make fully automatic algorithmic analysis pos-
sible. At the same time, making the semantic link is easy for humans who however
cannot deal well with massive amounts of data. In [5] we argue that by using
the right visual analytics tools for the analysis of massive collections of move-
ment data, it is possible to eﬀectively support human analysts in understanding
movement behaviors and mobility patterns.

Figure 5 shows a subset of raw GPS measurements presented in so-called
space-time cube. The large amount of position records referring to the same
territory over a long time period makes it virtually impossible to do the analysis
by purely visual methods.

The paper [5] proposes a framework where interactive visual interfaces are
synergistically combined with database operations and computational process-
ing. The generic database techniques are used for basic data processing and ex-
traction of relevant objects and features. The computational techniques, which
are specially devised for movement data, aggregate and summarize these objects
and features and thereby enable the visualization of large amounts of informa-
tion. The visualization enables human cognition and reasoning, which, in turn,
direct and control the further analysis by means of the database, computational,
and visual techniques. Interactive visual interfaces embrace all the tools.

Thus, in order to detect and interpret signiﬁcant places visited by the mov-
ing entities, the positions of stops are extracted from the data by means of
appropriate database queries. Then, clustering methods are applied to detect
frequently visited places. Interactive visual displays put the results in the spa-
tial and temporal contexts. The spatial positions of the stops can be observed on

Visual Analytics: Deﬁnition, Process, and Challenges

169

Fig. 5. A visual display of a large amount of position records is unreadable and not
suitable for analysis.

Fig. 6. Positions of stops have been extracted from the database. By means of cluster-
ing, frequently visited places have been detected.

170

D. Keim et al.

Fig. 7. The temporal histograms show the distribution of the stops in the frequently
visited places (Figure 6) with respect to the weekly (left) and daily (right) cycles.

a map (Figure 6) or 3D spatial view. Temporal histograms (Figure 7) are used
to explore the temporal distribution of the stops throughout the time period and
within various temporal cycles (daily, weekly, etc.). These complementary views
allow a human analyst to understand the meanings or roles of the frequently
visited places.

In order to detect and interpret typical routes of the movement between the
signiﬁcant places, the analyst ﬁrst applies a database query to extract sequences
of position records between the stops, from which trajectories (time-referenced
lines) are constructed. Then, clustering is applied with the use of specially de-
vised similarity measures. The results are computationally generalized and sum-
marized and displayed in the spatial context (Figure 8).

7.2 Multilevel Visualization of the Worldwide Air

Transportation Network

The air transportation network has now become more dense and more complex
at all geographical levels. Its dynamic no more rests on simple territorial logics.
The challenge is to gain insightful understandings on how the routes carrying the
densest traﬃc organize themselves and impact the organization of the network
into sub-communities at lower levels. At the same time, subnetworks grow on
their own logic, involving tourism, economy or territorial control, and inﬂuence
or ﬁght against each other. Because of the network size and complexity, its study
can no more rely on traditional world map and requires novel visualization. A
careful analysis of the network structural properties, requiring recent results on
small world phenomenon, reveals its multilevel community structure.

The original network is organized into a top level network of communi-
ties (Figure 9(a)). Each component can then be further decomposed into sub-
communities. Capitals such as New York, Chicago, Paris or London (Figure 9(b))
clearly attract most of the international traﬃc and impose routes to ﬂy the world
around because of airline partnerships (economical logic). Asia (Figure 9(c))
clearly stands apart from these core hubs because of strong territorial ties en-
dorsed by national Asian airline companies (territorial logic). Visualization of
social networks such as the worldwide air transportation is challenged by the
necessity to scale with the growing size of network data while being able to oﬀer

Visual Analytics: Deﬁnition, Process, and Challenges

171

Fig. 8. A result of clustering and summarization of movement data: the routes between
the signiﬁcant places.

readable visual representations and ﬂuid interaction. Visualization today brings
the ﬁeld of social sciences close to the study of complex systems and promises
to deliver new knowledge across these disciplines [7,3,10].

8 Conclusions

The problems addressed by Visual Analytics are generic. Virtually all sciences
and many industries rely on the ability to identify methods and models, which
can turn data into reliable and provable knowledge. Ever since the dawn of mod-
ern science, researchers needed to ﬁnd methodologies to create new hypotheses,
to compare them with alternative hypotheses, and to validate their results. In
a collaborative environment this process includes a large number of specialized
people each having a diﬀerent educational background. The ability to commu-
nicate results to peers will become crucial for scientiﬁc discourse.

Currently, no technological approach can claim to give answers to all three

key questions that have been outlined in the ﬁrst section, regarding the

– relevance of a speciﬁc information
– adequacy of data processing methods and validity of results
– acceptability of the presentation of results for a given task

172

D. Keim et al.

(a) World air transportation network.

(b) USA and world hubs.

(c) Asia.

Fig. 9. Multilevel Visualization of the Worldwide Air Transportation Network

Visual Analytics: Deﬁnition, Process, and Challenges

173

Visual Analytics research does not focus on speciﬁc methods to address these
questions in a single “best-practice”. Each speciﬁc domain contributes a reper-
toire of approaches to initiate an interdisciplinary creation of solutions.

Visual Analytics literally maps the connection between diﬀerent alternative
solutions, leaving the opportunity for the human user to view these options in
the context of the complete knowledge generation process and to discuss these
options with peers on common ground.

References

1. Aigner, W., Miksch, S., M¨uller, W., Schumann, H., Tominski, C.: Visual meth-
ods for analyzing time-oriented data. IEEE Transactions on Visualization and
Computer Graphics 14(1), 47–60 (2008)

2. Amar, R.A., Eagan, J., Stasko, J.T.: Low-level components of analytic activity in

information visualization. In: INFOVIS, p. 15 (2005)

3. Amiel, M., Melan¸con, G., Rozenblat, C.: R´eseaux multi-niveaux: l’exemple des

´echanges a´eriens mondiaux. M@ppemonde 79(3) (2005)

4. Andrienko, G., Andrienko, N., Jankowski, P., Keim, D., Kraak, M.-J.,
MacEachren, A., Wrobel, S.: Geovisual analytics for spatial decision support:
Setting the research agenda. Special issue of the International Journal of Geo-
graphical Information Science 21(8), 839–857 (2007)

5. Andrienko, G., Andrienko, N., Wrobel, S.: Visual analytics tools for analysis of

movement data. ACM SIGKDD Explorations 9(2) (2007)

6. Andrienko, N., Andrienko, G.: Exploratory Analysis of Spatial and Temporal

Data. Springer, Heidelberg (2005)

7. Auber, D., Chiricota, Y., Jourdan, F., Melan¸con, G.: Multiscale visualization of

small world networks. In: INFOVIS (2003)

8. Card, S.K., Mackinlay, J., Shneiderman, B.: Readings in Information Visualiza-

tion: Using Vision to Think. Morgan Kaufmann, San Francisco (1999)

9. Ceglar, A., Roddick, J.F., Calder, P.: Guiding knowledge discovery through in-

teractive data mining, pp. 45–87. IGI Publishing, Hershey (2003)

10. Chiricota, Y., Melan¸con, G.: Visually mining relational data. International Review

on Computers and Software (2005)

11. Das, A.: Semantic approximation of data stream joins. IEEE Transactions on
Knowledge and Data Engineering 17(1), 44–59 (2005), Member-Johannes Gehrke
and Member-Mirek Riedewald

12. Dix, A., Finlay, J.E., Abowd, G.D., Beale, R.: Human-Computer Interaction (.),

3rd edn. Prentice-Hall, Inc., Upper Saddle River (2003)

13. Duda, R., Hart, P., Stock, D.: Pattern Classiﬁcation. John Wiley and Sons Inc,

Chichester (2000)

14. Dykes, J., MacEachren, A., Kraak, M.-J.: Exploring geovisualization. Elsevier

Science, Amsterdam (2005)

15. Engel, K., Hadwiger, M., Kniss, J.M., Rezk-salama, C., Weiskopf, D.: Real-time

Volume Graphics. A. K. Peters, Ltd., Natick (2006)

16. Ester, M., Sander, J.: Knowledge Discovery in Databases - Techniken und An-

wendungen. Springer, Heidelberg (2000)

17. Forsell, C., Seipel, S., Lind, M.: Simple 3d glyphs for spatial multivariate data.

In: INFOVIS, p. 16 (2005)

174

D. Keim et al.

18. Han, J., Kamber, M. (eds.): Data Mining: Concepts and Techniques. Morgan

Kaufmann, San Francisco (2000)

19. Hand, D., Mannila, H., Smyth, P. (eds.): Principles of Data Mining. MIT Press,

Cambridge (2001)

20. Inselberg, A., Dimsdale, B.: Parallel Coordinates: A Tool for Visualizing Multi-
variate Relations (chapter 9), pp. 199–233. Plenum Publishing Corporation, New
York (1991)

21. Jacko, J.A., Sears, A.: The Handbook for Human Computer Interaction. Lawrence

Erlbaum & Associates, Mahwah (2003)

22. Johnson, C., Hanson, C. (eds.): Visualization Handbook. Kolam Publishing (2004)
23. Keim, D., Ertl, T.: Scientiﬁc visualization (in german). Information Technol-

ogy 46(3), 148–153 (2004)

24. Keim, D., Ward, M.: Visual Data Mining Techniques (chapter 11). Springer, New

York (2003)

25. Keim, D.A., Ankerst, M., Kriegel, H.-P.: Recursive pattern: A technique for visu-
alizing very large amounts of data. In: VIS ’95: Proceedings of the 6th conference
on Visualization ’95, Washington, DC, USA, p. 279. IEEE Computer Society
Press, Los Alamitos (1995)

26. Keim, D.A., Panse, C., Sips, M., North, S.C.: Pixel based visual data mining of

geo-spatial data. Computers &Graphics 28(3), 327–344 (2004)

27. Kerren, A., Stasko, J.T., Fekete, J.-D., North, C.J. (eds.): Information Visualiza-

tion. LNCS, vol. 4950. Springer, Heidelberg (2008)

28. Kr´uger, J., Schneider, J., Westermann, R.: Clearview: An interactive context pre-
serving hotspot visualization technique. IEEE Transactions on Visualization and
Computer Graphics 12(5), 941–948 (2006)

29. Maimon, O., Rokach, L. (eds.): The Data Mining and Knowledge Discovery Hand-

book. Springer, Heidelberg (2005)

30. Meliou, A., Chu, D., Guestrin, C., Hellerstein, J., Hong, W.: Data gathering tours

in sensor networks. In: IPSN (2006)

31. Mitchell, T.M.: Machine Learning. McGraw-Hill, Berkeley (1997)
32. Naumann, F., Bilke, A., Bleiholder, J., Weis, M.: Data fusion in three steps:
Resolving schema, tuple, and value inconsistencies. IEEE Data Eng. Bull. 29(2),
21–31 (2006)

33. North, C.: Toward measuring visualization insight. IEEE Comput. Graph.

Appl. 26(3), 6–9 (2006)

34. Perner, P. (ed.): Data Mining on Multimedia Data. LNCS, vol. 2558. Springer,

Heidelberg (2002)

35. Schumann, H., M¨uller, W.: Visualisierung - Grundlagen und allgemeine Metho-

den. Springer, Heidelberg (2000)

36. Shneiderman, B.: Tree visualization with tree-maps: 2-d space-ﬁlling approach.

ACM Trans. Graph. 11(1), 92–99 (1992)

37. Shneiderman, B., Plaisant, C.: Designing the User Interface. Addison-Wesley,

Reading (2004)

38. Spence, R.: Information Visualization. ACM Press, New York (2001)
39. Thomas, J.J., Cook, K.A.: Illuminating the Path. IEEE Computer Society Press,

Los Alamitos (2005)

40. Tricoche, X., Scheuermann, G., Hagen, H.: Tensor topology tracking: A visual-
ization method for time-dependent 2d symmetric tensor ﬁelds. Comput. Graph.
Forum 20(3) (2001)

41. Unwin, A., Theus, M., Hofmann, H.: Graphics of Large Datasets: Visualizing a

Million (Statistics and Computing). Springer, New York (2006)

Visual Analytics: Deﬁnition, Process, and Challenges

175

42. van Wijk, J.J.: The value of visualization. In: IEEE Visualization, p. 11 (2005)
43. Widom, J.: Trio: A system for integrated management of data, accuracy, and

lineage. In: CIDR, pp. 262–276 (2005)

44. Yi, J.S., Kang, Y.a., Stasko, J.T., Jacko, J.A.: Toward a deeper understanding
of the role of interaction in information visualization. IEEE Trans. Vis. Comput.
Graph. 13(6), 1224–1231 (2007)

",False,2008.0,{},False,False,bookSection,False,7DLBYPHS,[],self.user,False,False,False,False,http://link.springer.com/10.1007/978-3-540-70956-5_7,,"Visual Analytics: Definition, Process, and Challenges",7DLBYPHS,False,False
PPNESMHH,5RUCHG4U,"Organization Science
Vol. 16, No. 4, July–August 2005, pp. 409–421
issn 1047-7039(cid:1) eissn 1526-5455(cid:1) 05(cid:1) 1604(cid:1) 0409

informs ®

doi 10.1287/orsc.1050.0133
© 2005 INFORMS

Organizing and the Process of Sensemaking

Karl E. Weick, Kathleen M. Sutcliffe

Department of Management and Organizations, Ross School of Business, University of Michigan, 701 Tappan,

Ann Arbor, Michigan 48109-1234 {karlw@umich.edu, ksutclif@umich.edu}

Organization and Strategy, University of California, Irvine, Irvine, California 92697, dobstfel@uci.edu

David Obstfeld

Sensemaking involves turning circumstances into a situation that is comprehended explicitly in words and that serves as

a springboard into action. In this paper we take the position that the concept of sensemaking ﬁlls important gaps in
organizational theory. The seemingly transient nature of sensemaking belies its central role in the determination of human
behavior, whether people are acting in formal organizations or elsewhere. Sensemaking is central because it is the primary
site where meanings materialize that inform and constrain identity and action. The purpose of this paper is to take stock of
the concept of sensemaking. We do so by pinpointing central features of sensemaking, some of which have been explicated
but neglected, some of which have been assumed but not made explicit, some of which have changed in signiﬁcance over
time, and some of which have been missing all along or have gone awry. We sense joint enthusiasm to restate sensemaking
in ways that make it more future oriented, more action oriented, more macro, more closely tied to organizing, meshed
more boldly with identity, more visible, more behaviorally deﬁned, less sedentary and backward looking, more infused
with emotion and with issues of sensegiving and persuasion. These key enhancements provide a foundation upon which to
build future studies that can strengthen the sensemaking perspective.

Key words: sensemaking; interpreting; articulation; identity; power

Sensemaking involves

the ongoing retrospective
development of plausible images that rationalize what
people are doing. Viewed as a signiﬁcant process of
organizing, sensemaking unfolds as a sequence in which
people concerned with identity in the social context of
other actors engage ongoing circumstances from which
they extract cues and make plausible sense retrospec-
tively, while enacting more or less order into those ongo-
ing circumstances. Stated more compactly and more
colorfully, “[S]ensemaking is a way station on the road
to a consensually constructed, coordinated system of
action” (Taylor and Van Every 2000, p. 275). At that way
station, circumstances are “turned into a situation that is
comprehended explicitly in words and that serves as a
springboard to action” (p. 40). These images imply three
important points about the quest for meaning in orga-
nizational life. First, sensemaking occurs when a ﬂow
of organizational circumstances is turned into words and
salient categories. Second, organizing itself is embodied
in written and spoken texts. Third, reading, writing, con-
versing, and editing are crucial actions that serve as the
media through which the invisible hand of institutions
shapes conduct (Gioia et al. 1994, p. 365).

The emerging picture is one of sensemaking as a pro-
cess that is ongoing, instrumental, subtle, swift, social,
and easily taken for granted. The seemingly transient
nature of sensemaking (“a way station”) belies its cen-
tral role in the determination of human behavior. Sense-
making is central because it is the primary site where

meanings materialize that inform and constrain identity
and action (Mills 2003, p. 35). When we say that mean-
ings materialize, we mean that sensemaking is, impor-
tantly, an issue of language, talk, and communication.
Situations, organizations, and environments are talked
into existence.

Explicit efforts at sensemaking tend to occur when the
current state of the world is perceived to be different
from the expected state of the world, or when there is
no obvious way to engage the world. In such circum-
stances there is a shift from the experience of immersion
in projects to a sense that the ﬂow of action has become
unintelligible in some way. To make sense of the disrup-
tion, people look ﬁrst for reasons that will enable them to
resume the interrupted activity and stay in action. These
“reasons” are pulled from frameworks such as institu-
tional constraints, organizational premises, plans, expec-
tations, acceptable justiﬁcations, and traditions inherited
from predecessors. If resumption of the project is prob-
lematic, sensemaking is biased either toward identifying
substitute action or toward further deliberation.

Sensemaking is about

the interplay of action and
interpretation rather than the inﬂuence of evaluation
on choice. When action is the central focus, interpre-
tation, not choice, is the core phenomenon (Laroche
1995, p. 66; Lant 2002; Weick 1993, pp. 644–646).
Scott Snook (2001) makes this clear in his analysis of a
friendly ﬁre incident over Iraq in April 1994 when two
F-15 pilots shot down two friendly helicopters, killing

409

410

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

26 people. As Snook says, this is not an incident where
F-15 pilots “decided” to pull the trigger.

I could have asked, “Why did they decide to shoot?”
However, such a framing puts us squarely on a path
that leads straight back to the individual decision maker,
away from potentially powerful contextual features and
right back into the jaws of the fundamental attribution
error. “Why did they decide to shoot?” quickly becomes
“Why did they make the wrong decision?” Hence, the
attribution falls squarely onto the shoulders of the deci-
sion maker and away from potent situation factors that
inﬂuence action. Framing the individual-level puzzle as
a question of meaning rather than deciding shifts the
emphasis away from individual decision makers toward a
point somewhere “out there” where context and individ-
ual action overlap (cid:1) (cid:1) (cid:1) (cid:1) Such a reframing—from decision
making to sensemaking—opened my eyes to the possibil-
ity that, given the circumstances, even I could have made
the same “dumb mistake.” This disturbing revelation,
one that I was in no way looking for, underscores the
importance of initially framing such senseless tragedies
as “good people struggling to make sense,” rather than
as “bad ones making poor decisions” (pp. 206–207).

To focus on sensemaking is to portray organizing
as the experience of being thrown into an ongoing,
unknowable, unpredictable streaming of experience in
search of answers to the question, “what’s the story?”
Plausible stories animate and gain their validity from
subsequent activity. The language of sensemaking cap-
tures the realities of agency, ﬂow, equivocality, tran-
sience, reaccomplishment, unfolding, and emergence,
realities that are often obscured by the language of
variables, nouns, quantities, and structures. Students of
sensemaking understand that the order in organizational
life comes just as much from the subtle, the small, the
relational, the oral, the particular, and the momentary as
it does from the conspicuous, the large, the substantive,
the written, the general, and the sustained. To work with
the idea of sensemaking is to appreciate that smallness
does not equate with insigniﬁcance. Small structures and
short moments can have large consequences.

We take the position that the concept of sensemaking
ﬁlls important gaps in organizational theory. We reaf-
ﬁrm this idea and take stock of the sensemaking concept
ﬁrst by highlighting its distinctive features descriptively,
using an extended example of pediatric nursing. Next
we summarize the distinctive features of sensemaking
conceptually and discuss intraorganizational evolution,
instigations, plausibility, and identity. Finally, we sum-
marize the distinctive features of sensemaking prospec-
tively and examine future lines of work that may develop
from ideas about institutions, distributed sensemaking,
power, and emotion. We conclude with a brief descrip-
tion of gaps in organizational theory that the concept of
sensemaking ﬁlls.

The Nature of Organized Sensemaking:
Viewed Descriptively
Organizational sensemaking is ﬁrst and foremost about
the question: How does something come to be an event
for organizational members? Second, sensemaking is
about the question: What does an event mean? In the
context of everyday life, when people confront some-
thing unintelligible and ask “what’s the story here?” their
question has the force of bringing an event into exis-
tence. When people then ask “now what should I do?”
this added question has the force of bringing meaning
into existence, meaning that they hope is stable enough
for them to act into the future, continue to act, and to
have the sense that they remain in touch with the con-
tinuing ﬂow of experience.

While these descriptions may help delimit sensemak-
ing, they say little about what is organizational in all of
this. The answer is that sensemaking and organization
constitute one another: “Organization is an attempt to
order the intrinsic ﬂux of human action, to channel it
toward certain ends, to give it a particular shape, through
generalizing and institutionalizing particular meanings
and rules” (Tsoukas and Chia 2002, p. 570). We need to
grasp each to understand the other. The operative image
of organization is one in which organization emerges
through sensemaking, not one in which organization pre-
cedes sensemaking or one in which sensemaking is pro-
duced by organization.

A central theme in both organizing and sensemak-
ing is that people organize to make sense of equivocal
inputs and enact this sense back into the world to make
that world more orderly. Basic moments in the process
of sensemaking are illustrated in the following account,
where a nurse describes what she did while caring for
a baby whose condition began to deteriorate (Benner
1994, pp. 139–140)1:

Nurse: I took care of a 900-gram baby who was about
26 or 27 weeks many years ago who had been doing
well for about two weeks. He had an open ductus that
day. The difference between the way he looked at 9 a.m.
and the way he looked at 11 a.m. was very dramatic.
I was at that point really concerned about what was going
to happen next. There are a lot of complications of the
patent ductus, not just in itself, but the fact that it causes
a lot of other things. I was really concerned that the baby
was starting to show symptoms of all of them.

Interviewer: Just in that two hours?

Nurse: You look at this kid because you know this kid,
and you know what he looked like two hours ago. It is a
dramatic difference to you, but it’s hard to describe that
to someone in words. You go to the resident and say:
“Look, I’m really worried about X, Y, Z,” and they go:
“OK.” Then you wait one half hour to 40 minutes, then
you go to the Fellow (the teaching physician supervising
the resident) and say: “You know, I am really worried
about X, Y, Z.” They say: “We’ll talk about it on rounds.”

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

411

Interviewer: What is the X, Y, Z you are worried about?

Nurse: The fact that the kid is more lethargic, paler, his
stomach is bigger, that he is not tolerating his feedings,
that his chem strip (blood test) might be a little strange.
All these kinds of things. I can’t remember the exact
details of this case; there are clusters of things that go
wrong. The baby’s urine output goes down. They sound
like they are in failure. This kind of stuff. Their pulses
go bad, their blood pressure changes. There are a million
things that go on. At this time, I had been in the unit a
couple or three years.

Sensemaking Organizes Flux
Sensemaking starts with chaos. This nurse encounters
“a million things that go on” and the ongoing poten-
tial for “clusters of things that go wrong”—part of an
almost inﬁnite stream of events and inputs that surround
any organizational actor. As Chia (2000, p. 517) puts it,
we start with “an undifferentiated ﬂux of ﬂeeting sense-
impressions and it is out of this brute aboriginal ﬂux of
lived experience that attention carves out and concep-
tion names.” As the case illustrates, the nurse’s sense-
making does not begin de novo, but like all organizing
occurs amidst a stream of potential antecedents and con-
sequences. Presumably within the 24-hour period sur-
rounding the critical noticing, the nurse slept, awoke,
prepared for work, observed and tended to other babies,
completed paper work and charts, drank coffee, spoke
with doctors and fellow nurses, stared at an elevator
door as she moved between hospital ﬂoors, and per-
formed a variety of formal and impromptu observations.
All of these activities furnish a raw ﬂow of activity from
which she may or may not extract certain cues for closer
attention.

Sensemaking Starts with Noticing and Bracketing
During her routine activities, the nurse becomes aware
of vital signs that are at variance with the “normal”
demeanor of a recovering baby. In response to the
interruption, the nurse orients to the child and notices
and brackets possible signs of trouble for closer atten-
tion. This noticing and bracketing is an incipient state
of sensemaking. In this context sensemaking means
basically “inventing a new meaning (interpretation) for
something that has already occurred during the orga-
nizing process, but does not yet have a name (italics
in original), has never been recognized as a sepa-
rate autonomous process, object, event” (Magala 1997,
p. 324).

The nurse’s noticing and bracketing is guided by men-
tal models she has acquired during her work, training,
and life experience. Those mental models may help her
recognize and guide a response to an open ductus con-
dition or sickness more generally. Such mental models
might be primed by the patient’s conditions or a priori
permit her to notice and make sense of those conditions

(Klein et al., in press). Some combination of mental
models and salient cues calls her attention to this partic-
ular baby between the hours of 9 to 11 with respect to
a bounded set of symptoms.

The more general point is that in the early stages of
sensemaking, phenomena “have to be forcibly carved
out of the undifferentiated ﬂux of raw experience and
conceptually ﬁxed and labeled so that they can become
the common currency for communicational exchanges”
(Chia 2000, p. 517). Notice that once bracketing occurs,
the world is simpliﬁed.

Sensemaking Is About Labeling
Sensemaking is about labeling and categorizing to stabi-
lize the streaming of experience. Labeling works through
a strategy of “differentiation and simple-location, iden-
tiﬁcation and classiﬁcation, regularizing and routiniza-
tion [to translate] the intractable or obdurate into a
form that is more amenable to functional deployment”
(Chia 2000, p. 517). The key phrase here is “functional
deployment.” In medicine, functional deployment means
imposing diagnostic labels that suggest a plausible treat-
ment. In organizing in general, functional deployment
means imposing labels on interdependent events in ways
that suggest plausible acts of managing, coordinating,
and distributing. Thus, the ways in which events are ﬁrst
envisioned immediately begins the work of organizing
because events are bracketed and labeled in ways that
predispose people to ﬁnd common ground. To gener-
ate common ground, labeling ignores differences among
actors and deploys cognitive representations that are able
to generate recurring behaviors: “For an activity to be
said to be organized, it implies that types of behavior in
types of situation are systematically connected to types
of actors (cid:1) (cid:1) (cid:1) (cid:1) An organized activity provides actors with
a given set of cognitive categories and a typology of
actions” (Tsoukas and Chia 2002, p. 573).

A crucial feature of these types and categories is that
they have considerable plasticity. Categories have plas-
ticity because they are socially deﬁned, because they
have to be adapted to local circumstances, and because
they have a radial structure. By radial structure we mean
that there a few central instances of the category that
have all the features associated with the category, but
mostly the category contains peripheral instances that
have only a few of these features. This difference is
potentially crucial because if people act on the basis
of central prototypic cases within a category, then their
action is stable; but if they act on the basis of periph-
eral cases that are more equivocal in meaning, their
action is more variable, more indeterminate, more likely
to alter organizing, and more consequential for adapting
(Tsoukas and Chia 2002, p. 574).

Sensemaking Is Retrospective
The nurse uses retrospect to make sense of the puzzles
she observes at 11:00. She recalls “what he looked like

412

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

two hours ago. It’s a dramatic difference.” Symptoms are
not discovered at 11:00. Instead, symptoms are created
at 11:00 by looking back over earlier observations and
seeing a pattern. The nurse alters the generic sensemak-
ing recipe, “how can I know what I think until I see what
I say,” into the medically more useful variant, “how can
I know what I’m seeing until I see what it was.”

Marianne Paget (1988, p. 56) has been especially sen-
sitive to the retrospective quality of medical work as
is evident in her description of mistakes in diagnosis:
“A mistake follows an act. It identiﬁes the character of
an act in its aftermath. It names it. An act, however, is
not mistaken; it becomes mistaken. There is a paradox
here, for seen from the inside of action, that is from the
point of view of an actor, an act becomes mistaken only
after it has already gone wrong. As it is unfolding, it
is not becoming mistaken at all; it is becoming.” When
people bracket a portion of streaming circumstances and
label them as a concern, a bad sign, a mistake, or an
opportunity, the event is at an advanced stage; the label
follows after and names a completed act, but the label-
ing itself fails to capture the dynamics of what is hap-
pening. Because mistakes and diagnoses are known in
the aftermath of activity, they are fruitfully described as
“complex cognitions of the experience of now and then.
They identify the too-lateness of human understanding”
(Paget 1988, pp. 96–97). So, “the now of mistakes col-
lides with the then of acting with uncertain knowledge.
Now represents the more exact science of hindsight, then
the unknown future coming into being” (Paget 1988,
p. 48).

Sensemaking Is About Presumption
To make sense is to connect the abstract with the con-
crete. In the case of medical action, “instances of illness
are concrete, idiosyncratic, and personal in their expres-
sion, and the stock of knowledge is abstract and ency-
clopedic. Interpretation and experimentation engage the
concrete, idiosyncratic, and personal with the abstract
and impersonal” (Paget 1988, p. 51). It is easy to miss
this linkage and to portray sensemaking as more cere-
bral, more passive, more abstract than it typically is.
Sensemaking starts with immediate actions, local con-
text, and concrete cues, as is true for the worried nurse.
She says to the resident, “Look, I’m really worried about
X, Y, Z.”

What is interesting about her concerns is that she is
acting as if something is the case, which means any fur-
ther action tests that hunch but may run a risk for the
baby. To test a hunch is to presume the character of
the illness and to update that presumptive understand-
ing through progressive approximations: “The [medical]
work process unfolds as a series of approximations
and attempts to discover an appropriate response. And
because it unfolds this way, as an error-ridden activity,
it requires continuous attention to the patient’s condition
and to reparation” (Paget 1988, p. 143).

Sensemaking Is Social and Systemic
The nurse’s sensemaking is inﬂuenced by a variety of
social factors. These social factors might include previ-
ous discussions with the other nurses on duty, an off-
hand remark about the infant that might have been made
by a parent, interaction with physicians—some of whom
encourage nurses to take initiative and some who do
not—or the mentoring she received yesterday.

However, it is not just the concerned nurse and her
contacts that matter in this unfolding incident. Medi-
cal sensemaking is distributed across the healthcare sys-
tem, and converges on the tiny patient as much through
scheduling that involves cross-covering of one nurse’s
patients by another nurse (and through multiple brands
of infusion pumps with conﬂicting setup protocols) as
it does through the occasional appearance of the attend-
ing physician at the bedside. If knowledge about the
correctness of treatment unfolds gradually, then knowl-
edge of this unfolding sense is not located just inside the
head of the nurse or physician. Instead, the locus is sys-
temwide and is realized in stronger or weaker coordina-
tion and information distribution among interdependent
healthcare workers.

Sensemaking Is About Action
If the ﬁrst question of sensemaking is “what’s going on
here?,” the second, equally important question is “what
do I do next?” This second question is directly about
action, as is illustrated in this case, where the nurse’s
emerging hunch is intertwined with the essential task of
enlisting a physician to take action on the case. The talk
that leads to a continual, iteratively developed, shared
understanding of the diagnosis and the persuasive talk
that leads to enlistment in action both illustrate the “say-
ing” that is so central to organizational action. In sense-
making, action and talk are treated as cycles rather than
as a linear sequence. Talk occurs both early and late,
as does action, and either one can be designated as the
“starting point to the destination.” Because acting is an
indistinguishable part of the swarm of ﬂux until talk
brackets it and gives it some meaning, action is not
inherently any more signiﬁcant than talk, but it factors
centrally into any understanding of sensemaking.

Medical sensemaking is as much a matter of thinking
that is acted out conversationally in the world as it is a
matter of knowledge and technique applied to the world.
Nurses (and physicians), like everyone else, make sense
by acting thinkingly, which means that they simultane-
ously interpret their knowledge with trusted frameworks,
yet mistrust
those very same frameworks by testing
new frameworks and new interpretations. The underlying
assumption in each case is that ignorance and knowl-
edge coexist, which means that adaptive sensemaking
both honors and rejects the past. What this means is that
in medical work, as in all work, people face evolving
disorder. There are truths of the moment that change,

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

413

develop, and take shape through time. It is these changes
through time that progressively reveal that a seemingly
correct action “back then” is becoming an incorrect
action “now.” These changes also may signal a progres-
sion from worse to better.

Sensemaking Is About Organizing Through
Communication
Communication is a central component of sensemaking
and organizing: “We see communication as an ongo-
ing process of making sense of the circumstances in
which people collectively ﬁnd ourselves and of the
events that affect them. The sensemaking, to the extent
that it involves communication, takes place in interac-
tive talk and draws on the resources of language in order
to formulate and exchange through talk (cid:1) (cid:1) (cid:1) symbolically
encoded representations of these circumstances. As this
occurs, a situation is talked into existence and the basis
is laid for action to deal with it” (Taylor and Van Every
2000, p. 58). The image of sensemaking as activity that
talks events and organizations into existence suggests
that patterns of organizing are located in the actions and
conversations that occur on behalf of the presumed orga-
nization and in the texts of those activities that are pre-
served in social structures.

We see this in the present example. As the case illus-
trates, the nurse’s bracketed set of noticings coalesce
into an impression of the baby as urgently in need of
physician attention, but the nurse’s choice to articulate
her concerns ﬁrst to a resident and then to a Fellow pro-
duces little immediate result. Her individual sensemak-
ing has little inﬂuence on the organizing of care around
this patient as this passage shows (Benner 1994, p. 140):

(cid:1) (cid:1) (cid:1) At this time, I had been in the unit a couple or three
years. I was really starting to feel like I knew what was
going on but I wasn’t as good at throwing my weight
in a situation like that. And I talked to a nurse who had
more experience and I said, “Look at this kid,” and I told
her my story, and she goes: “OK.” Rounds started shortly
after that and she walks up to the Attending [Physician in
charge of patient] very quietly, sidles up and says: “You
know, this kid, Jane is really worried about this kid.”
She told him the story, and said: “He reminds me about
this kid, Jimmie, we had three weeks ago,” and he said:
“Oh.” Everything stops. He gets out the stethoscope and
listens to the kid, examines the kid and he says: “Call
the surgeons.” (Laughter) It’s that kind of thing where
we knew also what had to be done. There was no time
to be waiting around. He is the only one that can make
that decision. It was a case we had presented to other
physicians who should have made the case, but didn’t.
We are able in just two sentences to make that case to
the Attending because we knew exactly what we were
talking about. (cid:1) (cid:1) (cid:1) this particular nurse really knew exactly
what she was doing. [The Attending] knew she knew
what she was doing (cid:1) (cid:1) (cid:1) (cid:1) She knew exactly what button
to push with him and how to do it.

What we see here is articulation (Benner 1994, Winter
1987), which is deﬁned as “the social process by which
tacit knowledge is made more explicit or usable.” To
share understanding means to lift equivocal knowledge
out of the tacit, private, complex, random, and past to
make it explicit, public, simpler, ordered, and relevant
to the situation at hand (Obstfeld 2004). Taylor and Van
Every (2000, pp. 33–34) describe a process similar to
articulation: “A situation is talked into being through the
interactive exchanges of organizational members to pro-
duce a view of circumstances including the people, their
objects, their institutions and history, and their siting
[i.e., location as a site] in a ﬁnite time and place.” This
is what happens successively as the ﬁrst nurse trans-
lates her concerns for the second more powerful nurse,
who then rearticulates the case using terms relevant to
the Attending. The second nurse absorbs the complex-
ity of the situation (Boisot and Child 1999) by holding
both a nurse’s and doctor’s perspectives of the situation
while identifying an account of the situation that would
align the two. What is especially interesting is that she
tries to make sense of how other people make sense of
things, a complex determination that is routine in orga-
nizational life.

Summary
To summarize, this sequence highlights several distin-
guishing features of sensemaking, including its genesis
in disruptive ambiguity, its beginnings in acts of noticing
and bracketing, its mixture of retrospect and prospect,
its reliance on presumptions to guide action, its embed-
ding in interdependence, and its culmination in artic-
ulation that shades into acting thinkingly. Answers to
the question “what’s the story?” emerge from retrospect,
connections with past experience, and dialogue among
people who act on behalf of larger social units. Answers
to the question “now what?” emerge from presumptions
about the future, articulation concurrent with action, and
projects that become increasingly clear as they unfold.

The Nature of Organized Sensemaking:
Viewed Conceptually
Sensemaking as Intraorganizational Evolution
The preceding overview of early activities of sensemak-
ing and organizing that mobilize around moments of ﬂux
needs to be compressed if it is to guide research and
practice. One way to do that is to assume that “a sys-
tem can respond adaptively to its environment by mim-
icking inside itself the basic dynamics of evolutionary
processes” (Warglien 2002, p. 110). The basic evolution-
ary process assumed by sensemaking is one in which
retrospective interpretations are built during interdepen-
dent interaction. This framework is a variant of Donald

414

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

Campbell’s application of evolutionary epistemology to
social
life (1965, 1997). It proposes that sensemak-
ing can be treated as reciprocal exchanges between
actors (Enactment) and their environments (Ecological
Change) that are made meaningful (Selection) and pre-
served (Retention). However, these exchanges will con-
tinue only if the preserved content
is both believed
(positive causal linkage) and doubted (negative causal
linkage) in future enacting and selecting. Only with
ambivalent use of previous knowledge are systems able
both to beneﬁt from lessons learned and to update either
their actions or meanings in ways that adapt to changes
in the system and its context. For shorthand we will
call this model “enactment theory,” as has become the
convention in organizational work (e.g., Jennings and
Greenwood 2003). Graphically, the ESR sequence looks
like Figure 1.

If we conceptualize organizing as a sequence of eco-
logical change-enactment-selection-retention with the
results of retention feeding back to all three prior pro-
cesses, then the speciﬁc activities of sensemaking ﬁt
neatly into this more general progression of organizing.
The reciprocal relationship between ecological change
and enactment includes sensemaking activities of sens-
ing anomalies, enacting order into ﬂux, and being shaped
by externalities. The organizing process of enactment
incorporates the sensemaking activities of noticing and
bracketing. These activities of noticing and bracketing,
triggered by discrepancies and equivocality in ongo-
ing projects, begin to change the ﬂux of circumstances
into the orderliness of situations. We emphasize “begin”
because noticing and bracketing are relatively crude acts
of categorization and the resulting data can mean sev-
eral different things. The number of possible meanings
gets reduced in the organizing process of selection. Here
a combination of retrospective attention, mental mod-
els, and articulation perform a narrative reduction of

the bracketed material and generate a locally plausi-
ble story. Though plausible, the story that is selected is
also tentative and provisional. It gains further solidity
in the organizing process of retention. When a plausi-
ble story is retained, it tends to become more substan-
tial because it is related to past experience, connected
to signiﬁcant identities, and used as a source of guid-
ance for further action and interpretation. The close ﬁt
between processes of organizing and processes of sense-
making illustrates the recurring argument (e.g., Weick
1969, pp. 40–42) that people organize to make sense of
equivocal inputs and enact this sense back into the world
to make that world more orderly. The beauty of mak-
ing ESR the microfoundation of organizing and sense-
making is that it makes it easier to work with other
meso- and macro-level formulations that are grounded in
Campbell’s work (e.g., Aldrich 1999, Baum and Singh
1994, Ocasio 2001).

Instigations to Sensemaking
The idea that sensemaking is focused on equivocality
gives primacy to the search for meaning as a way to
deal with uncertainty (e.g., Mills 2003, p. 44). Thus,
we expect to ﬁnd explicit efforts at sensemaking when-
ever the current state of the world is perceived to be
different from the expected state of the world. This
means that sensemaking is activated by the question,
“same or different?” When the situation feels “differ-
ent,” this circumstance is experienced as a situation of
discrepancy (Orlikowski and Gash 1994), breakdown
(Patriotta 2003), surprise (Louis 1980), disconﬁrmation
(Weick and Sutcliffe 2001), opportunity (Dutton 1993),
or interruption (Mandler 1984, pp. 180–189). Diverse
as these situations may seem, they share the proper-
ties that in every case an expectation of continuity is
breached, ongoing organized collective action becomes

Figure 1 The Relationship Among Enactment, Organizing, and Sensemaking

Ongoing updating

Retrospect

extracted cues

Identity

plausibility

Ecological

change

Enactment

Selection

Retention

Source. Jennings and Greenwood (2003; adapted from Weick 1979, p. 132).

Feedback of  identity on
selection and enactment

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

415

disorganized, efforts are made to construct a plausible
sense of what is happening, and this sense of plausibil-
ity normalizes the breach, restores the expectation, and
enables projects to continue.

Questions of “same or different” tend to occur under
one of three conditions: situations involving the dramatic
loss of sense (e.g., Lanir 1989), situations where the loss
of sense is more mundane but no less troublesome (e.g.,
Westley 1990), and unfamiliar contexts where sense is
elusive (e.g., Orton 2000). Methodologically, it is hard
to ﬁnd people in the act of coping with disconﬁrmations
that catch them unawares (see Westrum 1982 for a clear
exception). Such outcroppings can be found, however, if
we examine how everyday situations sometimes present
us with either too many meanings or too few. For exam-
ple, managing any kind of process (e.g., a production
routine) with its interconnected processes of anticipation
and retrospection (Patriotta 2003) creates equivocality
of time (e.g., is this a fresh defect, or has it happened
for some time?) and equivocality of action (e.g., do I
have the resources to correct this defect?). Regardless
of whether there are too many meanings or too few, the
result is the same. Actors are faced with ﬂeeting sense
impressions that instigate sensemaking.

While scholars have a strong interest in conscious
sensemaking and in making the sensemaking process
more visible, they also agree with Gioia and Mehra
(1996, p. 1,228), who suggest that much of organiza-
tional life is routine and made up of situations that do
not demand our full attention. As they note, people’s
sense can be “modiﬁed in intricate ways out of aware-
ness via assimilation of subtle cues over time” (p. 1,229).
Acknowledgement of this facet of sensemaking is impor-
tant if only to avoid the impression that “routine organi-
zational life is devoid of sense” (Gioia and Mehra 1996,
p. 1,229).

Plausibility and Sensemaking
Sensemaking is not about
truth and getting it right.
Instead, it is about continued redrafting of an emerging
story so that it becomes more comprehensive, incorpo-
rates more of the observed data, and is more resilient
in the face of criticism. As the search for meanings
continues, people may describe their activities as the
pursuit of accuracy to get it right. However, that descrip-
tion is important mostly because it sustains motivation.
People may get better stories, but they will never get
the story. Furthermore, what is plausible for one group,
such as managers, often proves implausible for another
group, such as employees. In an important study of cul-
ture change, Mills (2003, pp. 169–173) found that sto-
ries tend to be seen as plausible when they tap into an
ongoing sense of current climate, are consistent with
other data, facilitate ongoing projects, reduce equivocal-
ity, provide an aura of accuracy (e.g., reﬂect the views

of a consultant with a strong track record), and offer a
potentially exciting future.

The idea that sensemaking is driven by plausibil-
ity rather than accuracy (Weick 1995, p. 55) conﬂicts
with academic theories and managerial practices that
assume that the accuracy of managers’ perceptions deter-
mine the effectiveness of outcomes. The assumption that
accuracy begets effectiveness builds on a long stream
of research on environmental scanning, strategic plan-
ning, rational choice, and organizational adaptation (e.g.,
Duncan 1972, Pfeffer and Salancik 1978) and persists,
for example, in current theorizing on search and adaptive
learning (e.g., Gavetti and Levinthal 2000) and strate-
gic decision making (e.g., Bukszar 1999).

However, studies assessing the accuracy of manager’s
perceptions are rare (see Sutcliffe 1994, Starbuck and
Mezias 1996 for exceptions), and those studies that
have been done suggest that managers’ perceptions are
highly inaccurate (Mezias and Starbuck 2003). This may
explain why some scholars propose that the key prob-
lem for an organization is not to accurately assess scarce
data, but to interpret an abundance of data into “action-
able knowledge” (Bettis and Prahalad 1995). These cri-
tiques have raised the question of the relative importance
and role of executives’ perceptual inputs relative to their
interpretations of these inputs. Kruglanski (1989) argues,
for example, that perceptual accuracy should be treated
as pragmatic utility, judged only by its usefulness for
beneﬁcial action.

A focus on perceptual accuracy is grounded in models
of rational decision making: A given problem is evalu-
ated in relation to stable goals and a course of action
chosen from a set of alternatives. In this model, accurate
information is important in evaluating the feasibility and
utility of alternative actions, and accurate perceptions
increase decision quality. However, actual organizations
do not ﬁt this conception. Problems must be brack-
eted from an amorphous stream of experience and be
labeled as relevant before ongoing action can be focused
on them. Furthermore, managers with limited attention
face many such issues at the same time, often eval-
uating several situations, interpretations, choices, and
actions simultaneously. Thus, inaccurate perceptions are
not necessarily a bad thing, as Mezias and Starbuck
(2003) conclude. People do not need to perceive the
current situation or problems accurately to solve them;
they can act effectively simply by making sense of cir-
cumstances in ways that appear to move toward gen-
eral long-term goals. Managerial misperceptions may
not curtail effective performance if agents have learning
mechanisms and operate in a context where there are
incentives to improve performance (Mezias and Starbuck
2003, p. 15; Winter 2003, p. 42).

The important message is that if plausible stories keep
things moving, they are salutary. Action-taking generates

416

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

new data and creates opportunities for dialogue, bargain-
ing, negotiation, and persuasion that enriches the sense
of what is going on (Sutcliffe 2000). Actions enable peo-
ple to assess causal beliefs that subsequently lead to new
actions undertaken to test the newly asserted relation-
ships. Over time, as supporting evidence mounts, signif-
icant changes in beliefs and actions evolve.

Identity and Sensemaking
Identity construction is seen by many to be one of the
two basic properties that differentiate sensemaking from
basic cognitive psychology (Gililand and Day 2000,
p. 334). The other property is the use of plausibility as
the fundamental criterion of sensemaking. Mills (2003)
made a similar point when she organized her study of
culture change at Nova Scotia Power around identity
construction, which “is at the root of sensemaking and
inﬂuences how other aspects, or properties of the sense-
making process are understood” (Mills 2003, p. 55).

Discussions of organizational

identity tend to be
anchored by Albert and Whetten’s (1985) description of
identity as that which is core, distinctive, and enduring
about the character of the organization. From the per-
spective of sensemaking, who we think we are (identity)
as organizational actors shapes what we enact and how
we interpret, which affects what outsiders think we are
(image) and how they treat us, which stabilizes or desta-
bilizes our identity. Who we are lies importantly in the
hands of others, which means our categories for sense-
making lie in their hands. If their images of us change,
our identities may be destabilized and our receptiveness
to new meanings increase. Sensemaking, ﬁltered through
issues of identity, is shaped by the recipe “how can I
know who we are becoming until I see what they say
and do with our actions?”

The pathway from image change to identity change is
demonstrated in Gioia and Thomas (1996). Their work
suggests that if managers can change the images that
outsiders send back to the organization, and if insiders
use those images to make sense of what their actions
mean, then these changes in image will serve as a cat-
alyst for reﬂection and redrafting of how the organiza-
tion deﬁnes itself. The controversy implicit in Gioia and
Thomas’s ﬁndings is the suggestion that identity may not
be nearly as enduring as ﬁrst thought, and may be more
usefully conceptualized as a variable, mutable continuity
(Gioia et al. 2000). If this were found to be the case,
then identity would turn out to be an issue of plausibility
rather than accuracy, just as is the case for many issues
that involve organizing and sensemaking.

Gioia and Chittipeddi (1991) set the stage for many
of the current concerns with identity and image in their
early ﬁnding that sensemaking is incomplete unless there
is sensegiving, a sensemaking variant undertaken to cre-
ate meanings for a target audience. The reﬁnement of
this demonstration is the ﬁnding that
the content of

sensegiving (present versus future image) and the tar-
get (insider versus outsider) affect how people interpret
the actions they confront. Yet to be examined is the
effect of efforts at sensegiving on the sensemakers. In
the sensemaking recipe “how can I know what I think
until I see what I say?” sensegiving corresponds to the
saying. However, notice that the saying is problematic,
you do not really know what you think until you do say
it. When you hear yourself talk, you see more clearly
what matters and what you had hoped to say. Sensegiv-
ing therefore may affect the sensemaker as well as the
target. For example, in Gioia and Chittipeddi’s study,
those administrators trying to move a university’s iden-
tity and image into the category “top 10 university” may
themselves have thought differently about this issue as
they articulated their campaign to improve the univer-
sity’s reputation.

It is clear that the stakes in sensemaking are high
when issues of identity are involved. When people face
an unsettling difference, that difference often translates
into questions such as who are we, what are we doing,
what matters, and why does it matter? These are not
trivial questions. As Coopey et al. (1997, p. 312, cited
in Brown 2000) note,

Faced with events that disrupt normal expectations and,
hence, the efﬁcacy of established patterns of meaning and
associated behavior, individuals attempt to make sense
of ambiguous stimuli in ways that respond to their own
identity needs. They are able to draw creatively on their
memory—especially their personal experience—in com-
posing a story that begins to make sense of what is
happening while potentially enhancing their feelings of
self-esteem and self-efﬁcacy. The story is a sufﬁciently
plausible account of “what is happening out there?” that
it can serve as a landscape within which they and others
might be able to make commitments and to act in ways
that serve to establish new meanings and new patterns of
behavior.

The outcomes of such processes, however, are not
always sanguine. This was the case in Bristol Royal
Inﬁrmary’s (BRI) continuation of a pediatric cardiac
surgery program for almost 14 years in the face of
data showing a mortality rate roughly double the rate
of any other center in England (Weick and Sutcliffe
2003, p. 76). The board of inquiry that investigated this
incident concluded that there was a prevailing mindset
among people at BRI that enabled them to “wish away
their poor results” as a “run of bad luck” even though
“there was evidence sufﬁcient to put the Unit on notice
that there were questions to be answered as regards the
adequacy of the service” (Kennedy 2001, pp. 247–248).
That mindset prevailed partly because surgeons con-
structed their identity as that of people learning complex
surgical procedures in the context of unusually challeng-
ing cases. The dangerous omission in this identity was
that the resources they used for learning were minimal.

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

They did not collect detailed data about their own prior
performance, solicit input from other members of the
surgical team, or observe the work of other surgeons
who were more skilled at this procedure until formal
complaints were ﬁled against pediatric surgeons.

The Nature of Organized Sensemaking:
Viewed Prospectively
Considering the modest amount of empirical work on
sensemaking that has accumulated so far, the question
of “future directions” pretty much takes care of itself.
Almost any kind of work is likely to enhance our under-
standing of a largely invisible, taken-for-granted social
process that is woven into communication and activ-
ity in ways that seem to mimic Darwinian evolution.
We brieﬂy discuss institutionalization, distributed sense-
making, power, and emotion to illustrate a few of the
many ways in which present thinking about sensemaking
might be enhanced.

Sensemaking and Institutional Theory
We have treated organizing as activity that provides a
more ordered social reality by reducing equivocality.
A crucial question is whether that reality gets renego-
tiated in every social interaction or whether, as Zucker
(1983) puts it, “institutionalization simply constructs the
way things are: alternatives may be literally unthinkable”
(p. 5). The tension inherent in these otherwise “cool”
positions is evident when Czarniawska (2003, p. 134)
observes that “Intentional action never leads to intended
results, simply because there is a lot of intentional action
directed at different aims in each time and place. Insti-
tutionalization, like power, is a post factum description
of the resultant of all those efforts combined with the
random events that accompanied them.”

Discussions of sensemaking often include words
like “construct,” “enact,” “generate,” “create,” “invent,”
“imagine,” “originate,” and “devise.” Less often do we
ﬁnd words like “react,” “discover,” “detect,” “become
aware of,” or “comply with.” This asymmetry suggests
that people who talk about sensemaking may exagger-
ate agency and may be reluctant to assume that peo-
ple internalize and adopt whatever is handed to them,
as Zucker suggests. An example of such exaggeration
might be the statement, “sensemaking is the feedstock
for institutionalization” (Weick 1995, p. 36). Institution-
alists might well argue that the causal arrow in this
assertion points in the wrong direction. The causal arrow
neglects evidence showing that organizational members
are socialized (indoctrinated) into expected sensemak-
ing activities and that ﬁrm behavior is shaped by broad
cognitive, normative, and regulatory forces that derive
from and are enforced by powerful actors such as mass
media, governmental agencies, professions, and interest
groups (Lounsbury and Glynn 2001). In other words,

417

“no organization can properly be understood apart from
its wider social and cultural context” (Scott 1995,
p. 151).

These diverse positions can begin to be reconciled
if we focus on mechanisms that link micro-macro lev-
els of analysis and if we pay as much attention to
structuring and conversing as we do to structures and
texts. One way to further such reconciliation is to fol-
low the lead of Hedstrom and Swedberg (1998), who
argue that when we want to explain change and vari-
ation at the macrolevel of analysis, we need to show
“how macro states at one point in time inﬂuence the
behavior of individual actors, and how these actions
generate new macro states at a later time” (p. 21).
Sensemaking can provide micromechanisms that link
macrostates across time through explication of cognitive
structures associated with mimetic processes, agency,
the mobilization of resistance, alternatives to confor-
mity such as independence, anticonformity, and unifor-
mity (Weick 1979, p. 115), and ways in which ongoing
interaction generates the taken for granted. Examples of
such mechanisms are found in Elsbach’s (2002) descrip-
tion of institutions within organizations and in descrip-
tions of “conventions” in the French Convention School
of institutionalists’ thought (Storpor and Salais 1997,
pp. 15–43).

this (see the important

The juxtaposition of sensemaking and institutional-
ism has been rare, but there are recent efforts to cor-
rect
integration proposed by
Jennings and Greenwood 2003). For example, Klaus
Weber’s (2003) study of globalization and convergence
speciﬁcally connects the sensemaking and macroinstitu-
tional perspectives. Weber focuses on the content rather
than the process of sensemaking. He argued that the
media provides corporate vocabularies, and that cor-
porate social structures direct the distribution of these
vocabularies among actors. His ﬁndings suggest
that
while institutions in the form of public discourse deﬁne
and impose the problems to which corporate actors
respond, those public institutions do not appear to direct
the solutions. Thus, public discourse appears to direct
corporate attention, set agendas, and frame issues, but it
is less critical for supplying response repertoires. Weber
concludes that the relationship between institutions and
corporate sensemaking is not linear; the use of corpo-
rate sensemaking vocabularies tends to be triggered by
institutions, but institutions have less inﬂuence over what
happens subsequent to triggering.

Distributed Sensemaking
The rhetoric of “shared understanding,” “common
sense,” and “consensus,” is commonplace in discussions
of organized sensemaking. However, the haunting ques-
tions remain: Are shared beliefs a necessary condition
for organized action (Lant 2002, p. 355), and is the con-
struct of collective belief theoretically meaningful (Porac

418

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

et al. 2002, p. 593)? The drama associated with such
questions is demonstrated by Hughes et al. (1992) in
their study of reliability in the UK air trafﬁc control
system:

If one looks to see what constitutes this reliability, it can-
not be found in any single element of the system. It is
certainly not to be found in the equipment (cid:1) (cid:1) (cid:1) for a period
of several months during our ﬁeld work it was failing
regularly (cid:1) (cid:1) (cid:1) (cid:1) Nor is it to be found in the rules and proce-
dures, which are a resource for safe operation but which
can never cover every circumstance and condition. Nor is
it to be found in the personnel who, though very highly
skilled, motivated and dedicated, are as prone as peo-
ple everywhere to human error. Rather we believe it is
to be found in the cooperative activities of controllers
across the “totality” of the system, and in particular in the
way that it enforces the active engagement of controllers,
chiefs, and assistants with the material they are using and
with each other (cited in Woods and Cook 2000, p. 164).

Promising lines of development would seem to occur
if work on distributed cognition (Hutchins 1995), heed-
ful interrelating (Weick and Roberts 1993), and variable
disjunction of information2 (Turner 1978, p. 50) were
focused less on the assembling and diffusing of preex-
isting meaning and more on collective induction of new
meaning (see Laughlin and Hollingshead 1995 for labo-
ratory investigations of this issue). When information is
distributed among numerous parties, each with a differ-
ent impression of what is happening, the cost of recon-
ciling these disparate views is high, so discrepancies and
ambiguities in outlook persist. Thus, multiple theories
develop about what is happening and what needs to be
done, people learn to work interdependently despite cou-
plings loosened by the pursuit of diverse theories, and
inductions may be more clearly associated with effec-
tiveness when they provide equivalent rather than shared
meanings.

Sensemaking and Power
Sensemaking strikes some people as naïve with regard
to the red meat of power, politics, and critical theory.
People who are powerful, rich, and advantaged seem to
have unequal access to roles and positions that give them
an unequally strong position to inﬂuence the construc-
tion of social reality (Mills 2003, p. 153). Sensemaking
discussions do tend to assume that meanings survive as
a result of voting (e.g., Weick 1995, p. 6), with the pro-
viso that sometimes the votes are weighted equally and
sometimes they are not.

Enhancements of sensemaking that pay more attention
to power will tend to tackle questions such as how does
power get expressed, increase, decrease, and inﬂuence
others? Preliminary answers are that power is expressed
in acts that shape what people accept, take for granted,
and reject (Pfeffer 1981). How does such shaping occur?
Through things like control over cues, who talks to

whom, proffered identities, criteria for plausible stories,
actions permitted and disallowed, and histories and ret-
rospect that are singled out. To shape hearts and minds
is to inﬂuence at least seven dimensions of sensemak-
ing: the social relations that are encouraged and dis-
couraged, the identities that are valued or derogated, the
retrospective meanings that are accepted or discredited,
the cues that are highlighted or suppressed, the updat-
ing that is encouraged or discouraged, the standard of
accuracy or plausibility to which conjectures are held,
and the approval of proactive or reactive action as the
preferred mode of coping.

Sensemaking and Emotion
Magala (1997, p. 324) argued that perhaps the most
important lost opportunity in the 1995 book Sensemak-
ing in Organizations was fuller development of a theory
of organizational sentiments. Such a theory was “hinted
at but ignored.” The opening for further development of
emotional sensemaking was the property that projects
are ongoing, and when interrupted generate either neg-
ative emotions when resumption is thwarted or positive
emotions when resumption is facilitated. If emotion is
restricted to events that are accompanied by autonomic
nervous system arousal (Berscheid and Ammazzalorso
2003, p. 312; Schachter and Singer 1962), if the detec-
tion of discrepancy provides the occasion for arousal
(Mandler 1997), and if arousal combines with a posi-
tive or negative valenced cognitive evaluation of a sit-
uation (e.g., a threat to well-being or an opportunity to
enhance well-being), then sensemaking in organizations
will often occur amidst intense emotional experience.
Consider the case of high task interdependence. As the
interdependent partners “learn more about each other
and move toward closeness by becoming increasingly
dependent on each other’s activities for the performance
of their daily behavioral routines and the fulﬁllment of
their plans and goals, the number and strength of their
expectancies about each other increase. As a result, their
opportunities for expectancy violation, and for emotional
experience also increase” (Berscheid and Ammazzalorso
2003, p. 317). When an important expectancy is vio-
lated, the partner becomes less familiar, less safe, and
more of a stranger. In the face of an emotional outburst,
people often ask in disbelief “what did I do?!” That is
the wrong question. The better question is “what did you
expect” (Berscheid and Ammazzalorso 2003, p. 318)?
Expectations hold people hostage to their relationships
in the sense that each expectancy can be violated, and
generates a discrepancy, an emotion, and a valenced
interpretation. If I expect little, there is little chance
for discrepancy and little chance for emotion. However,
when “an outside event produces negative emotion for an
individual in a close relationship, the individual’s part-
ner may be less likely to remain tranquil and supportive
than a superﬁcial partner might be because the partner

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

419

is likely to be experiencing emotion him or herself; the
partner’s emotional state, in turn, may interfere with the
partner’s ability to perform as the individual expects”
(Berscheid and Ammazzalorso 2003, p. 324).

Further exploration of emotion and sensemaking is
crucial to clear up questions such as whether intraorga-
nizational institutions are better portrayed as cold cog-
nitive scripts built around rules or as hot emotional
attitudes built around values (Elsbach 2002, p. 52).

Conclusions
To deal with ambiguity, interdependent people search
for meaning, settle for plausibility, and move on. These
are moments of sensemaking, and scholars stretch those
moments, scrutinize them, and name them in the belief
that they affect how action gets routinized, ﬂux gets
tamed, objects get enacted, and precedents get set. Work
to date suggests that the study of sensemaking is use-
ful for organizational studies because it ﬁlls several
gaps. Analyses of sensemaking provide (1) a micro-
mechanism that produces macro-change over
time;
(2) a reminder that action is always just a tiny bit ahead
of cognition, meaning that we act our way into belated
understanding; (3) explication of predecisional activities;
(4) description of one means by which agency alters
institutions and environments (enactment); (5) opportu-
nities to incorporate meaning and mind into organiza-
tional theory; (6) counterpoint to the sharp split between
thinking and action that often gets invoked in explana-
tions of organizational life (e.g., planners versus doers);
(7) background for an attention-based view of the ﬁrm;
(8) a balance between prospect in the form of anticipa-
tion and retrospect in the form of resilience; (9) reinter-
pretation of breakdowns as occasions for learning rather
than as threats to efﬁciency; and (10) grounds to treat
plausibility, incrementalism, improvisation, and bounded
rationality as sufﬁcient to guide goal-directed behavior.
Analyses of sensemaking also suggest important capa-
bilities and skills that warrant attention and development.
For example, the concept of enacted environments sug-
gests that constraints are partly of one’s own making and
not simply objects to which one reacts; the concept of
sensemaking suggests that plausibility rather than accu-
racy is the ongoing standard that guides learning; the
concept of action suggests that it is more important to
keep going than to pause, because the ﬂow of experience
in which action is embedded does not pause; and, the
concept of retrospect suggests that so-called stimuli for
action such as diagnoses, plans for implementation, and
strategies are as much the products of action as they are
prods to action.

Taken together, these properties suggest that increased
skill at sensemaking should occur when people are
socialized to make do, be resilient, treat constraints as
self-imposed, strive for plausibility, keep showing up,

use retrospect to get a sense of direction, and articulate
descriptions that energize. These are micro-level actions.
They are small actions, but they are small actions with
large consequences.

Acknowledgments
The authors thank two anonymous reviewers, Senior Editor
Alan Meyer, and Gary Klein for constructive comments on
previous versions of this paper.

Endnotes
1The terms “open ductus” and “complications of the patent
ductus” referenced by the nurse in her description refer to a
condition formally known as patent ductus arteriosus. Patent
ductus arteriosus is a condition where the ductus arteriosus,
a blood vessel that allows blood to bypass the baby’s lungs
before birth, fails to close after birth. The word “patent” means
open. If the patent ductus is not closed, the infant is at risk of
developing heart failure or a heart infection.
2“(cid:1) (cid:1) (cid:1) a complex situation in which a number of parties han-
dling a problem are unable to obtain precisely the same
information about the problem so that many differing interpre-
tations of the problem exist” (Turner 1978, p. 50).

References
Albert, S., D. Whetten. 1985. Organizational identity. L. L. Cummings,
B. M. Staw, eds. Research in Organizational Behavior, Vol. 7.
JAI Press, Greenwich, CT, 263–295.

Aldrich, H. 1999. Organizations Evolving. Sage, Thousand Oaks, CA.

Baum, J. A. C., J. V. Singh. 1994. Evolutionary Dynamics of Orga-

nizations. Oxford University Press, Oxford, UK.

Benner, P. 1994. The role of articulation in understanding practices
and experience as sources of knowledge in clinical nursing.
J. Tully, ed. Philosophy In An Age of Pluralism: The Philoso-
phy of Charles Taylor In Question. Cambridge University Press,
New York, 136–155.

Berscheid, E., H. Ammazzalorso. 2003. Emotional experience in close
relationships. G. J. Fletcher, M. S. Clark, eds. Blackwell Hand-
book of Social Psychology: Interpersonal Process. Blackwell,
Malden, MA, 308–330.

Bettis, R. A., C. K. Prahalad. 1995. The dominant logic: Retrospective

and extension. Strategic Management J. 16 5–14.

Boisot, M., J. Child. 1999. Organizations as adaptive systems in
complex environments: The case of China. Organ. Sci. 10(3)
237–252.

Brown, A. 2000. Making sense of inquiry sensemaking. J. Manage-

ment Stud. 37 45–75.

Bukszar, E., Jr. 1999. Strategic bias: The impact of cognitive biases

on strategy. Canadian J. Admin. Sci. 16 105–117.

Campbell, D. T. 1965. Variation and selective retention in socio-
cultural evolution. H. R. Barringer, G. I. Blanksten, R. Mack, eds.
Social Change in Developing Areas. Schenkman, Cambridge,
MA, 19–49.

Campbell, D. T. 1997. From evolutionary epistemology via selection
theory to a sociology of scientiﬁc validity. Evolution Cognition
3(1) 5–38.

Chia, R. 2000. Discourse analysis as organizational analysis. Organi-

zation 7(3) 513–518.

420

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

Czarniawska, B. 2003. Constructionism and organization studies.
R. I. Westwood, S. Clegg, eds. Debating Organization: Point-
Counterpoint in Organization Studies. Blackwell, Malden, MA,
128–139.

Duncan, R. B. 1972. Characteristics of organizational environments

and perceived uncertainty. Admin. Sci. Quart. 17 313–327.

Dutton, J. E. 1993. The making of organizational opportunities: An
interpretive pathway to organizational change. L. L. Cummings,
B. M. Staw, eds. Research in Organizational Behavior, Vol. 15.
JAI Press, Greenwich, CT, 195–226.

Elsbach, K. D. 2002. Intraorganizational institutions. J. A. C. Baum,
ed. The Blackwell Companion to Organizations. Blackwell,
Malden, MA, 37–57.

Gavetti, G., D. Levinthal. 2000. Looking forward and looking back-
ward: Cognitive and experiential search. Admin. Sci. Quart. 45
113–137.

Gililand, S. W., D. V. Day. 2000. Business management. F. T.
Durso, ed. Handbook of Applied Cognition. Wiley, New York,
315–342.

Gioia, D. A., K. Chittipeddi. 1991. Sensemaking and sensegiv-
ing in strategic change initiation. Strategic Management J. 12
433–448.

Gioia, D. A., A. Mehra. 1996. Book review: Sensemaking in Organi-

zations. Acad. Management Rev. 21(4) 1226–1230.

Gioia, D. A., J. B. Thomas. 1996. Identity, image, and issue interpreta-
tion: Sensemaking during strategic change in academia. Admin.
Sci. Quart. 41 370–403.

Gioia, D. A., M. Schultz, K. Corley. 2000. Organizational

iden-
tity, image, and adaptive instability. Acad. Management Rev. 25
63–81.

Gioia, D. A., J. B. Thomas, S. M. Clark, K. Chittipeddi. 1994.
Symbolism and strategic change in academia: The dynamics of
sensemaking and inﬂuence. Organ. Sci. 5 363–383.

Hedström, P., R. Swedberg. 1998. Social mechanisms: An introduc-
tory essay. P. Hedström, R. Swedberg, eds. Social Mechanisms:
An Analytical Approach to Social Theory, Ch. 1. Cambridge
University Press, Cambridge, UK, 1–31.

Hughes, J., D. Randall, D. Shapiro. 1992. Faltering from ethnography
to design. Comput. Supported Cooperative Work (CSCW) Proc.,
Toronto, Ontario, Canada, 1–8.

Hutchins, E. 1995. Cognition in the Wild. MIT Press, Cambridge,

MA.

Jennings, P. D., R. Greenwood. 2003. Constructing the iron cage:
Institutional
theory and enactment. R. Westwood, S. Clegg,
eds. Debating Organization: Point-Counterpoint in Organization
Studies. Blackwell, Malden, MA, 195–207.

Kennedy, I. 2001. Learning from Bristol: The Report of the Public
Inquiry into Children’s Heart Surgery at Bristol Royal Inﬁrmary
1984–1995. Her Majesty’s Stationer, London, UK.

Klein, G., J. K. Phillips, E. L. Rall, D. A. Peluso. In press. A
data/frame theory of sensemaking. Unpublished manuscript.
R. R. Hoffman, ed. Expertise Out of Context: Proc. 6th Internat.
Conf. Naturalistic Decision Making. Erlbaum, Mahwah, NJ.

Kruglanski, A. W. 1989. The psychology of being “right”: The prob-
lem of accuracy in social perception and cognition. Psych. Bull.
106 395–409.

Lanir, Z. 1989. The reasonable choice of disaster—The shooting down
of the Libyan airliner on 21 February 1973. J. Strategic Stud.
12 479–493.

Lant, T. K. 2002. Organizational cognition and interpretation.
J. A. C. Baum, ed. The Blackwell Companion to Organizations.
Blackwell, Oxford, UK, 344–362.

Laroche, H. 1995. From decision to action in organizations: Decision-

making as a social representation. Organ. Sci. 6(1) 62–75.

Laughlin, P. R., A. B. Hollingshead. 1995. A theory of collec-
tive induction. Organ. Behavior Human Decision Processes 61
94–107.

Louis, M. R. 1980. Surprise and sensemaking: What newcomers expe-
rience in entering unfamiliar organizational settings. Admin. Sci.
Quart. 25 226–251.

Lounsbury, M., M. A. Glynn. 2001. Cultural entrepreneurship:
Stories, legitimacy, and the acquisition of resources. Strategic
Management J. 22(6) 545–564.

Magala, S. J. 1997. The making and unmaking of sense. Organ. Stud.

18(2) 317–338.

Mandler, G. 1984. Mind and Body. Free Press, New York.
Mandler, G. 1997. Human Nature Explored. Oxford, New York.
Mezias, J. M., W. H. Starbuck. 2003. Managers and their inaccurate
perceptions: Good, bad or inconsequential? British J. Manage-
ment 14(1) 3–19.

Mills,

J. H. 2003. Making Sense of Organizational Change.

Routledge, London, UK.

Obstfeld, D. 2004. Saying more and less of what we know: The
social processes of knowledge creation, innovation, and agency.
Unpublished manuscript, University of California-Irvine, Irvine,
CA.

Ocasio, W. 2001. How do organizations think? T. K. Lant, Z. Shapira,
eds. Organizational Cognition: Computation and Interpretation.
Erlbaum, Mahwah, NJ, 39–60.

Orlikowski, W. J., D. C. Gash. 1994. Technological frames: Making
sense of information technology in organizations. ACM Trans.
Inform. Systems 2 174–207.

Orton, J. D. 2000. Enactment, sensemaking, and decision making:
Redesign processes in the 1976 reorganization of US intelli-
gence. J. Management Stud. 37 213–234.

Paget, M. A. 1988. The Unity of Mistakes. Temple University Press,

Philadelphia, PA.

Patriotta, G. 2003. Sensemaking on the shop ﬂoor: Narratives
of knowledge in organizations. J. Management Stud. 40(2)
349–376.

Pfeffer, J. 1981. Power in Organizations. Pitman, Marshﬁeld, MA.
Pfeffer, J., G. R. Salancik. 1978. The External Control of Organi-
zations: A Resource Dependence Perspective. Harper and Row,
New York.

Porac, J. F., M. J. Ventresca, Y. Mishina. 2002. Interorganizational
cognition and interpretation. J. A. C. Baum, ed. The Black-
well Companion to Organizations. Blackwell, Malden, MA,
579–598.

Schachter, S., J. E. Singer. 1962. Cognitive, social, and physiological

determinants of emotional state. Psych. Rev. 69 379–399.

Scott, R. W. 1995. Institutions and Organizations. Sage, Thousand

Oaks, CA.

Snook, S. 2001. Friendly Fire. Princeton University, Princeton, NJ.
Starbuck, W. H., J. Mezias. 1996. Opening Pandora’s box: Studying
the accuracy of managers’ perceptions. J. Organ. Behavior 17
99–117.

Storpor, M., R. Salais. 1997. Worlds of Production: The Action

Frameworks of the Economy. Harvard, Cambridge, MA.

Sutcliffe, K. M. 1994. What executives notice: Accurate perceptions
in top management teams. Acad. Management J. 37 1360–1378.

Weick, Sutcliffe, and Obstfeld: Organizing and the Process of Sensemaking
Organization Science 16(4), pp. 409–421, © 2005 INFORMS

421

Sutcliffe, K. M. 2000. Organizational environments and organiza-
tional information processing. F. M. Jablin, L. L. Putnam, eds.
The New Handbook of Organizational Communication. Sage,
Thousand Oaks, CA, 197–230.

Taylor, J. R., E. J. Van Every. 2000. The Emergent Organization:
Communication as Its Site and Surface. Erlbaum, Mahwah, NJ.
Tsoukas, H., R. Chia. 2002. Organizational becoming: Rethinking

organizational change. Organ. Sci. 13(5) 567–582.

Turner, B. 1978. Man Made Disasters. Wykeham Press, London, UK.
Warglien, M. 2002. Intraorganizational evolution. J. A. C. Baum, ed.
The Blackwell Companion to Organizations. Blackwell, Malden,
MA, 98–118.

Weber, K. 2003. Does globalization lead to convergence? The evo-
lution of organizations’ cultural repertoires in the biomedical
industry. Unpublished dissertation, University of Michigan, Ann
Arbor, MI.

Weick, K. 1969. The Social Psychology of Organizing. Addison-

Wesley, Reading, MA.

Weick, K. E. 1979. The Social Psychology of Organizing, 2nd ed.

Addison-Wesley, Reading, MA.

Weick, K. E. 1993. The collapse of sensemaking in organizations:

The Mann Gulch disaster. Admin. Sci. Quart. 38 628–652.

Weick, K. E. 1995. Sensemaking in Organizations. Sage, Thousand

Oaks, CA.

Weick, K. E., K. H. Roberts. 1993. Collective mind in organiza-
tions: Heedful interrelating on ﬂight decks. Admin. Sci. Quart.
38 357–381.

Weick, K. E., K. M. Sutcliffe. 2001. Managing the Unexpected.

Jossey-Bass, San Francisco, CA.

Weick, K. E., K. M. Sutcliffe. 2003. Hospitals as cultures of entrap-
ment: A re-analysis of the Bristol Royal Inﬁrmary. California
Management Rev. 45(2) 73–84.

Westley, F. R. 1990. Middle managers and strategy: Microdynamics

of inclusion. Strategic Management J. 11 337–351.

Westrum, R. 1982. Social intelligence about hidden events. Knowl-

edge 3 381–400.

Winter, S. 1987. Knowledge and competence as strategic assets.
D. Teece, ed. The Competitive Challenge—Strategies for Indus-
Innovation and Renewal. Bollinger, Cambridge, MA,
trial
159–184.

Winter, S. 2003. Mistaken perceptions: Cases and consequences.

British J. Management 14 39–45.

Woods, D. D., R. I. Cook. 2000. Perspectives on human error: Hind-
sight biases and local rationality. F. T. Durso, ed. Handbook of
Applied Cognition. Wiley, New York, 141–172.

Zucker, L. G. 1983. Organizations as institutions. S. B. Bacharach, ed.
Research in the Sociology of Organizations, Vol. 2. JAI Press,
Greenwich, CT, 1–48.

",False,2011.0,{},False,False,bookSection,False,PPNESMHH,[],self.user,False,False,False,False,http://www.elgaronline.com/view/9781849807623.00024.xml,,Organization Science,PPNESMHH,False,False
2355LVL5,XA4SVP3L,Parsing Error,False,2018.0,{},False,False,conferencePaper,False,2355LVL5,[],self.user,False,False,False,False,http://dl.acm.org/citation.cfm?doid=3173574.3174209,,A Visual Interaction Framework for Dimensionality Reduction Based Data Exploration,2355LVL5,False,False
9BL2E5F8,ERVZICSJ,"BiSet: Semantic Edge Bundling with Biclusters for Sensemaking

Maoyuan Sun, Peng Mi, Chris North and Naren Ramakrishnan

Fig. 1. An overview of BiSet. Entities are represented in lists.
In the space between each neighboring pair of lists, BiSet adds a
“in-between” layer, displaying edges. BiSet bundles edges based on biclusters and allows users to directly manipulate bundles. The
bundles can reveal task-oriented semantic insights about coordinated relationships. BiSet also applies accumulated highlighting to
entities, bundles and edges to indicate highly shared entities and relationships.

Abstract— Identifying coordinated relationships is an important task in data analytics. For example, an intelligence analyst might
want to discover three suspicious people who all visited the same four cities. Existing techniques that display individual relationships,
such as between lists of entities, require repetitious manual selection and signiﬁcant mental aggregation in cluttered visualizations
to ﬁnd coordinated relationships.
In this paper, we present BiSet, a visual analytics technique to support interactive exploration of
coordinated relationships. In BiSet, we model coordinated relationships as biclusters and algorithmically mine them from a dataset.
Then, we visualize the biclusters in context as bundled edges between sets of related entities. Thus, bundles enable analysts to infer
task-oriented semantic insights about potentially coordinated activities. We make bundles as ﬁrst class objects and add a new layer,
“in-between”, to contain these bundle objects. Based on this, bundles serve to organize entities represented in lists and visually reveal
their membership. Users can interact with edge bundles to organize related entities, and vice versa, for sensemaking purposes. With
a usage scenario, we demonstrate how BiSet supports the exploration of coordinated relationships in text analytics.
Index Terms—Bicluster, coordinated relationship, semantic edge bundling

1 INTRODUCTION
Analysts often face difﬁcult challenges in exploring complex relations
and identifying meaningful ones for sensemaking [39]. Current vi-
sual analysis tools emphasize individual relationships and just display
simple ones. This makes it hard for analysts to see more complex re-
lationships (e.g., coordinated relationship). Coordinated relationships
are grouped relations between sets of entities of different types (e.g.,
three people who all visited the same four cities). Due to the complex-
ity, compared with simple relationship, coordinated relationship needs
more cognitive effort for exploration.

Existing techniques that display individual relationships, such as
between lists of entities, require repetitious manual selection and sig-
niﬁcant mental aggregation in cluttered visualizations to ﬁnd coordi-
nated relationships. For example, Jigsaw [19] provides a List View
to support exploring relationships between lists of entities (e.g., peo-
ple, location, date, organization, etc.). In the List View, Jigsaw applies
visual links between related entities to show their connections and con-
trols the shading of colors for entities to indicate their co-occurrence.
With these visual encodings, in Jigsaw, users can recognize relations

• Maoyuan Sun, Peng Mi, Chris North and Naren Ramakrishnan are all
with the Discovery Analytics Center, Department of Computer Science,
Virginia Tech. E-mail: {smaoyuan | mipeng | north | naren}@cs.vt.edu.

Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of
publication xx Aug. 2015; date of current version 25 Oct. 2015.
For information on obtaining reprints of this article, please send
e-mail to: tvcg@computer.org.

between entities without much effort, but these relations are limited
to simple individual ones (e.g., a person visited three cities). Users
have to repetitiously click entities, visually check and mentally com-
pare their linked entities to identify coordinated relationships. Since
Jigsaw’s List View does not provide clear visual clues on coordinated
relations, users have to manually test all possible entities before they
ﬁnally ﬁnd a meaningful one. This potentially forces users to solve a
combinatorial problem of selection without much support. Thus, due
to deﬁcient clues to direct user selections, tools like Jigsaw have lim-
ited capabilities to support exploring coordinated relationships.

Visual analytics can potentially better support this by computation-
ally ﬁnding complex relationships and revealing them in context. This
enables analysts to see complex relations with other data (e.g., enti-
ties in lists). Speciﬁcally, we can compute coordinated relationships
with biclustering algorithms and display them in context using edge
bundling. In this case, edge bundles can reveal semantic insights from
coordinated relationships, which is meaningful from a task-oriented
perspective. The reason is that edges are bundled using semantic edge
bundling that is based on results of biclustering algorithms, rather than
using spatial edge bundling which is based on spatial proximity to sim-
plify visual representations.

Biclustering algorithms compute coordinated relationships as bi-
clusters. A bicluster can be considered a grouped relationship between
two sets of entities, where each entity in one set is connected with all
in another. Figure 2 shows an example of a bicluster that indicates
a coordinated relation between three people and four locations. It is
clear that a bicluster can bundle edges that link pairs of related entities,
and group entities that belong to the same coordinated set. Biclusters
provide a conceptual format to present coordinated relationships in an

organized manner. To take advantage of this for sensemaking, a ﬁve-
level design framework for bicluster visualizations has been proposed
in [49]. However, existing techniques are inefﬁcient to support explor-
ing coordinated relationships, and few attempt to adapt biclusters to
facilitate this by following the design framework. Thus, it is still chal-
lenging to design a technique that can take advantage of biclusters and
make them usable to support coordinated relationship explorations.

Fig. 2. An example of a bicluster, indicating a coordinated relationship
between three people and four locations. (A) presents all connections
between each pair of related entities from the two domains. (B) shows
the result of bundling edges in this bicluster. (C) demonstrates the result
of both bundling edges and grouping entities in this bicluster.

To address such challenges, we present BiSet, a visual analytics
technique to support interactively exploring coordinated relationships
with biclusters. Our key contributions in this paper are as follows:

1) We formalize coordinated relationships as biclusters and algo-

rithmically mine them from a dataset.

2) We visualize the biclusters in context as bundled edges between
sets of related entities. These bundles enable analysts to infer semantic
insights about potentially coordinated activities.

3) We make bundles as the ﬁrst class objects and add a new layer
“in-between” lists to contain these bundle objects. We allow users to
direct manipulate bundles for organizing entities represented in lists.
4) We apply interactions to both edge bundles and entities for re-
vealing and organizing relevant information in a bidirectional way.
Users can interact with edge bundles to forage and organize relevant
entities and, vice versa, for sensemaking purposes.

5) We present a usage scenario to demonstrate how BiSet can sup-

port the coordinated relationship exploration in text analytics.

2 RELATED WORK
Four key aspects are involved in BiSet: biclustering, list layout, visual
link and edge bundling, which outlines the discussion of related work.

2.1 Biclusters and Bicluster-Chains
Biclustering attempts to ﬁnd both subsets of entities and subsets of
dimensions with the restriction that for each identiﬁed subset of en-
tities, they identically behave within the corresponding subset of di-
mensions [36]. Biclusters are computational results from biclustering
algorithms that identify coordinated relations between two entity sets.
An entity set refers to a set of unique objects from a speciﬁc domain
(e.g., people) extracted from a dataset (e.g., documents).

Relationship between two entity sets. Given two entity sets E and
F, a (binary) relationship R (E, F) between E and F is a subset of
E × F (the Cartesian product of E and F). We say that E is connected
to F. There are different ways to model relationship R in different sce-
narios. In text analytics, R can be determined by word co-occurrence
in documents or semantic meanings identiﬁed with natural language
processing. For example, person X is related to city Y , since they are
mentioned in the same document or based on semantic meanings of
some sentences that indicate person X visited city Y .
Bicluster. We deﬁne a bicluster (E(cid:48), F(cid:48)) on R (E, F) as a set E(cid:48) ⊆ E
and a set F(cid:48) ⊆ F such that E(cid:48) × F(cid:48) ⊆ R. That is, there is a relationship
between each element of E(cid:48) with every element of F(cid:48). We use |E(cid:48)| +
|F(cid:48)| to denote the size of a bicluster (E(cid:48), F(cid:48)) where |E(cid:48)| and |F(cid:48)| are
the cardinality of E(cid:48) and F(cid:48). In addition, bicluster (E(cid:48), F(cid:48)) is thin if
there is only one entity in either E(cid:48) or F(cid:48).

Closed bicluster. A bicluster (E(cid:48),F(cid:48)) is closed if:

(i) For every entity e ∈ E − E(cid:48), there is some entity f ∈ F(cid:48) such that
(ii) For every entity f ∈ F −F(cid:48), there is some entity e ∈ E(cid:48) such that

(e, f ) /∈ R, and
(e, f ) /∈ R.

Algorithms for bicluster mining typically aim to ﬁnd closed biclus-
ters. These algorithms (e.g., CHARM [56] and LCM [51]) function
level-wise with regard to one domain (e.g., E), wherein they attempt to
mine closed biclusters involving one entity of E, then closed biclusters
involving two entities of E, and so on. The key parameter inﬂuencing
such mining is the size of a bicluster in terms of the other domain (e.g.,
F), also referred to as the minimum support threshold. The setting of
this parameter is done heuristically by users; a low threshold will yield
a plethora of biclusters whereas a stringent (high) threshold will yield
few (or no) biclusters. Typically, users begin with a high threshold and
gradually lower it until it yields a sufﬁcient number of biclusters [56].
In this paper, we use CHARM and LCM, although any biclustering
algorithm can be utilized in BiSet.

Biclusters logically aggregate multiple individual relations to form
coordinated sets, so they provide an opportunity to visually bundle
edges between entities. Bicluster-based edge bundles organize edges
in a semantic manner, potentially revealing semantic insights. For ex-
ample, four suspicious people may collude about a terrorist attack,
since they are all related to the same three terrorist organizations. This
is different from spatial edge bundling that bundle edges based on spa-
tial proximity to reduce visual clutter [58].

Fig. 3. An example of a bicluster-chain consisting of two biclusters.
(A) presents all edges between related entities. (B) shows that the two
biclusters connect together as a chain by their shared phone numbers.

Bicluster-chains. Based on shared entities, if there are any, multi-
ple biclusters (consisting of different pairs of domains) can connect to
form bicluster-chains. With compositional mining methods [29, 54],
bicluster-chains can be identiﬁed from a dataset. Figure 3 shows an
example of a bicluster-chain with two biclusters. One shows coordi-
nated relations between three people and four phone numbers, and the
other presents relations between three phone numbers and four loca-
tions. They share three phone numbers. One possible semantic insight
revealed from this chain is: three people may visit the same four cities,
since they called each other via four phone numbers, and phone calls
from three of these numbers were all reported at the four cities.

A ﬁve-level design framework for bicluster visualizations has
been proposed based on ﬁve hierarchical levels of relationships poten-
tially existing in a dataset [49]. Keywords corresponding to the ﬁve
levels are: entity, group, bicluster, chain and schema. Entity-level
relations refer to those between two individual entities, while group-
level relations are relations between one individual entity and a group
of entities. Bicluster-level and chain-level relations represent two lev-
els of coordinated relations: biclusters and bicluster-chains. The lat-
ter is more complex than the former, since a bicluster-chain consists
of multiple biclusters. Schema-level relations indicates database-like
patterns in a dataset, which reveals the overview of a dataset. Relations
in higher levels (e.g., bicluster-level and chain-level) are usually con-
structed based on those in lower levels (e.g., entity-level and group-
level), so relations in lower levels provide a critical support for the
exploration and interpretation of those in higher levels. These ﬁve lev-
els of relations systematically present the space of relationship, which
works as an important guideline for us to follow. Speciﬁcally, it guides
us to identify potential tasks that BiSet needs to support, by consider-
ing the implicit linkage of these ﬁve levels.

2.2 Visualizations for Exploring Coordinated Relations
List and matrix views are two layouts that can potentially support ex-
ploring coordinated relationships. Two corresponding visualization
techniques are parallel coordinates [28] and scatterplot matrix [6]. The
former uses spatial position to present attributes as individual axes at
once, while the latter embeds multiple scatterplots in a matrix layout
that shows all pairwise combinations of attributes [37]. They are use-
ful for discovering correlation, which potentially can be adapted for
exploring coordinated relationship. Brushing is a common interaction
technique in both of them to explore correlation [4, 45] and it helps
users manually ﬁnd correlations by selecting a group of records in an
axis or in a region. However, similar to the problem of Jigsaw’s List
View, users are not clearly directed to know where they should brush.
Moreover, brushing is designed for selecting a group of entities, rather
than an individual one, so it is hard to use this to support users to
delve into detailed information of a coordinated relation. Thus, nei-
ther parallel coordinates nor scatterplot matrix, without any adaption,
can effectively support visual exploration of coordinated relationship.
List (or list view) holds a similar concept to that of parallel coor-
dinates which manages entities in lists by domains. Instead of using
brushing to select a group of entities, in a list view, users usually select
individual entities. ConTour [38] and Jigsaw [19] are two examples
that applied a list layout to support meaningful relations discovery.
The problem with Jigsaw’s List View is that its visual representation
is explicit but interaction is implicit. Different from Jigsaw, ConTour
suffers from the problem of implicit visual representation. ConTour
highlights related entities when users hover an entity. This serves as
explicit visual clues to direct users’ attention to some potential targets.
However, ConTour does not show visual links between related entities.
Just with color changes, it is hard for users to visually discriminate
one coordinated relationship from another, especially when there are
many and some of them overlap each other. ConTour applies recursive
nesting to reduce such visual clutter, but it requires entity duplication,
which may confuse users. BiSet is inspired by the good parts of both
Jigsaw and ConTour, which provides both visual links and visual clues
to guide users for coordinated relationship exploration.

Matrix is a preferred layout for exploring coordinated relationship
(in bicluster-level) that has been well explored in bioinformatics do-
main (e.g., BicAt [3], Bicluster viewer [22], BicOverlapper 2.0 [43],
BiGGEsTS [18], BiVoc [21], Expression Proﬁler [31] and GAP [55]).
By reordering and duplicating rows and columns, biclusters can be vi-
sually revealed in a matrix [21, 22]. However, similar to the problem
of ConTour, row and column duplication in a matrix may cause more
confusion than entity duplication in a list. In addition, interacting with
entities in a matrix is not as easy as that in a list. Thus, compared with
list, matrices are not ﬂexible enough to support coordinated relation-
ship exploration (e.g., drilling down to details of relations), although
it can provide a visual representation for such relationship.

Node-link diagrams in the context of multivariate or heteroge-
neous networks can also be used to visualize coordinated relationships.
For example, PivotPaths [12] applies a modiﬁed node-link diagram to
support relationship exploration in heterogenous networks.
It sepa-
rates a 2D space into three regions to contain entities from different
domain (e.g., authors, articles and keywords). This explicitly sepa-
rates nodes in a node-link diagram into different groups based on do-
mains, which is similar to a list view. Entities in the middle region
are horizontally aligned. Edges are presented to show connections be-
tween entities in two neighboring domains. By following edges, users
can manually discover coordinated relationships in PivotPaths (e.g.,
ﬁnding three co-authors of four papers with the same two keywords).
OnionGraph [44] used a similar visual representation to show bibli-
ographic networks. Pretorius et al. also applied a similar layout to
show structures of multivariate graphs and edge labels [40]. In their
method, edge labels are listed in the middle region. Related entities are
highlighted when users select these labels. This is a good example of
enabling interactivity on edges for revealing relevant nodes. However,
their method lacks the ability to manage nodes via interactions on the
labels. Related nodes are highlighted but they do not move close to
the selected label. Thus, users still have to navigate in a graph (e.g.,

scrolling up and down) to ﬁnd highlighted nodes, if the graph is large.
Hybrid layout combines two or more layouts together. Usually it
uses a list or a node-link diagram as the basic layout and replaces enti-
ties with matrices (or other types of visualizations). Matchmaker [34]
and VisBrick [33] took a list view as the major layout, while Bixplorer
[14], Furby [47] and NodeTrix [23] applied a node-link diagram as
the key layout. The former group organizes relations in lists and the
latter group uses a 2D space to manage relations. Since entities are re-
placed with relations, it is difﬁcult to further explore detailed informa-
tion (e.g., entities and entity-level relations) to interpret a coordinated
relationship. This may be even harder here than using a matrix layout.
In BiSet, we choose list as the key layout. The detailed rationale for

our decision is discussed in Section 3.2.

2.3 Visual Links and Edge Bundling
Visual links (e.g., edges in graphs) are important for assisting visual
navigation [24] and indicate certain types of relations (e.g., causality
[57]). By following links, users can navigate their foci from one part
of a visualization to another, or across different visualizations [52] or
applications [53]. This helps to direct users to potentially related con-
tent for comparison and evaluation [9, 46] or assist users to explore
visually hidden (or being covered) content [16]. However, cases are
not always optimistic. If too many edges exist, a visual layout (e.g.,
graph) will become a hairball of visual clutter [37]. Edge bundling is a
useful technique to reduce visual clutter and reveal high level edge pat-
terns by visually aggregating edges based on certain rules (e.g., force-
directed model [26], image-based rule [50], geometry-based rule [11],
etc.). However, there are two major problems with these edge bundling
techniques: 1) spatial-based bundling in the visual level (losing rela-
tions in the data level), and 2) lack of interactions on edge bundles.

Traditional edge bundling techniques simply group edges based on
spatial proximity (e.g., the position of nodes or edges), which may ig-
nore some implicit relations in a dataset. Since visual adjacency is
determined by layout algorithms (e.g., force-directed layout), rather
than knowledge discovery algorithms (e.g., biclustering), bundling vi-
sually adjacent edges does not guarantee that a bundle of these indi-
vidual relations reﬂect meaningful semantic insights from the dataset.
To deal with such problems, a hierarchical edge bundling technique is
proposed in [25] that bundles adjacent edges by considering the hier-
archical relations in a dataset. However, compared with coordinated
relationships, hierarchical relationships are relatively simple because
they can not reveal high level semantic insights implied by the co-
ordination of individual relationships. Despite this, the hierarchical
edge bundling technique inspires our design of BiSet that bundle edges
based on coordinated relationship. This potentially enables users to in-
fer semantic insights from these edge bundles.

Deﬁcient interaction on edge bundles is another problem with exist-
ing edge bundling techniques. Such bundles have limited capabilities
to support users exploring the space of relationship, although they help
to reduce visual clutter. This partially results from the previous prob-
lem since edge bundles depend on the layout of nodes. If positions of
nodes change, the existing bundles may also change. This means that
these edge bundles are not stable. In BiSet, we map algorithmically
discovered coordinated relationships to edge bundles. This assures
that each bundle visually presents a certain coordinated relationship.
Thus, bundles in BiSet are more independent from the layout of nodes
than existing techniques. Based on this, BiSet allows interactions on
edge bundles which enables users to manipulate bundles to forage re-
lated information, and organize spatializations for synthesis [2].

3 DESIGN REQUIREMENT ANALYSIS
3.1 A Design Trade-off
Figure 2 shows that a bicluster can both bundle connections and group
entities and this can be clearly conveyed in a list view. However, cases
are not always that easy. When certain entities belong to more than
one bicluster, it becomes difﬁcult to visually group all entities of the
same bicluster together. Figure 4 shows an example of this case. There
are three biclusters indicating three different coordinated relations be-
tween people and locations. They share some entities (e.g., P1 and

to show connections between the newly added entities and those from
one domain of the existing matrix. This leads to two problems: entity
duplication and direction (row or column) selection. To build the new
matrix, entities from one domain of the existing matrix have to be du-
plicated to form either row names or column names in the new matrix.
If replicated entities work as the row names in the new matrix, newly
added ones will be the column names, and vice versa. Compared with
matrices, in a list whenever a group of entities are to be added or re-
moved, we can easily add or remove one list. Thus, compared with the
other two layouts, lists organize entities in a consistent manner.

P2 are associated with both bicluster A and bicluster B). This brings
about an Euler diagram problem [1, 42] when visually grouping enti-
ties. When the number of shared entities increases, it becomes more
difﬁcult to present a visual representation that show the membership
of these entities without replicating some of them. This suggests a
key design trade-off: entity-centric versus relationship-centric, which
means that we cannot easily achieve the goal of clearly presenting both
entities (without duplications) and relationships (without separations).
Techniques such as bubble sets [10] and untangling Euler diagrams
[41] attempt to balance this trade-off by using a 2D space. They show
relationships with their members in a calculated spatial layout with
certain boundaries. However, they do not scale up well. In a list, there
is just one dimension to use for organizing entities, so it is even harder
to balance this trade-off.

Entity-centric requires that entities that belong to a certain domain
(e.g., people in Figure 4) should be listed in a certain order without
duplication. The positions of entities can be reordered to fulﬁll some
purpose (e.g., listing names in an alphabetical order or ranking them
based on frequency in documents). Since entities cannot be duplicated,
relationships that consist of shared entities may be separated apart.
Thus, an entity-centric design can help to avoid the ambiguity caused
by entity replication, but it costs the completeness of relationships.

Fig. 5. A 2D space is sliced into two types of subspaces in a list view.

Organized alternate subspaces for entities and relationships. In
a list view, a 2D space is sliced into two types of subspaces, where en-
tities and relationships appear alternately. Figure 5 gives an example
of such slicing that generates three subspaces for entities and two sub-
spaces for relationships. This provides an opportunity to leverage the
design trade-off by using the relationship subspace. In node-link dia-
grams, the space of entities is intertwined with that of relations so there
is no clear boundary between the two spaces. Matrices potentially em-
phasize the space of relationships (e.g., the structure of a dataset) [17],
so it is difﬁcult to support simple relationship (e.g., entity-level or
group-level relations) exploration in a matrix. Visually, in a matrix,
the proportion of the space for relations (the total area of all cells in
a matrix) is larger than that for entities (one column and one row in
the matrix) and the ratio of the two increases when the size of a matrix
gets larger. Thus, compared with the other two layouts, lists slice a 2D
space for entities and relations in a clearly organized and usable way.

3.3 Requirements Description with Identiﬁed Tasks
Based on the design trade-off and the selected layout, we identify four
important requirements with key tasks that BiSet needs to support.

R1: Entity and relationship encodings. Efﬁcient visual encodings
for both entity and relationship are necessary which should attempt
to achieve four important goals. First, visual encodings should assist
users to discriminate entities from relations (a). Then visual encodings
(particularly for relationships) should potentially help to reduce visual
clutter (b). Third, visual encodings should provide clues to reveal the
membership of entities (c). Finally, visual encodings should reﬂect the
changes when the state of entities or relations updates (d).

R2: Four types of exploration. There are four possible types of
exploration in a list layout based on the space slicing: i) from entity to
entity, ii) from entity to relationship, iii) from relationship to relation-
ship, and iv) from relationship to entity. The ﬁrst type of exploration
refers to when users start from entities and focus on ﬁnding related
entities. The second one may happen in the scenario where users take
the strategy of following the clues from identiﬁed meaningful entities
[30] and search for more relevant information. The third type of ex-
ploration may be performed when users seek additional relationships
based on current one. This is a possible case for text analytics which
has been reported in [48]. The last one may be used to compare sev-
eral hypotheses, which has been identiﬁed as a common intelligence
analysis strategy [7]. For example, several biclusters share some indi-
vidual relations, but they still have their unique ones. This may form
(partially) conﬂicted relations that lead to different hypotheses. In this

Fig. 4. A detailed example to illustrate the Euler diagram problem that
arises when visually presenting the membership of entities in the do-
main of people shared by three biclusters. This problem indicates a key
design trade-off: entity-centric versus relationship-centric.

Relationship-centric requires that entities that belong to the same
relationships (e.g., biclusters) should be placed near each other, which
visually preserves the completeness of relationships. To achieve this,
entities may be duplicated, particularly when several relationships
share two or more entities. A relationship-centric design (e.g., bub-
ble sets technique [10]) can maintain the integrity of relationships with
the cost of entity duplication. This may confuse users, especially when
they see some entities appear at several different positions in a list.
3.2 Layout Candidate Selection
Three layouts can be potentially applied to show coordinated relation-
ships: node-link diagram, matrix, and list (including parallel coordi-
nates). A detailed discussion about applying them in bicluster visual-
izations are addressed in [49]. Because of the following two advan-
tages, we choose list as the major layout of BiSet.

Consistent, domain-based entity management. In a list layout,
entities are organized in a consistent manner by domains. In a node-
link diagram, nodes are placed either randomly or based on certain
layout algorithms (e.g., force-directed layout [15]), so it is hard to sep-
arate entities of one domain (e.g., people) from those of another (e.g.,
location). Compared with a node-link diagram, a matrix is a better
organized layout, where relations are more readable [17]. Entities in a
matrix are organized in two orthogonal directions based on domains.
However, when a new group of entities are to be added, it is impossi-
ble to add them in the existing matrix. A new matrix has to be built

case, users may have to compare these biclusters by drilling down to
detailed level of information (e.g., entities) for competing them.

There are two ways to perform the four types of exploration: from
one to many and from many to many. For instance, users may want to
ﬁnd related entities based on one or multiple biclusters. Based on this,
we identify eight user tasks involved in such explorations, which are
summarized in Table 1. For each type of exploration, users can start
from either an entity (or a bicluster) or multiple entities (or biclusters)
and then try to ﬁnd relevant entities or biclusters. A user’s analytical
process may consist of a series of these tasks that iteratively forage rel-
evant information and identify some meaningful pieces [30]. Detailed
examples of these tasks are addressed and labeled in Section 5.

R3: Organizing entities and relationships. Entities and relation-
ships should be visually represented in an organized way. This can
help users to easily ﬁnd useful information. In addition, users may
want to make changes to the automatically generated layouts so that
they can organize entities or relationships in personalized, meaningful
ways (e.g., using spatialization) for sensemaking [2].

R4: Retrieving original data for reference. To evaluate algorith-
mically discovered coordinated relationships, users may refer to the
content from the original dataset (e.g., documents) because they need
contextual information to help them to interpret and further evaluate
these relations [19]. BiSet should attempt to efﬁciently direct users to
useful information, rather than keep them from reading documents.

4 BISET TECHNIQUE
Three key aspects are involved in BiSet: coordinated relationship dis-
covery in data level; bundles as objects and a “in-between” layer in
visual level; and interactions to support four types of exploration and
two ways of organizing information. In this section, we discuss them
in detail and explain how identiﬁed design requirements are satisﬁed.

4.1 Data Level: Bicluster Discovery
Coordinated relationship discovery is the fundamental step in BiSet
since it determines how edges are bundled. In BiSet, we formalize co-
ordinated relationships as biclusters. Suppose that entities have been
extracted from a dataset (e.g., documents) with named entity recogniz-
ers such as LingPipe [5] or similar tools. We use closed itemset algo-
rithms (e.g., LCM [51] and CHARM [56]) to discover biclusters based
on extracted entities. Each unique pair of entity types (e.g., people and
location, people and date, location and date, etc.) is considered a type
of coordinated relationship and is computed separately to generate re-
sults that include all unique pairs of entity types. Results are stored in
a database and associated with the dataset under investigation.

The mined biclusters indicate different coordinated relationships
and some of them may share entities and relations in entity-level or
group-level. This suggests that some entities and edges (individual re-
lationships) are members of biclusters. Thus, membership in BiSet, in
the data level, can be considered from two aspects: entity and edge.

4.2 Visual Level: Bundles as Objects and “In-between”
In BiSet, we propose two important concepts to balance the key design
trade-off: making bundles as ﬁrst class objects and adding a new layer
“in-between” lists to contain bundle objects. The former enables users
to directly manipulate relationships (relationship-centric) and the lat-
ter helps to visually reveal membership of entities in two neighboring
lists without duplicating entities (entity-centric).

Making Bundles as Objects In BiSet, we make bundles the ﬁrst
class objects so users can directly manipulate them for sensemaking
purposes (e.g., organizing information). BiSet bundles edges based
on computed biclusters that reﬂect algorithmically discovered coordi-
nated relationships. Different from spatial edge bundling techniques
that emphasize bundling based on spatial proximity, BiSet bundles
edges based on coordinated relationships that reveal task-oriented se-
mantic insights. This assures that edge bundles remain stable, regard-
less of the positions of associated entities. Thus, bundles potentially
enables users to use space (e.g., vertical position) to organize informa-
tion (e.g., entities) (for R3), and safely retrieve related information by
interacting with edge bundles (for R2, iii and iv).

Adding an “in-between” Layer To make bundles usable, we add
a new layer, called “in-between”, visually locating in the space be-
tween two neighboring entity-lists. It contains bundles and edges (e.g.,
those that do not belong to any coordinated relationship). In this layer,
BiSet allows users to manipulate bundles for sensemaking (e.g., orga-
nizing entities and checking their membership), so bundles can support
users interactively exploring coordinated relationships.
4.2.1 Semantic Edge Bundling in BiSet
BiSet has two types of edges: independent and associated, which are
mutually exclusive. The former refers to edges that do not belong to
any coordinated relationship and the latter are those that can form one
or more coordinated relationships. For instance, in Figure 6, the edge
on top in (A) is an independent edge and other edges are associated
ones. Independent edges can reﬂect entity-level and group-level rela-
tionships, but they are not associated with others to form coordinated
relationship. Based on membership, associated edges that belong to
the same bicluster can be aggregated and represented as an edge bun-
dle. BiSet takes the following three steps to bundle edges (for R1(b)).

Fig. 6. Three modes in the “in-between” layer for displaying edges. (A)
is the edge only mode that shows all edges between related entities. (B)
is the hybrid mode, which presents bundles with individual edges. (C) is
the bundle only mode that just displays bundles.

1) Grouping edges based on membership. We separate associated
edges into different groups based on their associated biclusters. For

those in multiple biclusters, we duplicate and assign them respectively
to multiple groups, so each group has a complete number of edges.

2) Bundling edges based on groups. For each group obtained from
the previous step, we bundle all its edges together and visually replace
these edges with a rectangle to indicate an edge bundle.

3) Connecting bundles with entities. We link bundles and entities
based on membership. This assures that entities and their associated
bundles are fully connected (for R1(c)).

This bicluster-based edge bundling can potentially reduce visual
clutter and clearly present a coordinated relationship (for R1(b)). As
is shown in Figure 6, compared with (A), (B) clearly illustrates the
coordinated relationship between four people and ﬁve phone numbers.
In fact, BiSet supports three modes to show edges: edge only mode
(EM), hybrid mode (HM) and bundle only mode (BM), shown as (A),
(B) and (C) respectively in Figure 6. In EM, BiSet shows edges with-
out bundling. In HM, BiSet shows independent edges and bundles. In
BM, BiSet just displays bundles. The three modes attempt to meet dif-
ferent user needs. For example, EM potentially reveals the overview
of relationships between two entity sets (e.g., (A) in Figure 6). BM
enables users to focus on analysis just with coordinated relationships.
HM can help to visually organize groups of individual relations into
multiple levels (e.g., coordinated bundles with individual entity-level
relationships). In BiSet, users can switch modes during their analysis.
An example of using semantic edge bundling in BiSet is shown in Fig-
ure 7, which reduces 164 edges to 9 bundles. In this example, we use
LCM to calculate biclusters and set the minimum support parameter
to three, which assures that each calculated bicluster has at least three
entities in one of the two related domains (here is the people’s name).

Fig. 7. A semantic edge bundling example in BiSet. (A) shows the orig-
inal 164 edges. (B) After semantic edge bundling, there are 9 bundles.

In addition to improving readability, bundles in BiSet preserve the
coordinated attribute of entities and edges. This enables users to infer
semantic meanings about potentially coordinated activities. For exam-
ple, why are the four people all related with the ﬁve phone numbers
in Figure 6? Perhaps they colluded about a terrorist assault. Such se-
mantic insights cannot be easily revealed from separated entity-level
or group-level of relations. Thus, edge bundles in BiSet serve two im-
portant roles: improving readability and revealing semantic insights.

4.2.2 Visual Encoding in BiSet
BiSet uses four major visual channels [37] to encode bundles, entities
and edges: shape, size, color and position. Figure 8 shows a detailed
example of visual encodings in BiSet.

Shape and Size In BiSet, entities and bundles are represented
as rectangles (e.g., (1) and (2) in Figure 8). Edges are visualized as
B´ezier curves. We choose B´ezier curves since they can generate more
smooth edges, compared with polylines [32].

Length, width and font size are three speciﬁc types of size channel
used in BiSet. Rectangles indicating entities are equal in length, while
those representing bundles are not. BiSet applies a linear mapping
function to determine the length of a bundle based on the total number
of its related entities. In a bundle, BiSet uses two colored regions (light
green and light gray) to indicate the proportion between its related
entities in the left list and those in the right list. In an entity rectangle,
a small rectangle is presented on the left to indicate its frequency in
a dataset. The length of these small rectangles is determined by the
frequency of the associated entities. Based on these with different
color encoding and position, users can easily discriminate entities from
bundles (for R1(a)). In addition, the width of edges can reﬂect results
of user selections. For instance, in Figure 8, compared with the width
of those in (3), the width of edges in (8) is larger, since two relevant
entities are selected. Moreover, when hovering an selected entity or
bundle, related entities will be displayed in larger fonts. This helps
users to review relevant information of previous selections (for R1(c)).

Fig. 8. Visual encodings in BiSet. (1), (2) and (3) present the normal
state of an entity, a bundle and edges, respectively. (4) and (4’) show the
selected state of an entity and a bundle with accumulated highlighting.
(5) and (5’) present the mouseover state of an entity and a bundle. (6)
shows accumulated highlighting of an entity. (7) presents the highlight-
ing state of edges. (8) shows the accumulated highlighting of edges.

Color Coding BiSet applies color coding to entities, bundles and
edges to indicate their states. In BiSet, entities and bundles have three
different states (normal, mouseover and selected), and edges has two
different states (normal and highlighting). In Figure 8, (1), (2) and (3)
respectively present the normal state of an entity, a bundle and edges;
(4) and (4’) show the selected state of an entity and a bundle; (5) and
(5’) illustrate the mouseover state of an entity and a bundle; and (7)
and (8) demonstrate the highlighting state of edges. In addition, two
different colored borders (blue and black) are used to help users further
discriminate the mouseover state from the selected state (for R1(d)).
When hovering an entity or a bundle, a blue border will be added to the
rectangle. This border will change to black after user selection, which
indicates that the state has changed from mouseover to selected.

Accumulated highlighting is important in BiSet, which is triggered
by mouseover and selection. Different from simple highlighting, ac-
cumulated highlighting provides useful visual clues (e.g., darker in
orange) for shared entities (for R1(c)) and bundles. BiSet applies ac-
cumulated highlighting to entities, bundles and edges by increasing
the shading of their colors. For example, in Figure 8, the entity in
(6) is in darker orange, compared with those in (4) and (5), since its
highlighting is accumulated based on selections of the entity in (4) and
AMTRAK, and the mouseover on (5).

Position Position is used to organize entities and bundles in BiSet.
A set of entities of a certain domain (e.g., people) is organized as an
entity-list. In between two neighboring entity-lists (the “in-between”
layer), there is one relationship-list that contains coordinated relation-
ships as biclusters (visually as edge bundles).

In entity-lists, the positions of entities can be determined in three
ways (for R3):
in an alphabetical order, based on frequency, or
based on the order of (one-side) associated bundles. Alphabetical and
frequency-based ordering can help to organize entities. However, they
may lead to a severe problem of membership separation, since entities

belonging to the same bicluster may not be listed close to each other.
This results from the trade-off discussed in Section 3.1. To balance
this trade-off, BiSet provides the third approach to organize the po-
sition of entities based on the order of (one-side) associated bundles.
For example, entities in the left list in Figure 8 are ordered based on
their associated biclusters in the middle list.

The larger a bicluster’s size is, the more important it is likely to
be. A bicluster in larger size contains more information, so it is more
likely to reveal potentially meaningful coordinated relationships. With
this rationale, we apply a greedy algorithm, listed below, to organize
entities based on the size of their associated biclusters.

Algorithm 1: Get positions of entities associated with biclusters
input : curPos, initialized as 0 for the top position

orderedBics, a list of ordered biclusters
bicEntDict, a dictionary stores entities for each bicluster
entBicDict, a dictionary stores biclusters for each entity
rankedEntSet, a set stores entities already ranked

totalRank += bicluster.rank;
num += 1;

if not entity in rankedEntSet then
bicList = entBicDict(entity);
totalRank = 0, num = 0;
foreach bicluster in bicList do

1 foreach bic in orderedBics do
entList = bicEntDict(bic);
2
foreach entity in entList do
3
4
5
6
7
8
9
10
11
12
13
14
15 end
16 orderedEnts = entList.sort(sel f .rank);
17 foreach entity in orderedEnts do
18
19 end

end
entity.rank = totalRank / num;
rankedEntSet.add(entity);

entity.pos = curPos++

end

end

BiSet also allows automatically changing positions of groups of en-
tities (for R3) by dragging a bundle in the “in-between” layer. Figure
9 shows such an example. After dragging a bundle, two groups of en-
tities in its two neighboring lists automatically move to new positions.
In the “in-between” layer, BiSet supports two ways to organize po-
sitions of bundles: automatically adjusting positions based on related
entities, and manually dragging and moving bundles to new positions.
The former allows listing bundles based on the positions of entities
in either or both neighboring list(s), while the latter enables users to
adjust the automatically generated layout based on their ad hoc needs
(e.g., synthesis with created spatializations).

4.3 Interaction: Exploring and Organizing Information
Revealing and organizing information in a bidirectional manner serves
as a key design principle for interactions in BiSet. By enabling users
to directly interact with entities in entity-lists, and bundles in the “in-
between” layer, BiSet can potentially support four types of exploration
and two ways of organizing information.

Revealing Information BiSet supports bidirectional information
revealing. Speciﬁcally, users can ﬁnd relevant bundles by selecting or
hovering over entities, and vice versa (for R2, ii and iv). Relevant
entities or bundles in entity-lists or “in-between” layer will also be
highlighted in BiSet (for R2, i and iii), when users interact with an
entity or a bundle. This means that when users select (or hover over)
entities, BiSet can use accumulated highlighting to reveal four types
of potentially relevant information: 1) entities in the same list that
belong to the same bicluster(s) with the selected entities, 2) entities
in other list(s) that are related with the selected entities, 3) bundles in
neighboring “in-between” layer that are directly connected with the

Fig. 9. Dragging a bundle in the “in-between” layer. Entities associated
with this bundle automatically move to their new positions.

selected entities, and 4) bundles in other relationship-list(s) that are
associated with the selected entities via bicluster-chain(s). 1) and 2)
satisfy the requirement of T1 and T2 (from entity to entity), and 3)
and 4) support T3 and T4 (from entity to relationship). Similarly,
when users select (or hover over) bundles, BiSet can reveal the same
four types of information, but they are related with selected bundles.
In this case, 1) and 2) fulﬁll the requirement of T7 and T8 (from
relationship to entity), and 3) and 4) can support T5 and T6 (from
relationship to relationship). For example, in Figure 8 when users
hover the entity in (5), three entities in the left list and six entities in the
right list are highlighted. Of the six entities, the FBI one is in darker
orange, which indicates that it is shared with another bundle (bicluster)
on the bottom. The bundle on top is also highlighted, since it is directly
related to the entity. With such ways of revealing information, BiSet
can support the four types of exploration.

Fig. 10. The document view from bundles in BiSet. (A) shows bicluster
ID, related document and associated entities. (B) shows the content of a
document. (C) lists all document ID(s) in the data with a search function.

Organizing Information BiSet supports two ways of organizing
information, which is also bidirectional. Users can organize the posi-
tion of entities based on bundles, and vice versa. BiSet uses vertical
positions to visually externalize the organized entities and bundles. As
discussed above, for bundles in the “in-between” layer, BiSet can not
only automatically organize them based on the position of related en-
tities, but also enable users to manually adjust their positions by drag-
ging and moving them. For entities, BiSet provides three options to
automatically order them in an entity-list. When users drag and move
a bundle, associated entities in two neighboring lists move with it, as
is shown in Figure 9. This enables users to manually adjust positions
of a group of related entities by using bundles. Thus, in BiSet, entities
and bundles can mutually impact each other, which provides a ﬂexible
way for users to organize information in lists.

BiSet also allows users to review documents directly from bundles
and entities with a right click menu (for R4), shown in Figure 10.
When ﬁnding an interesting bundle or entity, users can use a right click
menu to open a popup view where relevant documents are listed. This
view is on top of the view for relationship exploration. After reading,
users can quickly return to previous view by closing it.

5 USAGE SCENARIO
In this section, we walk through a text analytics scenario to demon-
strate how BiSet supports an analyst to identify a coordinated activity.
We use The Sign of the Crescent dataset [27] which contains 41 ﬁc-
tional intelligence reports regarding a coordinated terrorist plot in three
US cities, and each plot involves a group of (at least four) suspicious
people. In fact, 24 of them are relevant to the plot. We used LCM
[51] to generate biclusters from the dataset with the minimum support
parameter set to 3, which assured that each bicluster has at least three
entities from one domain. This resulted in 284 unique entities, 495
relationships, and 337 biclusters (including 122 thin biclusters).

Suppose that Sarah is an intelligence analyst. She is assigned a task
to read intelligence reports and identify potential terrorist threats with
key persons. She opens BiSet, selects four identiﬁed domains (people,
location, phone number and date) and starts her analysis. Figure 11
shows the key steps of Sarah’s analytical process.

Rapid discovery of insights from coordinated relationships with
bidirectional exploration. Sarah begins analysis by hovering indi-
vidual entities in the list of people’s names. BiSet highlights related
bundles and entities, each time when she hovers the mouse over an
entity (T1 and T3). Immediately she ﬁnds that A. Ramazi is active
in three bundles, which indicates that this person is involved in three
coordinated activities (T3). Sarah selects it (Figure 11 (1)) to focus
on highlighted entities of the three bundles (T8). She ﬁnds that A.
Ramazi is involved in two cells with ﬁve other people. One is in Ger-
many and the other is more broad including four countries. A. Ramazi
is the only person connecting the two cells, and there are two over-
lapped subgroups of people involved in the broader cell. Moreover,
each subgroup has its unique person (B. Dhaliwal and F. Goba) (T7).
BiSet quickly directs Sarah to such insights with just one click. With-
out BiSet, Sarah has to select and deselect many entities to ﬁnd these
meaningful semantic insights, particularly with tools that only display
simple relationships (e.g., entity-level or group-level), such as Jigsaw.
Easily gaining insights from bundles for exploration in entity-
space. Then Sarah decides to explore the two overlapped subgroups,
because she wants to know what brings the unique people to them. She
checks B. Dhaliwal ﬁrst by hovering the mouse over it. After this, two
bundles are highlighted (T3). By following edges from them, Sarah
ﬁnds that they share two people’s name and three locations, but the
bigger one (shown in Figure 11 (2)) is related to a new name (H. Pakes)
(T8). Then she examines F. Goba in the same way. This time three
bundles and three names are highlighted (T1 and T3), and one name
(M. Galab) has a high frequency. This quickly catches Sarah’s atten-
tion, so she decides to temporarily pause the analytical branch of B.
Dhaliwal, and moves on with the branch of F. Goba. Sarah hovers the
mouse over M. Galab to check whether it leads to more information
(T1 and T3). However, it turns out that no additional bundles or names
are highlighted. Sarah realizes that people potentially related with M.
Galab have already been highlighted in her current workspace. The
bundle (shown in Figure 11 (3) as the black dot box in the middle)
clearly reveals the people related with M. Galab, and their activities
are all in the US (T7). With this bundle, Sarah easily acquires this
key insight revealed by a group of locations. The relations revealed in
this bundle are important, and Sarah infers that the three people (M.
Galab, Y. Mosed and Z. al Shibh) may work on something together
in the US. Thus, she decides to ﬁnd more relevant information by fol-
lowing this tail [30] (T2, T4 and T7). Without bundles, Sarah has to
mentally aggregate such pieces of information (18 edges crossed with
each other) to gain this insight (e.g., in Jigsaw’s List View).

Visually connecting semantic insights using bundles for explo-
ration in relationship-space. Sarah selects the same bundle. BiSet
highlights relevant bundles that potentially form bicluster chains with

the selected one (T5). She ﬁnds that ﬁve bundles, in the space be-
tween the location list and the phone number list, are highlighted, and
two bundles, in the space between the phone number list and the date
list, are highlighted. Relevant entities in lists are also highlighted (T7).
In the two lists of newly highlighted bundles, Sarah ﬁnds that there
are two big ones (relatively longer in width shown in Figure 11 (4))
in each list. These two bundles seem useful since they contain more
relations. Sarah wants to investigate these ﬁrst and tries to check how
bundles from different relationship lists are connected (T6). For bun-
dles between the location list and the phone number list (from top to
bottom), Sarah ﬁnds that the ﬁrst bundle and the third one share two
locations (Charlottesville and Virginia) with the selected bundle, and
other highlighted bundles just share one location with the selected one
(T6). Compared with the third bundle, the ﬁrst one is related with
more locations that are not associated the selected bundle (T7). Sarah
wants to focus on information highly connected with the selected bun-
dle, rather than more additional information. Thus, she considers the
third bundle, in this case, a useful one. Using the same strategy in an-
other relationship-list, she ﬁnds that the bigger bundle is more useful.
After this, Sarah hides edges of other bundles with the right click
menu to keep a clear view. Then her workspace shows that three bun-
dles connected to each other through two shared locations and three
shared phone numbers. Sarah feels that she has found a good number
of relations, connecting four groups of entities, which may reﬂect a
suspicious activity. Therefore, she decides to read relevant documents
to ﬁnd details of such connections and generate her hypothesis.

Efﬁciently directing to relevant documents based on bundle ob-
jects. The three connected bundles direct Sarah to eight reports, which
are all relevant to the plot. Sarah quickly goes through these reports
by referring to the entities with bright shading in the four connected
groups (shown in Figure 11 (4)). The darker shading of an entity in-
dicates that it is shared more times. Sarah uses this to help keep her
attention to more important entities in reports. After reading the re-
ports, she identiﬁes a potential threat with four key persons.

Aiding hypothesis generation. By visually following the bicluster-
chain (linked bundles, shown in Figure 11 (4)), Sarah quickly remem-
bers what she has already read [2]. Finally she makes a hypothesis of
the identiﬁed attack as follows:

F. Goba, M. Galab and Y. Mosed, following the commands from A.

Ramazi, plan to attack AMTRAK Train 19 at 9:00 am on April 30.

6 CONCLUSION AND FUTURE WORK
We present BiSet, a visual analytics technique, which bundles edges
based on biclusters to reveal task-oriented semantic insights about po-
tentially coordinated activities for sensemaking. In BiSet, we make
edge bundles the ﬁrst class objects that enable users to directly ma-
nipulate relationships. In addition, we add a new layer, “in-between”,
containing bundles, which assists to visually reveal membership of en-
tities in neighboring entity-lists without entity duplication.

By applying interactions on bundles and entities, BiSet enables
bidirectional information foraging to support exploration between the
entity-space and the relationship-space. Moreover, BiSet allows or-
ganizing entities in lists, either automatically based on the positions
of associated bundles or manually by dragging and moving bundles.
With a usage scenario, we demonstrate how BiSet can potentially sup-
port an analyst to explore coordinated activities. However, there are
still three challenges that need further explorations.

C1: Seriation in lists. In BiSet, we apply a greedy approach to
order entities in lists based on bicluters in neighboring relationship-
list(s). Compared with traditional ordering (in an alphabetical order
or by frequency), this attempts to keep entities, belonging to the same
biclusters (especially those in a larger size), close to each other. How-
ever, this approach may lead to a membership separation problem for
smaller sized biclusters (those with a small number of entities), espe-
cially if some of their entities are shared by bigger sized biclusters. In
fact, this brings about an ordering problem in lists when considering
the design trade-off discussed before. Seriation methods, commonly
used in matrices [35], can reveal clustering structure by permuting the
presentation order [8]. However, applying a seriation method to order-

Fig. 11. A process of ﬁnding a major threat plot with key steps. (1): Based on A. Ramazi, ﬁnding that there are two similar bundles and two cells.
(2): One name and two bundles are highlighted when hovering the mouse over B. Dhaliwal. (3): Three names and three bundles are highlighted
when exploring F. Goba. (4) Referring to the four connected groups of useful entities for hypothesis generation.

ing entities in lists, based on coordinated relationships, is challenging.
Seriation in lists based on biclusters may better organize entities and
reduce edge crossings between shared entities and edge bundles.

C2: Variant visualizations in lists. BiSet chooses list as its ma-
jor layout to organize entities and applies interactions to bundles to
enhance its capability to utilize the relationship-space. However, or-
ganizing entities in a list may not always be the best choice. For exam-
ple, entities of locations (e.g., Chicago, Boston, Seattle, etc.) may bet-
ter reveal meaningful, contextual information, if visualized in a map,
rather than simply listed as vertically piled bars. Similar to the tech-
nique proposed in [20], it is possible to substitute some entity-lists
with variant types of visualizations in BiSet, which may help users to
infer contextual clues. If such substitution could be switched on and
off, users would infer semantic insights from lists, and gain contextual
clues from variant views (e.g., maps).

C3: Semantic Interactions in lists. Semantic interaction provides
an efﬁcient way to facilitate visual analytics by enabling users to im-
plicitly steer machine learning algorithms to support the human rea-
soning process [13]. This allows users to modify algorithmic outcomes
for sensemaking purposes. Currently BiSet does not support this. Se-
mantic interactions on bundles for organizing entities, such as split-
ting and merging bundles (similar to those in NodeTrix [23]), can help
users to adjust algorithmically discovered biclusters and efﬁciently re-
trieve relevant information. In addition, merging bundles also provides
a good opportunity for BiSet to scale up to a large dataset.

ACKNOWLEDGMENTS
The authors wish to thank Kurt Luther for his feedback on the initial
version of BiSet, and Junyang Chen and Bin He for supporting the

backend development of BiSet. This research was supported in part
by a grant from L-3 Communications and NSF grant IIS-1447416.

REFERENCES
[1] B. Alsallakh, L. Micallef, W. Aigner, H. Hauser, S. Miksch, and
P. Rodgers. Visualizing Sets and Set-typed Data: State-of-the-Art and
Future Challenges. In EuroVis-STARs. Eurographics Association, 2014.
large high-
resolution displays for sensemaking. In Proceedings of the Conference
on Human Factors in Computing Systems, pages 55–64. ACM, 2010.

[2] C. Andrews, A. Endert, and C. North. Space to think:

[3] S. Barkow, S. Bleuler, A. Preli´c, P. Zimmermann, and E. Zitzler. Bicat: a
biclustering analysis toolbox. Bioinformatics, 22(10):1282–1283, 2006.
[4] R. A. Becker and W. S. Cleveland. Brushing scatterplots. Technometrics,

[5] B. Carpenter. Phrasal queries with lingpipe and lucene: ad hoc genomics

29(2):127–142, 1987.

text retrieval. In TREC, 2004.

[6] D. B. Carr, R. J. Littleﬁeld, W. Nicholson, and J. Littleﬁeld. Scatterplot
matrix techniques for large n. Journal of the American Statistical Associ-
ation, 82(398):424–436, 1987.

[7] G. Chin Jr, O. A. Kuchar, and K. E. Wolf. Exploring the analytical pro-
cesses of intelligence analysts. In Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, pages 11–20. ACM, 2009.

[8] J. Chuang, C. D. Manning, and J. Heer. Termite: Visualization techniques
for assessing textual topic models. In Proceedings of International Work-
ing Conference on Advanced Visual Interfaces, pages 74–77. ACM, 2012.
[9] C. Collins and S. Carpendale. VisLink: Revealing Relationships
Amongst Visualizations. Visualization and Computer Graphics, IEEE
Transactions on, 13(6):1192–1199, 2007.

[10] C. Collins, G. Penn, and S. Carpendale. Bubble sets: Revealing set re-
lations with isocontours over existing visualizations. Visualization and

Computer Graphics, IEEE Transactions on, 15(6):1009–1016, 2009.

[11] W. Cui, H. Zhou, H. Qu, P. C. Wong, and X. Li. Geometry-based edge
clustering for graph visualization. Visualization and Computer Graphics,
IEEE Transactions on, 14(6):1277–1284, 2008.

[12] M. D¨ork, N. H. Riche, G. Ramos, and S. Dumais. Pivotpaths: Strolling
through faceted information spaces. Visualization and Computer Graph-
ics, IEEE Transactions on, 18(12):2709–2718, 2012.

[13] A. Endert, P. Fiaux, and C. North. Semantic interaction for sensemak-
ing: inferring analytical reasoning for model steering. Visualization and
Computer Graphics, IEEE Transactions on, 18(12):2879–2888, 2012.

[14] P. Fiaux, M. Sun, L. Bradel, C. North, N. Ramakrishnan, and A. En-
dert. Bixplorer: Visual analytics with biclusters. Computer, 46(8):90–94,
2013.

[15] T. M. Fruchterman and E. M. Reingold. Graph drawing by force-directed
placement. Software: Practice and experience, 21(11):1129–1164, 1991.
[16] T. Geymayer, M. Steinberger, A. Lex, M. Streit, and D. Schmalstieg.
Show me the invisible: visualizing hidden content. In Proceedings of the
Conference on Human Factors in Computing Systems, pages 3705–3714.
ACM, 2014.

[17] M. Ghoniem, J. Fekete, and P. Castagliola. A comparison of the readabil-
ity of graphs using node-link and matrix-based representations. In Infor-
mation Visualization, IEEE Symposium on, pages 17–24. IEEE, 2004.

[18] J. P. Gonc¸alves, S. C. Madeira, and A. L. Oliveira. Biggests: integrated
environment for biclustering analysis of time series gene expression data.
BMC Research Notes, 2(1):124, 2009.

[19] C. Gorg, Z. Liu, J. Kihm, J. Choo, H. Park, and J. Stasko. Combining
computational analyses and interactive visualization for document explo-
ration and sensemaking in jigsaw. Visualization and Computer Graphics,
IEEE Transactions on, 19(10):1646–1663, 2013.

[20] S. Gratzl, N. Gehlenborg, A. Lex, H. Pﬁster, and M. Streit. Domino: Ex-
tracting, Comparing, and Manipulating Subsets Across Multiple Tabular
Datasets. Visualization and Computer Graphics, IEEE Transactions on,
20(12):2023–2032, 2014.

[21] G. A. Grothaus, A. Mufti, and T. Murali. Automatic layout and visual-

ization of biclusters. Algorithms for Molecular Biology, 1(1):15, 2006.

[22] J. Heinrich, R. Seifert, M. Burch, and D. Weiskopf. Bicluster viewer:
a visualization tool for analyzing gene expression data. In Advances in
Visual Computing, pages 641–652. Springer, 2011.

[23] N. Henry, J. Fekete, and M. J. McGufﬁn. Nodetrix: a hybrid visualization
of social networks. Visualization and Computer Graphics, IEEE Trans-
actions on, 13(6):1302–1309, 2007.

[24] I. Herman, G. Melanc¸on, and M. S. Marshall. Graph visualization and
navigation in information visualization: A survey. Visualization and
Computer Graphics, IEEE Transactions on, 6(1):24–43, 2000.

[25] D. Holten. Hierarchical edge bundles: Visualization of adjacency rela-
tions in hierarchical data. Visualization and Computer Graphics, IEEE
Transactions on, 12(5):741–748, 2006.

[26] D. Holten and J. J. Van Wijk. Force-directed edge bundling for graph
visualization. In Computer Graphics Forum, volume 28, pages 983–990.
Wiley Online Library, 2009.

[27] F. Hughes and D. Schum. Discovery-proof-choice, the art and science of
the process of intelligence analysis-preparing for the future of intelligence
analysis. Washington, DC: Joint Military Intelligence College, 2003.

[28] A. Inselberg and B. Dimsdale. Parallel coordinates. In Human-Machine

Interactive Systems, pages 199–233. Springer, 1991.

[29] Y. Jin, T. M. Murali, and N. Ramakrishnan. Compositional mining of
multirelational biological datasets. ACM Transactions on Knowledge Dis-
covery from Data, 2(1):1–35, Mar. 2008.

[30] Y.-a. Kang, C. Gorg, and J. Stasko. Evaluating visual analytics systems
for investigative analysis: Deriving design principles from a case study. In
Visual Analytics Science and Technology, 2009. VAST 2009. IEEE Sym-
posium on, pages 139–146. IEEE, 2009.

[31] M. Kapushesky, P. Kemmeren, A. C. Culhane, S. Durinck, J. Ihmels,
C. K¨orner, M. Kull, A. Torrente, U. Sarkans, J. Vilo, et al. Expression
proﬁler: next generation-an online platform for analysis of microarray
data. Nucleic acids research, 32(suppl 2):W465–W470, 2004.

[32] A. Lambert, R. Bourqui, and D. Auber. Winding Roads: Routing edges

into bundles. Computer Graphics Forum, 29(3):853–862, Aug. 2010.

[33] A. Lex, H. Schulz, M. Streit, C. Partl, and D. Schmalstieg. VisBricks:
Multiform Visualization of Large, Inhomogeneous Data. IEEE Transac-
tions on Visualization and Computer Graphics, 17(12):2291–2300, 2011.
[34] A. Lex, M. Streit, C. Partl, K. Kashofer, and D. Schmalstieg. Compara-
tive Analysis of Multidimensional, Quantitative Data. Visualization and

Computer Graphics, IEEE Transactions on, 16(6):1027–1035, 2010.

[35] I. Liiv. Seriation and matrix reordering methods: An historical overview.
Statistical Analysis and Data Mining: The ASA Data Science Journal,
3(2):70–91, 2010.

[36] S. C. Madeira and A. L. Oliveira. Biclustering algorithms for biologi-
cal data analysis: a survey. Computational Biology and Bioinformatics,
IEEE/ACM Transactions on, 1(1):24–45, 2004.

[37] T. Munzner. Visualization Analysis and Design. CRC Press, 2014.
[38] C. Partl, A. Lex, M. Streit, H. Strobelt, A. M. Wassermann, H. Pﬁster, and
D. Schmalstieg. ConTour: Data-Driven Exploration of Multi-Relational
Datasets for Drug Discovery. Visualization and Computer Graphics,
IEEE Transactions on, 20(12):1883–1892, 2014.

[39] P. Pirolli and S. Card. The sensemaking process and leverage points for
analyst technology as identiﬁed through cognitive task analysis. In Pro-
ceedings of international conference on intelligence analysis, volume 5,
pages 2–4, 2005.

[40] A. J. Pretorius and J. J. Van Wijk. Visual inspection of multivariate
graphs. In Computer Graphics Forum, volume 27, pages 967–974. Wiley
Online Library, 2008.

[41] N. H. Riche and T. Dwyer. Untangling euler diagrams. IEEE Transac-
tions on Visualization and Computer Graphics, 16(6):1090–1099, 2010.
[42] P. Rodgers. A survey of euler diagrams. Journal of Visual Languages &

Computing, 25(3):134–155, 2014.

[43] R. Santamar´ıa, R. Ther´on, and L. Quintales. Bicoverlapper 2.0: visual

analysis for gene expression. Bioinformatics, page btu120, 2014.

[44] L. Shi, Q. Liao, H. Tong, Y. Hu, Y. Zhao, and C. Lin. Hierarchical fo-
cus+ context heterogeneous network visualization. In Paciﬁc Visualiza-
tion Symposium (PaciﬁcVis), 2014 IEEE, pages 89–96. IEEE, 2014.

[45] H. Siirtola and K.-J. R¨aih¨a. Interacting with parallel coordinates. Inter-

acting with Computers, 18(6):1278–1309, 2006.

[46] M. Steinberger, M. Waldner, M. Streit, A. Lex, and D. Schmalstieg.
Context-Preserving Visual Links. Visualization and Computer Graphics,
IEEE Transactions on, 17(12):2249–2258, 2011.

[47] M. Streit, S. Gratzl, M. Gillhofer, A. Mayr, A. Mitterecker, and
S. Hochreiter. Furby: fuzzy force-directed bicluster visualization. BMC
bioinformatics, 15(Suppl 6):S4, 2014.

[48] M. Sun, L. Bradel, C. L. North, and N. Ramakrishnan. The role of in-
teractive biclusters in sensemaking. In Proceedings of the Conference on
Human Factors in Computing Systems, pages 1559–1562. ACM, 2014.

[49] M. Sun, C. North, and N. Ramakrishnan. A Five-Level Design Frame-
work for Bicluster Visualizations. Visualization and Computer Graphics,
IEEE Transactions on, 20(12):1713–1722, 2014.

[50] A. Telea and O. Ersoy.

Image-Based Edge Bundles: Simpliﬁed Visu-
alization of Large Graphs. Computer Graphics Forum, 29(3):843–852,
2010.

[51] T. Uno, T. Asai, Y. Uchida, and H. Arimura. An efﬁcient algorithm for
enumerating closed patterns in transaction databases. In Discovery Sci-
ence, pages 16–31. Springer, 2004.

[52] C. Viau and M. J. McGufﬁn. ConnectedCharts: Explicit Visualization
of Relationships between Data Graphics. Computer Graphics Forum,
31(3pt4):1285–1294, June 2012.

[53] M. Waldner, W. Puff, A. Lex, M. Streit, and D. Schmalstieg. Visual links
In Proceedings of Graphics Interface 2010, pages

across applications.
129–136. Canadian Information Processing Society, 2010.

[54] H. Wu, J. Vreeken, N. Tatti, and N. Ramakrishnan. Uncovering the plot:
detecting surprising coalitions of entities in multi-relational schemas.
Data Mining and Knowledge Discovery, 28(5-6):1398–1428, 2014.

[55] H.-M. Wu, Y.-J. Tien, and C.-h. Chen. Gap: A graphical environment
for matrix visualization and cluster analysis. Computational Statistics &
Data Analysis, 54(3):767–778, 2010.

[56] M. J. Zaki and C.-J. Hsiao. Efﬁcient algorithms for mining closed item-
sets and their lattice structure. Knowledge and Data Engineering, IEEE
Transactions on, 17(4):462–478, 2005.

[57] H. Zhang, M. Sun, D. D. Yao, and C. North. Visualizing trafﬁc causality
for analyzing network anomalies. In Proceedings of the 2015 ACM Inter-
national Workshop on International Workshop on Security and Privacy
Analytics, pages 37–42. ACM, 2015.

[58] H. Zhou, P. Xu, X. Yuan, and H. Qu. Edge bundling in information visu-

alization. Tsinghua Science and Technology, 18(2):145–156, 2013.

",False,2016.0,{},False,False,journalArticle,False,9BL2E5F8,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7192715/,,BiSet: Semantic Edge Bundling with Biclusters for Sensemaking,9BL2E5F8,False,False
L239LNLV,NPGU7FZK,"WeSearch: Supporting Collaborative Search and 

Sensemaking on a Tabletop Display 

Daniel Wigdor 

Microsoft Surface 

Redmond, WA, USA 

Meredith Ringel Morris 

Jarrod Lombardo 

Microsoft Research 
Redmond, WA, USA 
merrie@microsoft.com 

Averro 

Bellevue, WA, USA 

jarrod@network-science.net 

dwigdor@microsoft.com 

 

ABSTRACT 
Groups of users often have shared information needs  – for 
example,  business  colleagues  need  to  conduct  research 
relating to joint projects and students must work together on 
group  homework  assignments.  In  this  paper,  we  introduce 
WeSearch, a  collaborative  Web  search  system  designed  to 
leverage  the  benefits  of  tabletop  displays  for  face-to-face 
collaboration  and  organization  tasks.  We  describe  the 
design of WeSearch and explain the interactions it affords. 
We  then  describe  an  evaluation  in  which  eleven  groups 
used  WeSearch  to  conduct  real  collaborative  search  tasks. 
Based on our study‟s findings, we analyze the effectiveness 
of the features introduced by WeSearch. 

Author Keywords 
Interactive  tables,  surface  computing,  tabletop  computing, 
collaborative search, Web search, sensemaking. 

ACM Classification Keywords 
H5.3.  Information  interfaces  and  presentation  (e.g.,  HCI): 
Group  and  Organization  Interfaces:  computer-supported 
cooperative work.  

INTRODUCTION 
Web search is often considered a solitary activity, but there 
are  many  situations  in  which  groups  of  people  share  an 
information need, and may benefit from the ability to search 
the  Web  collaboratively  in  both  education  [1,  15,  33]  and 
workplace [5, 9, 19] scenarios.  

Several  researchers  have  begun  to  introduce  technologies 
that  support  collaborative  search  tasks.  For  example, 
SearchTogether  [18]  is  a  browser  plug-in  that  facilitates 
remote  collaboration  on  Web  search.  Unlike  the  designers 
of  SearchTogether,  however,  our  focus  is  on  supporting 
collaborative search among co-located group members. For 
example, business colleagues may need to find information 
related  to  a  question  that  arises  during  the  course  of  a 
meeting; students working together in the library on a joint 
homework project may need to find materials to include in 
their  report;  and  family  members  gathered  in  their  home 

Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted  without fee  provided  that  copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the  full  citation on the  first  page. To copy  otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CSCW 2010, February 6–10, 2010, Savannah, Georgia, USA. 
Copyright 2010 ACM  978-1-60558-795-0/10/02...$10.00. 
 

 

 

 

Figure 1. A group of students conducts a collaborative Web 

search using the WeSearch tabletop application. 

may  wish  to  explore  topics  such  as  researching  joint 
purchases,  planning  an  upcoming  vacation,  or  seeking 
medical information to assist a loved one.    

Proposed  systems  for  supporting  co-located  collaborative 
Web search generally provide each group member with her 
own  device,  sometimes  supplemented  by  a  shared  display. 
For  example,  CoSearch  [1]  provides  a  mobile  phone  for 
each  user,  plus  one  shared  PC  display  for  the  group. 
Cerchiamo  [24]  provides  a  dedicated  PC  for  each  of  two 
collaborators that shows role-specific content, plus a shared 
wall  display  providing  joint  information.  Maekawa  et  al. 
describe  a  system  [16]  which  divides  a  Web  page  into 
sections  and  puts  one  section  on  each  user‟s  personal 
mobile  device.  WebGlance  [22]  lets  a  group  browse  the 
Web together by providing an interface on each user‟s PDA 
that controls a browser shown on a large wall display.  

These  prior  systems‟  rationale  for  providing  each  group 
member  with  a  personal  device  is  to  enable  all  group 
members  to  participate  and  work  in  parallel.  This  is 
necessary  because  traditional  PCs  (and  many  large  wall 
displays) permit only a single mouse or keyboard to interact 
at  a  time,  which  can  result  in  frustration  for  the  group 
member  who  is  not  “driving”  the  input  devices  [1]. 
However,  providing  separate  devices  for  each  co-located 
group  member  has  some  drawbacks,  such  as  reduced 
awareness  [1],  which  may  be  particularly  problematic  for 
collaborative 
awareness 
information has been found to be valuable [18, 23].    

applications,  where 

search 

Multi-touch  tabletop  technologies  (e.g.,  [2,  7])  provide  a 
promising  platform  for  co-located  collaborative  search 
applications.  Such 
technologies  enable  simultaneous 

participation  by  all  group  members,  while  providing  a 
shared  display  to  facilitate  awareness.  Indeed,  groups 
already  gather  around  traditional  tables  in  many  of  the 
situations in which  the needs for collaborative search arise 
(in  business  meetings,  in  classrooms  and  libraries,  and  at 
home), making next-generation, interactive tables a logical 
venue  for  supporting  information-finding  tasks.  Tables‟ 
large  size  affords  spatially  organizing  content,  which  also 
makes them well-suited to search and sensemaking tasks.     

A  few  researchers  have  begun  to  explore  the  harmony 
between  tabletops  and  information-seeking  tasks,  although 
they  have  focused  on  specialized  domains.  TeamSearch 
[17]  provides  a  visual  query  language  to  support  tabletop 
search over a tagged image database. The Personal Digital 
Historian  [29]  also  supports  image  search,  by  filtering  a 
tagged  collection  based  on  who  is  shown  in  the  photo, 
where it was taken, or when it was taken. Físchlár-DT [31] 
enables  partners  to  collaboratively  explore  a  collection  of 
video  clips  on  a  tabletop  display,  and  Cambiera  [13] 
supports 
investigation  of  a  database  of  news  clips 
associated with the VAST visual analytics challenge.  

(and 

search 

In this paper, we introduce WeSearch, a system designed to 
support  collaborative  Web 
subsequent 
sensemaking)  for  groups  of  up  to  four  co-present  users 
gathered  around  a  multi-touch  tabletop  display.      We  first 
articulate design criteria specific to tabletop search systems, 
and  introduce  user  interface  features  to  address  those 
criteria.  We  then  describe  an  evaluation  of  WeSearch,  in 
which  student,  co-worker,  and  family  groups  conducted 
real-world  search  tasks.  We  discuss  the  findings  from  our 
study, reflecting on the effectiveness of each of WeSearch‟s 
features. 

WESEARCH 
In order to explore the potential for interactive tabletops to 
support collaborative Web search, we developed WeSearch. 
WeSearch 
tabletop  application  which  supports 
collaborative  Web  search,  browsing,  and  sensemaking 
among groups of up to four people.  

is  a 

Design Criteria 
When  designing  WeSearch,  our  goal  was  to  leverage  the 
affordances  of  tabletops  that  would  benefit  collaborative 
search  tasks.  Examples  of  such  affordances  include  high 
levels  of  awareness  from  sharing  a  single  display  and  the 
affordances  of  large  horizontal  surfaces  for  spatially 
organizing content [21].  

In 

Prior  work  on  collaborative  Web  search  tools  for  the  PC 
informed  our  system‟s  design. 
their  work  on 
SearchTogether  [18],  Morris  and  Horvitz  identify  three 
traits that are important for facilitating collaborative search 
tasks.  Awareness  of  other  group  members‟  activities, 
facilities  for  supporting  division  of  labor  among  group 
members,  and  facilities  for  persistence  of  a  search  session 
in  order  to  facilitate  asynchronous  collaboration  and 
resumption  of  multi-session,  exploratory  search  tasks. 
Morris and Horvitz also noted that, in initial evaluations of 

SearchTogether,  participants  requested  richer  sensemaking 
[25] support. Studies of the CoSense system [23] reiterated 
the  importance  of  sensemaking  support  for  collaborative 
Web search tools.  

Research  from  the  surface  computing  community  has 
shown  that  there  are  several  challenges  in  adapting 
horizontal surfaces to productivity tasks [20]. Text entry on 
tabletops  is  one  challenge  [35];  virtual  keyboards  are  not 
nearly  as  efficient  as  their  physical  counterparts,  and 
appropriate  alternative  techniques  are  still  a  subject  of 
ongoing  research  [12].  Clutter  is  also  a  challenge  of 
adapting search and sensemaking applications to tabletops, 
since  displaying  content  for  multiple  users  on  a  single 
display is a constant challenge for single display groupware 
systems  [32].  The  clutter  issue  is  further  compounded  by 
the  information-intensive  nature  of  Web  search  tasks,  as 
well  as  by  the  tendency  of  tabletops  to  utilize  projected 
displays;  XGA  (1024  x  768)  is  still  the  predominant 
projector resolution, so while most tabletops are larger than 
PC  monitors,  they  may 
in  fact  have  fewer  pixels. 
Orientation is also a challenge for tabletop system designers 
[30], since what‟s right-side-up for some group members is 
upside-down  for  others;  Web  search  exacerbates  this 
challenge,  since  text-heavy  applications  are  particularly 
challenging to use from odd viewing angles [34]. 

In light of these findings from the collaborative search and 
tabletop communities, our design goals for WeSearch were: 

  Support awareness among group members. 
  Support division of labor among group members. 
  Enable  the  shared  search  to  persist  beyond  a  single 

session. 

  Support  sensemaking  as  an  integral  part  of  the 

collaborative search process. 

  Provide facilities for reducing the frequency of virtual-

keyboard text entry. 

  Reduce clutter on the shared display. 
  Address the orientation challenges posed by text-heavy 

tabletop applications. 

Next,  we  describe  the  features  of  WeSearch,  and  explain 
how they address these design goals.  

System Description 
Since  collaborative  search  and  sensemaking  are  data-
intensive  tasks,  we  designed  WeSearch  for  a  large-form-
factor  surface,  4x6  [10]  (Figure  1).  4x6  is  a  custom-built, 
standing-height  interactive  tabletop  measuring  4  feet  wide 
by 6 feet long (1.2 m x 1.8 m). The display is top-projected 
by  two  tiled  XGA  projectors,  for  a  total  display  resolution 
of  1024  x  1536  pixels.  The  table  is  illuminated  from 
beneath by infrared light, and touch inputs are detected by a 
vision system.  

The  4x6  table  can  receive  multiple,  simultaneous  touch 
inputs,  but  cannot  associate  inputs  with  a  particular  group 
member.  Users  can  freely  rotate  all  objects;  because  our 
tabletop  hardware  is  not  user-differentiating,  we  use 

Browser Controls 
We have designed the WeSearch browser with the needs of 
touch-based  interaction  in  mind.  The  browser  can  be 
moved, rotated, and scaled using direct touch manipulations 
[8].  Because  touches  on  the  browser  are  by  default 
interpreted  as  manipulations,  we  augment  the  browser‟s 
border with buttons to enable additional actions (pan, link, 
and  clips),  shown  in  Figure  3.  A  future  implementation  of 
WeSearch might remap these buttons to gestures; we chose 
to  sidestep  the  issue  of  gesture  selection  by  using  buttons 
which  are  held  with  one  hand  while  the  browser  is 
manipulated  with  the  second  hand.  The  buttons  must  be 
held  to  maintain  the  mode  in  order  to  reduce  errors  [28]. 
Horizontal  and  vertical  scrolling  are  accomplished  by 
holding  the  “pan”  button  with  one  hand  while  using  the 
other  hand  to  pull  the  Web  page‟s  content  in  the  desired 
direction,  and  link-following  is  accomplished  by  holding 
the  “link”  button  with  one  hand  while  tapping  the  desired 
link with the other. 

Clips 
Holding a browser‟s “clips” button divides the current web 
page  into  multiple  smaller  chunks  (Figure  3b);  a  user  can 
grab  a  chunk  with  his  other  hand  and  drag  it  beyond  the 
borders  of  the  browser,  where  it  will  become  a  separate 
entity that we call a clip. Upon releasing the “clips” button, 
the  browser  page returns to  its  undivided  state.  In  order  to 
divide  a  Web  page  into  clips,  WeSearch  parses  the  DOM 
(document object model) of each page when it is loaded; we 
then create clip boundaries surrounding  DOM objects such 
as paragraphs, lists, and images. The ability to divide a page 
into  clips  supports  division  of  labor  and  readability  by 
enabling  different  group  members  to  claim  responsibility 
over distinct portions of a page‟s  contents, which can then 
be  individually  rotated  into  a  proper  reading  orientation; 
clips  also  support  clutter  reduction  –  the  small  chunks  of 
relevant content can remain open on the table and the parent 
page  can  be  closed.  Our  clips  build  upon  prior  work 
demonstrating  the  value  of  micro-mobility  of  content  on 
tables, such as DocuBits [4], which enabled easy transfer a 
screen-captured  chunk  of 
images 
between a tabletop and associated supplementary displays. 

text  documents  or 

 

Figure  2.  A  WeSearch  session.  Each  group  member  has  a 
color-coded toolbar in which they can enter queries or urls, 
and  a  marquee  containing  awareness  information.  Spread 
around the table are several browsers, clips, and containers. 

 
objects‟ orientation as a proxy for identity (i.e., whether an 
object  is  right-side  up  for  the  North,  South,  East,  or  West 
edge  of  the  tabletop).  Because  of  the  readability  issues 
concerning text on tabletops [34], orientation of text-heavy 
documents  such as  Web  pages  seems  like  a  reliable  proxy 
for  the  identity  of  the  currently-interacting  user,  and  is 
consistent  with  how  users  utilize  orientation  of  objects  to 
denote ownership [14, 27]. 

Toolbars 
When  WeSearch  initializes,  it  displays  four  color-coded 
toolbars  (one  per  group  member),  one  along  each  edge  of 
the  tabletop  (Figure  2).  If  desired,  these  toolbars  can  be 
repositioned 
direct-touch 
manipulations.  The  color  of  a  user‟s  toolbar  is  associated 
with him in other aspects of the user interface.  

re-oriented 

and 

through 

Touching  the  toolbar‟s  text  field  opens  a  virtual  keyboard 
that enables users to enter urls or query terms. Tapping the 
toolbar‟s “go” button opens the WeSearch browser (Figure 
3a) to that url (if the terms begin with “http” or “www”) or 
opens a search engine page containing search results for the 
terms entered. 

(A) 

 (B)

 

Figure  3.  (A)  The  WeSearch  browser  provides  a  panel  of  kinesthetically-held  buttons  to  distinguish  between  direct 
manipulations  (translation,  rotation,  and scaling),  and  actions such  as  panning, link-following,  and  dividing a  page  into  clips. 
(B) When the clips button is held, a webpage is automatically divided based on the underlying DOM. The resulting clips can be 
pulled out of a page and treated as individual objects. 

 

Figure 4. A clip tagged by both the green and red users. 

 

 

Figure  5.  The clips-search  function  creates four  types  of  clips 
directly  from  query  terms:  related  keywords,  Web  search 
results, images, and news article summaries. 

Clips can be moved, rotated, and scaled in the same manner 
as  browser  windows.  Like  browsers,  clips  are  augmented 
with  a  “link”  button  that,  when  depressed,  interprets 
touches as clicks on links rather than direct manipulations. 
Clips  are  also  augmented  by  a  “tag”  button;  pressing  this 
opens  a  virtual  keyboard, and  the  user  can augment  a  clip 
with tags. Tags are displayed on the clip in the color of the 
user  who  entered  them  (Figure  4).  We  included the  ability 
to augment clips with tags in order to support sensemaking 
as a key part of the task. 

Clips-Search 
In addition to creating clips by pulling chunks out of  Web 
pages in WeSearch browser windows, users can also create 
clips  directly  by  pressing  the  “clips”  button  in  lieu  of  the 
“go”  button  after  they  have  entered  query  terms  into  their 
toolbar.  This  sends  the  query  to  a  search  engine  via  its 
public API, and automatically creates four piles of five clips 
each in front of the user (Figure 5) one containing the most 
relevant  images  for  the  query,  another  containing  snippets 
describing  related  Web  pages,  a  third  containing  news 
article  summaries  on  that  topic,  and  a  fourth  containing 
suggested  related  query  keywords.  The  clips-search  button 
provides  another  easy  way  for  groups  to  divide  labor 
amongst  themselves,  if  each  chooses  to  take  responsibility 
for a different content type. 

Marquee 
Another  component  of  the  WeSearch  interface  is  the 
marquee  region  of  each  user‟s  toolbar  (Figure  6).  The 
marquee  displays  a  slowly  flowing  stream  of  text  and 
images  that  reflect  the  group  members‟  activities:  query 
terms  used,  titles  of  pages  opened  in  browsers,  and  clips 
created.  The  marquee  is  visually  similar  to  an  interface 
current  [11],  but  the  marquee‟s  content  is  generated 
automatically  based  on  users‟  actions,  while  objects  must 
be  manually  added  to  interface  currents.  The  colored 
borders  surrounding  marquee  items  indicate  which  user‟s 

Figure  6.  Each  toolbar  contains  a  marquee,  where  search 
terms,  page  titles,  and  clips  from  all  group  members  slowly 
scroll  past  for  awareness  and  readability.  Terms  from  the 
marquee can be reused via dragging onto the search box. 

activities  each  item represents.  Scroll  buttons  at  either  end 
of the marquee enable the user to manually rewind or fast-
forward  the  display,  in  order  to  review  the  content.  The 
marquee  addresses  the  issue  of  awareness  of  group 
members‟  activities,  as  well  as  offering  a  solution  to  the 
challenge of reading text at odd orientations by giving each 
group  member  a  right-side-up  view  of  key  bits  of  textual 
material from other team members. 

Marquee  items  are  interactive.  Pressing  and  holding  a 
marquee  item  causes  the  corresponding  original  clip  or 
browser (if still open) to become highlighted in the pressing 
user‟s  color  and  to  blink,  offering  a  method  of  mitigating 
the  clutter  issue  by  simplifying  the  process  of  finding 
content  within  a  crowded  UI.  Marquee  items  (and  clips) 
also provide another opportunity to reduce the frustration of 
virtual keyboard text entry: users can drag items out of the 
marquee and onto the toolbar‟s text area, in order to re-use 
the text they contain; clips can also be used in this manner. 
For  example,  the  “keyword  suggestion”  clips  created  by  a 
clips-search  can  be  dragged  directly  onto  the  textbox  in 
order to save the effort of manually re-typing those terms. 

Containers 
Another  type  of  sensemaking  support  WeSearch  provides, 
in addition to the ability to create and tag clips, is the ability 
for  clips  to  be  organized  in  containers,  which  a  user  can 
create via a button on the toolbar (Figure 7). The ability to 
create  collections  of  material  from  disparate  websites  has 
been  demonstrated  by  prior  systems  such  as  Dontcheva  et 
al.‟s  system  [3],  Hunter  Gatherer  [26],  and  Google 
this  concept  by  offering 
Notebook.  We  expand  on 
containers  designed 
in  multi-user,  direct 
manipulation  environments,  and  augmented  by  features 
such as the ability to “search by example.” 

for  use 

WeSearch offers several container variants, which organize 
clips in different manners, such as via lists, grids, and free-
form  positioning.  The  virtual  keyboards  can  be  used  to 
specify  a  title  for  the  container.  Containers  can  be 
translated,  rotated,  and  scaled  through  direct  manipulation 
interactions  on  their  background,  and  clips  can  be  added 
and removed from containers via drag-and-drop.  

Search by Example 
In  addition  to  providing  a  mechanism  for  organizing  a 
group‟s  clips,  containers  also  facilitate  discovering  new 

 

Figure  7.  Clips  can  be  organized  within  containers. 
Containers  also  provide  a  “search  by  example” 
capability, suggesting search terms related to a group of 
clips along the container’s bottom edge (in this case, the 
terms “top breed”). 

information  via  their  “search  by  example”  functionality. 
Every time a clip is added to or removed from a container, 
the  search  preview  region  at  the  bottom  of  that  container 
updates,  to  indicate  what  query  the  system  thinks  is 
suggested  by  the  container‟s  current  clips  (Figure  7). 
Pressing  the  “search”  button  adjacent  to  the  preview 
executes  the  search,  opening  the  search  results  in  a  new 
browser  window.  The  suggested  queries  are  generated  by 
analyzing  what  terms  (excepting  stopwords)  a  group  of 
clips  has  in  common;  if  there  are  no  common  terms,  the 
algorithm  instead  composes  a  query  by  choosing  a  salient 
term  from  each  clip;  saliency  is  determined  by  heuristics 
including  the  frequency  with  which  a  term  appears  and 
whether the term is a proper noun. This functionality helps 
to reduce the need for tedious virtual keyboard text entry. 

Metadata 
WeSearch  automatically  associates  several 
types  of 
metadata  with  clips,  structured  around  the  six  journalistic 
interrogatives: 

  Who: The identity of the user who created the clip. 
  What: The content type of the clip (text, image, etc). 
  Where: The URL of the Web page the clip is from. 
  When: The timestamp of the clip‟s creation. 
  Why: The tags associated with the clip. 
  How:  The  query  keywords  used  to  find  the  clip  (or  to 

find its parent Web page). 

Exportable Record 
These  metadata  are  used  to  create  a  record  of  the  group‟s 
search session. This record is saved as an XML file, with an 
accompanying  XSL  file  that  enables  the  records  to  be 
viewed  in  any  standard  Web  browser.  Figure  8  shows  a 
sample record, viewed after a WeSearch session on a group 
member‟s  PC.  Pressing  the  “save”  button  on  a  toolbar 
creates  this  record,  as  well  as  creating  a  session  file  that 
captures the current application state, enabling the group to 
reload  and  resume  the  WeSearch  session  at  a  later  time. 
This  supports  our  design  goal  of  persistence  by  providing 

 

Figure  8. WeSearch  exports  the  session  record  as XML 
with  an  accompanying  XSL  file  that  enables  users  to 
view the record from any Web browser for post-meeting 
reflection and sensemaking. 

both persistence of the session for resumption by the group 
on the table at a later time, as well as persistence in terms of 
an  artifact  (the  XML  record) 
that  can  be  viewed 
individually  away  from 
tabletop  computer.  The 
metadata included in the record also supports sensemaking 
of  the  search  process  by  exposing  detailed  information 
about  the  lineage  of  each  clip  (i.e.,  which  group  member 
found  it,  how  they  found  it,  etc),  as  well  as  information 
about the assignment of clips to containers. 

the 

EVALUATION 
We conducted an evaluation of WeSearch in order to learn 
more about how groups would use a tabletop collaborative 
search system, and whether the features we included in light 
of our design criteria were effective.  

there 

We decided to conduct an observational study rather than a 
formal  experiment,  since 
is  no  clear  baseline 
condition  to  compare  to,  as  there  are  currently  no  other 
tabletop  Web  search  systems  of  which  we  are  aware. 
Comparing  to  a  naïve  system  (such  as  running  a  standard 
Web  browser  on  a  table)  would  yield  little  additional 
insight, as we already understand the shortcomings of such 
systems  based  on  our  own  experiences,  prior  work  on 
tabletop  systems,  and  prior  work  on  collaborative  search 
(and chose our design criteria based on those issues!). 

We  decided  to  recruit  groups  who  had  an  existing  shared 
information need, in order to observe the use of  WeSearch 
for  real,  ecologically  valid  tasks.  Questions  we  hoped  to 
address through our evaluation included: 

  What  types  of  search  tasks  would  groups  want  to 

conduct using an interactive tabletop? 

  How  will  groups  use  the  WeSearch  application;  what 
types  of  work  styles  and  group  dynamics  does  it 
engender? 

  Were  WeSearch‟s 

interface  features  effective 

in 

achieving our design goals? 

Participants 
We recruited 44 participants (11 groups with four members 
per  group)  from  outside  our  institution.  In  one  of  the 
groups, two members had an unexpected absence, resulting 
in  42  participants 
total.  Group  members  had  prior 
relationships – three were family groups (each consisting of 
two  adults  plus  children),  three  were  groups  of  college 
students  taking  classes  together,  and  five  (including  the 
two-member group) were groups of colleagues who worked 
together.  Participants‟  ages  ranged  from  10  to  54  years. 
43% were female. Participants did not have backgrounds in 
computer  science,  usability,  or  related  fields.  When  asked 
to describe their Web search skills, 1 self-rated as a novice, 
19 as average, and 22 as above-average. 

Methodology 
Sessions began with a “group interview,” during which the 
experimenter asked the group members to describe a shared 
information  need  that  is  typical  in  their  interactions  with 
one  another.  They  were  then  asked  to  describe  how  their 
group  would  normally  go  about 
this 
information  need  (i.e.,  what  tools  would  they  use,  how 
would they divide up the task, etc.). The tasks groups chose 
were: 

investigating 

  Select  the  location  for  their  company‟s  next  offsite 

meeting (colleagues: administrative assistants) 

  Conduct  background  research  on  a  plaintiff  in  a 

pending court case (colleagues: paralegals) 

  Learn about local  businesses  who might be in need of 

financial services (colleagues: bankers) 

  Research  and  describe  antimicrobial  peptides  for 

identifying bacteria (colleagues:  food scientists) 

  Compile  a  list  of  commonly  reported  problems  with  a 
their 

piece  of 
customers (colleagues:  technical support technicians) 

technology  commonly  utilized  by 

  Shop for a new home computer (family) 
  Plan an upcoming ski trip to Colorado (family) 
  Plan an upcoming trip to Montana (family) 
  Research treatments for Alzheimer‟s disease (students) 
  Learn  about  art  classes  they  might  take  together 

(students) 

  Plan  a  trip  to  Austin,  Texas  (students)  (Note  that  this 
student  group  was  unable  to  agree  upon  their  search 
task, so we prompted them with a variant of the travel 
planning  task  used  in  Paul  &  Morris‟  collaborative 
sensemaking study [23].) 

After  the  initial  interview,  groups  completed  a  tutorial,  in 
which  an  experimenter  demonstrated  each  of  WeSearch‟s 
features,  and  participants  were  able  to  try  using  each 
feature. Participants were also free to ask the experimenter 
for  assistance  during the  study  (for  instance, if  they  forgot 
how to access a particular feature). 

After completing the tutorial, groups were given 30 minutes 
to  use  WeSearch to  conduct  the  joint  search  task  they  had 
specified  during  the  group  interview.  All  interactions  with 
WeSearch were logged automatically. Sessions were video 
recorded,  and  each  session  was  observed  by 
three 
experimenters who took notes.   

the  conclusion  of 

At 
the  session,  group  members 
individually  completed  questionnaires  soliciting  feedback 
on their experience using WeSearch.  

RESULTS 
Participants  reported  that  WeSearch  was  easy  to  learn 
(median  score  of  6  on  a  7-point  Likert  scale),  and  was 
helpful in completing their chosen task (median = 5). In this 
section,  we  revisit  the  seven  challenges  we  designed  our 
system  to  address,  and  examine  how  they  impacted  the 
WeSearch  experience  based  on  log,  observation,  and 
questionnaire data. 

Awareness 
WeSearch was designed to support awareness among group 
members through (1) use of a single, shared display and (2) 
the marquee feature, which displays other group members‟ 
query terms and clips in a slowly moving stream atop each 
user‟s toolbar. 

Participants strongly agreed that seeing everyone‟s  content 
on  the  shared  display  was  a  useful  aspect  of  the  system 
(median  =  6),  and  reported  generally  high  awareness  of 
other group members‟ activities (median = 5). Additionally, 
several  users  commented  on  the  benefits  of  the  shared 
tabletop form-factor. For example, when asked to compare 
using  WeSearch 
their  current  method  of  finding 
information as a group, one user commented that she liked 
how  WeSearch‟s  shared  display  offered  the  ability  to 
“glance  over  and  see  that  someone  found  something 
interesting,” another said she liked “being able to see other 
people's  pages  &  searches  at  the  same  time,”  and  another 
reported liking that “we are able to see what each person is 
working on and can expand on that particular idea.” 

to 

The  marquee  feature  proved  successful  in  facilitating 
additional awareness, particularly  of the search terms other 
group  members  had 
typed.  One  user  described  her 
experience  with  the  marquee  by  noting,  “I  found  search 
terms coming up that I did not anticipate and this improved 
the value and speed of the searches,” and another reported 
that  “seeing  other  people's  search  queries  reminded  me  of 
things  to  search  for.”  During  the  sessions,  we  observed 
many  instances  where  the marquee  sparked awareness  and 
discussion.  For  example,  among  the  group  searching  for  a 
location  for  a  company  offsite  meeting,  one  participant 
questioned  another,  “Why  did  you  put  „weddings‟?”  when 
she  saw  that  term  scroll  past  in  her  marquee,  and  learned 
that her colleague thought wedding venues might also have 
catering  facilities  appropriate  for  corporate  meetings.  The 
marquee  made  one  of  the  bankers  aware  that  his  partner 
was searching for businesses in an area he didn‟t anticipate, 
causing  him  to  comment,  “oh,  you  did  „city  of  <anon>‟?” 

it  provided  him  of 

when he  saw  her  search  terms  in his marquee.  One  of  the 
paralegals  commented  in  surprise  when  he  noticed  a 
colleague‟s  search  term,  “McDonald‟s,”  appear  in  the 
marquee, and his colleague explained that the plaintiff had 
in  the  past  sued  that  restaurant  chain.  One  participant 
commented  that he  felt  that the  collaborative  aspect  of  the 
system  was  most  clear  in  the  marquee  feature,  due  to  the 
awareness 
the  keywords  other 
participants  were  using.  Another  participant  removed 
search terms that seemed most useful from the marquee and 
arranged  them  in  a  pile  near  his  toolbar,  in  order  to  help 
him  keep  track  of  which  directions  of  the  group‟s 
investigation 
promising.  However, 
participants  also  commented  that  they  felt  the  marquee 
sometimes became too cluttered; in particular, they felt that 
the  clips  that  appeared  in  the  marquee  took  up  too  much 
space,  and  were  not  as  valuable  as  the  search  terms,  since 
clips were already easily visible by glancing at the tabletop. 

appeared  most 

Division of Labor 
One  motivation  behind  the  design  of  WeSearch‟s  clips 
feature  was  to  facilitate  division  of  labor  among  group 
members. We envisioned that group members might divide 
up  responsibility  for  portions  of  a  web  page,  for  instance, 
by dividing it into several clips and each claiming some, or 
that users might find the different clip types returned by the 
clips-search  feature  (news, 
images,  web  results,  and 
keyword  suggestions)  to  be  a  convenient  way  to  allocate 
different media types among themselves.  However, groups 
did not  use  clips  to  divide  labor  in  this  manner,  but rather 
used clips mainly for sensemaking and clutter reduction.  

However,  the  large,  shared  display  provided  by  WeSearch 
seemed to adequately support division of labor by providing 
each  group  member  with  space  to  interact  and  facilitating 
conversation  and  awareness  through  co-presence  and  co-
visibility.  All  groups  followed  a  divide-and-conquer 
strategy for their search task, wherein group members each 
were assigned sub-tasks (either by a de facto “leader,” or by 
each  volunteering  for  a  particular  task);  group  members 
then  communicated  orally  to  resolve  questions,  update 
others  on  the  status  of  their  subtask,  and  to  share  and 
compare key findings.  

Persistence 
WeSearch  provides  persistence  of  information  through  the 
ability to save and reload the table‟s state, and the ability to 
export  an  XML 
record  containing  metadata  about 
containers  and  the  clips  they  contain,  such  as  who  found 
each item,  what  terms they  used  to  find it,  what  tags  were 
added  to  it,  and  a  link  back  to  the  url  where  each  clip 
originated.  Because  our  evaluation  took  place  during  a 
single  session, participants did not have  the  opportunity  to 
make  use  of  this  record  feature,  although  we  did  show 
participants their record webpage at the end of the session, 
to  get  their  reactions.  Participants  strongly  agreed  that  the 
ability  to  export  a  record  for  later  viewing  away  from  the 
tabletop  was  a  valuable  feature  (median  =  6).  Despite  the 
fact that the task was only occurring during a single session, 

9  of  the  11  groups  used  the  “save”  button  at  least  once 
during the task, in order to export their record and capture 
the table‟s current state, further emphasizing the value users 
attribute to persistence features. 

Sensemaking 
WeSearch  offered  several  features  designed  to  support  an 
integrated  search  and  sensemaking  cycle,  including  the 
ability to create clips representing the key portions of  web 
pages,  the  ability  to  tag  clips,  and  the  ability  to  organize 
clips in various types of containers.  

Participants appreciated the ability to create clips, although 
they preferred creating clips by selecting chunks of pages in 
their  browsers  rather  than through  the  clips-search  feature, 
which  created  clips  directly  from  search terms.  They  rated 
the ability to break up pages into clips as significantly more 
useful  (median  =  6)  than  the  ability  to  do  a  clips-search 
directly (median = 5), as confirmed by a Wilcoxon test (z = 
-2.54,  p  =  .01).  Participants  frequently  broke  pages  into 
clips (previewing the available clips by using the browser‟s 
“clips” button 74.7 times per group, and removing from the 
browser  38.3  clips  per  group,  on  average),  but  rarely 
executed  a  clips-search  (only  2.2  times  per  group,  on 
average).  The  relative  unpopularity  of  the  clips-search 
feature  seemed 
issue,  with 
participants commenting that the number of clips created at 
once  via  the  search  feature  was  overwhelming  (the  feature 
by  default  created  20  clips  relevant  to  the  current  query); 
for  example,  one  user  noted,  “clips-search  …  added  too 
much clutter to the table.”  

to  be  a  clutter 

largely 

While  making  clips  from  web  pages  was  quite  popular, 
participants rarely chose to add tags to their clips  – only 2 
of the 11 groups used this feature at all; the slowness of text 
entry  with  virtual  keyboards  may  have  contributed  to  the 
disuse  of  the  tagging  feature.  Several  participants  also 
suggested  improving  upon  our  current  clip  feature  by 
enabling manual demarcation of clips (such as by circling a 
region  of  the  page  with  one‟s  finger),  in  addition  to  the 
automatic, DOM-based chunking that our system used. 

The  ability  to  organize  clips  in  containers  in  order  to  help 
make  sense  of  the  information  the  group  had  discovered 
was  popular  among  our  participants.  All  groups  created  at 
least one container, with an average of 5.7 per group. Of the 
four  container  types,  the  list  format  was  the  most  popular; 
however,  several  participants  requested  the  ability  to 
change  a  container‟s  organizational  style  on-the-fly  (rather 
than  have  to  choose  a  style  a  priori),  so  that  they  could 
preview what their content looked like organized in varying 
fashions (lists, grids, etc.).  

While we had anticipated that containers would be used for 
sensemaking at the group, rather than individual, level, this 
was  not  typically  the  case;  participants  seemed  to  feel 
ownership  over  the  containers  they  had  created,  and 
typically each group member had one (or more) containers 
of his own. The exceptions were two of the family groups – 
in one, one of the parents created a container and instructed 

the  other  family  members  that  each  should  place  clips 
related  to  his  favorite  candidate  laptops  inside  it;  in  the 
other, one of the children created a container about weather 
conditions  in  Montana  and  instructed  the  other  family 
members  to  “throw  any  clips  you  find  for  weather  stuff 
here.” Generally, however, participants felt awkward about 
using  a  container  someone  else  had  created,  apologizing 
when they did so (such as when one participant exclaimed 
“oops, I was naming your container!”) or asking for explicit 
permission, such as when one colleague asked another “can 
I  use  your  bucket?”  Groups  that  followed  this  “individual 
sensemaking”  trend  typically  followed  up  with  a  round  of 
collective  sensemaking,  where  they  discussed  what  was  in 
each container; for example, near the end of the task one of 
the  administrative  assistants  declared  “now  it‟s  time  to 
compare [containers].” 

The metadata (who, what, where, when, why, and how) that 
was automatically associated with each clip and exposed in 
the  exported  session  records  was  designed  to  support 
individual  sensemaking  and  reflection  away  from  the 
tabletop; however, since our experiment took place during a 
single,  around-the-table  session,  we  did  not  have  the 
opportunity to evaluate that sensemaking feature. 

Text Entry 
As expected, participants reported that entering text (query 
terms, urls, tags, etc.) using our tabletop‟s virtual keyboards 
was  slower  and 
than  using  traditional 
keyboards,  expressing  disagreement  with  the  statement 
“typing  on  the  virtual  keyboard  was  easy”  (median  =  3), 
and providing several comments to that effect, such as “take 
the  keyboard  off  the  table  and  make  it  separate  hardware. 
Typing was a bottleneck to my efficiency.” 

less  pleasant 

WeSearch attempted to mediate this problem by facilitating 
easy  reuse  of  text,  such  as  by  enabling  reuse  of  terms 
appearing in the  marquee  and  clips,  and  by  the  “search  by 
example”  feature  that  suggests  search  keywords  based  on 
the set of clips in a container.  Participants made use of the 
search by example feature, though not extensively; 7 of the 
11 groups used it at least once, with an average of 2.8 uses 
per group.  

The  ability  to  reuse  items  appearing  in the  marquee  was  a 
very  popular  alternative  to  typing,  and  enabled  group 
members to leverage text entry  work already done by their 
teammates.  Groups  used  this  feature  an  average  of  16.8 
times  each  (66.2%  of  all  words  entered  in  the  query  box 
were from re-used, rather than typed, text), and participants 
strongly  agreed  that  the  ability  to  reuse  text  from  the 
marquee was helpful (median = 6). For example, when the 
group  of  administrative  assistants  was  searching  for  a 
location  for  their  company‟s  offsite  meeting,  one  woman 
said  to  another  “thanks  for  typing  „conference‟”  as  she 
pulled  that  term  from  her  marquee  and  appended  it  to  her 
own  query.  Another  participant  had  begun  typing  a  long 
keyword  when  he  saw  it  appear  in  his  marquee  because 
another  group  member  had  typed  it  already  –  “why  am  I 

typing this when it‟s on the marquee?” he said, abandoning 
typing and adding the marquee text to his query instead. 

Clutter 
WeSearch  allowed  participants  to  extract  clips  containing 
the most relevant content from web pages so that the pages 
themselves  could  be  closed,  thus  reducing  clutter  on  the 
tabletop.  Our  system  also  employed  a  large  form-factor 
table with a tiled projector system in order to alleviate space 
issues.  Nonetheless,  clutter  remained  one  of  the  most 
problematic aspects of the WeSearch experience, prompting 
tense dialogue between participants such as “will you move 
your  stuff?”,  “you‟re  taking  up  my  space,  man”,  and  “I‟ll 
just start stealing Dad‟s space.” These remarks also reflect 
participants‟  sense  of  ownership  over  a  portion  of  space, 
reconfirming  Scott  et  al.‟s  findings  regarding  territoriality 
on  tabletop  displays  [27].  Participants  strongly  agreed  that 
the table was too cluttered during the activity (median = 6), 
and 18 participants (43%) mentioned clutter as a problem in 
their 
responses.  Common 
suggestions  made  by  our  participants  on  the  post-task 
questionnaire included the ability to group items and mark 
them for simultaneous deletion (i.e., deleting an entire pile 
of  clips  at  once),  and  allowing  users  to  hide  or  minimize 
their  toolbars  (which  are  currently  always  visible  in 
WeSearch).  

questionnaire 

free-form 

Orientation 
As  mentioned  in  the  “Awareness”  section,  the  marquee 
appeared to successfully address the issue of reading text at 
odd  orientations  by  providing  a  user-oriented  view  of  the 
search  terms  other  group  members  were  typing.  However, 
WeSearch  permitted  clips  and  browser  windows  to  be 
freely  rotated  via  two-fingered  manipulations  in  order  to 
permit  the  use  of  orientation  for  communicative  purposes, 
as  suggested  by  Kruger  et  al  [14].  Participants  frequently 
performed  rotations  unintentionally  (while  scaling  or 
moving documents), and spent time trying to return browser 
windows  to  straight  orientations  –  even  slight  angles 
seemed  to  annoy  our  participants,  several  of  whom 
requested  that  browsers  and  clips  should  “snap”  to  align 
with  table  edges  rather  than  be  fully  rotatable  (such  as  in 
[30]).  One  participant  captured  this  sentiment  on  his 
questionnaire,  indicating  that  we  should  “change  the 
sensitivity  to  rotation.  I  spent  too  much  time  having  to 
continually readjust the rotation.” 

DISCUSSION 
In  light  of  our  findings,  we  revisit  the  questions  that 
motivated our evaluation: 

What  types  of  search  tasks  would  groups  want  to  conduct 
using an interactive tabletop? 

types 

(families, 

In  our  study  we  had  the  opportunity  to  observe  groups  of 
several 
colleagues) 
conducting  collaborative  searches  on  topics  of  interest  to 
them.  Our  post-study  questionnaire  also  asked  participants 
to discuss any other information needs they had where they 
might want to use a system such as  WeSearch. In addition 

students, 

and 

to providing examples of several specific productivity tasks 
(i.e.  job  or  school-related  joint research  tasks) and  family-
oriented tasks (i.e. shopping, travel planning, entertainment 
planning)  involving  collaboration  among  groups  of  peers, 
several  participants  also  suggested  that  a  collaborative 
tabletop  search  system  might  be  valuable  in  situations 
where a professional and a customer work together, such as 
a librarian and a student finding information together, or a 
salesperson  helping  a  client  to  explore  various  product 
options together. 

How will groups use the WeSearch application; what types 
of work styles and group dynamics does it engender? 

WeSearch  supported  natural  transitions  between  closely- 
and loosely-coupled (i.e., parallel) work styles, with groups 
typically  dividing  a  search  task  into  sub-tasks  on  which 
members worked in parallel, with interwoven discussion of 
items  of  interest  to  the  group  at  large,  such  as  interesting 
results discovered. 

Were  WeSearch’s  interface  features  effective  in  achieving 
our design goals? 

Awareness:  WeSearch  adequately  supported  awareness 
among  group  members;  the  marquee  feature,  in  particular, 
provided heightened awareness of the query terms used by 
other  group  members,  which  stimulated  discussions  about 
search strategy among group members. 

Division of Labor: While the clips feature did not support 
division  of  labor  as  originally  envisioned,  the  tabletop 
environment  itself,  which  supported  individual  action  and 
shared communication, was adequate for enabling effective 
division of work among group members. 

Persistence:  The  system‟s  record-exporting  and  state-
saving  features  received  positive  reactions  from  our 
participants. 

their 

to 
Sensemaking:  WeSearch  enabled  group  members 
integrate 
activities 
seamlessly. The ability to create clips and to organize them 
in  containers  was  highly  valued,  although 
tagging 
capabilities were underutilized. 

sensemaking 

search 

and 

the 

Text Entry: The ability to reuse existing text rather than re-
typing  everything  on  virtual  keyboards  was  highly 
appreciated,  although  participants  still  found  the  occasions 
when  they  needed  to  use  the  virtual  keyboards  to  be 
frustrating.  Augmenting 
tabletop  with  physical 
keyboards (perhaps fixed in place around the bezels  of the 
display so that they don‟t add to the clutter problem) might 
address  this  frustration.  Since  text  entry  was  a  key 
frustration  for  users,  we  expect  that  offering  physical 
keyboards may change work styles from what we observed; 
however, evaluating WeSearch without physical keyboards 
to  optimize 
offered  value 
productivity 
common  hardware 
configuration, such as by maximizing text reusability. 

in  understanding  how 

systems 

this 

for 

to 

the 

Clutter: The ability to create a clip and then close the web 
page  it  came  from  helped  to  reduce  clutter  somewhat. 
Despite this, clutter remained a key frustration  when using 
WeSearch  due 
large  number  of  documents 
participants  explored  during  their  search  tasks.  Further 
clutter  reduction  techniques,  such  as  ZoomScapes  [6], 
might  prove  valuable.  A  higher-resolution  display  would 
also  reduce  clutter,  since  objects  could  be  legible  at  a 
smaller  size.  Note  that  popular  commercially-available 
tabletops such as DiamondTouch [2] and Microsoft Surface 
would  not  be  appropriate  for  collaborative  search  and 
sensemaking tasks due to their small sizes (since the custom 
table we used in our study was over twice as large as these 
technologies and still suffered from clutter issues). We note 
that a common suggestion made by participants of allowing 
the  toolbar  to  be  minimized  would  reduce  its  benefits  for 
awareness,  and  so  alternative  clutter-reduction  approaches 
would be preferable.  

Orientation:  While  the  marquee  provided  group  members 
with  a  way  to  see  other  users‟  text  at  a  right-side-up 
orientation,  group  members  were  more  concerned  with 
maintaining  a  right-side-up  orientation  for  items  that 
belonged  to  them  –  the  freedom  to  reorient  objects  freely 
confused  them  and  they  did  not  seem  interested  in  the 
communicative  benefits  of  orientation; 
they 
indicated  a  strong  preference  for  forced  rectilinear 
orientations for text-heavy objects such as web browsers. 

rather, 

In  light  of  these  findings,  we  believe  that  WeSearch 
effectively  illustrates  the  potential  of  interactive  tabletops 
as  a  platform  for  collaborative  search  tasks,  by  easily 
facilitating  awareness  and  division  of  labor,  which  PC-
based  collaborative  search  tools  struggle  to  enable  [1,  18], 
and by offering interface solutions to challenges specific to 
the tabletop form-factor, such as text-entry and clutter. Note 
that  by  choosing  a  holistic  evaluation,  we  were  unable  to 
evaluate alternative designs for any particular UI innovation 
introduced  by  WeSearch;  while  such  assessments  would 
also be beneficial, we leave them to future work. 

CONCLUSION AND FUTURE WORK 
In this paper, we investigated whether interactive tabletops 
could be an effective platform for facilitating collaborative 
Web  search.  The  contributions  of  this  work  included:  (1) 
Identifying  seven  design  criteria  for  successful  tabletop 
search  systems,  based  on  a  review  of  literature  from  both 
the  collaborative  search  and  tabletop  communities;  (2) 
WeSearch,  a  system  that  addresses  these  design  criteria 
through  a  combination  of  novel  interface  features;  and  (3) 
An evaluation of WeSearch, in which 42 participants (in 11 
groups)  performed 
thereby 
providing data on the effectiveness of WeSearch‟s interface 
features as well as on how groups work together using this 
new type of system. 

real-world  search 

tasks, 

We  found  that  WeSearch‟s  marquee  feature  enhanced 
awareness  of  group  members‟  search  terms,  clips  and 
containers supported sensemaking as an integral part of the 

search process, and text-reuse and report-exporting features 
were  highly  valued.  The  main  obstacles  to  a  smooth 
tabletop  search  experience  were  hardware-based:  larger, 
higher-resolution  tabletops  would  alleviate  clutter  issues, 
and  the integration  of  physical  keyboards  would  provide  a 
more positive text-entry experience. 

The  success  of  WeSearch 
in  facilitating  co-located 
collaborative  search  shows  the  potential  for  tabletop 
computers  to  serve  this  need.  Our  design  criteria,  system, 
and study  findings provide a  first step toward adapting the 
properties of tabletop displays for group Web search tasks.  

REFERENCES 
1.  Amershi, S. and Morris, M.R. CoSearch: A System for Co-
located Collaborative Web Search.  CHI 2008, 1647-1656. 

2.  Dietz, P. and Leight, D. DiamondTouch: A Multi-User Touch 

Technology.  UIST 2001, 219-226. 

3.  Dontcheva, M., Drucker, S., Wade, G., Salesin, D., and Cohen, 

M. Summarizing Personal Web Browsing Sessions.  UIST 
2006, 115-124.  

4.  Everitt, K., Shen, C., Ryall, K., and Forlines, C. DocuBits and 
Containers: Providing e-Document Micro-mobility in a Walk-
Up Interactive Tabletop Environment.  Interact 2005. 

5.  Fidel, R., Bruce, H., Peitersen, A., Dumais, S., Grudin, J., and 

Poltrock, S. Collaborative Information Retrieval. Review of 
Information Behavior Research 1, 1 (2000). 

6.  Guimbretiere, F., Stone, M., and Winograd, T. Fluid 

Interaction with High-Resolution Wall-Size Displays.  UIST 
2001, 21-30. 

7.  Han, J. Low-Cost Multi-Touch Sensing through Frustrated 

Total Internal Reflection.  UIST 2005, 115-118. 

8.  Hancock, M., Vernier, F. D., Wigdor, D., Carpendale, S., and 

Shen, C. Rotation and Translation Mechanisms for Tabletop 
Interaction.  Tabletop 2006, 79-86. 

9.  Hansen, P., and Jarvelin, K. Collaborative Information 

Retrieval in an Information-Intensive Domain. Information 
Processing and Management 41, 5 (2005), 1101-1119. 

10. Hartmann, B., Morris, M.R., Benko, H., and Wilson, A. 

Augmenting Interactive Tables with Mice & Keyboards. UIST 
2009. 

11. Hinrichs, U., Carpendale, S., and Scott, S. D. Interface 

currents: supporting fluent face-to-face collaboration. In ACM 
SIGGRAPH 2005 Sketches. 

12. Hinrichs, U., Hancock, M., Collins, C., and Carpendale, S. 

Examination of Text-Entry Methods for Tabletop Displays.  
IEEE Tabletop 2007, 105-112. 

13. Isenberg, P. and Fisher, D. Collaborative Brushing and 
Linking for Co-located Visual Analytics of Document 
Collections.  Eurographics/IEEE-VGTC Symposium on 
Visualization (EuroViz 2009), 1031-1038. 

14. Kruger, R., Carpendale, M.S.T., Scott, S.D., and Greenberg, S. 

Roles of Orientation in Tabletop Collaboration: 
Comprehension, Coordination and Communication. Computer 
Supported Cooperative Work, Volume 14, Issue 5-6, December 
2004, 501-537.  

15. Large, A., Beheshti, J., and Rahman, T. Gender Differences in 

Collaborative Web Searching Behavior: An Elementary 

School Study. Information Processing and Management 38, 3 
(2002), 427-443. 

16. Maekawa, T., Hara, T., and Nishio, S. A Collaborative Web 

Browsing System for Multiple Mobile Users.  PERCOM 2006. 

17. Morris, M.R., Paepcke, A., and Winograd, T. TeamSearch: 

Comparing Techniques for Co-Present Collaborative Search of 
Digital Media.  IEEE Tabletop 2006, 97-104. 

18. Morris, M.R. and Horvitz, E. SearchTogether: An Interface for 

Collaborative Web Search.  UIST 2007, 3-12. 

19. Morris, M.R. A Survey of Collaborative Web Search 

Practices.  CHI 2008, 1657-1660. 

20. Morris, M.R., Brush, A.J.B., and Meyers, B. A Field Study of 
Knowledge Workers‟ Use of Interactive Horizontal Displays.  
IEEE Tabletop 2008, 113-120. 

21. Morris, M.R. Supporting Effective Interaction with Tabletop 
Groupware. Stanford University Technical Report (Doctoral 
Dissertation), April 2006. 

22. Paek, T., Agrawala, A., Basu, S., Drucker, S., Kristjansson, T., 

Logan, R., Toyoma, K., and Wilson, A. Toward Universal 
Mobile Interaction Shared Displays.  CSCW 2004, 266-269. 

23. Paul, S. and Morris, M.R. CoSense: Enhancing Sensemaking 

for Collaborative Web Search.  CHI 2009, 1771-1780. 

24. Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., and 

Back, M. Algorithmic Mediation for Collaborative 
Exploratory Search.  SIGIR 2008, 315-322. 

25. Russell, D.M., Stefik, M.J., Pirolli, P., and Card, S.K. The 

Cost Structure of Sensemaking.  CHI 1993, 269-276. 

26. schraefel, m.c., Zhu, Y., Modjeska, D., Wigdor, D., and Zhao, 

S. Hunter-Gatherer. Interaction Support for the Creation and 
Management of Within-Web-Page Collections.  WWW 2002. 

27. Scott, S.D., Carpendale, M.S.T., and Inkpen, K. Territoriality 

in Collaborative Tabletop Workspaces.  CSCW 2004, 294-303. 

28. Sellen, A., Kurtenbach, G., and Buxton, W. (1992). The 

prevention of mode errors through sensory feedback. Human 
Computer Interaction, 7(2). p. 141-164. 

29. Shen, C., Lesh, N., Vernier, F., Forlines, C., and Frost, J. 

Building and Sharing Digital Group Histories.  CSCW 2002, 
324-333. 

30. Shen, C. Vernier, F., Forlines, C., and Ringel, M. 

DiamondSpin: An Extensible Toolkit for Around-the-Table 
Interaction.  CHI 2004, 167-174. 

31. Smeaton,  A.F.,  Lee,  H.,  Foley,  C.,  and  McGivney,  S. 
Collaborative  Video  Searching  on  a  Tabletop.  Multimedia 
Systems Journal, 2006, Vol. 12, No. 4, 375-391. 

32. Stewart,  J.,  Bederson,  B.,  and  Druin,  A.  Single  Display 
Groupware:  A  Model  for  Co-present  Collaboration.    CHI 
1999, 286-293. 

33. Twidale,  M.,  Nichols,  D.,  and  Paice,  C.  Browsing  is  a 
and 

Information  Processing 

Collaborative 
Management 33, 6 (1997), 761-783. 

Process. 

34. Wigdor,  D.  and  Balakrishnan,  R.  Empirical  Investigation  into 
the  Effect  of  Orientation  on  Text  Readability  in  Tabletop 
Displays.  ECSCW 2005, 205-224.  

35.Wigdor, D., Penn, G., Ryall, K., Esenther, A., Shen, C. Living 
with  a  Tabletop:  Analysis  and  Observations  of  Long  Term 
Office Use of a Multi-Touch Table.  Tabletop 2007. 

",False,2010.0,{},False,False,conferencePaper,False,L239LNLV,[],self.user,False,False,False,False,http://portal.acm.org/citation.cfm?doid=1718918.1718987,,WeSearch: supporting collaborative search and sensemaking on a tabletop display,L239LNLV,False,False
YA4MYR2A,UPR74L2P,"Eurographics Conference on Visualization (EuroVis) (2015)
R. Borgo, F. Ganovelli, and I. Viola (Editors)

STAR – State of The Art Report

Visualizing High-Dimensional Data:

Advances in the Past Decade

S. Liu1, D. Maljovec1, B. Wang1, P.-T. Bremer2 and V. Pascucci1

1Scientiﬁc Computing and Imaging Institute, University of Utah

2Lawrence Livermore National Laboratory

Abstract
Massive simulations and arrays of sensing devices, in combination with increasing computing resources, have
generated large, complex, high-dimensional datasets used to study phenomena across numerous ﬁelds of study.
Visualization plays an important role in exploring such datasets. We provide a comprehensive survey of advances
in high-dimensional data visualization over the past 15 years. We aim at providing actionable guidance for data
practitioners to navigate through a modular view of the recent advances, allowing the creation of new visualiza-
tions along the enriched information visualization pipeline and identifying future opportunities for visualization
research.

Categories and Subject Descriptors (according to ACM CCS):
Generation—Line and curve generation

I.3.3 [Computer Graphics]: Picture/Image

1

Introduction

With the ever-increasing amount of available computing
resources, our ability to collect and generate a wide vari-
ety of large, complex, high-dimensional datasets continues
to grow. High-dimensional datasets show up in numerous
ﬁelds of study, such as economy, biology, chemistry, polit-
ical science, astronomy, and physics, to name a few. Their
wide availability, increasing size and complexity have led
to new challenges and opportunities for their effective vi-
sualization. The physical limitations of the display devices
and our visual system prevent the direct display and instanta-
neous recognition of structures with higher dimensions than
two or three. In the past decade, a variety of approaches have
been introduced to visually convey high-dimensional struc-
tural information by utilizing low-dimensional projections
or abstractions: from dimension reduction to visual encod-
ing, and from quantitative analysis to interactive exploration.
A number of surveys have focused on different aspects of
high-dimensional data visualization, such as parallel coordi-
nates [Ins09, HW13], quality measures [BTK11], clutter re-
duction [ED07], visual data mining [HG02, Kei02, DOL03],
and interactive techniques [BCS96]. High-dimensional as-
pects of scientiﬁc data have also been investigated within the
surveys [BH07,KH13]. The surveys [WB94,Cha06,Mun14]
focus on the various aspects of visual encoding techniques

c(cid:13) The Eurographics Association 2015.

for multivariate data. These papers provide a valuable sum-
mary of existing techniques and inspiring discussions of fu-
ture directions in their respective domains. However, few
surveys in the past decade have aimed at providing a general,
coherent, and uniﬁed picture that addresses the full spectrum
of techniques for visualizing high-dimensional data.

We provide a comprehensive survey of advances in high-
dimensional data visualization over the past 15 years, with
the following objectives: providing actionable guidance for
data practitioners to navigate through a modular view of the
recent advances, allowing the creation of new visualizations
along the enriched information visualization pipeline, and
identifying opportunities for future visualization research.

Our contributions are as follows. First, we propose a cat-
egorization of recent advances based on the information vi-
sualization (InfoVis) pipeline [CMS99] enriched with cus-
tomized action-driven classiﬁcations (Figure 2, Section 2).
We further assess the amount of interplay between user in-
teractions and pipeline-based categorization and put user in-
teractions into a measurable context (Table 1, Section 6).
Second, we highlight key contributions of each advancement
(Sections 3, Section 4, Section 5). In particular, we provide
an extensive survey of visualization techniques derived from
topological data analysis (Section 3.5, Section 4.4), a new
area of study that provides a multi-scale and coordinates-

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

free summary of high-dimensional data [Car09]. Further-
more, we connect advances in high-dimensional data visu-
alization with volume rendering and machine learning (Sec-
tion 7). Finally, we reﬂect on our categorization with respect
to actionable tasks, and identify emerging future directions
in subspace analysis, model manipulation, uncertainty quan-
tiﬁcation, and topological data analysis (Section 8).

Figure 1: Interactive survey website for paper navigation.

2 Survey Method and Categorization

We conduct a thorough literature review based on relevant
works from major visualization venues, namely Visweek,
EuroVis, PaciﬁcVis, and the journal IEEE Transactions on
Visualization and Computer Graphics (TVCG) from the pe-
riod between 2000 and 2014. To ensure the survey covers
the state-of-the-art, we further selectively search through ref-
erences within the initial set of papers. Beyond the visual-
ization ﬁeld, we also dedicate special attention to the ex-
ploratory data analysis techniques in the statistics commu-
nity. Through such a rigorous search process, we have iden-
tiﬁed more than 200 papers that focus on a wide spectrum
of techniques for high-dimensional data visualization. To
help organize the large quantity of papers, we have produced
an interactive survey website (www.sci.utah.edu/
~shusenl/highDimSurvey/website, based on the
SurVis [Bec14] framework; a screen shot is shown in Fig-
ure 1) that allows readers to interactively select and ﬁlter
papers through various tags. However, due to the space limi-
tation, only a subset of the complete list of references (avail-
able through the survey website) is mentioned in the paper.
As illustrated in Figure 2, we base our main catego-
rization on the three transformation steps of the informa-
tion visualization pipeline [CMS99] (and its minor varia-
tion in [BTK11]), namely, data transformation, visual map-
ping, and view transformation. Each category is enriched
with novel, customized subcategories. Data transformation
(Section 3) corresponds to the analysis-centric methods such
as dimension reduction, regression, subspace clustering, fea-
ture extraction, topological analysis, data sampling, and ab-
straction. Visual mapping (Section 4), the key for most vi-
sual encoding tasks, focuses on organizing the information
from the data transformation stage for visual representa-

tion. This category includes visual encodings based on axes
(e.g., scatterplots and parallel coordinate plots), glyphs, pix-
els, and hierarchical representations; together with anima-
tion and perception. View transformation (Section 5) corre-
sponds to methods focusing on screen space and rendering,
including illustrative rendering for various visual structures,
as well as screen space measures for reducing clutter or arti-
facts and highlighting important features.

Such a design allows us to easily classify the core con-
tribution of vastly different methods that operate on en-
tirely different objects, but at the same time, reveal their
interconnections through the linked pipeline. In addition,
the pipeline-based categorization provides the reader with
a modular view of the recent advances, allowing new sys-
tems to be conﬁgured based on possibilities provided by the
reviewed methods.

User interactivity is an integral part within each pro-
cessing step of the pipeline, as illustrated in Figure 2.
Based on the amount of user interaction, we can classify
all high-dimensional data visualization methods into three
categories: computation-centric, interactive exploration, and
model manipulation. The distinction between interactive ex-
ploration and model manipulation is made to emphasize a
particular manipulation paradigm, where the underlying data
model is modiﬁed based on interaction to reﬂect user inten-
tion. A summary of the interplay between processing steps
and interactions is illustrated in Table 1, where user interac-
tions are put into a measurable context. The corresponding
details are discussed in Section 6.

3 Data Transformation

We start by describing different types of high-dimensional
datasets. We then give an in-depth discussion on the action-
driven subcategories centered around typical analysis tech-
niques during data transformation, namely, dimension re-
duction, clustering (in particular, subspace clustering) and
regression analysis. We focus especially on their usages in
visualization methods. In addition, we pay special attention
to topological data analysis, which is a promising emerging
ﬁeld.
3.1 High-Dimensional Data

We provide an overview of the different aspects of high-
dimensional datasets, to deﬁne the scope of our discussion
and highlight distinct properties of these datasets. Our dis-
cussions on different data types are inspired by the book by
Munzner [Mun14].
Data Types. In our survey, we limit our exposition to
table-based data, and exclude (potentially high-dimensional)
graph/network data from the discussion. A high-dimensional
dataset is commonly modeled as a point cloud embedded in
a high-dimensional space, with the values of attributes cor-
responding to the coordinates of the points. Based on the un-
derlying model of the data and the analysis and visualization

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

Figure 2: Categorization based on transformation steps within the information visualization pipeline, with customized action-
driven subcategories.

goals, the attributes consist of input parameters and output
observations, and the data could be modeled as a scalar or
vector-valued function (where the function values are based
on the output observations) on the point cloud deﬁned by the
input parameters. Topological data analysis (Section 3.5) ap-
plies to both point cloud data and functions on point cloud
data (e.g., [GBPW10, SMC07]), while regression analysis
(Section 3.4) typically applies to the latter (e.g., [PBK10]).
Attribute Types. The attribute type (e.g., nominal vs. nu-
merical) can greatly impact the visualization method. In
many ﬁelds and applications, the value of the attributes is
nominal in nature. However, most commonly available high-
dimensional data visualization techniques such as scatter-
plots or parallel coordinate plots are designed to handle
numerical values only. When utilizing these methods for
visualizing nominal data, information overlapping and vi-
sual elements stacking usually exist. One way to address
the challenge is mapping the nominal values to numeri-
cal values [RRB∗04] (e.g. as implemented in the Xmdv-
Tool [War94]). Through such a mapping, each axis is used
more efﬁciently and the spacing becomes more meaningful.
In the Parallel Sets work [BKH05], the authors introduce a
new visual representation that adapts the notion of parallel
coordinates but replaces the data points with a frequency-
based visual representation that is designed for nominal
data. The Conjunctive Visual Form [Wea09] allows users to
rapidly query nominal values with certain conjunctive rela-
tionships through simple interactions. The GPLOM (Gener-
alized Plot Matrix) [IML13] extends the Scatterplot Matrix
(SPLOM) to handle nominal data.
Spatiotemporal Data. Some recent advances focus on de-
veloping visual encoding that capture the spatiotemporal as-

c(cid:13) The Eurographics Association 2015.

pects of high-dimensional data. Visual analysis of the ﬁnan-
cial time series data is explored in the work by Ziegler et
al. [ZJGK10]. The work presented by Tam et al. [TFA∗11]
studies facial dynamics utilizing the analysis of time-series
data in parameter space. Datasets with spatial information
such as multivariate volumes [BDSW13] or multi-spectral
images [LAK∗11] are very common in scientiﬁc visualiza-
tion, and numerous methods have been introduced within the
scientiﬁc visualization domain, see [BH07, KH13] for com-
prehensive surveys on these topics. We discuss the intrinsic
interconnections between these two areas in Section 7.
3.2 Dimension Reduction

Dimension reduction techniques are key components for
many visualization tasks. Existing work either extends the
state-of-the-art techniques, or improves upon their capabili-
ties with additional visual aid.
Linear Projection. Linear projection uses linear transfor-
mation to project the data from a high-dimensional space to
a low-dimensional one. It includes many classical methods,
such as Principal component analysis (PCA), Multidimen-
sional scaling (MDS), Linear discriminate analysis (LDA),
and various factor analysis methods.

PCA [Jol05] is designed to ﬁnd an orthogonal linear
transformation that maximizes the variance of the result-
ing embedding. PCA can be calculated by an eigende-
composition of the data’s covariance matrix or a singular
value decomposition of the data matrix. The interactive PCA
(iPCA) [JZF∗09] introduces a system that visualizes the re-
sults of PCA using multiple coordinated views. The system
allows synchronized exploration and manipulations among
the original data space, the eigenspace, and the projected
space, which aids the user in understanding both the PCA

Source DataData TransformationViewTransformationVisualMappingTransformedDataVisualStructureViewsUserUser InteractionsDimension Reduction linear projection[KC03], non-linear DR[WM04],Control Points Projection[DST04],Distance Metric[LMZ∗14], Precision Measures[LV09]Data TransformationVisual MappingView TransformationAxis BasedScatterplot Matrix[WAG06],Parallel Coordinate[JJ09],Radial Layout[LT13],Hybrid Construction[YGX∗09, CvW11]Illustrative RenderingIllustrative PCP[MM08], Illuminated 3D scatterplot[SW09],PCP density basedtransfer function[JLJC05]Subspace ClusteringDimension Space Exploration [TFH11, YRWG13],Subset of Dimension[TMF∗12],Non-Axis-Parallel Subspace[Vid11, AWD12]Topological Data AnalysisMorse-Smale Complex[GBPW10, CL11],Reeb Graph & Contour Tree[PSBM07],Topological Features[WSPVJ11]Regression AnalysisOptimization & Design Steering[BPFG11, DW13],Structural Summaries[PBK10, GBPW10]GlyphsPer-Element Glyphs[CCM10, GWRR11][CCM13],Multi-Object Glyphs [War08, CGSQ11]Pixel-OrientedJigsaw Map,Pixel Bar Charts[KHL01],Value & RelationDispaly[YHW∗07]Hierarchy BasedDimension Hierarchy[WPWR03], Topology-based Hierarchy[HW10, OHWS13],Others[ERHH11]AnimationGGobi[SLBC03],TripAdvisorND[NM13],Rolling the Dice[EDF08]EvaluationScatterplot Guideline[SMT13],PCPs Effectiveness,[HVW10],Animation [HR07]Continuous Visual RepresentationContinuous Scatterplot[BW08], Continuous Parallel Coordiante[HW09, LT11],Splatterplots[MG13]Accurate Color BlendingHue-Preserving Blending[KGZ∗12],Weaving vs. Blending[HSKIH07]Image Space MetricsClutter Reduction[AdOL04, JC08],Pargnostics[DK10],Pixnostic[SSK06]S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

process and the dataset. When visualizing labeled data, class
separation is usually desired. Methods such as LDA aim to
provide a linear projection that maximizes the class separa-
tion. The recent work by Koren et. al. [KC03] generalizes
PCA and LDA by providing a family of ﬂexible linear pro-
jections to cope with different kinds of data.
Non-linear Dimension Reduction. There are two distinct
groups of techniques in non-linear dimension reduction, un-
der either the metric or non-metric setting. The graph-based
techniques are designed to handle metric inputs, such as
Isomap [TDSL00], Local Linear Embedding (LLE) [RS00],
and Laplacian Eigenmap (LE) [BN03], where a neighbor-
hood graph is used to capture local distance proximities and
to build a data-driven model of the space.

The other group of techniques address non-metric prob-
lems commonly referred to as non-metric MDS or stress-
based MDS by capturing non-metric dissimilarities. The
fundamental idea behind the non-metric MDS is to mini-
mize the mapping error directly through iterative optimiza-
tions. The well-known Shepard-Kruskal algorithm [Kru64]
begins by ﬁnding a monotonic transformation that maps the
non-metric dissimilarities to the metric distances, which pre-
serves the rank-order of dissimilarity. Then, the resulting
embedding is iteratively improved based on stress. The pro-
gressive and iterative nature of these methods has been ex-
ploited recently by Williams et al. [WM04], where the user
is presented with a coarse approximation from partial data.
The reﬁnement is on-demand based on user inputs.
Control Points Based Projection. For handling large and
complex datasets, the traditional linear or non-linear di-
mension reductions are limited by their computational efﬁ-
ciency. Some recent developments, e.g., [DST04, PNML08,
PEP∗11a, JPC∗11, PSN10], utilize a two-phases approach,
where the control points (anchor points) are projected ﬁrst,
followed by the projection of the rest of the points based on
the control points location and local features preservation.
Such designs lead to a much more scalable system. Further-
more, the control points allow the user to easily manipulate
and modify the outcome of the dimension reduction compu-
tation to achieve desirable results.
Distance Metric. For a given dimension reduction algo-
rithm, a suitable distance metric is essential for the com-
putation outcome as it is more likely to reveal important
structural information. Brown et al. [BLBC12] introduce
the distance function learning concept, where a new dis-
tance metric is calculated from the manipulation of point
layouts by an expert user. In [Gle13], the author attempts
to associate a linear basis with a certain meaningful con-
cept constructed based on user-deﬁned examples. Machine
learning techniques can then be employed to ﬁnd a set of
simple linear bases that achieve an accurate projection ac-
cording to the prior examples. The structure-based analysis
method [LMZ∗14] introduces a data-driven distance metric

inspired by the perceptual processes of identifying distance
relationships in parallel coordinates using polylines.
Dimension Reduction Precision Measure. One of the fun-
damental challenges in dimension reduction is assessing and
measuring the quality of the resulting embeddings. Lee et al.
introduce the ranking-based metric [LV09] that assesses the
ranking discrepancy before and after applying dimension re-
duction. This technique is then generalized [MLGH13] and
used for visualizing dimension reduction quality. A projec-
tion precision measure is introduced in [SvLB10], where a
local precision score is calculated for each point with a cer-
tain neighborhood size. In the distortion-guided exploration
work [LWBP14], several distortion measures are proposed
for different dimension reduction techniques, where these
measures aid in understanding the cause of highly distorted
areas during interactive manipulation and exploration. For
MDS, the stress can be used as a precision measure. Seifert
et al. [SSK10] further develop this idea by incorporating the
analysis and visualization for better understanding of the lo-
calized stress phenomena.
3.3 Subspace Clustering

Clustering is one of the most widely used data-driven
analysis methods. Instead of providing an in-depth discus-
sion on all clustering techniques, in this survey, we fo-
cus on subspace clustering techniques which have a great
impact for understanding and visualizing high-dimensional
datasets. Dimension reduction aims to compute one sin-
gle embedding that best describes the structure of the data.
However, this could become ineffective due to the increas-
ing complexity of the data. Alternatively, one could perform
subspace clustering, where multiple embeddings can be gen-
erated through clustering either the dimensions or the data
points, for capturing various aspects of the data.
Dimension Space Exploration. Guided by the user, dimen-
sion space exploration methods interactively group relevant
dimensions into subsets. Such exploration allows us to better
understand their relationships and to identify shared patterns
among the dimensions. Turkay et al. introduce a dual visual
analysis model [TFH11] where both the dimension embed-
ding and point embedding can be explored simultaneously.
Their later improvement [TLLH12] allows for the group-
ing of a collection of dimensions as a factor, which per-
mits effective exploration of the heterogeneous relationships
among them. The Projection Matrix/Tree work [YRWG13]
extends a similar concept to allow a recursive exploration of
both the dimension space and data space. Several visual en-
coding methods also rely on the concept of dimension space
exploration. These methods are discussed in Section 4.3.
Clustering Subsets of Dimensions. Comparing to the di-
mension space exploration, where the user is responsible
for identifying patterns and relationships, subspace clus-
tering/ﬁnding methods automatically group related dimen-
sions into clusters. Subspace clustering ﬁlters out the in-
terferences introduced by irrelevant dimensions, allowing

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

lower-dimensional structures to be discovered. These meth-
ods, such as ENCLUS [CFZ99], originate from the data
mining and knowledge discovery community. They in-
troduce some very interesting exploration strategies for
high-dimensional datasets, and can be particularly effec-
tive when the dimensions are not
tightly coupled. The
TripAdvisorND [NM13] system employs a sightseeing
metaphor for high-dimensional space navigation and explo-
ration. It utilizes subspace clustering to identify the sights
for the exploration. The subspace search and visualization
work [TMF∗12] utilizes the SURFING (subspaces rele-
vant for clustering) [BPR∗04] algorithm to search the high-
dimensional space and automatically identiﬁes a large can-
didate set of interesting subspaces. For the work presented
by Ferdosi et al. [FBT∗10], morphological operators are ap-
plied on the density ﬁeld generated from the (3D) PCA pro-
jection of the high-dimensional data for identifying subspace
clusters.
Non-Axis-Aligned Subspaces. Instead of clustering the di-
mensions, which essentially creates axis-aligned linear sub-
spaces, identifying non-axis-aligned subspaces is a more
ﬂexible alternative. Projection Pursuit [FT74] is one of the
earliest works aimed at automatically identifying the inter-
esting non-axis-aligned subspaces. Projections are consid-
ered to be more interesting when they deviate more from a
normal distribution. Some advances have been made in the
machine learning community to perform non-axis-aligned
subspace clustering [Vid11]. Instead of clustering the dimen-
sions, the points are grouped together for sharing similar lin-
ear subspaces. In particular, we assume the complex struc-
ture of the data can be approximated by a mixture of linear
subspaces (of varying dimensions), and each of the linear
subspaces corresponds to a set of points where their rela-
tionships can be approximately captured by the same linear
subspace.

For very high-dimensional data, the subspace ﬁnding
algorithms typically have a relatively high computational
complexity. By utilizing random projection, Anand et
al. [AWD12] introduce an efﬁcient subspace ﬁnding algo-
rithm for data with thousands of dimensions. It generates a
set of candidate subspaces through random projections and
presents the top-scoring subspaces in an exploration tool.
3.4 Regression Analysis

Regression analysis in high dimension is an extensive and
active ﬁeld of research in its own right. We make no attempt
to survey the entire area, but rather focus on the interplay
between visualization and regression analysis.
Optimization and Design Steering. Pure optimization
problems often are not the focus in the visualization com-
munity. What is more common are design steering methods
where, in addition to a multivariate input space, the user has
one or several output or response variables they want to ex-
plore (e.g., [BPFG11,TWSM∗11]), where the results require
a qualitative examination, or are used to inform decisions.

c(cid:13) The Eurographics Association 2015.

HyperMoVal [PBK10] is a software system used for val-
idating regression models against actual data. It uses sup-
port vector regression (SVR) [SS04b] to ﬁt a model to high-
dimensional data, highlights discrepancies between the data
and the model, and computes sensitivity information on the
model. The software allows for adding more model param-
eters to reﬁne their regression to an acceptable level of ac-
curacy. Berger et al. [BPFG11] utilize two different types of
regression models (SVR and nearest neighbor regression) to
analyze a trade-off study in performance car engine design.
Utilizing the predictive power of the regression, they are able
to provide a guided navigation of the high-dimensional space
centered around a user-selected focal point. The user adjusts
the focal point through multiple linked views, and sensitiv-
ity and uncertainty information are encoded around the focal
point.

Tuner [TWSM∗11] begins as an automated adaptive sam-
pling algorithm where a sparse sampling of the parame-
ter space is reﬁned by building a Gaussian Process Model
(GPM) (see [RW06] for a good overview) and using adap-
tive sampling to focus additional samples in areas with ei-
ther a high goodness of ﬁt or high uncertainty. The software
then relies heavily on user interaction to study the sensitivi-
ties with respect to each input parameter and steers the com-
putation toward the user-deﬁned optimal solution. Demir et
al. [DW13] improve the effectiveness of GPMs by utiliz-
ing a block-wise matrix inversion scheme that can be im-
plemented on the GPU, greatly increasing efﬁciency. In ad-
dition, their method involves progressive reﬁnement of the
GPM and can be halted at any point, if the improvement be-
comes insigniﬁcant.

Most of these methods convey sensitivity information
through user exploration of the input space. In Section 4.2,
explicit visual encodings for understanding sensitivity infor-
mation are also discussed.
Structural Summaries. Researchers have also used re-
gression to summarize data as in the works by Reddy et
al. [RPH08] and Gerber et al. [GBPW10]. Both methods
summarize the structures of the data via skeleton repre-
sentations. Reddy et al. [RPH08] use a clustering algo-
rithm followed by construction of a minimum spanning
tree of the cluster centroids in order to determine possible
trends in the data. These trends are then ﬁtted with princi-
ple curves [HS89] which go through the medial-axis of the
data. HDViz [GBPW10], on the other hand, approximates a
topological segmentation (for more details, see Section 3.5)
and constructs an inverse linear regression for each segment
of the data. In both examples, regression is used as a post-
processing step of the algorithms in order to present sum-
maries of the extracted subsets of the data.
3.5 Topological Data Analysis

A crucial step in gaining insights from large, complex,
high-dimensional data involves feature abstraction, extrac-
tion, and evaluation in the spatiotemporal domain for ef-

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

fective exploration and visualization. Topological data anal-
ysis (TDA), a new ﬁeld of study (see [Zom05, BDFF∗08,
EH08,EH10,Car09,Ghr08] for seminal works and surveys),
has provided efﬁcient and reliable feature-driven analysis
and visualization capabilities. Speciﬁcally, the construction
of topological structures [Ree46, Sma61] from scalar func-
tions on point clouds (e.g., Morse-Smale complexes, con-
tour trees, and Reeb graphs) as “summaries” over data is at
the core of such TDA methods. Reeb graphs/contour trees
capture very different structural information of a real-valued
function compared to the Morse-Smale complexes as the for-
mer is contour-based and the latter is gradient-based (Figure
3). They both provide meaningful abstractions of the high-
dimensional data, which reduces the amount of data needed
to be processed or stored; and they utilize sophisticated hier-
archical representations capturing features at multiple scales,
which enables progressive simpliﬁcations of features differ-
entiating small and large scale structures in the data.
Morse-Smale Complexes. The Morse-Smale complex
(MSC) [EHNP03, EHZ03] describes the topology of a func-
tion by clustering the points in the domain into regions of
monotonic gradient ﬂow, where each region is associated
with a sink-source pair deﬁned by local minima and max-
ima of the function. The MSC can be represented using a
graph where the vertices are critical points and the edges
are the boundaries of areas of similar gradient behavior. The
simpliﬁcation of the MSC is obtained by removing pairs
of vertices in the graph and updating connectivities among
their neighboring vertices, merging nearby clusters by redi-
recting the gradient ﬂow. MSCs have been shown to be ef-
fective in identifying, ordering, and selectively removing
features of large-scale data in scientiﬁc visualizations (e.g.,
[BEHP04, GBPH08, GNP∗05]).

HDViz [GBPW10] employs an approximation of the
MSC (in high dimensions) to analyze scalar functions on
point cloud data. It creates a hierarchical segmentation of
the data by clustering points based on their monotonic ﬂow
behavior, and designs new visual metaphors based on such
a segmentation. Correa et al. [CL11] suggest that by con-
sidering a different type of neighborhood structure, we can
improve the accuracy in the extracted topology compared to
those obtained within HDViz.
Reeb Graphs and Contour Trees. The Reeb graph of a
real-valued function describes the connectivity of its level
sets. A contour tree is a special case of Reeb graph that arises
in simply-connected domains. The Reeb graph stores infor-
mation regarding the number of components at any function
value as well as how these components split and merge as the
function value changes. Such an abstraction offers a global
summary of the topology of the level sets and enables the
development of compact and effective methods for modeling
and visualizing scientiﬁc data, especially in high dimensions
(i.e., [NLC11, SMC07]).
algorithms

computing

Efﬁcient

for

the

contour

tree [CSA03] and Reeb graph [PSBM07] in arbitrary dimen-
sions have been developed. A generalization of the contour
tree has been introduced by Carr et al. [CD14, DCK∗12]
called the joint contour net (JCN), which allows for the
analysis of multi-ﬁeld data.

Figure 3: Contour- and gradient-based topological structure
of a 2D scalar function.

Other Topological Features. Ghrist [Ghr08] and Carls-
son [Car09] both offer several applications of TDA and in
particular highlight the topological theory used in a study
of statistics of natural images [LPM03]. Mapper [SMC07]
decomposes data into a simplicial complex resembling a
generalized Reeb graph, and visualizes the data using a
graph structure with varying node sizes. The software is
shown to extract salient features in a study of diabetes by
correctly classifying normal patients and patients with two
causes of diabetes. Wang et al. [WSPVJ11] utilize TDA
techniques developed by Silva et al. [dSMVJ09] to re-
cover important structures in high-dimensional data con-
taining non-trivial topology. Speciﬁcally, they are interested
in high-dimensional branching and circular structures. The
circle-valued coordinate functions are constructed to repre-
sent such features. Subsequently, they perform dimension re-
duction on the data while ensuring such structures are visu-
ally preserved.

4 Visual Mapping

Visual mapping plays an essential role in converting the
analysis result or the original dataset into visual structures
based on various visual encodings. Here, we divide the ap-
proaches based on their structural patterns, compositions,
and movements (i.e., animations). In addition, the methods
that evaluate the effectiveness of visual encoding are also
discussed.
4.1 Axis-Based Methods

Axis-based methods refer to visual mappings where el-
ement relationships are expressed through axes represent-
ing the dimensions/variables. These methods include some
of the most well-known visual mapping approaches, such as
scatterplot matrices (SPLOMs) and parallel coordinate plots
(PCPs).
Scatterplot Matrix. A scatterplot matrix, or SPLOM, is a

c(cid:13) The Eurographics Association 2015.

2D Scalar function Reeb Graph/Contour Tree/Merge Tree Morse-Smale Complex S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

collection of bivariate scatterplots that allows users to view
multiple bivariate relationships simultaneously. One of the
primary drawbacks of SPLOMs is the scalability. The num-
ber of the bivariate scatterplots increases quadratically with
respect to the dataset’s dimensionality. Numerous studies
have introduced methods for improving the scalability of
SPLOMs by automatically or semi-automatically identify-
ing more interesting plots.

Scagnostics are a set of measures designed for identify-
ing interesting plots originally introduced by John W. Tukey.
The recent works of Wilkinson et al. [WAG05, WAG06] ex-
tend the concept to include nine measures capturing proper-
ties such as outliers, shape, trend, and density. In addition,
they improve the computational efﬁciency by using graph-
theoretic measures. Scagnostics is also extended to handle
time series data [DAW13]. Guo [Guo03] introduces an inter-
active feature selection method for ﬁnding interesting plots
by evaluating the maximum conditional entropy of all pos-
sible axis-parallel scatterplots. The rank by feature frame-
work [SS04a, SS06] allows users to choose a ranking crite-
rion, such as histogram distribution properties and correla-
tion coefﬁcients between axes, for scatterplots in SPLOMs.
Data class labels can play an important role in identifying
interesting plots and selecting a meaningful ranking order.
Sips et al. utilize class consistency [SNLH09] as a quality
metric for 2D scatterplots. The class consistency measure
is deﬁned by the distance to the class’s center or entropies
of the spatial distributions of classes. Tatu et al. [TAE∗09]
introduce different metrics for ranking the “interestingness”
of scatterplots and PCPs for both classiﬁed and unclassiﬁed
datasets. For data with labels, a class density measure and a
histogram density measure are adopted as ranking functions
for the scatterplots.

The ranking order provides only an indirect way to as-
sess the scatterplots, Lehmann et al. [LAE∗12] introduces a
system for visually exploring all the plots as a whole. By re-
ordering the rows and columns in the SPLOMs, this method
groups relevant plots in the spatial vicinity of one another. In
addition, an abstraction can be obtained from the reordered
SPLOM to provide a global view.
Parallel Coordinates. Compared to a SPLOM, where only
bivariate relationships can be directly expressed, the Paral-
lel Coordinate Plot (PCP) [Ins09, ID91] allows patterns that
highlight multivariate relations to be revealed by showing
all the axes at once (typically, in a vertical layout). How-
ever, due to the linear ordering of the PCP axes, for a given
n-dimensional dataset, there are n! permutations of the or-
dering of the axes. Each of the orderings highlights certain
aspects of the high-dimensional structure. Therefore, one of
the signiﬁcant challenges when dealing with PCPs is deter-
mining an appropriate order of the axes. In addition, as the
number of points increases, the line density in the PCP in-
creases dramatically, which can lead to overplotting and vi-
sual clutter thus hindering the discovery of patterns.

c(cid:13) The Eurographics Association 2015.

A few methods have proposed metrics for ordering
the axes automatically. Tatu et al. [TAE∗09] introduce
PCP ranking methods for both classiﬁed and unclassiﬁed
datasets. For unlabeled data, the Hough space measure is
used, and for labeled data, a similarity measure and overlap
measures are adopted. Ferdosi et al. introduce a dimension
ordering method [FR11] that is applicable for both PCPs
and SPLOMs utilizing the subspace analysis method from
their earlier work [FBT∗10] discussed in the Section 3.3.
Johansson and Johansson [JJ09] propose an interactive sys-
tem adopting a weighted combination of quality metrics for
dimension selection and automatic ordering of the axes to
enhance visual patterns such as clustering and correlation.
Hurley et al. utilize Eulerian tours and Hamiltonian decom-
positions of complete graphs, which represent the relation-
ship between the dimensions, in their recent work [HO10] to
address the axis ordering challenge.

Clutter reduction is another important aspect in PCPs, es-
pecially for large point counts. Peng et al. [PWR04] were
able to reduce clutter for both SPLOMs and PCPs without
altering the information content simply by reordering the di-
mensions. A focus+context visualization scheme can also be
used for reducing the clutter and highlighting the essential
features in the PCP [NH06]. In this context, the overview
captures both the outliers and the trends in the dataset. The
outliers are indicated by single lines, and the trends that cap-
ture the overall relationship between axes are approximated
by polygon strips. The selected data items are emphasized
through visual highlighting. In addition, several of the clut-
ter reduction methods employing screen space measures are
discussed in detail in Section 5.4.

Finally, many visual encoding improvements exist for
PCPs. Progressive PCPs [RZH12] demonstrate the power of
a progressive reﬁnement scheme for enhancing the ability
of PCPs to handle large datasets. In the work of Dang et
al. [DWA10], density is expressed by stacking overlapping
elements. For the PCP case, a 3D visualization is presented,
where either the edges are stacked as curves or the points on
the axes are stacked vertically as dots.
Radial Layout. The star coordinate plot [Kan00], also re-
ferred to as a bi-plot [HGM∗97], is a generalization of the
axis-aligned bivariate scatterplot. The star coordinate axes
represent the unit basis vectors of an afﬁne projection. The
user is allowed to modify the orientation and the length of
the axes as a way of altering the projection. However, due
to the unbounded manipulation, star coordinates may pro-
duce afﬁne projections where substantial distortion occurs.
Lehmann et al. extend the star coordinate concept with an
orthographic constraint [LT13] to restrict the generated pro-
jection to be orthographic, which better preserves the struc-
ture of the original dataset.

Similar to the star coordinates, Radviz [HGM∗97] adopts
a circular pattern.The difference is that Radviz does not de-
ﬁne an explicit projection matrix. In Radviz, n dimensional

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

anchors are placed along the perimeter of a circle, each rep-
resenting one of the dimensions of an n-dimensional dataset.
A spring model is constructed for each point, where one end
of a spring is attached to a dimensional anchor and the other
is attached to the data point. The point is then displayed
where the sum of the spring forces equals zero. Albuquerque
et al. [AEL∗10] devise a RadViz quality measure allowing
automatic optimization of the dimensional anchor layout.

DataMeadow [EST07] introduces a radial visual encod-
ing named DataRoses, which is represented as a PCP laid
out radially as opposed to linearly. Lastly, PolarEyez [JN02]
introduces a focus+context visualization where the high-
dimensional function parameter space is encoded in a radial
fashion around a user-controlled focal point. Data near the
focal point is represented with more precision, and the focal
point can be altered to focus on different parts of the data.

Figure 4: Scattering points in parallel coordinates by Yuan et
al. [YGX∗09].

Hybrid Construction. The axis-based methods can also be
combined to create new visualizations. The scattering points
in parallel coordinate work [YGX∗09] (Figure 4) embeds a
MDS plot between a pair of PCP axes. The ﬂexible linked
axes work [CvW11] is a generalization of the PCP and the
SPLOM. The tool gives the user the ability to create new
conﬁgurations by drawing and linking axes in either scat-
terplot or PCP style. Proposed by Fanea et al., the integra-
tion of parallel coordinate and star glyphs [FCI05] provides
a way to “unfold” the overlapped values in the PCP axis in
3D space. In this work [FCI05], each axis in the PCP is re-
placed with a star glyph that represents the values across all
points, and then each high-dimensional point is described as
a set of line segments in 3D connecting the individual values
in the star glyphs.

In addition, there is a number of visual representations
that derive from the the well-known visual mappings. Angu-
lar histograms [GPL∗11] introduced a novel visual encoding
that improves the scalability of PCPs by overcome the over-
plotting issue. The tiled PCP [CMR07] adopts a row-column
2D conﬁguration instead of the 1D linear layout of the tra-
ditional PCP for simultaneous visualization of multiple time
steps and variables.
4.2 Glyphs

Chernoff faces [Che73] are one of the ﬁrst attempts to map
a high-dimensional data point into a single glyph. The sys-

tem works by mapping different facial features to separate
dimensions. In a few recent works, glyphs have been utilized
to provide statistical and sensitivity information in order to
present trends in the data. By utilizing local linear regression
to compute partial derivatives around sampled data points
and representing the information in terms of glyph shape,
sensitivity information can be visually encoded into scatter-
plots [CCM09, CCM10, GWRR11, CCM13].

Correa et al. [CCM09] aimed at incorporating uncertainty
information into PCA projections and k-means clustering
and accomplished this goal by augmenting scatterplots with
tornado plots. Together these glyphs encode uncertainty and
partial derivative information. The idea of mapping sen-
sitivity information to a line segment through each data
point has been extended in their later work [CCM10] with
the introduction of the ﬂow-based scatterplot (FBS) that
highlights functional relationships between inputs and out-
puts. The works by Guo et al. [GWRR11] and Chan et
al. [CCM13] attempt to provide more than a single partial
derivative information into their scatterplots by experiment-
ing with different glyph shapes such as star plots among oth-
ers. [GWRR11] also uses a bar chart similar to the tornado
plot used in [CCM09], and [CCM13] provides two other in-
terpretations. The ﬁrst is a generalization of the FBS called
the generalized sensitivity scatterplot (GSS). By using or-
thogonal regression, GSSs can represent the partial deriva-
tive of any variable with respect to any other variable. The
other is a fan glyph that works similarly to the star glyph,
allowing for viewing multiple partial derivatives, but rather
than displaying magnitude as in the star glyph, the fan glyph
highlights the direction of each partial derivative, since all
line segments are normalized in length.

The methods described above all deal with encoding ex-
tra information per data point into glyphs, but the DICON
system [CGSQ11] attempts to show the trend of data within
a collection of data points by visually encoding statistical
information about the set of points being represented. DI-
CON uses dynamic icons based on treemap visualization to
encode clusters of data into separate icons, and allows the
user to interactively merge, split, ﬁlter, regroup, and high-
light clusters or data within clusters. Due to the interactive
nature, the authors have developed a stabilized Voronoi lay-
out that allows data within the treemap to maintain spatial
coherence as the user edits the clusters. They further encode
skew and kurtosis into the shape of the icon before applying
the Voronoi algorithm, thus allowing for statistical details to
be presented.

Finally, Ward [War08] gives a thorough, practical treat-
ment of generating and organizing effective glyphs for mul-
tivariate data, paying particular attention to the common pit-
falls involving the use of glyphs.
4.3 Pixel-Oriented Approaches

In an effort to encode the maximal amount of informa-
tion, several works have targeted dense pixel displays. Re-

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

searchers have focused on encoding data values as individ-
ual pixels and creating separate displays, or subwindows, for
each dimension.

Some of the earliest works in this area date back to the mid
1990s [KK94,AKpK96]. VisDB [KK94] visualizes database
queries by creating a 2D image for each dimension involved
in the query and mapping individual values of a dimension
to pixels. The mapped data is sorted and colored by rele-
vance such that the data most related to the query appears
in the center of the image, and the data spirals outward as
it loses relevance to the query. Circle segments [AKpK96]
arrange multidimensional data in a radial fashion with equal
size sectors being carved out for each dimension.

The pixel concept can be applied to bar charts to create
pixel bar charts [KHL∗01]. Pixel bar charts ﬁrst separate
data into separate bars based on one dimension or attribute,
and it can also split the data along the orthogonal direction
using another dimension, although most results are reported
using only one direction for splitting data. Once split, the
data points are sorted along the horizontal axis within the
bars using one dimension and ordered along the vertical axis
using another dimension. Wattenberg introduces the jigsaw
map [Wat05], which again maps data points to pixels and
uses discrete space-ﬁlling curves in order to ﬁll a 2D plane
in a more sensible fashion than a comparative treemap lay-
out.

The Value and Relation (VaR) displays

[YPS∗04,
YHW∗07] combine the recursive pattern displays [KKA95]
with MDS in order to lay out the separate subwindows
such that similar dimensions are placed closer together. A
latter iteration [YHW∗07] enhances the work by provid-
ing more robust visualizations including jigsaw maps, scat-
terplot glyphs, and a novel concept known as the Rainfall
metaphor geared at establishing the relationship of all di-
mensions to a single dimension of interest.
4.4 Hierarchy-Based Approaches

For visualizing high-dimensional datasets, hierarchical vi-
sual representations are used to capture dimensional rela-
tionships, represent contour tree structure, and provide new
visual encodings for representing high-dimensional struc-
tures.
Dimension Hierarchies. Large numbers of dimensions hin-
der our ability to navigate the data space and cause scala-
bility issues for visual mapping. A hierarchical organization
of dimensions explicitly reveals the dimension relationships,
helping to alleviate the complexity of the dataset. Yang et
al. propose an interactive hierarchical dimension ordering,
spacing, and ﬁltering approach [WPWR03] based on dimen-
sion similarity. The dimension hierarchy is represented and
navigated by a multiple ring structure (InterRing [YWR02]),
where the innermost-ring represents the coarsest level in the
hierarchy.
Topology-Based Hierarchies. In Section 3.5, we have dis-

c(cid:13) The Eurographics Association 2015.

cussed topological structures, which can provide a ranking
of features with the help of persistence simpliﬁcation and
thus be treated as a hierarchy.

Various visual metaphors have been designed for con-
tour trees [PCMS09, WBP12]. In particular, variations of
landscapes have been proposed [BMW∗12,
topological
DBW∗12, HW10, OHJS10, OHJ∗11, WBP07]. These visual
metaphors have, or potentially have, capabilities for the visu-
alization of high-dimensional datasets. In particular, Weber
et al. [WBP07] have presented such a metaphor for visu-
ally mapping the contour tree of high-dimensional functions
to a 2D terrain where the relative size, volume, and nest-
ing of the topological features are preserved. Harvey and
Wang [HW10] have extended this work by computing all
possible planar landscapes and they are able to preserve ex-
actly the volumes of the high-dimensional features in the
areas of the terrain. In addition, the works of Oesterling et
al. [OHJS10, OHJ∗11] have used this same metaphor to vi-
sualize a related structure, the join tree. They use a novel
high-dimensional interpolation scheme in order to estimate
the density from the raw data points, and visually map the
density as points on top of their generated terrains.

Oesterling et al. [OHWS13] continued this line of work
by creating a linked view software system including user in-
teractions into the analysis by allowing users to brush and
link with PCPs and PCA projections of the data. In addition,
they have presented a new method of sorting the features
based on either persistence, cluster size, or cluster stability,
thus adjusting the placement of features in the topological
landscape.
Other Hierarchical Structures. In the structure-based
brushes work [FWR00], a data hierarchy is constructed to
be visualized by both a PCP and a treemap [Shn92], allow-
ing users to navigate among different levels-of-detail and se-
lect the feature(s) of interest. The structure decomposition
tree [ERHH11] presents a novel technique that embeds a
cluster hierarchy in a dimensional anchor-based visualiza-
tion using a weighted linear dimension reduction technique.
It provides a detail plus overview structural representation,
and conveys coordinate value information in the same con-
struction. The system supports user-guided pruning, opti-
mization of the decision tree, and encoding the tree structure
in an explorable visual hierarchy. Kreuseler et al. present a
novel visualization technique [KS02] for visualizing com-
plex hierarchical graphs in a focus+context manner for vi-
sual data mining tasks.
4.5 Animation

Many techniques for visualizing high-dimensional data
utilize animated transitions to enhance the perception of
point and structure correspondences among multiple rele-
vant plots.

The GGobi system [SLBC03] provides a mechanism for
calculating a continuous linear projection transition between
any pair of linear projections based on the principal an-

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

gles between them. In the Rolling the Dice work [EDF08],
a transition between any pair of scatterplots in a SPLOM
is made possible by connecting a series of 3D tran-
sitions between scatterplots that share an axis. Rnav-
Graph [WO11] constructs a graph connecting a number of
interesting scatterplots. A smooth animation is generated be-
tween all scatterplots that are connected by an edge. The
TripAdvisorND [NM13] system allows users to explore the
neighborhood of a subspace by tilting the projection plane
using a polygonal touchpad interface.
4.6 Perception Evaluation

The design goal of visual mapping and encoding is to di-
rectly convey the information to the user through visual per-
ception. The evaluation of this mapping is vitally important
in determining the effectiveness of the overall visualization.
Sedlmair et al. have carried out an extensive investigation
of the effectiveness of visual encoding choices [SMT13],
including 2D scatterplot, interactive 3D scatterplot, and
SPLOMs. Their ﬁndings reveal that the 2D scatterplot is
often decent, and certain dimension reduction techniques
provide a good alternative. In addition, SPLOMs some-
times add additional value, and the interactive 3D scatter-
plot rarely helps and often hurts the perception of class
separation. The efﬁcacy of several PCP variants for clus-
ter identiﬁcation has been studied in [HVW10]. The com-
parison is performed among nine PCP variations based on
existing methods and combinations of them. The evalu-
ation reveals that, aside from the scatterplots embedded
into parallel coordinates [YGX∗09], a number of seemingly
valid improvements do not result in signiﬁcant performance
gains for cluster identiﬁcation tasks. Heer et al. investi-
gate the animated transition effectiveness between statistic
graphs [HR07] such as bar charts, pie charts, and scatter-
plots. Their results reveal that animated transitions, when
used appropriately, can signiﬁcantly improve graphical per-
ception.

5 View Transformation

View transformations dictate what we ultimately see on
the screen. As pointed out by Bertini et al. [BTK11], the
view transformation can also be described as the rendering
process that generates images in the screen space.
5.1

Illustrative Rendering

Illustrative rendering describes methods that focus on
achieving a speciﬁc visual style by applying custom-
designed rendering algorithms. The illustrative PCPs
work [MM08] provides a set of artistic style rendering
techniques for enhancing parallel coordinate visualization.
Some of the rendering techniques include spline-based edge
bundling, opacity-based hints to convey cluster density, and
shading effects to illustrate local line density. Illuminated
scatterplots [SW09] (Figure 5) classify points based on
the eigenanalysis of the covariance matrix, and give the

user the opportunity to see effects such as planarity and
linearity when visualizing dense scatterplots. Johansson et
al. [JLJC05] reveal structures in PCPs by adopting the trans-
fer function concept commonly used in volume rendering.
Based on user input, the transfer function maps the line den-
sities into different opacities to highlight features.

Illustrative rendering techniques are also used for high-
lighting the focused areas, such as the well-known Table-
Lens approach [RC94] for visualizing large tables. Such a
magic lens based approach permits fast exploration of an
area of interest without presenting all the details, therefore,
reduces clutter in the view. MoleView [HTE11], for visual-
izing scatterplots and graphs, adopts a semantic lens for al-
lowing users to focus on the area of interest and keep the in-
focused data unchanged while simplifying or deforming the
rest of data to maintain context. A survey on the distortion-
oriented magic lens techniques is presented by Leung and
Apperley [LA94].

Figure 5: Illuminated 3D scatterplot by Sanftmann et
al. [SW09].

5.2 Continuous Visual Representation

For most high-dimensional visualization techniques, a
discrete visual representation is assumed since each element
corresponds to a data point. However, due to limitations such
as visual clutter and computational cost, many applications
prefer a continuous representation.

The work of Bachthaler and Weiskopf [BW08] presents a
mathematical model for constructing a continuous scatter-
plot. The follow-up work [BW09] introduces an adaptive
rendering extension for continuous scatterplots increasing
the rendering efﬁciency. This concept is extended to create
continuous PCPs [HW09] based on the point and line duality
between scatterplots and parallel coordinates. The authors
propose a mathematical model that maps density from a con-
tinuous scatterplot [BW08] to parallel coordinates. Lehmann
et al. introduce a feature detection algorithm design for con-
tinuous PCPs [LT11].

Clutter caused by overlapping in PCPs and scatterplots
occludes data distribution and outliers. In the splatterplot
work [MG13], the authors introduce a hybrid representation
for scatterplots to overcome the overdraw issue when scal-
ing to very large datasets. The proposed abstraction auto-
matically groups dense data points into an abstract contour
representation and renders the rest of the area using selected
representatives, thus preserving the visual cue for outliers. A

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

splatting framework for extracting clusters in PCPs is pre-
sented by [ZCQ∗09], where a polyline splatter is introduced
for cluster detection, and a segment splatter is used for clut-
ter reduction.
5.3 Accurate Color Blending

When rendering semi-transparent objects, color blending
methods have a signiﬁcant impact on the perception of order
and structure.

As

stated in the Hue-Preserving Color-Blending
work [KGZ∗12], the commonly adopted alpha-compositing
can result in false colors that may lead to a deceiving visual-
ization. The authors propose a data-driven machine learning
model
for optimizing and predicting a hue-preserving
blending. This model can be applied to high-dimensional
visualization techniques such as illustrative PCPs [MM08],
where a depth ordering clue is better preserved. In the
Weaving vs. Blending work [HSKIH07],
the authors
investigate the effectiveness of two color mixing schemes:
color blending and color weaving (interleaved pattern). The
results indicate that color weaving allows users to better
infer the value of individual components; however, as the
number of components increases, the advantage of color
weaving diminishes.
5.4

Image Space Metrics

As discussed in Section 4.1, a number of quality mea-
sures have been proposed to analyze the visual structure and
automatically identify interesting patterns in PCPs or scat-
terplots. In this section, we discuss the image space based
quality measures that are applied in the screen space.

Arterode et al. propose a method [AdOL04] for uncov-
ering clusters and reducing clutter by analyzing the density
or frequency of the plot. Image processing based techniques
such as grayscale manipulation and thresholding are used to
achieve the desired visualization. Johansson et al. introduce
a screen space quality measure for clutter reduction [JC08]
to address the challenge of very large datasets. The metric
is based on distance transformation, and the computation is
carried out on the GPU for interactive performance.

Pargnostics [DK10], a portmanteau for parallel coordi-
nates and diagnostics (similar to Scagnostics [WAG05]), is
a set of screen space measures for identifying distance pat-
terns among pairs of axes in PCPs. The metrics include line
crossings, crossing angles, convergence, and over-plotting.
For each of the metrics, the system provides ranked views for
pairs of axes, allowing the user to guide exploration and vi-
sualization. Pixnostic [SSK06] is an image space based qual-
ity metric for ranking interestingness for pixel based (Sec-
tion 4.3) visualization such as Pixel Bar Chars [KHL∗01].

6 User Interaction

As illustrated in Figure 2,

interaction is integrated
with each of the processing steps. An alternative sub-
categorization for each of the processing steps based on the

c(cid:13) The Eurographics Association 2015.

amount of user interaction is shown in Table 1. In this cat-
egorization, each step is further divided into computation-
centric approaches, interactive exploration, and model ma-
nipulation. In both of the recent surveys [MPG∗14,TJHH14]
on the user interaction in visualization applications, the level
of integration between the computation and visualization
(indicate user interaction) is used for classifying the meth-
ods. In many ways, their classiﬁcations are aligned with our
proposed approach, with the distinction that our discussion is
directly connected to the information visualization pipeline.
6.1 Computation-Centric Approaches

input

such as

Computation-centric approaches require only limited
setting initial parameters. These
user
methods center around algorithms designed for well-
deﬁned computational problems such as dimension re-
duction [RS00, MRC02, KC03, WM04], subspace analy-
sis [CFZ99, TMF∗12, FBT∗10, AWD12], regression anal-
[BPFG11, BPFG11], quality metric based rank-
ysis
ing [WAG05, TAE∗09], etc. Computation-centric ap-
proaches exist at each of the processing steps, but are most
concentrated in the data transformation step.
6.2 Interactive Exploration

Interactive methods navigate, query, and ﬁlter the exist-
ing model interactively for more effective visual communi-
cation. In this section, we focus only on representative meth-
ods where the interactive exploration mechanism is their key
contribution.

In the data transformation step, the interactive explo-
ration scheme allows users to guide progressive dimen-
sion reduction, where a partial result is presented upon re-
quest [WM04]. In works by Turkay et al. [TFH11,TLLH12]
and Yuan et al. [YRWG13], a subset of dimensions is inter-
actively selected and explored in dimension space.

In the visual mapping step, there are large number of
methods focused on interactive exploration and querying
the high-dimensional dataset. Such methods play an impor-
tant role in the knowledge Discovery in Databases (KDD)
process, where the term visual data mining [KK96, Kei02,
DOL03] is used to describe these applications. Interac-
tive ﬁltering, zooming, distortion, linking and brushing, or
a combination of them have been adopted to include the
user as part of the exploring and querying process. Po-
laris [STH02] is a visual query and analysis system de-
signed for relational databases. This system is later de-
veloped into the well-known commercial product Tableau.
Stolte et al. introduce an approach for zooming along one
or more dimensions for multi-scale exploration by travers-
ing a graph [STH03]. In this system, relational queries can
be deﬁned by visual speciﬁcations allowing fast incremen-
tal development and intuitive understanding of the data.
Hao et al. have introduced the Intelligent Visual Analyt-
ics Queries [HDK∗07]. Their approach utilizes correlation
and similarity measurements for mining data relationships.
We believe new research directions could stem from visual

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

Computation-Centric

Interactive Exploration

Model Manipulation

Data Transformation
dimension
reduc-
tion [WM04],
subspace
ﬁnding [TMF∗12], regres-
sion analysis
progres-
interactive,
sive
reduc-
tion [WM04], dimension
space exploration [TFH11]
user-guided
embedding
manipulation [LWBP14],
control point based projec-
tion [JPC∗11]

dimension

parallel
axis

Visual Mapping
co-
automatic
ordinate
reorder-
ing [PWR04], scatterplot
ranking [WAG05]
visual querying and ﬁl-
tering [SVW10], animated
transition [SLBC03]

[JC08],

View Transformation
quality metrics in image
continu-
space
ous
visual
representa-
tion [BW08]
interactive magic lens ef-
fects [HTE11], illuminated
3D scatterplot [SW09]

distance function learning
[BLBC12, Gle13], visual
to parameter
interaction
[HBM∗13]

transfer
[JLJC05],

PCP
tion
projection
tion [PdSABD∗12]

func-
inverse
extrapola-

Table 1: The transformation pipelines intertwine with user interaction. The subcategorizing is based on the different levels of
user involvement.

data mining and visual queries. The Select and Slice Ta-
ble [SVW10] allows users to study the relationships between
data subsets and the semantic zone (user-deﬁned areas of in-
terest). The semantic zones are arranged along one axis of
the table, while the data subsets are arranged along the other
axis. In addition, the method enables the combination and
manipulation of the semantic zones for further exploration.
More recent works [GLG∗13, GGL∗14] by Gratzl et al. in-
troduce some very interesting interactive methods for rank-
ings multi-attributes and explore subsets of tabular datasets.
Both of the works of Poco et al. [PEP∗11b] and Sanft-
mann and Weiskopf [SW12] present methods for navigat-
ing a 3D projection. However, their approaches are quite
different. The method introduced by Poco et al. [PEP∗11b]
focuses on enhancing the visual encoding and exploration
usability of a 3D projection calculated by the Least-Square
Projection [PNML08] algorithm. On the other hand, Sanft-
mann and Weiskopf [SW12] present an interpolation scheme
for generating 3D rigid body rotations between a pair of 3D
axis-aligned scatterplots that share a common axis.

In the view transformation step, interactivity is inherent in
both the magic lens based methods [HTE11,LA94], and illu-
minated 3D scatterplots [SW09] (discussed in Section 5.1).
6.3 Model Manipulation

Model manipulation techniques represent a class of meth-
ods that integrate user manipulation as part of the algorithm,
and update the underlying model to reﬂect the user input to
obtain new insights.

Take the distance function learning work [BLBC12],
for example. The initial embedding is created using a
default distance measure. Through interaction, the initial
point layout is modiﬁed based on the expert user’s domain
knowledge. The system then adjusts the underlying dis-
tance model to reﬂect the user input. Hu et al. present a
method [HBM∗13] for improving the translation of user in-
teraction to algorithm input (visual to parameter interaction)

for distance learning scenarios. The explainers [Gle13] are
projection functions created from a set of user-deﬁned anno-
tations.

The control point based projection methods [DST04,
PNML08,PEP∗11a,JPC∗11,PSN10] update the overall pro-
jection result based on user manipulation of the control
points. In the iLAMP method [PdSABD∗12], inverse pro-
jection extrapolation is used for generating synthetic mul-
tidimensional data out of existing projections for param-
eter space exploration. In the Local Clustering Operation
work [GXWY10], the visual structure is modiﬁed in PCPs
through user-guided deformation operators. Finally, Liu et
al. [LWBP14] allow for direct manipulation of the dimen-
sion reduction embedding to resolve structural ambiguities.
The interactively updated distortion measure is used for
feedback during manipulation.

7 Connections with Related Fields

We investigate the connections between recent advances
in high-dimensional data visualization and related ﬁelds in
the hope of inspiring new research directions.
7.1 Multivariate Volume Visualization

Multivariate volume visualization and high-dimensional
visualization are often studied under different contexts: the
former is normally considered as scientiﬁc visualization re-
search [BH07,KH13], while the latter is mostly studied from
the perspective of information visualization and visual ana-
lytics. In addition, they focus on different kinds of data and
attempt to accomplish distinct goals.

Despite the differences, recent advances in both areas
have shown that they share a number of fundamental tech-
niques and principles. Standard high-dimensional data visu-
alization techniques, such as PCPs, scatterplots, and dimen-
sion reduction, have found their way into the multivariate
volume visualization literature. For example, the scattering
points in parallel coordinates work [YGX∗09] is adopted

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

by [GXY12] as a design space for multivariate volume trans-
fer functions. In the work of Liu et al. [LWT∗14a], dynamic
projection and subspace analysis are utilized for exploring
the high-dimensional parameter space of volumetric data.
We believe useful and interesting techniques may be devel-
oped by sharing ideas and discovering new connections be-
tween these two ﬁelds.
7.2 Machine Learning

Machine learning algorithms under many situations have
been treated as “black box” approaches, and the param-
eter tuning process can be tedious and unpredictable. To
resolve such a challenge, several visualization approaches
have been introduced to aid the understanding of the various
machine learning algorithms. Tzeng et al. present a visual-
ization system that helps users design neural networks more
efﬁciently [TM05]. The works of Teoh and Ma [TM03] and
van den Elzen and van Wijk [vdEvW11] investigate visual-
ization methods for interactively constructing and analyzing
decision trees. Visualization has also been used to aid model
validation [Rd00, MW10]. Numerous challenges for under-
standing machine learning algorithms coincide with high-
dimensional visualization. We believe high-dimensional vi-
sualization will play an important role in designing, tuning,
and validating machine learning algorithms.

8 Reﬂections and Future Directions

One of our primary objectives in presenting this survey is
to provide actionable guidance for data practitioners to nav-
igate through a modular view of the recent advances. To do
so, we provide a categorization of recent works along an en-
riched information visualization pipeline. We reﬂect on the
chosen categories and subcategories (as described brieﬂy in
Section 2) and describe on a high level how they provide
actionable guidance. To allow the creation of new visualiza-
tions along the pipeline, one should think beyond data tasks
to be performed in any single stage, and focus on under-
standing how results from one stage could be utilized most
effectively in the remaining stages. We argue that the sub-
categories discussed during each pipeline stage correspond
to sets of actionable items or toolsets that the data practi-
tioner could choose from and rely upon. The combinations
of techniques they chose to apply are largely data-driven
and application-dependent. Nevertheless, the techniques sur-
veyed following our categorization aim to provide a modular
view during the design process.

We now discuss the challenges addressed by the tech-
niques surveyed in the paper, and those that remain to be
tackled. Our discussion is partially inspired by Donoho’s
AMS lecture [Don00] where he discusses the curses
and blessings of dimensionality when it comes to high-
dimensional data analysis.

Data analysis, falls under the data transformation stage
within our categorization. Some of the surveyed, standard
data analysis tasks are widely applicable for studying var-

c(cid:13) The Eurographics Association 2015.

ious aspects of high-dimensional data: dimension reduc-
tion for feature selection and extraction; clustering for ex-
ploratory data mining and classiﬁcation; regression for rela-
tionship inference and prediction. However, we identify sev-
eral different directions in which we expect to see further
progress, namely: robust analysis and data de-noising; multi-
scale analysis; data skeletonization; and high-dimensional
approximations. First, more advanced regression techniques
could be developed that are robust to noise and outliers,
in particular, a new class of regression techniques inspired
by geometric and topological intuititions (e.g., [GBPW10]).
Second, topological data analysis has built-in capabilities in
separating features from noise at multi-scales; such a multi-
scale notion is expected to be transferrable to a larger class of
analysis techniques. Third, developing frameworks to extract
as well as to simplify “skeletons” from high-dimensional
data can be extremely useful for visual data abstraction
and exploration (e.g., [SMC07]). Finally, as pointed out by
Donoho [Don00], perhaps there exists new notions of high-
dimensional approximation theory, where we make different
regularity assumptions and obtain a very different picture in
approximating high-dimensional functions. Approximating
the Morse-Smale complex in high dimension is considered
such an example.

During visual mapping, our surveyed techniques convert
the analysis result into visual structures with various vi-
sual encodings. Development of new analysis results, for ex-
ample, new approximations of high-dimensional structures,
would inevitably lead to new visual metaphors (e.g., in the
case of topological landscape [HW10, DBW∗12]). Under
visual mapping and view transformation, we also see var-
ious methods aimed at summarizing trends in data, such
as glyph representations, edge bundling in PCPs, splatting
as presented in splatterplots [MG13] and PCP-based splat-
ters [ZCQ∗09], and hierarchical approaches. These could be
further enhanced with new data skeletonization techniques.
Finally, we identify a few opportunities for future visual-

ization research and discuss them in detail.
Subspace Clustering. Finding interesting projections
(views) has been an active and important research area for vi-
sualizing high-dimensional data. The motivation behind the
various view selection schemes can be traced back to much
earlier work such as projection pursuit [FT74].

Along a similar line of research, scatterplot ranking meth-
ods [SS04a, WAG06, TAE∗09] are introduced to automati-
cally identify the interesting scatterplots. However, a scat-
terplot matrix captures only limited bivariate relationships.
Subspace selection methods [CFZ99, BPR∗04], originally
developed in the data mining community, have recently been
adapted for high-dimensional data visualization [FBT∗10,
TMF∗12] to capture more complicated multivariate struc-
tures. Despite the added ﬂexibility, the search is still lim-
ited to axis-aligned subspaces. Recent advances in machine
learning, such as subspace clustering (e.g., [Vid11]), assume

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

the high-dimensional dataset can be represented by a mix-
ture of low-dimensional linear subspaces with mixed dimen-
sions. Such methods produce non-axis-aligned subspaces,
which work well for datasets where different dimensions are
closely related. In addition, instead of capturing a single lin-
ear subspace, they can approximate non-linear structures by
ﬁtting together multiple linear subspaces.

We believe exploring various (non-axis-aligned) subspace
clustering methods will lead to new developments in high-
dimensional view selection techniques (e.g., some of the re-
cent work by the authors [LWT∗14a, LWT∗14b]).
Model Manipulation. We have seen an emerging user
interaction paradigm,
referred to as model manipula-
tion [BLBC12,PdSABD∗12,Gle13,HBM∗13] in this survey.
What differentiates the model manipulation interaction from
other types of interaction is the change of the underlying data
model to reﬂect user intention. These model manipulation
based methods allow users to easily transfer their domain
knowledge into the exploratory analysis process, allowing
for effective analysis and visualization. However, since such
interactive manipulations give users an enormous amount of
freedom, one of the main challenges in model manipulation
is to understand whether or not the manipulation faithfully
conveys the user intention. Rigorous validation between the
user intended operations and manipulation outcomes is es-
sential for evaluating the effectiveness and usability of these
methods.
Uncertainty Quantiﬁcation. Along with the large-scale and
high dimensionality of the data, information pertaining to
uncertainty is becoming increasingly available and impor-
tant. The addition of uncertainty information within visual-
izations has been deemed a top research problem in scien-
tiﬁc visualization [Joh04], due to the greater availability of
this information from simulation and quantiﬁcation, and the
importance of understanding data quality, conﬁdence, and
error issues when interpreting scientiﬁc results. Some recent
works in high-dimensional data visualization have focused
on analyzing the uncertainty stemming from the input data or
with respect to the accuracy of a ﬁtted model (see Section 3.4
and [ZSWR06]). We believe the extensions and generaliza-
tions of existing uncertainty visualization capabilities (e.g.,
[DKLP02, PWB∗09, SZD∗10]) to high-dimensional data is
one of the important future directions.

Another interesting aspect of uncertainty quantiﬁcation
is based on uncertainty-aware visual analytics discussed
by Correa et al. [CCM09], and further explored by Liu et
al. [LWBP14] and Schreck et al. [SvLB10], where the un-
certainty (e.g., bias and distortions) arises from the Data
transformation step. The work by Correa et al. [CCM09]
measures the uncertainty introduced by three common
Data transformation techniques; and the works of Liu et
al. [LWBP14] and Schreck et al. [SvLB10] quantiﬁes the
amount of distortion for projection techniques. While these
methods apply to the uncertainty stemming from the Data

transformation step, more work can be done to deﬁne mea-
sures of uncertainty associated with the two latter processing
steps in the visualization pipeline, namely Visual mapping
and View transformation.
Topological Data Analysis and Visualization. Another
important and interesting recent advance is the introduc-
tion of TDA to visualization (e.g., [GBPW10, WSPVJ11,
DCK∗12]). TDA provides an interesting alternative for cap-
turing the structure in high-dimensional data. Since topolog-
ical structures are typically scale-invariant, designing mean-
ingful and effective visual encodings that capture their inher-
ent properties is essential for future development. Approx-
imation algorithms exist for computing topological struc-
tures in high dimensions; therefore, it is important to strike
a balance between speed and accuracy, and to convey ap-
propriately the approximation error in the visualization.
Some initial work has been done to provide bounds or es-
timations on the accuracy of these approximated models
(e.g., [GBPW10, CL11, TFO09]).
Other Directions. Finally, as discussed in Section 7, ﬁelds
such as multivariate volume visualization and machine
learning share a number of common research problems
with high-dimensional data visualization. Finding connec-
tions and sharing ideas among these related topics will likely
not only yield interesting future research directions, but also
help resolve many challenges in high-dimensional data visu-
alization.

Acknowledgments

The ﬁrst two authors contributed equally to this work.
This work was performed in part under the auspices of the
US DOE by LLNL under Contract DE-AC52-07NA27344.,
LLNL-CONF-658933. This work is also supported in
part by NSF 0904631, DE-EE0004449, DE-NA0002375,
DE-SC0007446, DE-SC0010498, NSG IIS-1045032, NSF
EFT ACI-0906379, DOE/NEUP 120341, DOE/Codesign
P01180734.
References
[AdOL04] ARTERO A., DE OLIVEIRA M., LEVKOWITZ H.: Un-
covering clusters in crowded parallel coordinates visualizations.
In IEEE Symposium on Information Visualization (2004), pp. 81–
88. 11

[AEL∗10] ALBUQUERQUE G., EISEMANN M., LEHMANN
D. J., THEISEL H., MAGNOR M.: Improving the visual analy-
sis of high-dimensional datasets using quality measures. In IEEE
Symposium on Visual Analytics Science and Technology (2010),
IEEE, pp. 19–26. 8

[AKpK96] ANKERST M., KEIM D. A., PETER KRIEGEL H.:
Circle segments: A technique for visually exploring large mul-
In Proceedings of IEEE Visualization,
tidimensional data sets.
Hot Topic Session. (1996). 9

[AWD12] ANAND A., WILKINSON L., DANG T. N.: Visual pat-
tern discovery using random projections. In IEEE Conference on
Visual Analytics Science and Technology (2012), IEEE, pp. 43–
52. 5, 11

[BCS96] BUJA A., COOK D., SWAYNE D. F.: Interactive high-

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

dimensional data visualization. Journal of Computational and
Graphical Statistics 5, 1 (1996), pp. 78–99. 1

[BDFF∗08] BIASOTTI S., DE FLORIANI L., FALCIDIENO B.,
FROSINI P., GIORGI D., LANDI C., PAPALEO L., SPAGNUOLO
M.: Describing shapes by geometrical-topological properties of
real functions. ACM Computing Surveys 40, 4 (2008), 12:1–
12:87. 6

[BDSW13] BISWAS A., DUTTA S., SHEN H.-W., WOODRING
J.: An information-aware framework for exploring multivari-
ate data sets. IEEE Transactions on Visualization and Computer
Graphics 19, 12 (2013), 2683–2692. 3

[Bec14] BECK F.: Survis. https://github.com/fabian-beck/survis,

2014. 2

[BEHP04] BREMER P.-T., EDELSBRUNNER H., HAMANN B.,
PASCUCCI V.: A topological hierarchy for functions on trian-
gulated surfaces. IEEE Transactions on Visualization and Com-
puter Graphics 10, 385-396 (2004). 6

[BH07] BÜRGER R., HAUSER H.: Visualization of multi-variate
scientiﬁc data. EuroGraphics State of the Art Reports (STARs)
(2007), 117–134. 1, 3, 12

[BKH05] BENDIX F., KOSARA R., HAUSER H.: Parallel sets:
visual analysis of categorical data. In IEEE Symposium on Infor-
mation Visualization (2005), pp. 133–140. 3

[BLBC12] BROWN E. T., LIU J., BRODLEY C. E., CHANG R.:
Dis-function: Learning distance functions interactively. In IEEE
Conference on Visual Analytics Science and Technology (2012),
IEEE, pp. 83–92. 4, 12, 14

[BMW∗12] BEKETAYEV K., MOROZOV D., WEBER G. H.,
ABZHANOV A., HAMANN. B.: Geometry–preserving topolog-
ical landscapes. In Proceedings of the Workshop at SIGGRAPH
Asia (2012), pp. 155–160. 9

[BN03] BELKIN M., NIYOGI P.: Laplacian eigenmaps for dimen-
sionality reduction and data representation. Neural computation
15, 6 (2003), 1373–1396. 4

[BPFG11] BERGER W., PIRINGER H., FILZMOSER P.,
GRÖLLER E.:
Uncertainty-aware exploration of continu-
ous parameter spaces using multivariate prediction. Computer
Graphics Forum 30, 3 (2011), 911–920. 5, 11

[BPR∗04] BAUMGARTNER C., PLANT C., RAILING K.,
KRIEGEL H.-P., KROGER P.: Subspace selection for clustering
high-dimensional data. In Fourth IEEE International Conference
on Data Mining (2004), IEEE, pp. 11–18. 5, 13

[BTK11] BERTINI E., TATU A., KEIM D.: Quality metrics in
high-dimensional data visualization: an overview and system-
IEEE Transactions on Visualization and Computer
atization.
Graphics 17, 12 (2011), 2203–2212. 1, 2, 10

[BW08] BACHTHALER S., WEISKOPF D.: Continuous scatter-
plots. IEEE Transactions on Visualization and Computer Graph-
ics 14, 6 (2008), 1428–1435. 10, 12

[BW09] BACHTHALER S., WEISKOPF D.: Efﬁcient and adaptive
rendering of 2-d continuous scatterplots. Computer Graphics Fo-
rum 28, 3 (2009), 743–750. 10

[Car09] CARLSSON G.: Topology and data. Bullentin of the

American Mathematical Society 46, 2 (2009), 255–308. 2, 6

[CCM09] CORREA C., CHAN Y.-H., MA K.-L.: A framework
for uncertainty-aware visual analytics. In IEEE Symposium on
Visual Analytics Science and Technology (2009), pp. 51–58. 8,
14

[CCM10] CHAN Y.-H., CORREA C., MA K.-L.: Flow-based
scatterplots for sensitivity analysis. In IEEE Symposium on Vi-
sual Analytics Science and Technology (2010), IEEE, pp. 43–50.
8

c(cid:13) The Eurographics Association 2015.

[CCM13] CHAN Y.-H., CORREA C., MA K.-L.: The general-
ized sensitivity scatterplot. IEEE Transactions on Visualization
and Computer Graphics 19, 10 (2013), 1768–1781. 8

[CD14] CARR H., DUKE D.: Joint contour nets.

IEEE Trans-
actions on Visualization and Computer Graphics 20, 8 (2014),
1100–1113. 6

[CFZ99] CHENG C.-H., FU A. W., ZHANG Y.: Entropy-based
subspace clustering for mining numerical data. In Proceedings of
the ﬁfth ACM SIGKDD international conference on Knowledge
discovery and data mining (1999), ACM, pp. 84–93. 5, 11, 13

[CGSQ11] CAO N., GOTZ D., SUN J., QU H.: Dicon: Interactive
visual analysis of multidimensional clusters. IEEE Transactions
on Visualization and Computer Graphics 17, 12 (2011), 2581–
2590. 8

[Cha06] CHAN W. W.-Y.: A survey on multivariate data visu-
alization. Department of Computer Science and Engineering.
Hong Kong University of Science and Technology 8, 6 (2006),
1–29. 1

[Che73] CHERNOFF H.: The use of faces to represent points in
k-dimensional space graphically. Journal of the American Statis-
tical Association 68, 342 (1973), 361–368. 8

[CL11] CORREA C., LINDSTROM P.: Towards robust topology
IEEE Transactions on Visualization

of sparsely sampled data.
and Computer Graphics 17, 12 (2011), 1852–1861. 6, 14

[CMR07] CAAT M., MAURITS N., ROERDINK J.: Design and
evaluation of tiled parallel coordinates visualization of multi-
channel eeg data. IEEE Transactions on Visualization and Com-
puter Graphics 13, 1 (2007), 70–79. 8

[CMS99] CARD S. K., MACKINLAY J. D., SHNEIDERMAN B.:
Readings in information visualization: using vision to think.
Morgan Kaufmann, 1999. 1, 2

[CSA03] CARR H., SNOEYINK J., AXEN U.: Computing con-
tour trees in all dimensions. Computational Geometry 24, 2
(2003), 75 – 94. Special Issue on the Fourth CGC Workshop
on Computational Geometry. 6

[CvW11] CLAESSEN J., VAN WIJK J.: Flexible linked axes for
multivariate data visualization. IEEE Transactions on Visualiza-
tion and Computer Graphics 17, 12 (2011), 2310–2316. 8

[DAW13] DANG T. N., ANAND A., WILKINSON L.: Timeseer:
Scagnostics for high-dimensional time series. IEEE Transactions
on Visualization and Computer Graphics 19, 3 (2013), 470–483.
7

[DBW∗12] DEMIR D., BEKETAYEV K., WEBER G. H., BRE-
MER P.-T., PASCUCCI V., HAMANN. B.: Topology exploration
with hierarchical landscapes. In Proceedings of the Workshop at
SIGGRAPH Asia (2012), pp. 147–154. 9, 13

[DCK∗12] DUKE D., CARR H., KNOLL A., SCHUNCK N., NAM
H. A., STASZCZAK A.: Visualizing nuclear scission through a
multiﬁeld extension of topological analysis. IEEE Transactions
on Visualization and Computer Graphics 18, 12 (2012), 2033–
2040. 6, 14

[DK10] DASGUPTA A., KOSARA R.: Pargnostics: Screen-space
metrics for parallel coordinates. IEEE Transactions on Visualiza-
tion and Computer Graphics 16, 6 (2010), 1017–1026. 11

[DKLP02] DJURCILOV S., KIM K., LERMUSIAUX P., PANG A.:
Visualizing scalar volumetric data with uncertainty. Computers
and Graphics 26 (2002), 239–248. 14

[DOL03] DE OLIVEIRA M. C. F., LEVKOWITZ H.: From visual
data exploration to visual data mining: A survey. IEEE Transac-
tions on Visualization and Computer Graphics 9, 3 (2003), 378–
394. 1, 11

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

[Don00] DONOHO D. L.: High-dimensional data analysis: The
curses and blessings of dimensionality. AMS Lecture: Math
Challenges of the 21st Century, 2000. 13

[dSMVJ09] DE

SILVA V., MOROZOV D., VEJDEMO-
Persistent cohomology and circular
In Proceedings 25th Annual Symposium on

JOHANSSON M.:
coordinates.
Computational Geometry (2009), pp. 227–236. 6

[DST04] DE SILVA V., TENENBAUM J. B.: Sparse multidimen-
sional scaling using landmark points. Tech. rep., Technical re-
port, Stanford University, 2004. 4, 12

[DW13] DEMIR I., WESTERMANN R.: Progressive high-quality
response surfaces for visually guided sensitivity analysis. Com-
puter Graphics Forum 32, 3pt1 (2013), 21–30. 5

[DWA10] DANG T. N., WILKINSON L., ANAND A.: Stacking
graphic elements to avoid over-plotting. IEEE Transactions on
Visualization and Computer Graphics 16, 6 (2010), 1044–1052.
7

[ED07] ELLIS G., DIX A.: A taxonomy of clutter reduction for
IEEE Transactions on Visualization

information visualisation.
and Computer Graphics 13, 6 (2007), 1216–1223. 1

[EDF08] ELMQVIST N., DRAGICEVIC P., FEKETE J.-D.:
Rolling the dice: Multidimensional visual exploration using scat-
IEEE Transactions on Visualization
terplot matrix navigation.
and Computer Graphics 14, 6 (2008), 1539–1148. 10

[EH08] EDELSBRUNNER H., HARER J.: Persistent homology –

a survey. Contemporary Mathematics 453 (2008), 257. 6

[EH10] EDELSBRUNNER H., HARER J.: Computational Topol-
ogy - an Introduction. American Mathematical Society, 2010.
6

[EHNP03] EDELSBRUNNER H., HARER J., NATARAJAN V.,
PASCUCCI V.: Morse-Smale complexes for piece-wise linear
3-manifolds. In Proceedings 19th Annual symposium on Com-
putational geometry (2003), pp. 361–370. 6

[EHZ03] EDELSBRUNNER H., HARER J., ZOMORODIAN A. J.:
Hierarchical Morse-Smale complexes for piecewise linear 2-
manifolds. Discrete and Computational Geometry 30, 87-107
(2003). 6

[ERHH11] ENGEL D., ROSENBAUM R., HAMANN B., HAGEN
H.: Structural decomposition trees. Computer Graphics Forum
30, 3 (2011), 921–930. 9

[EST07] ELMQVIST N., STASKO J., TSIGAS P.: Datameadow:
A visual canvas for analysis of large-scale multivariate data. In
IEEE Symposium on Visual Analytics Science and Technology
(2007), pp. 187–194. 8

[FBT∗10] FERDOSI B. J., BUDDELMEIJER H., TRAGER S.,
WILKINSON M. H., ROERDINK J. B.: Finding and visualizing
relevant subspaces for clustering high-dimensional astronomical
data using connected morphological operators. In IEEE Sympo-
sium on Visual Analytics Science and Technology (2010), IEEE,
pp. 35–42. 5, 7, 11, 13

[FCI05] FANEA E., CARPENDALE S., ISENBERG T.: An inter-
active 3d integration of parallel coordinates and star glyphs. In
IEEE Symposium on Information Visualization (2005), pp. 149–
156. 8

[FR11] FERDOSI B. J., ROERDINK J. B.: Visualizing high-
dimensional structures by dimension ordering and ﬁltering us-
ing subspace analysis. Computer Graphics Forum 30, 3 (2011),
1121–1130. 7

[FT74] FRIEDMAN J., TUKEY J.: A projection pursuit algorithm
for exploratory data analysis. IEEE Transactions on Computers
C-23, 9 (1974), 881–890. 5, 13

[FWR00] FUA Y.-H., WARD M., RUNDENSTEINER E.:
Structure-based brushes: a mechanism for navigating hierarchi-
cally organized data and information spaces. IEEE Transactions
on Visualization and Computer Graphics 6, 2 (2000), 150–159.
9

[GBPH08] GYULASSY A., BREMER P.-T., PASCUCCI V.,
HAMANN B.: A practical approach to Morse-Smale complex
computation: Scalability and generality. IEEE Transactions on
Visualization and Computer Graphics 14, 6 (2008), 1619–1626.
6

[GBPW10] GERBER S., BREMER P., PASCUCCI V., WHITAKER
R.: Visual exploration of high dimensional scalar functions.
IEEE Transactions on Visualization and Computer Graphics 16,
6 (2010), 1271–1280. 3, 5, 6, 13, 14

[GGL∗14] GRATZL S., GEHLENBORG N., LEX A., PFISTER H.,
STREIT M.: Domino: Extracting, comparing, and manipulating
subsets across multiple tabular datasets. IEEE Transactions on
Visualization and Computer Graphics 20, 12 (2014), 2023–2032.
12

[Ghr08] GHRIST R.: Barcodes: The persistent topology of data.
Bulletin of the American Mathematical Society 45 (2008), 61–75.
6

[Gle13] GLEICHER M.: Explainers: Expert explorations with
IEEE Transactions on Visualization and

crafted projections.
Computer Graphics 19, 12 (2013), 2042–2051. 4, 12, 14

[GLG∗13] GRATZL S., LEX A., GEHLENBORG N., PFISTER H.,
STREIT M.: Lineup: Visual analysis of multi-attribute rankings.
IEEE Transactions on Visualization and Computer Graphics 19,
12 (2013), 2277–2286. 12

[GNP∗05] GYULASSY A., NATARAJAN V., PASCUCCI V., BRE-
MER P.-T., HAMANN B.: Topology-based simpliﬁcation for fea-
In Proceedings of IEEE
ture extraction from 3D scalar ﬁelds.
Visualization (2005), pp. 535–542. 6

[GPL∗11] GENG Z., PENG Z., LARAMEE R., ROBERTS J.,
WALKER R.: Angular histograms: Frequency-based visualiza-
tions for large, high dimensional data. IEEE Transactions on Vi-
sualization and Computer Graphics 17, 12 (2011), 2572–2580.
8

[Guo03] GUO D.: Coordinating computational and visual ap-
proaches for interactive feature selection and multivariate clus-
tering. Information Visualization 2, 4 (2003), 232–246. 7

[GWRR11] GUO Z., WARD M., RUNDENSTEINER E., RUIZ C.:
Pointwise local pattern exploration for sensitivity analysis.
In
IEEE Conference on Visual Analytics Science and Technology
(2011), pp. 131–140. 8

[GXWY10] GUO P., XIAO H., WANG Z., YUAN X.: Interactive
local clustering operations for high dimensional data in parallel
In IEEE Paciﬁc Visualization Symposium (2010),
coordinates.
pp. 97–104. 12

[GXY12] GUO H., XIAO H., YUAN X.: Scalable multivariate
volume visualization and analysis based on dimension projection
and parallel coordinates. IEEE transactions on visualization and
computer graphics (2012). 13

[HBM∗13] HU X., BRADEL L., MAITI D., HOUSE L., NORTH
IEEE
C.: Semantics of directly manipulating spatializations.
Transactions on Visualization and Computer Graphics 19, 12
(2013), 2052–2059. 12, 14

[HDK∗07] HAO M., DAYAL U., KEIM D., MORENT D.,
SCHNEIDEWIND J.: Intelligent visual analytics queries. In IEEE
Symposium on Visual Analytics Science and Technology (2007),
pp. 91–98. 11

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

[HG02] HOFFMAN P. E., GRINSTEIN G. G.: A survey of visu-
alizations for high-dimensional data mining. Information visual-
ization in data mining and knowledge discovery (2002), 47–82.
1

[HGM∗97] HOFFMAN P., GRINSTEIN G., MARX K., GROSSE
I., STANLEY E.: DNA visual and analytic data mining. In Pro-
ceedings of IEEE Visualization (1997), pp. 437–441. 7

[HO10] HURLEY C., OLDFORD R.: Pairwise display of high-
dimensional information via eulerian tours and hamiltonian de-
compositions. Journal of Computational and Graphical Statis-
tics 19, 4 (2010). 7

[HR07] HEER J., ROBERTSON G. G.: Animated transitions in
statistical data graphics. IEEE Transactions on Visualization and
Computer Graphics 13, 6 (2007), 1240–1247. 10

[HSKIH07] HAGH-SHENAS H., KIM S.,

[HS89] HASTIE T., STUETZLE W.: Principal curves. Journal of
the American Statistical Association 84, 406 (1989), 502–516. 5
INTERRANTE V.,
HEALEY C.: Weaving versus blending: a quantitative assessment
of the information carrying capacities of two alternative methods
for conveying multivariate data with color. IEEE Transactions on
Visualization and Computer Graphics 13, 6 (2007), 1270–1277.
11

[HTE11] HURTER C., TELEA A., ERSOY O.: Moleview: An at-
tribute and structure-based semantic lens for large element-based
plots. IEEE Transactions on Visualization and Computer Graph-
ics 17, 12 (2011), 2600–2609. 10, 12

[HVW10] HOLTEN D., VAN WIJK J. J.: Evaluation of cluster
identiﬁcation performance for different PCP variants. Computer
Graphics Forum 29, 3 (2010), 793–802. 10

[HW09] HEINRICH J., WEISKOPF D.: Continuous parallel co-
IEEE Transactions on Visualization and Computer

ordinates.
Graphics 15, 6 (2009), 1531–1538. 10

[HW10] HARVEY W., WANG Y.: Topological landscape en-
sembles for visualization of scalar-valued functions. Computer
Graphics Forum 29, 3 (2010), 993–1002. 9, 13

[HW13] HEINRICH J., WEISKOPF D.: State of the art of parallel
coordinates. STAR Proceedings of Eurographics 2013 (2013),
95–116. 1

[ID91]

INSELBERG A., DIMSDALE B.: Parallel coordinates. In
Human-Machine Interactive Systems. Springer, 1991, pp. 199–
233. 7
[IML13]

IM J.-F., MCGUFFIN M., LEUNG R.: Gplom: The gen-
eralized plot matrix for visualizing multidimensional multivariate
data. IEEE Transactions on Visualization and Computer Graph-
ics 19, 12 (2013), 2606–2614. 3

[Ins09]

INSELBERG A.: Parallel Coordinates : Visual Multidi-
mensional Geometry and its Applications. Springer, 2009. 1,
7

[JC08]

JOHANSSON J., COOPER M.: A screen space quality
method for data abstraction. Computer Graphics Forum 27, 3
(2008), 1039–1046. 11, 12

[JJ09]

JOHANSSON S., JOHANSSON J.: Interactive dimensional-
ity reduction through user-deﬁned combinations of quality met-
rics. IEEE Transactions on Visualization and Computer Graphics
15, 6 (2009), 993–1000. 7

[JLJC05]

JOHANSSON J., LJUNG P., JERN M., COOPER M.: Re-
vealing structure within clustered parallel coordinates displays.
In IEEE Symposium on Information Visualization (2005), IEEE,
pp. 125–132. 10, 12

[JN02]

JAYARAMAN S., NORTH C.: A radial focus+context vi-
In Proceedings of

sualization for multi-dimensional functions.
IEEE Visualization (2002), pp. 443–450. 8

c(cid:13) The Eurographics Association 2015.

[Joh04]

problems.
14

JOHNSON C. R.: Top scientiﬁc visualization research
IEEE Computer Graphics and Applications (2004).

[Jol05]

JOLLIFFE I.: Principal component analysis. Wiley Online

Library, 2005. 3

[JPC∗11]

JOIA P., PAULOVICH F., COIMBRA D., CUMINATO J.,
IEEE
NONATO L.: Local afﬁne multidimensional projection.
Transactions on Visualization and Computer Graphics 17, 12
(2011), 2563–2571. 4, 12

JEONG D. H., ZIEMKIEWICZ C., FISHER B., RIB-
ARSKY W., CHANG R.:
iPCA: An interactive system for pca-
based visual analytics. Computer Graphics Forum 28, 3 (2009),
767–774. 3

[JZF∗09]

[Kan00] KANDOGAN E.: Star coordinates: A multi-dimensional
visualization technique with uniform treatment of dimensions. In
IEEE Information Visualization Symposium, Late Breaking Hot
Topics (2000), pp. 9–12. 7

[KC03] KOREN Y., CARMEL L.: Visualization of labeled data
using linear transformations. In IEEE Symposium on Information
Visualization (2003), pp. 121–128. 4, 11

[Kei02] KEIM D. A.: Information visualization and visual data
IEEE Transactions on Visualization and Computer

mining.
Graphics 8, 1 (2002), 1–8. 1, 11

[KGZ∗12] KUHNE L., GIESEN J., ZHANG Z., HA S., MUELLER
K.: A data-driven approach to hue-preserving color-blending.
IEEE Transactions on Visualization and Computer Graphics 18,
12 (2012), 2122–2129. 11

[KH13] KEHRER J., HAUSER H.: Visualization and visual anal-
ysis of multifaceted scientiﬁc data: A survey. IEEE Transactions
on Visualization and Computer Graphics 19, 3 (2013), 495–513.
1, 3, 12

[KHL∗01] KEIM D., HAO M., LADISCH J., HSU M., DAYAL
U.: Pixel bar charts: a new technique for visualizing large multi-
attribute data sets without aggregation. In IEEE Symposium on
Information Visualization (2001), pp. 113–120. 9, 11

[KK94] KEIM D., KRIEGEL H.-P.: Visdb: database exploration
using multidimensional visualization. Computer Graphics and
Applications, IEEE 14, 5 (1994), 40–49. 9

[KK96] KEIM D. A., KRIEGEL H.-P.: Visualization techniques
for mining large databases: A comparison. Knowledge and Data
Engineering, IEEE Transactions on 8, 6 (1996), 923–938. 11

[KKA95] KEIM D., KRIEGEL H.-P., ANKERST M.: Recursive
pattern: a technique for visualizing very large amounts of data.
In Proceedings of IEEE Visualization (1995), pp. 279–286, 463.
9

[Kru64] KRUSKAL J. B.: Multidimensional scaling by optimiz-
ing goodness of ﬁt to a nonmetric hypothesis. Psychometrika 29,
1 (1964), 1–27. 4

[KS02] KREUSELER M., SCHUMANN H.: A ﬂexible approach
for visual data mining. IEEE Transactions on Visualization and
Computer Graphics 8, 1 (2002), 39–51. 9

[LA94] LEUNG Y. K., APPERLEY M. D.: A review and taxon-
omy of distortion-oriented presentation techniques. ACM Trans-
actions on Computer-Human Interaction 1, 2 (1994), 126–160.
10, 12

[LAE∗12] LEHMANN D. J., ALBUQUERQUE G., EISEMANN
M., MAGNOR M., THEISEL H.: Selecting coherent and relevant
plots in large scatterplot matrices. Computer Graphics Forum 31,
6 (2012), 1895–1908. 7

[LAK∗11] LAWRENCE J., ARIETTA S., KAZHDAN M., LEPAGE

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

D., O’HAGAN C.: A user-assisted approach to visualizing mul-
tidimensional images. IEEE Transactions on Visualization and
Computer Graphics 17, 10 (2011), 1487–1498. 3

[LMZ∗14] LEE J. H., MCDONNELL K. T., ZELENYUK A.,
IMRE D., MUELLER K.: A structure-based distance metric for
high-dimensional space exploration with multidimensional scal-
ing. IEEE Transations on Visualization and Computer Graphics
20, 3 (2014), 351–364. 4

[LPM03] LEE A. B., PEDERSEN K. S., MUMFORD D.: The non-
linear statistics of high-contrast patches in natural images. Inter-
national Journal of Computer Vision 54, 1-3 (2003), 83–103. 6
[LT11] LEHMANN D., THEISEL H.: Features in continuous par-
allel coordinates. IEEE Transactions on Visualization and Com-
puter Graphics 17, 12 (2011), 1912–1921. 10

[LT13] LEHMANN D. J., THEISEL H.: Orthographic star coordi-
nates. IEEE Transactions on Visualization and Computer Graph-
ics 19, 12 (2013), 2615–2624. 7

[LV09] LEE J. A., VERLEYSEN M.: Quality assessment of di-
mensionality reduction: Rank-based criteria. Neurocomputing
72, 7 (2009), 1431–1443. 4

[LWBP14] LIU S., WANG B., BREMER P.-T., PASCUCCI
V.: Distortion-guided structure-driven interactive exploration of
high-dimensional data. Computer Graphics Forum 33, 3 (2014),
101–110. 4, 12, 14

[LWT∗14a] LIU S., WANG B., THIAGARAJAN J. J., BREMER
P.-T., PASCUCCI V.: Multivariate volume visualization through
dynamic projections. Large Data Analysis and Visualization
(LDAV), 2014 IEEE Symposium on (2014). 13, 14

[LWT∗14b] LIU S., WANG B., THIAGARAJAN J. J., BREMER
P.-T., V V. P.: Visual exploration of high-dimensional data: Sub-
space analysis through dynamic projections. Tech. Rep. UUSCI-
2014-003, SCI Institute, University of Utah, 2014. 14

[MG13] MAYORGA A., GLEICHER M.: Splatterplots: Overcom-
ing overdraw in scatter plots. IEEE Transactions on Visualization
and Computer Graphics 19, 9 (2013), 1526–1538. 10, 13

[MLGH13] MOKBEL B., LUEKS W., GISBRECHT A., HAMMER
B.: Visualizing the quality of dimensionality reduction. Neuro-
computing 112 (2013), 109–123. 4

[MM08] MCDONNELL K. T., MUELLER K.: Illustrative paral-
lel coordinates. Computer Graphics Forum 27, 3 (2008), 1031–
1038. 10, 11

[MPG∗14] MUHLBACHER T., PIRINGER H., GRATZL S., SEDL-
MAIR M., STREIT M.: Opening the black box: Strategies for in-
creased user involvement in existing algorithm implementations.
IEEE Transactions on Visualization and Computer Graphics 20,
12 (2014), 1643–1652. 11

[MRC02] MORRISON A., ROSS G., CHALMERS M.: A hy-
brid layout algorithm for sub-quadratic multidimensional scaling.
In IEEE Symposium on Information Visualization (2002), IEEE,
pp. 152–158. 11

[Mun14] MUNZNER T.: Visualization Analysis and Design. CRC

Press, 2014. 1, 2

[MW10] MIGUT M., WORRING M.: Visual exploration of clas-
In IEEE Symposium on
siﬁcation models for risk assessment.
Visual Analytics Science and Technology (2010), pp. 11–18. 13
[NH06] NOVOTNY M., HAUSER H.: Outlier-preserving fo-
IEEE Trans-
cus+context visualization in parallel coordinates.
actions on Visualization and Computer Graphics 12, 5 (2006),
893–900. 7

[NLC11] NICOLAU M., LEVINE A. J., CARLSSON G.: Topology
based data analysis identiﬁes a subgroup of breast cancers with a

unique mutational proﬁle and excellent survival. In Proceedings
of the National Academy of Sciences (2011), vol. 108, pp. 7265–
7270. 6

[NM13] NAM J. E., MUELLER K.: Tripadvisor-nd: A tourism-
inspired high-dimensional space exploration framework with
IEEE Transactions on Visualization and
overview and detail.
Computer Graphics 19, 2 (2013), 291–305. 5, 10

[OHJ∗11] OESTERLING P., HEINE C., JANICKE H., SCHEUER-
MANN G., HEYER G.: Visualization of high-dimensional point
clouds using their density distribution’s topology. IEEE Trans-
actions on Visualization and Computer Graphics 17, 11 (2011),
1547–1559. 9

[OHJS10] OESTERLING P., HEINE C., JÄNICKE H., SCHEUER-
MANN G.: Visual analysis of high dimensional point clouds us-
ing topological landscape. In IEEE Paciﬁc Visualization Sympo-
sium (2010), pp. 113–120. 9

[OHWS13] OESTERLING P., HEINE C., WEBER G., SCHEUER-
MANN G.: Visualizing nd point clouds as topological landscape
proﬁles to guide local data analysis. IEEE Transactions on Visu-
alization and Computer Graphics 19, 3 (2013), 514–526. 9

[PBK10] PIRINGER H., BERGER W., KRASSER J.: Hypermoval:
Interactive visual validation of regression models for real-time
In Proceedings of the 12th Eurographics / IEEE -
simulation.
VGTC Conference on Visualization (2010), EuroVis’10, Euro-
graphics Association, pp. 983–992. 3, 5

[PCMS09] PASCUCCI V., COLE-MCLAUGHLIN K., SCORZELLI
G.: The toporrery: Computation and presentation of multi-
resolution topology. Mathematical Foundations of Scientiﬁc Vi-
sualization, Computer Graphics, and Massive Data Exploration
(2009), 19–40. 9

[PdSABD∗12] PORTES DOS SANTOS AMORIM E., BRAZIL
E. V., DANIELS J., JOIA P., NONATO L. G., SOUSA M. C.:
ilamp: Exploring high-dimensional spacing through backward
multidimensional projection. In IEEE Conference on Visual An-
alytics Science and Technology (2012), IEEE, pp. 53–62. 12, 14
[PEP∗11a] PAULOVICH F., ELER D., POCO J., BOTHA C.,
MINGHIM R., NONATO L.: Piece wise laplacian-based projec-
tion for interactive data exploration and organization. Computer
Graphics Forum 30, 3 (2011), 1091–1100. 4, 12

[PEP∗11b] POCO J., ETEMADPOUR R., PAULOVICH F., LONG
T., ROSENTHAL P., OLIVEIRA M., LINSEN L., MINGHIM R.:
A framework for exploring multidimensional data with 3d pro-
jections. Computer Graphics Forum 30, 3 (2011), 1111–1120.
12

[PNML08] PAULOVICH F., NONATO L., MINGHIM R., LEV-
KOWITZ H.: Least square projection: A fast high-precision mul-
tidimensional projection technique and its application to docu-
IEEE Transactions on Visualization and Com-
ment mapping.
puter Graphics 14, 3 (2008), 564–575. 4, 12

[PSBM07] PASCUCCI V., SCORZELLI G., BREMER P.-T., MAS-
CARENHAS A.: Robust on-line computation of reeb graphs: Sim-
plicity and speed. ACM Transactions on Graphics 26, 3 (2007).
6

[PSN10] PAULOVICH F., SILVA C., NONATO L.: Two-phase
mapping for projecting massive data sets. IEEE Transactions on
Visualization and Computer Graphics 16, 6 (2010), 1281–1290.
4, 12

[PWB∗09] POTTER K., WILSON A., BREMER P.-T., WILLIAMS
D., PASCUCCI V., JOHNSON C.: Ensemblevis: A ﬂexible ap-
proach for the statistical visualization of ensemble data. In Pro-
ceedings of IEEE Workshop on Knowledge Discovery from Cli-
mate Data: Prediction, Extremes, and Impacts (2009). 14

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

[PWR04] PENG W., WARD M. O., RUNDENSTEINER E. A.:
Clutter reduction in multi-dimensional data visualization using
dimension reordering. In IEEE Symposium on Information Visu-
alization (2004), IEEE, pp. 89–96. 7, 12

[RC94] RAO R., CARD S. K.: The table lens: merging graph-
ical and symbolic representations in an interactive focus+ con-
In Proceedings of
text visualization for tabular information.
the SIGCHI conference on Human factors in computing systems
(1994), ACM, pp. 318–322. 10

[Rd00] RHEINGANS P., DESJARDINS M.: Visualizing high-
In Proceedings of IEEE

dimensional predictive model quality.
Visualization (2000), pp. 493–496. 13

[Ree46] REEB G.: Sur les points singuliers d’une forme de pfaff
completement intergrable ou d’une fonction numerique [on the
singular points of a complete integral pfaff form or of a numerical
function]. Comptes Rendus Acad. Science Paris 222 (1946), 847–
849. 6

[RPH08] REDDY C. K., POKHARKAR S., HO T. K.: Generating
hypotheses of trends in high-dimensional data skeletons. In IEEE
Symposium on Visual Analytics Science and Technology (2008),
IEEE, pp. 139–146. 5

[RRB∗04] ROSARIO G. E., RUNDENSTEINER E. A., BROWN
D. C., WARD M. O., HUANG S.: Mapping nominal values to
numbers for effective visualization. Information Visualization 3,
2 (2004), 80–95. 3

[RS00] ROWEIS S. T., SAUL L. K.: Nonlinear dimensionality
reduction by locally linear embedding. Science 290, 5500 (2000),
2323–2326. 4, 11

[RW06] RASMUSSEN C. E., WILLIAMS C. K. I.: Gaussian Pro-
cesses for Machine Learning (Adaptive Computation and Ma-
chine Learning). The MIT Press, 2006. 5

[RZH12] ROSENBAUM R., ZHI J., HAMANN B.: Progressive
In IEEE Paciﬁc Visualization Symposium

parallel coordinates.
(2012), pp. 25–32. 7

[Shn92] SHNEIDERMAN B.: Tree visualization with tree-maps: 2-
d space-ﬁlling approach. ACM Transactions on graphics (TOG)
11, 1 (1992), 92–99. 9

[SLBC03] SWAYNE D. F., LANG D. T., BUJA A., COOK D.:
GGobi: evolving from XGobi into an extensible framework for
interactive data visualization. Computational Statistics & Data
Analysis 43, 4 (2003), 423–444. 9, 12

[Sma61] SMALE S.: On gradient dynamical systems. The Annals

of Mathematics 74 (1961), 199–206. 6

[SMC07] SINGH G., MEMOLI F., CARLSSON G.: Topological
methods for the analysis of high dimensional data sets and 3d ob-
ject recognition. In Symposium on Point Based Graphics (2007),
pp. 91–100. 3, 6, 13

[SMT13] SEDLMAIR M., MUNZNER T., TORY M.: Empiri-
cal guidance on scatterplot and dimension reduction technique
IEEE Transactions on Visualization and Computer
choices.
Graphics 19, 12 (2013), 2634–2643. 10

[SNLH09] SIPS M., NEUBERT B., LEWIS J. P., HANRAHAN P.:
Selecting good views of high-dimensional data using class con-
sistency. Computer Graphics Forum 28, 3 (2009), 831–838. 7

[SS04a] SEO J., SHNEIDERMAN B.: A rank-by-feature frame-
work for unsupervised multidimensional data exploration using
low dimensional projections. In IEEE Symposium on Informa-
tion Visualization (2004), IEEE, pp. 65–72. 7, 13

[SS04b] SMOLA A. J., SCHÖLKOPF B.: A tutorial on support
vector regression. Statistics and Computing 14, 3 (2004), 199–
222. 5

c(cid:13) The Eurographics Association 2015.

[SS06] SEO J., SHNEIDERMAN B.: Knowledge discovery in
high-dimensional data: Case studies and a user survey for the
rank-by-feature framework. IEEE Transactions on Visualization
and Computer Graphics 12, 3 (2006), 311–322. 7

[SSK06] SCHNEIDEWIND J., SIPS M., KEIM D. A.: Pixnostics:
Towards measuring the value of visualization. In IEEE Sympo-
sium on Visual Analytics Science and Technology (2006), IEEE,
pp. 199–206. 11

[SSK10] SEIFERT C., SABOL V., KIENREICH W.: Stress maps:
analysing local phenomena in dimensionality reduction based vi-
sualisations. In IEEE International Symposium on Visual Analyt-
ics Science and Technology. (2010). 4

[STH02] STOLTE C., TANG D., HANRAHAN P.: Polaris: a sys-
tem for query, analysis, and visualization of multidimensional re-
lational databases. IEEE Transactions on Visualization and Com-
puter Graphics 8, 1 (2002), 52–65. 11

[STH03] STOLTE C., TANG D., HANRAHAN P.: Multiscale vi-
sualization using data cubes. IEEE Transactions on Visualization
and Computer Graphics 9, 2 (2003), 176–187. 11

[SvLB10] SCHRECK T., VON LANDESBERGER T., BREMM S.:
Techniques for precision-based visual analysis of projected data.
Information Visualization 9, 3 (2010), 181–193. 4, 14

[SVW10] SHRINIVASAN Y. B., VAN WIJK J. J.: Supporting ex-
ploratory analysis with the select & slice table. Computer Graph-
ics Forum 29, 3 (2010), 803–812. 12

[SW09] SANFTMANN H., WEISKOPF D.:

Illuminated 3d scat-
terplots. Computer Graphics Forum 28, 3 (2009), 751–758. 10,
12

[SW12] SANFTMANN H., WEISKOPF D.: 3d scatterplot naviga-
tion. IEEE Transactions on Visualization and Computer Graph-
ics 18, 11 (2012), 1969–1978. 12

[SZD∗10] SANYAL J., ZHANG S., DYER J., MERCER A., AM-
BURN P., MOORHEAD R. J.: Noodles: A tool for visualization
of numerical weather model ensemble uncertainty. IEEE Trans-
actions on Visualization and Computer Graphics 16, 6 (2010),
1421 – 1430. 14

[TAE∗09] TATU A., ALBUQUERQUE G., EISEMANN M.,
SCHNEIDEWIND J., THEISEL H., MAGNOR M., KEIM D.:
Combining automated analysis and visualization techniques for
effective exploration of high-dimensional data. In IEEE Sympo-
sium on Visual Analytics Science and Technology (2009), IEEE,
pp. 59–66. 7, 11, 13

[TDSL00] TENENBAUM J. B., DE SILVA V., LANGFORD J. C.:
A global geometric framework for nonlinear dimensionality re-
duction. Science 290, 5500 (2000), 2319–2323. 4

[TFA∗11] TAM G. K. L., FANG H., AUBREY A. J., GRANT
P. W., ROSIN P. L., MARSHALL D., CHEN M.: Visualization
of time-series data in parameter space for understanding facial
dynamics. Computer Graphics Forum 30, 3 (2011), 901–910. 3
[TFH11] TURKAY C., FILZMOSER P., HAUSER H.: Brushing
dimensions-a dual visual analysis model for high-dimensional
data. IEEE Transactions on Visualization and Computer Graph-
ics 17, 12 (2011), 2591–2599. 4, 11, 12

[TFO09] TAKAHASHI S., FUJISHIRO I., OKADA M.: Applying
manifold learning to plotting approximate contour trees. IEEE
Transactions on Visualization and Computer Graphics 15, 6
(2009), 1185–1192. 14

[TJHH14] TURKAY C., JEANQUARTIER F., HOLZINGER A.,
HAUSER H.: On computationally-enhanced visual analysis of
heterogeneous data and its application in biomedical informatics.
In Interactive Knowledge Discovery and Data Mining in Biomed-
ical Informatics. Springer, 2014, pp. 117–140. 11

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

[TLLH12] TURKAY C., LUNDERVOLD A., LUNDERVOLD A. J.,
HAUSER H.: Representative factor generation for the interactive
visual analysis of high-dimensional data. IEEE Transactions on
Visualization and Computer Graphics 18, 12 (2012), 2621–2630.
4, 11

[TM03] TEOH S. T., MA K.-L.: Paintingclass: interactive con-
struction, visualization and exploration of decision trees. In Pro-
ceedings of the ninth ACM SIGKDD international conference on
Knowledge discovery and data mining (2003), ACM, pp. 667–
672. 13

[TM05] TZENG F.-Y., MA K.-L.: Opening the black box - data
driven visualization of neural networks. In Proceedings of IEEE
Visualization (2005), pp. 383–390. 13

[TMF∗12] TATU A., MAAS F., FARBER I., BERTINI E.,
SCHRECK T., SEIDL T., KEIM D.: Subspace search and vi-
sualization to make sense of alternative clusterings in high-
dimensional data. In IEEE Conference on Visual Analytics Sci-
ence and Technology (2012), IEEE, pp. 63–72. 5, 11, 12, 13

[TWSM∗11] TORSNEY-WEIR T., SAAD A., MOLLER T., HEGE
H.-C., WEBER B., VERBAVATZ J., BERGNER S.: Tuner: Princi-
pled parameter ﬁnding for image segmentation algorithms using
IEEE Transactions on Vi-
visual response surface exploration.
sualization and Computer Graphics 17, 12 (2011), 1892–1901.
5

[vdEvW11] VAN DEN ELZEN S., VAN WIJK J.: Baobabview:
Interactive construction and analysis of decision trees. In IEEE
Conference on Visual Analytics Science and Technology (2011),
pp. 151–160. 13

[Vid11] VIDAL R.: A tutorial on subspace clustering. IEEE Sig-

nal Processing Magazine (2011). 5, 13

[WAG05] WILKINSON L., ANAND A., GROSSMAN R.: Graph-
theoretic scagnostics. In IEEE Symposium on Information Visu-
alization (2005), vol. 0, p. 21. 7, 11, 12

[WAG06] WILKINSON L., ANAND A., GROSSMAN R.: High-
dimensional visual analytics: Interactive exploration guided by
pairwise views of point distributions. IEEE Transactions on Vi-
sualization and Computer Graphics 12, 6 (2006), 1363–1372. 7,
13

[War94] WARD M. O.: Xmdvtool: Integrating multiple methods
for visualizing multivariate data. In Proceedings of IEEE Visual-
ization (1994), pp. 326–333. 3

[War08] WARD M. O.: Multivariate data glyphs: Principles and
In Handbook of Data Visualization. Springer, 2008,

practice.
pp. 179–198. 8

[Wat05] WATTENBERG M.: A note on space-ﬁlling visualizations
and space-ﬁlling curves. In IEEE Symposium on Information Vi-
sualization (2005), pp. 181–186. 9

[WB94] WONG P. C., BERGERON R. D.: 30 years of multi-
In Proceedings of Sci-
dimensional multivariate visualization.
entiﬁc Visualization, Overviews, Methodologies, and Techniques
(1994), pp. 3–33. 1

[WBP07] WEBER G., BREMER P.-T., PASCUCCI V.: Topolog-
IEEE
ical landscapes: A terrain metaphor for scientiﬁc data.
Transactions on Visualization and Computer Graphics 13, 6
(2007), 1416–1423. 9

[WBP12] WEBER G. H., BREMER P.-T., PASCUCCI V.: Topo-
logical cacti: Visualizing contour-based statistics. Topological
Methods in Data Analysis and Visualization II Mathematics and
Visualization (2012), 63–76. 9

[Wea09] WEAVER C.: Conjunctive visual forms.

IEEE Trans-
actions on Visualization and Computer Graphics 15, 6 (2009),
929–936. 3

[WM04] WILLIAMS M., MUNZNER T.: Steerable, progressive
In IEEE Symposium on Information

multidimensional scaling.
Visualization (2004), pp. 57–64. 4, 11, 12

[WO11] WADDELL A., OLDFORD R. W.: RnavGraph: A visual-

ization tool for navigating through high-dimensional data. 10

[WPWR03] WANG J., PENG W., WARD M. O., RUNDEN-
STEINER E. A.:
Interactive hierarchical dimension ordering,
spacing and ﬁltering for exploration of high dimensional datasets.
In IEEE Symposium on Information Visualization (2003), IEEE,
pp. 105–112. 9

[WSPVJ11] WANG B., SUMMA B., PASCUCCI V., VEJDEMO-
JOHANSSON M.: Branching and circular features in high dimen-
sional data. IEEE Transactions on Visualization and Computer
Graphics 17, 12 (2011), 1902–1911. 6, 14

[YGX∗09] YUAN X., GUO P., XIAO H., ZHOU H., QU H.: Scat-
tering points in parallel coordinates. IEEE Transactions on Visu-
alization and Computer Graphics 15, 6 (2009), 1001–1008. 8,
10, 12

[YHW∗07] YANG J., HUBBALL D., WARD M. O., RUNDEN-
STEINER E. A., RIBARSKY W.: Value and relation display: in-
teractive visual exploration of large data sets with hundreds of
dimensions. IEEE Transactions on Visualization and Computer
Graphics 13, 3 (2007), 494–507. 9

[YPS∗04] YANG J., PATRO A., SHIPING H., MEHTA N., WARD
M., RUNDENSTEINER E.: Value and relation display for inter-
active exploration of high dimensional datasets. In IEEE Sympo-
sium on Information Visualization (2004), pp. 73–80. 9

[YRWG13] YUAN X., REN D., WANG Z., GUO C.: Dimen-
sion projection matrix/tree: Interactive subspace visual explo-
ration and analysis of high dimensional data. IEEE Transactions
on Visualization and Computer Graphics 19, 12 (2013), 2625–
2633. 4, 11

[YWR02] YANG J., WARD M. O., RUNDENSTEINER E. A.: In-
terring: An interactive tool for visually navigating and manipulat-
ing hierarchical structures. In IEEE Symposium on Information
Visualization (2002), IEEE, pp. 77–84. 9

[ZCQ∗09] ZHOU H., CUI W., QU H., WU Y., YUAN X., ZHUO
W.: Splatting the lines in parallel coordinates. Computer Graph-
ics Forum 28, 3 (2009), 759–766. 11, 13

[ZJGK10] ZIEGLER H., JENNY M., GRUSE T., KEIM D.: Visual
In IEEE
market sector analysis for ﬁnancial time series data.
Symposium on Visual Analytics Science and Technology (2010),
pp. 83–90. 3

[Zom05] ZOMORODIAN A. J.: Topology for Computing (Cam-
bridge Monographs on Applied and Computational Mathemat-
ics). Cambridge University Press, 2005. 6

[ZSWR06] ZAIXIAN X., SHIPING H., WARD M., RUNDEN-
STEINER E.: Exploratory visualization of multivariate data with
variable quality. In IEEE Symposium on Visual Analytics Science
and Technology (2006), pp. 183–190. 14

c(cid:13) The Eurographics Association 2015.

S. Liu, D. Maljovec, B. Wang, P.-T Bremer & V. Pascucci / Visualizing High-Dimensional Data:Advances in the Past Decade

ments. Valerio earned a Ph.D. in computer science at Purdue
University in May 2000, and a EE Laurea (Master), at the
University “La Sapienza” in Roma, Italy, in December 1993,
as a member of the Geometric Computing Group. His recent
research interest is in developing new methods for massive
data analysis and visualization.

Brief Biographies of the Authors

Shusen Liu received his bachelor degree in Biomedical
Engineering and Computer Science from Huazhong Univer-
sity of Science and Technology, China, in 2009, where he
worked at Wuhan National Laboratory for Optoelectronic on
GPU accelerated biophotonics applications. Currently he is
a PhD student at University of Utah. His research interests
lie primarily in high-dimensional data visualization and mul-
tivariate volume visualization.

Dan Maljovec is a graduate student working on his PhD
from the School of Computing at the University of Utah. Dan
has been a research assistant at the University of Utah’s Sci-
entiﬁc Computing and Imaging Institute since 2012. He re-
ceived his B.S. in computer science from Gannon University
in 2009. His research focuses on analysis and visualization
of high-dimensional scientiﬁc data and intuitive visualiza-
tion.

Bei Wang received her Ph.D. in Computer Science from
Duke University in 2010. She is currently a Research Sci-
entist at the Scientiﬁc Computing and Imaging Institute,
University of Utah. Her main research interests are com-
putational topology, computational geometry, scientiﬁc data
analysis and visualization. She is also interested in computa-
tional biology and bioinformatics, machine learning and data
mining. She is a member of the IEEE Computer Society.

Peer-Timo Bremer is a member of technical staff and
project leader at the Center for Applied Scientiﬁc Comput-
ing (CASC) at the Lawrence Livermore National Laboratory
(LLNL) and Associated Director for Research at the Center
for Extreme Data Management, Analysis, and Visualization
at the University of Utah. His research interests include large
scale data analysis, performance analysis and visualization
and he recently co-organized a Dagstuhl Perspectives work-
shop on integrating performance analysis and visualization.
Prior to his tenure at CASC, he was a postdoctoral research
associate at the University of Illinois, Urbana-Champaign.
Peer-Timo earned a Ph.D. in Computer science at the Uni-
versity of California, Davis in 2004 and a Diploma in Math-
ematics and Computer Science from the Leipniz University
in Hannover, Germany in 2000. He is a member of the IEEE
Computer Society and ACM.

Valerio Pascucci is the founding Director of the Center
for Extreme Data Management Analysis and Visualization
(CEDMAV) of the University of Utah. Valerio is also a Fac-
ulty of the Scientiﬁc Computing and Imaging Institute, a
Professor of the School of Computing, University of Utah,
and a DOE Laboratory Fellow, of the Paciﬁc Northwest Na-
tional Laboratory. Previously, Valerio was a Group Leader
and Project Leader in the Center for Applied Scientiﬁc Com-
puting at the Lawrence Livermore National Laboratory, and
Adjunct Professor of Computer Science at the University of
California Davis. Prior to his CASC tenure, he was a senior
research associate at the University of Texas at Austin, Cen-
ter for Computational Visualization, CS and TICAM Depart-

c(cid:13) The Eurographics Association 2015.

",False,2017.0,{},False,False,journalArticle,False,YA4MYR2A,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7784854/,,Visualizing High-Dimensional Data: Advances in the Past Decade,YA4MYR2A,False,False
SGYL3UXY,T4Y4B36G,"1
1
0
2

 

v
o
N
4

 

 
 
]

C
H
.
s
c
[
 
 

2
v
0
0
2
6

.

0
1
1
1
:
v
i
X
r
a

TopicViz: Semantic Navigation of Document

Collections

Jacob Eisenstein, Duen Horng “Polo” Chau, Aniket Kittur and Eric Xing

School of Computer Science
Carnegie Mellon University
Pittsburgh, PA 15215 USA

November 7, 2011

Abstract

When people explore and manage information, they think in terms of topics and themes.
However, the software that supports information exploration sees text at only the surface level.
In this paper we show how topic modeling – a technique for identifying latent themes across
large collections of documents – can support semantic exploration. We present TopicViz, an
interactive environment for information exploration. TopicViz combines traditional search and
citation-graph functionality with a range of novel interactive visualizations, centered around a
force-directed layout that links documents to the latent themes discovered by the topic model.
We describe several use scenarios in which TopicViz supports rapid sensemaking on large doc-
ument collections.

Introduction

1
As information repositories continue to expand and diversify, there is an urgent need for sys-
tems that help people explore and make sense of large document collections. While researchers
in information-seeking and related areas have developed increasingly effective interaction tech-
niques for navigating document collections [1, 16], these methods are hampered by a view of
language that is generally restricted to the surface level; such techniques are oblivious to the
semantic meaning behind the text. Meanwhile, researchers in machine learning and natural
language processing have developed powerful statistical methods for recovering latent seman-
tics [2], but the output of these methods is difﬁcult to present to both domain expert and novice
users alike.

In this paper, we introduce TopicViz, a new tool for searching and navigating large docu-
ment collections (Figure 1). TopicViz infers a set of topics that summarize the latent high-level
semantic organization of a collection, and provides a novel interactive view that exposes this
semantic organization using a force-directed layout. This layout permits a range of interactive

1

affordances, allowing users to gradually reﬁne their understanding of the search results and
citations links, while focusing in on key semantic distinctions of interest.

The analytic engine of our approach is the topic model – a powerful statistical technique
for identifying latent themes in a document collection [2]. Without any annotation, topic mod-
els can extract topics – sets of semantically-related words – and describe each document as a
mixture of these topics. For example, a given research paper might be characterized as 70%
human-computer interaction, and 30% machine learning.1 Topic models have been success-
fully applied to a broad range of text, and the extracted topics have been shown to cohere with
readers’ semantic judgments [4]. But while topic models are often motivated as a technique
to support information seeking, there has been little investigation of how users can understand
and exploit them.

One of the principle strengths of topic models is their ﬂexibility: topics need not correspond
to any predeﬁned taxonomy, but rather represent the latent structure inherent to the document
collection. However, this means that the content of each topic must somehow be conveyed
to the user. In topic modeling research, this issue is almost invariably addressed by showing
ranked lists of words and documents that are closely associated with each topic. But such
lists have undesirable properties:
it is difﬁcult to show more than a few entries per topic,
the meaning of individual terms may be unknown to non-experts;2 in addition, the numerical
scores for each word and topic are hard to interpret.

While hundreds of papers address the mathematical methodology of topic modeling, rel-
atively few take up the question of how topic models can support information exploration.
Our approach is distinguished from prior work in its emphasis on interaction: the user is em-
powered to manipulate the visualization by adding, rearranging or removing topics, and by
controlling the set of documents to visualize. The motivation for this design stems from our
focus on local information exploration: we aim to provide a deep understanding of a local
area of the information landscape that is relevant to the user’s goals, rather than a surface-level
static view of thousands of documents. We provide affordances for users to quickly focus in
on the topical distinctions that are relate to their goals, allowing them to interactively manip-
ulate topics within this space to better understand document-document, document-topic, and
topic-topic relationships.

2 Background
Topic models of document collections A topic model is a hierarchical probabilistic
model of document content [2]. Each topic is a probability distribution over words, β; ev-
ery word in every document is assumed to be randomly generated from one topic. In a given
document the proportion of words generated from each topic is given by a latent vector θd.
Thus, the matrix θ provides a succinct summary of the semantics of each document.

Both the topics β and the document descriptions θ can be obtained through ofﬂine statistical
inference [2, 10], without any need for manual annotation. Thus, topics need not correspond to

1This distinguishes topic models from more coarse-grained techniques that treat each document as a member of a

single cluster [7].

2For example, “muc”, “muc-6” and “muc-7” are three of the four most relevant terms for one of the topics learned
by our model. These terms are well-known to experts in natural language processing (they are the names of shared
research tasks), but are incomprehensible to an outsider.

2

Figure 1: The TopicViz environment. The main panel shows the initial presentation for a selected set
of documents, which are arranged in a force-directed layout controlled by the seven best-matching
topics. The upper-left panel shows the search results in list form, and the lower-left panel describes
the selected topic, “multilingual”.

3

“Morphology”

“Multilingual”

morphemes
morpheme

afﬁxes
afﬁx
kanji
endings
inﬂections
sufﬁxes

inﬂectional
katakana

bilingual

english-chinese

bitext

english-french
monolingual

melamed
cognates
hansard

japanese-english

systran

“Parsing”
nonterminal
nonterminals

adjoining

cfgs
cfg

subtree

non-terminal

subtrees
adjunction

non-terminals

Figure 2: A textual display of the top ten automatically-identiﬁed keywords from three topics
obtained from a dataset of research papers on computational linguistics. The topic names were
assigned manually.

any predeﬁned categories; indeed, this is why they are useful for exploratory analysis. In the
research literature, topic models are often displayed through textual tables showing the most
relevant words and documents for each topic, as in Figure 2.

In typical scenarios, the number of topics ranges from 10-200, and the number of docu-
ments can range from a few hundred to hundreds of thousands. The number of topics can be
determined automatically [20], or set in advance through interactive exploration by a domain
expert. Note that TopicViz is not currently designed to support the user in training new topic
models or exploring alternative topic model parametrizations. Rather, we target the case where
a topic model is trained in advance, to be used by many novices who are interested in a given
domain, such as legal documents or research literature. We consider the problem of supporting
end users to train new topic models to be an important area of future work.

Visualizing large document collections Most prior work on visualizing large document
collections can be divided into two high-level streams: citation graphs and static projection. In
citation graph approaches, each document is a node, and edges are used to represent citation
links; clustering is then performed over the resulting graph [19, 3]. The clusters are displayed
using techniques such as triangulation [19] or force-directed layout [3]. Such approaches are
well-suited to discover connected disciplines in science at a high-level, but do not consider the
textual content of individual documents.

Projection-based methods apply topic models [12, 9, 21] or related techniques like Latent
Semantic Analysis [14]. The high-dimensional document descriptions (θd in our notation from
earlier in this section) are then projected into two-dimensional coordinates for visualization
(Landauer et al. user color as an additional dimension [14]). A related, recent approach to use
visualize topic models is the work of Liu et al., who emphasize the temporal dimension by
placing it on the the X-axis of a graph that shows the evolution of topic strength and content
over time [15].

We differ from this prior work in our emphasis on document search and interactive sense-

4

making [8, 13]. Rather than viewing the entire collection and topic model in a single static
view, the user manipulates an ever-shifting subset of documents and topics. This approach is
driven by the intuition – dating back to early work on Scatter/Gather [7] – that only a small
corner of the topic space will be relevant for any given information search. We allow docu-
ments to be easily added and removed from the view, either through additional search queries
or by exploring citation links; similarly, topics can be moved and manipulated to reveal subtle
semantic distinctions. The remainder of the paper describes these affordances in greater detail.

3 Scenarios
The key idea behind TopicViz is to integrate a force-directed layout for topic models with an
integrated environment for expanding and reﬁning a document list. As in conventional doc-
ument search, the entrance point is the search query; however, rather than simply listing the
search results, they are visualized in an interactive force-directed layout with a range of affor-
dances. As these capabilities are best described by example, this section is centered around a
detailed novice user scenario and two briefer expert scenarios.3 The mechanisms underlying
TopicViz are described in detail in the following section.

3.1 Novice scenario
Consider an individual given the task of searching an unfamiliar research literature, with the
goal of identifying whether a particular technology can be applied to a commercial problem. In
our scenario, the individual is tasked with determining whether it is possible to automatically
identify names on foreign language websites, using a collection of 15,032 research papers on
computational linguistics [18].

The user begins by devising a query; with current tools like Google Scholar and Lexis
Nexis, the response to the query would be an ordered list of results. Only some of the resulting
documents will be relevant, and almost surely there will be relevant documents that do not
match the query. The user may then vary the search terms or navigate the citation links to try
to get a complete sense of the research literature in this unfamiliar area.

Now consider the same task, performed with TopicViz. The ﬁrst step is the same: the user
supplies a search query. The results are shown in a list (the top-left part of Figure 1). The
user then drags as many documents as desired into the main area, which is called the Topic
Field: each document is displayed as a node, and these nodes are surrounded by a ring of
topic centers. The topic centers are “pinned,” while the position of each document is set by
a force-directed layout in which the topics each exert an attractive force proportional to the
document’s topical relevance. Thus, documents with similar content will be located near each
other.

The size of each topic center is determined by its relevance to the documents in the ﬁeld,
and only the most relevant topics are shown. The panel on the lower-left shows the most
relevant words for each topic (selected by mouseover). The user can also see the relevance
of each topic to the documents in the ﬁeld both statically (by the document’s position) and

3Video of many of these affordances can be found at http://www.cs.cmu.edu/˜dchau/topicviz/

topicviz.mp4.

5

Figure 3: Rearranging the topic centers to view topic-topic relationships

6

Figure 4: By arranging the topic centers into two points, the documents are shown linearly by
relevance.

dynamically (by dragging the topic center around to see how the document nodes are affected).
The topic names are speciﬁed in advance, either manually by a domain expert, or through
automatic methods [17]; the user is free to rename topics with more familiar terms.

In our scenario, the user recognizes the topic multilingual as especially relevant to the
search – but other topics like morphology and Bayesian are not familiar. To better understand
if these topics are relevant, the user rearranges the topics, with multilingual in the upper-left
corner and the unfamiliar topics in an arc across the screen (Figure 3). From this view, the user
sees that morphology is related to multilingual, as several documents have strong connections
with both topics.

The user inspects the set of terms associated with the topic morphology (Figure 2). While
terms like “morpheme” and “inﬂection” are confusing, the user recognizes the terms “afﬁx”
and “sufﬁx” as referring to parts of individual words. Based on this insight, the user renames
the topic from morphology to subwords. While this name is not typically used in the research
literature, it helps the user relate the topic model to her pre-existing ontology.

Having identiﬁed morphology and multilingual as key topics of interest, the user again
rearranges the topics, placing the relevant topics in one corner of the screen and the others
in another corner. This causes the document nodes to form a line, with location governed by
relevance to the topics of the interest (Figure 4). The user now removes documents that are not
close to the desired topics by selecting and deleting their nodes.

7

The user has now culled the original list of query hits to a set of documents that are closely
related to multiple topics of interest. But the coverage of this document set depends on the
quality of the original query. To make sure that important documents have not been missed, the
user selects a subset of particularly promising documents and adds documents that cite them.
These new documents may not match the search query by name, but may still be relevant. The
user can now investigate the topical characteristics of these new documents and further reﬁne
the search.

Ultimately, the user arrives at a set of documents that reﬂect the underlying semantics of
the information search. By investigating the topic structure, the user has pruned away “false
positives” that match the query but are in fact irrelevant; by walking the citation graph, the
user has identiﬁed “false negatives” that are relevant but did not match the original query.
Morever, by interatively exploring the documents, topics, and terms that relate to the initial
query, the user acquires a deeper, structured understanding of the relevant area of the document
collection. This elucidates the speciﬁc role played by each document in the relevant research
literature, and contextualizes previously unknown themes, such as the topic morphology. The
user is now prepared to summarize the desired content, having obtained both a comprehensive,
high-precision list of documents and a clearer understanding of this area of research.

3.2 Expert scenarios
Determining author expertise We brieﬂy consider a scenario involving a user who has
more expertise in the domain of the document collection. Here, the expert wants to identify
the topical interests of several authors – perhaps to distinguish the speciﬁc contributions of
multiple authors on a single paper. To do this, the user searches for papers by each author
and drags them into the ﬁeld. However, unlike the previous view, the documents are pinned in
place, and the topics ﬂoat between them (such non-default behavior can be easily set using the
toolbar at the topic of the window). The edges in the force-directed layout are bidirectional, and
work identically in this setting; the user need only pin sets of documents for each author, and
then add relevant topics to the view. Figure 5 shows such a view for the relationship between
the three authors of a heavily-cited paper in computational linguistics; this view reveals that
the author to the upper-left has focused more on the speech topic; the author to the upper-right
has focused more on syntax and lexical semantics; and the author on the bottom has focused
more on the Bayesian and applications topics.

Direct manipulation 2D projections Finally, we consider a scenario in which an expert
user has a detailed understanding of the topic model, and wants to select a set of documents
that ﬁt a very speciﬁc semantic proﬁle. The novice scenario explored an affordance in which
documents were arranged on a spectrum between two topics (Figure 4). In fact, much more ex-
pressive arrangements are possible, yielding a direct-manipulation inferface for creating two-
dimensional projections.

Suppose that the expert user wants documents that describe multilingual analysis and
translation, but avoid syntax and parsing; in fact, let us suppose that parsing is completely
inappropriate due to technical constraints. The user can arrange the topic centers on a line, with
parsing to the far left and syntax slightly left of center, while locating the multilingual and
translation topics to the far right. Such a conﬁguration can be viewed as a one-dimensional

8

Figure 5: To compare topical emphasis of different authors, the expert user creates and pins “piles”
of documents for each author; the unpinned topic centers are pulled between them.

9

Figure 6: The expert user arranges the topic centers on the X and Y-axes to create a custom two-
dimensional projection of the topic space.

projection that assigns a large negative weight to parsing, a smaller negative weight to syn-
tax, and a equal positive weights to multilingual and translation. Next, the user wants to
distinguish documents that focus on morphology from those that focus on semantics – this
time using the Y-axis. The ﬁnal conﬁguration is shown in Figure 6. The user can now select
the documents in the desired subspace for further viewing and reﬁnement, as described in the
novice scenario.

Overall, we see that in two dimensions, the location of each topic center deﬁnes a projection
matrix that reduces the high-dimensional topic proportion vector to an easily viewable two
dimensional representation. By dragging topics further from the center, their absolute weight
is increased, causing them to exert a greater inﬂuence on the position of each document. Thus,
TopicViz offers an intuitive direct manipulation interface for designing projections that isolate
the desired region of topic space.

4 The TopicViz System
We now describe in more detail the mechanisms and affordances underlying the TopicViz sys-
tem. The core idea of TopicViz is to provide affordances for interactively exploring the topical
afﬁliations of a set of documents, while facilitating reﬁnement and expansions of the docu-

10

ment set. Thus, the main entry point is the search query, which will be familiar to users from
traditional information search interfaces. However, from this point, we diverge from prior ap-
proaches, emphasizing the direct manipulation design of novel 2D projections and interactive
exploration of document-topic and topic-topic relationships. The previous section described
the envisioned use cases for such interactions; we now describe the underlying mechanisms.

4.1 Document positioning
As described in Section 2, a topic model is deﬁned by the topic-term relations and the topic-
document relations. Both objects are high-dimensional: the topic-term matrix contains a row
for each topic, and a column for each word in the vocabulary; the topic-document matrix con-
tains a row for each document and a column for each topic. The number of documents and
vocabulary size are each typically in the thousands;4, and the numerical values are not intu-
itively meaningful on their own; rather, the structure of the topic model is best understood in a
relational setting. Thus, we present a document ﬁeld view incorporating topics and documents.
In the document ﬁeld, we see the relationship between documents and topics. Inspired by
“dust-and-magnet” approaches to information visualization, (e.g., [22]), we initially arrange
a ring of topic nodes around the outside of the ﬁeld, which act as magnets. Documents are
represented as nodes within this ﬁeld; edge weights are based on Hooke’s law, with 1 − θdi as
the force of the spring between topic i and document d. A document that is a near 100% match
for a given topic will be placed almost directly on that topic’s magnet; a document that is a
50% match for each of two topics will be positioned halfway between them. Thus, documents
that have similar topic proportions are located near each other, reﬂecting semantic differences
directly in the spatial layout. Visualizing such a high-dimensional model in 2D inevitably
causes information to be lost, but the force-directed layout permits interactive manipulation of
document nodes, allowing users to more closely examine regions of particular interest.

4.2 Document set reﬁnement
As the number of documents in a collection is typically in the thousands, it is not helpful to
view all of the documents at the same time. Our interface includes two affordances for selecting
sets of documents to visualize. The ﬁrst affordance – which is the entry point to interaction
with our system – is the search query. Just as in traditional search interfaces, the user enters
a query and receives a list of results (in a separate panel). These results can be sorted by
traditional metadata: titles, author, year, and venue. The user can then drag documents into the
document ﬁeld, which provides an intuitive graphical visualization of the semantic structure of
the search results. The second affordance permits the user to walk to the citation graph, adding
citing or cited documents for any set of documents already in the view; the citation links are
made visible.

By default, the set of topic magnets is dynamically updated to show the topics that are most
relevant to the documents currently in the ﬁeld. This feature can be turned off, allowing topics
to be added and deleted manually.

4In the scenario, the number of topics is 25; the vocabulary size is 18,743 (after pruning infrequent words) the

number of documents is 15,032.

11

4.3 Implementation
TopicViz is implemented through Shiftr [5], a Java platform designed to support interactive ex-
ploration and querying of large graph data with millions of nodes and edges. Shiftr builds on
the Prefuse library for force-directed layouts [11], providing a collection of fundamental oper-
ations over graph data: querying nodes by arbitrary node attributes; visualizing user-speciﬁed
subgraphs; and ﬂexible spatial arrangement for nodes through pinning and unpinning. Top-
icViz uses these lower-level operations to provide a force-directed layout interface for exploring
topic models of document content.

5 Future work
A key target for future work is empirical validation. Indeed, beyond the necessary task of eval-
uating the speciﬁc design decisions taken in TopicViz, we also believe that this tool can serve as
a platform for in situ user studies of whether and how topic models can best support document
set exploration and sensemaking. Speciﬁcally, we plan to develop a battery of information-
exploration tasks (similar to the email exploration tasks of Liu et al. [15]) and compare the
efﬁcacy of TopicViz with traditional search interfaces, as well as textual and table-based repre-
sentations of topic models.

From a visualization standpoint, we see several intriguing directions for future work. While
TopicViz offers an innovative take on the document-topic relationship, the connection between
topics and terms is still expressed through traditional term lists. We plan to explore whether
a more spatial visualization for this relationship would be possible, or whether an alternative
approach such as DocuBurst [6] could be incorporated in the TopicViz environment. We also
believe that an integrated presentation of document metadata such as time, authorship, and
venue would substantially improve the practical usability of the system. Finally, we are eager
to investigate the use of color as a third dimension, either to visualize such metadata, or to
enable gestalt high-level comparisons between document sets [14].

6 Summary
Topic models can give powerful insights on document collections – but only if used in combi-
nation with a comprehensible presentation and an interaction design built around the informa-
tion exploration process. TopicViz presents an interactive visualization that places topic models
in the context of a search interface, ﬁlling the same role currently played by keyword search.
We see two main advantages of our approach: it accounts for latent document semantics, and
provides an interactive spatial visualization that allows the user to rapidly focus on key areas
of interest.

Acknowledgments
This work was supported by the following grants: AFOSR FA9550010247, ONR N0001140910758,
NSF OCI-0943148, NSF IIS-0968484, NSF IIS-0713379, NSF CAREER DBI-0546594, and
an Alfred P. Sloan Fellowship.

12

References
[1] M. Baldonado and T. Winograd. SenseMaker: an information-exploration interface sup-
porting the contextual evolution of a user’s interests. In Proceedings of CHI, pages 11–18,
1997.

[2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. Journal of Machine

Learning Research, 3:993–1022, 2003.

[3] K. W. Boyack, R. Klavans, and K. B¨orner. Mapping the backbone of science. Sciento-

metrics, 64(3):351–374, 2005.

[4] J. Chang, J. Boyd-Graber, S. Gerrish, C. Wang, and D. M. Blei. Reading tea leaves: How

humans intepret topic models. In NIPS, 2009.

[5] D. H. Chau, A. Kittur, C. Faloutsos, and J. I. Hong. Shiftr: a user-directed, link-based
system for ad hoc sensemaking of large heterogeneous data collections. In Proceedings
of CHI, pages 3535–3536, 2009.

[6] C. Collins, S. Carpendale, and G. Penn. Docuburst: Visualizing document content using

language structure. Computer Graphics Forum, 28(3):1039–1046, 2009.

[7] D. Cutting, D. Karger, J. Pedersen, and J. Tukey. Scatter/gather: A cluster-based approach
to browsing large document collections. In Proceedings of SIGIR, pages 318–329, 1992.
[8] B. Dervin. An overview of sense-making research: concepts, methods, and results to

date. In Annual Meeting of the International Communication Association, 1983.

[9] H. et al. The NIH visual browser: An interactive visualization of biomedical research. In

Proceedings of IEEE Conference on Information Visualisation, 2009.

[10] T. Grifﬁths and M. Steyvers. Finding scientiﬁc topics, 2004.
[11] J. Heer, S. K. Card, and J. A. Landay. prefuse: a toolkit for interactive information

visualization. In Proceedings of CHI, pages 421–430, 2005.

[12] T. Iwata, T. Yamada, and N. Ueda. Probabilistic latent semantic visualization: Topic

model for visualizing documents. In Proceedings of KDD, 2008.

[13] C. C. Kuhlthau. Inside the search process: Information seeking from the users perspec-

tive. Journal of the American Society for Information Science, 42:361–371, 1991.

[14] T. K. Landauer, D. Laham, and M. Derr. From paragraph to graph: Latent semantic

analysis for information visualization. PNAS, 101:5214–5219, April 2004.

[15] S. Liu, M. X. Zhou, S. Pan, W. Qian, W. Cai, and X. Lian. Interactive, topic-based visual

text summarization and analysis. In Proceedings of CIKM, pages 543–552, 2009.

[16] G. Marchionini. Exploratory search: from ﬁnding to understanding. Communications of

the ACM, 49(4):46, 2006.

[17] Q. Mei, X. Shen, and C. Zhai. Automatic labeling of multinomial topic models.

In

Proceedings of KDD, pages 490–499, 2007.

[18] D. R. Radev, P. Muthukrishnan, and V. Qazvinian. The ACL anthology network corpus.
In Workshop on Text and Citation Analysis for Scholarly Digital Libraries, pages 54–61,
2009.

13

[19] H. Small. Visualizing science by citation mapping. Journal of the American Society for

Information Science, 50(9):799–813, 1999.

[20] Y. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical dirichlet processes. Journal of the

American Statistical Association, 101(576):1566–1581, 2006.

[21] L. van der Maaten and G. Hinton. Visualizing high-dimensional data using t-sne. Journal

of Machine Learning Research, 9:2579–2605, 2008.

[22] J. S. Yi, R. Melton, J. Stasko, and J. A. Jacko. Dust & magnet: multivariate information

visualization using a magnet metaphor. Information Visualization, 4:239–256, 2005.

14

",False,2011.0,{},False,False,journalArticle,False,SGYL3UXY,[],self.user,False,False,False,False,http://arxiv.org/abs/1110.6200,,TopicViz: Semantic Navigation of Document Collections,SGYL3UXY,False,False
PRCPUX25,7JQEDWA7,"Observation-Level Interaction with Statistical Models for Visual Analytics 

Alex Endert+      Chao Han*      Dipayan Maiti *     Leanna House*      Scotland Leman*      Chris North+ 

+ Department of Computer Science  

* Department of Statistics 

Virginia Tech 

 

 

ABSTRACT 

specifically  designed  for  visualizations  of  this  purpose.  Thus, 
many  visual  analytic  systems  are  fundamentally  based  on 
interaction  with  statistical  models  and  algorithms,  using 
visualization as the medium for the communication (i.e. where the 
interaction occurs). This communication is performed via direct 
interaction  with  the  parameters  of  the  model.  For  example, 
Interactive  Principal  Component  Analysis,  iPCA [3],  allows  the 
user to change the weight for each dimension in calculating the 
direction  of  projection  using  multiple  sliders  (one  slider  per 
dimension). Also, in an interactive visualization using MDS [4], 
the  user  can  weight  the  dissimilarities  in  the  calculation  of  the 
stress function through similar visual controls.  
In both instances, the model is made aware of the user input 
through  a  formal  and  direct  modification  of  a  parameter  (i.e. 
parameter  level  interaction).  The  drawback  of  this  type  of 
interaction  is  that  users  are  expected  to  be  experts  in  the 
underlying  model  that  generates  the  visualization.  Moreover,  as 
datasets continue to increase in size and dimensionality, directly 
adjusting dimensions or parameters creates an issue of scalability. 
Both interactive MDS [4] and object-centered MDS [5] also allow 
interactions such as “anchoring” points to provide the algorithm 
with user specified starting positions, either to test the sensitivity 
of the current visualization or to obtain an alternate spatial layout 
based  on  the  anchored  observations.  In  both  cases,  the  visual 
analytic system does not leverage the observation level interaction 
to obtain information about the parameters of the model. 
In  this  paper,  we  reevaluate  interaction  with  such  models, 
moving  away  from  parameter  level  interactions,  and  propose  to 
focus on interacting with data (i.e. observation level interaction). 
In contrast to parameter level interactions, users are familiar and 
comfortable  interacting  directly  with  the  data  in  a  spatial 
visualization, freely organizing and relocating observations as an 
integral  part  of  their  sensemaking  process  [6].  Thus,  it  is 
necessary for us to design models that are more tightly integrated 
with interaction at the observation level, rather than through visual 
controls of parameters. 
Our  framework  shields  users  from  the  technicalities  of  the 
model  and  allows  them  to  interact  freely  with  the  data  in  the 
visual  space. The  typical  steps  in  a  discovery  process  based  on 
such a framework will be as follows: 1) the visual analytic system 
provides  a  visualization  based  on  initial  values  of  model 
parameters,  2)  users 
inject 
understanding  and  semantic  reasoning  of  the  data,  3)  under  a 
certain  predefined  mapping  of 
the  user's  observation-level 
interaction to analytic reasoning, the parameters of the model are 
tuned  or  re-weighted  to  reflect  the  user's  understanding  of  the 
data,  and  finally  4) 
the  system  regenerates  an  updated 
visualization  based  on  the  new  parameter  values  of  the  model. 
The  process  continues  iteratively,  as  does  sensemaking,  for  the 
duration of the analytic process.  
We  show  examples  that  such  a  framework  can  be  applied  to 
dimension  reduction  algorithms  for  visual  analysis  of  high-
dimensional data. Our framework of incorporating user interaction 
can be applied to either deterministic or probabilistic methods. We 
demonstrate  this  on:  PPCA  (a  probabilistic  projection-based 

interact  with  observations 

to 

is 

facilitated 

In  visual  analytics,  sensemaking 

through 
interactive  visual  exploration  of  data.  Throughout  this  dynamic 
process, users combine their domain knowledge with the dataset 
to  create  insight.  Therefore,  visual  analytic  tools  exist  that  aid 
sensemaking  by  providing  various  interaction  techniques  that 
focus  on  allowing  users  to  change  the  visual  representation 
through adjusting parameters of the underlying statistical model. 
However,  we  postulate  that  the  process  of  sensemaking  is  not 
focused on a series of parameter adjustments, but instead, a series 
of perceived connections and patterns within the data.  Thus, how 
can models for visual analytic tools be designed, so that users can 
express  their  reasoning  on  observations  (the  data),  instead  of 
directly  on  the  model  or  tunable  parameters?  Observation  level 
(and  thus  “observation”)  in  this  paper  refers  to  the  data  points 
within  a  visualization.  In  this  paper,  we  explore  two  possible 
observation-level interactions, namely exploratory and expressive, 
within  the  context  of  three  statistical  methods,  Probabilistic 
Principal Component Analysis (PPCA), Multidimensional Scaling 
(MDS),  and  Generative  Topographic  Mapping  (GTM).  We 
discuss  the  importance  of  these  two  types  of  observation  level 
interactions, in terms of how they occur within the sensemaking 
process.  Further,  we  present  use  cases  for  GTM,  MDS,  and 
PPCA,  illustrating  how  observation  level  interaction  can  be 
incorporated into visual analytic tools. 
 
KEYWORDS:  observation-level 
statistical models. 
 
INDEX TERMS: H.5.0 [Human-Computer Interaction] 
1 

interaction,  visual  analytics, 

INTRODUCTION 
Visual  analytics 

is  “the  science  of  analytical  reasoning 
facilitated by interactive visual interfaces” [1]. The goal of visual 
analytics  (VA)  is  to  extract  information,  perform  exploratory 
analyses,  and  validate  hypotheses 
interactive 
exploration  process  known  as  sensemaking 
this 
sensemaking loop, users proceed through a complex combination 
of proposing and evaluating hypotheses and schemas about their 
data, with the ultimate goal of gaining insight (i.e. “making sense 
of”  the  data).  A  wide  variety  of  statistical  models  have  been 

through  an 

[2]. 

In 

{aendert, chaohan, dipayanm, lhouse, leman, north}@vt.edu 
 Blacksburg, VA 24061 

model),  MDS  (a  deterministic  stress  minimization  model),  and 
GTM  (a  probabilistic  manifold  learning  model).  However,  the 
fundamental framework can be applied to numerous other models. 
Finally,  we  discuss  the  tradeoffs  between  these  models  for 
observation-level interaction. 
2 

RELATED WORK 

these  algorithms. 

The three methods in this paper were chosen either because of 
their  wide  usage  or  flexibility  in  modelling  non-linear  data.  A 
large and growing body of literature has shown their successful 
applications in visualization. For example, PCA has gained a lot 
of  success  in  the  area  of  image  classification,  with  applications 
such  as  face  recognition  [7-10].  MDS  has  been  used  in  graph 
layout for network visualization [11-13] due to its rich distance 
information.  GTM  is  good  at  visualizing  unstructured  data  like 
newsgroup  text  collections,  web  navigation  datasets  [14],  and 
datasets  which  have  complicated  structure,  for  instance,  protein 
sequences [15] and the standard Swiss-Roll dataset [16].   
Research  has  gone  into  creating  systems  that  allow  for 
interaction  with 
iPCA  [3]  allows  direct 
interaction with the parameters of PCA, through the use of visual 
controls.  In  adjusting  these  parameters,  users  can  observe  the 
corresponding change in the visualization. Buja et al. demonstrate 
an  interactive  version  of  MDS  in  which  users  can  define  static 
locations of a number of observations, and the algorithm positions 
the remaining observations into the layout [4]. We would consider 
this an example of an observation-level interaction, as users can 
“test” the location of specific observations, and see how the layout 
(and  thus  the  algorithm)  responds.  However,  the  interaction  is 
directly on pairwise dissimilarities, instead of updating of global 
dimension  weights  based  on  the  user’s  positioning  of  the 
observations. 
interactive  MDS 
algorithm  using  “object-centric  interaction”,  where  users  can 
explore  alternative  positions  of  observations  by  moving  them 
within  the  spatialization  [5].  This  is  similar  to  our  concept  of 
observation-level interaction, in that the interaction is occurring in 
the spatialization. However, the movement of an observation is to 
discover the proportional error contribution, and not to adjust the 
parameters  of  MDS.  Another  example  of  interacting  directly  in 
the spatialization is “Dust & Magnet”, an interactive visualization 
allowing users to understand large, multivariate datasets [17]. It is 
based on the metaphor of magnets, which can attract observations 
that share the attributes of the magnet. Thus, in placing multiple 
magnets into specific locations in the space, users can gain insight 
into  the  structure  of  the  data  through  seeing  how  observations 
respond to the attractors. Therefore, the interaction is performed 
on attractors (i.e., parameters), not on the observations. 
From  this  work,  we  learn  that  statistical  methods  are  widely 
used in visual analytics, and approaches to making these methods 
interactive  have  been  proposed.  However,  interactivity  in  these 
cases mainly refers to direct manipulation of model parameters. 
With observation-level interaction, we focus on interacting with 
the  observations  within  the  spatial  metaphor,  and  handle  the 
corresponding parameter updates through our methods. 
3 

Similarly,  Broekens  et  al.  describe  an 

OBSERVATION-LEVEL INTERACTION 

In  general,  observation-level  interaction  refers  to  interactions, 
occurring  within  a  spatialization,  that  enable  users  to  interact 
directly  with  data  points  (i.e.,  observations).  A  spatialization  in 
this  context  refers  to  a  two-dimensional  layout  calculated  from 
high-dimensional  data  where  the  metaphor  of  relative  spatial 
proximity represents similarity between documents. That is, data 
points placed closer together are more similar. Observation-level 

in 

interaction 

interaction  may  exhibit 

the  algorithm  computes  similarity.  Thus,  when 

interactions  are  therefore  tightly  coupled  with  the  underlying 
mathematical  models  creating  the  layout,  thus  allowing  the 
models to update parameters based on the interaction occurring. 
While  numerous  forms  of 
these 
characteristics  (e.g.,  moving  clusters  of  documents,  marking 
regions of interest within the spatialization, etc.), in this paper we 
will  focus  on  one  –  movement  of  observations.  From  previous 
studies, we found that movement of observations (in those cases 
documents) closer together is one way for the user to externalize 
the  analytical  reasoning  that  those  documents  are  somehow 
similar [6]. In this study, the spatial rearrangement of documents 
was  an  integral  part  of  each  intelligence  analysts’  sensemaking 
process.  Further,  this  study  points  out  that  users  perform 
observation-level 
two  ways,  exploratory  or 
expressive, based on the particular analytical reasoning associated 
with the interaction, and also how the system responds.  
During an exploratory interaction, users utilize the algorithm to 
explore the data and the space. For example, through dragging one 
observation within the layout, users gain insight into the structure 
of  the  data  by  observing  how  other  data  reacts  given  the 
algorithm. While an observation is dragged through the layout, the 
algorithm  adjusts  the  layout  of  the  remaining  data  according  to 
how 
the 
observation  is  dragged  towards  a  cluster  of  data,  similar  data 
points attract, while dissimilar ones repel. Additional information 
such  as  a  list  of  similar  and  dissimilar  parameters  can  also  be 
displayed.  Through  this  process,  users  learn  about  a  single 
observation,  and  how  it  relates  to  the  other  observations  in  the 
dataset.  
An expressive interaction is different, in that it allows users to 
“tell” the model that the criteria (i.e. the parameters, weights) used 
for  calculating  the  similarity  need  to  be  adjusted  globally.  For 
example,  as  a  user  reads  two  documents,  she  denotes  they  are 
similar by dragging them close together. If this were exploratory, 
the two documents would repel again. However, in an expressive 
form of this interaction, it is the responsibility of the underlying 
mathematical  model  to  calculate  and  determine  why  these 
documents  are  similar,  and  update  the  model  generating  the 
spatial layout accordingly. Using the methods below, we illustrate 
how both expressive and exploratory forms of observation-level 
interaction  are  enabled  through  modifications  made  to  three 
common statistical methods (PPCA, MDS, and GTM).  
4  METHODS INTEGRATING OBSERVATION-LEVEL INTERACTION 
A probabilistic model assumes a sampling distribution for the 
observed data and an uncertainty over the model parameters (e.g. 
PPCA and GTM discussed in Section 4.1 and 4.3 respectively). A 
deterministic method makes no such assumptions about the data 
or the parameters (e.g. Weighted MDS, discussed in Section 4.2). 
House  et  al.  describe  in  detail  the  underpinnings  of  the 
probabilistic framework, termed as “Bayesian Visual Analytics” 
(BaVA) [18]. The BaVA process begins with an initial display of 
the data. In turn the user may assess the display and decide if it 
matches her mental model of the data. If it does not, the user may 
convey  her  cognitive  feedback  f(c)  by  adjusting  the  locations  of 
two  observations  to  convey  her  mental  model  about  the  two 
observations.  The  user  might  also  explore  an  alternative  spatial 
location  of  an  observation  and  see  how  the  other  observation 
responds  to  such  an  interaction.  In  short,  iterations  of  user 
interaction  and  subsequent  regeneration  of  the  visualization  are 
modelled  as  sequential  updating  of  maximum  a  posteriori 
estimates  of  parameters.  The  deterministic  version  of 
the 
framework, termed as “Visual to Parametric Interaction” (V2PI), 
also  starts  with  an  initial  display  and  upon  obtaining  a  user 
feedback  sequentially  updates  the  parameters,  but  the  updated 

values  of  the  parameters  are  such  that  they  minimize  some 
measure of discrepancy between the expected configuration of the 
data under the user’s reasoning and the original data [19].  
For each of the models discussed in this paper, we present an 
overview of the model, describe the modifications made to allow 
observation-level interaction, and show a use case demonstrating 
how an end-user can interact with each model. Given that each of 
these models is designed for different types of data (varying  in 
structure,  size,  and  nature  of  the  data),  the  example  use  cases 
below each use different datasets to match the intended use of the 
models  with  the  use  case.  The  use  cases  are  performed  in 
prototype visualizations to show a proof of concept, and we are 
actively  working  to  incorporate  these  models  into  more  fully 
featured tools.  
PPCA 
4.1 

4.1.1  Overview 

Principal  Component  Analysis  (PCA)  [20-22]  is  a  common, 
deterministic  method  used  to  summarize  data  in  a  reduced 
dimensional  form.  The  summary  is  a  projection  of  a  high-
dimensional  dataset  in  the  directions  with  the  largest  variance. 
When only two directions are chosen, PCA may produce a spatial 
representation or map of the data that is easy to visualize. One 
problem with PCA is that important structures (e.g., clusters) in 
data may not correlate with variance. Thus, PCA spatializations 
may mask information in the data that analysts may find useful.    
Probabilistic PCA [23] is, simply, a probabilistic form of PCA. 
This  means  that  PPCA  is  not  a  deterministic  algorithm,  but  a 
statistical  modeling  approach  (specifically,  a  factor  modeling 
approach) that estimates low-dimensional representations of high-
dimensional  data.  Let  d=[d1,…,dn]  represents  a  p×n  high-
dimensional  data  matrix,  where  n  represents  the  number  of 
observations, p represents the number of columns, and di (for i∈ 
{1,…,n})  represents  a  p×1  vector  for  observation  i.  Also,  let 
r=[r1,…,rn] represent a low-dimensional analogy of d, such that r 
is q×n and q<p.  For our purposes, we set q=2.  PPCA models d as 
a function of r,  

d W r
,
i
i

,

,
µσ

2

=

 
No Wr
i

(

I
µ σ
+
p

,

 

2

)

  
where, No(.,.) represents the Multivariate Normal Distribution; µ 
represents  a  p×1  mean-vector  of  d;  W  is  a  p×q  transformation 
matrix  known  as  the  factor  loadings  of  d;  Ip  is  a  p×p  identity 
matrix; and σ2 represents the variance of each dimension in d.  By 
convention,  PPCA  models  each  ri  with  a  Multivariate  Normal 
distribution centered at zero and with unit variance: ri~No(02, I2).  
In  turn,  the  conditional  posterior  distribution  for  ri  is  No(η,Σr), 
where 
 

η
=

2

W W I
W d
(
)
1
(
σµ−
ʹ′
+
−
2
r W W
Iσ
2
2
−
ʹ′
Σ=
σ
+
2

ʹ′

i

(

−

) 1

)
                               (1) 

 
A spatialization of data d that relies on PPCA plots the posterior 
expectation  η.  Similar  to  PCA,  the  coordinates  η  rely  on  the 
variability observed in d. To see this, let Σd represent the marginal 
variance of di, (Σd=V[di |W,µ,σ2]). Since Σd=W´W+I2σ2, we can 
-1W(di  -µ)  which  shows  that  the  relationship 
rewrite  η  as  η=Σd
between Σd and η is well defined.   
The  final  step  in  PPCA  is  to  estimate  the  model  parameters, 
{W, µ, σ2, Σd}. We take a Bayesian approach. We specify either 

 

 

 

reference or flat priors for each unknown (as suggested by [23] 
and use Maximum A Posteriori (MAP) estimators to assess (and 
plot)  η.  For  example,  when  we  assign  π(Σd)  ∝1,  the  posterior 
distribution for Σd  is an Inverse Wishart (IW) distribution,  

d

d

,

,

)

d

∝

1)

IW(

(
π Σ

nS p n p

− −                             (2) 
 
Where  Sd  represents  the  empirical  variance  of  d.  The  MAP 
estimate of Σd is Sd.   
4.1.2 

User Guided PPCA 

To enable analysts to guide PPCA via the data visualization, we 
take  advantage  of  the  relationship  between  Σd  and  η.  Namely, 
changes in Σd will effect η, and changes in η will effect Σd, when 
we invert Equation (1).   
After  obtaining  an  initial  PPCA  display,  the  user  adjusts  the 
locations of two observations; i.e., adjusts two columns in η. If 
the two observations are moved close to one another, the analyst 
is conveying that in her mental map, the observations are more 
similar  than  what  they  appear  in  the  display;  and,  if  the 
observations are dragged apart, the analyst is conveying that the 
observations differ more than what they appear.    
The  challenge  in  BaVA  is  to  parameterize  the  cognitive 
feedback  and  update  the  visualization  [18].  First,  we  determine 
the dimensions of the data d for which the adjusted observations 
are similar and different. Second, we transform the adjustments to 
η into a hypothetical p×p variance matrix. We denote this matrix 
by f(p), as it is a quantified version of f(c). In f(p), the dimensions for 
which the adjusted observations are similar have small variances 
and  the  dimensions  for  which  adjusted  observations  differ  have 
large variances. Third, we consider the hypothetical variance f(p) to 
be a realization of a Wishart distribution that has an expectation 
equal to Σd. Finally, we apply Bayesian sequential updating [24, 
25] to adjust Equation (2) by the parametric feedback f(p),  
 

(

π Σ=
d

d f
,

(

p

)

)

IW(
+

pS
− −
d

vf
+

(

p

)

,

p n v p
,

 

1)

where, υ is solved from a specification κ(κ ∈ [0,1]) made by the 
analyst  that  states  how  much  weight  to  place  on  the  feedback 
relative to the data. Namely, the updated MAP estimate for Σd is a 
weighted average of the empirical variance Sd and feedback f(p) 
 

MAP

(

)
Σ=
d

f

)

p
(
+

v
v n
+
 

 

S

d

n
v n
+

thus υ= nκ/(1-κ). Now, the PPCA projection of the data d that is 
based on MAP(Σd) will portray both information in the data and 
expert feedback. 
4.1.3 

Example 

A sensitive issue for taxpayers, parents, children, educators, and 
policy  makers  is  whether  an  increase  in  money  devoted  to 
education  will  increase  education  quality.  Money  provides  a 
means  to  buy  modern  textbooks,  employ  experienced  teachers, 
and provide a variety of classes and/or extra curricular activities. 
Although,  do  the  students  who  benefit  from  these  high-priced 
resources actually improve academically? 
In 1999, Dr. Deborah Guber compiled a dataset for pedagogical 
purposes  to  address  this  question  [26].  Based  on  the  following 
variables, 
the  academic  success, 
educational expenses, and other related variables in 1997 for each 
U.S. state: the average exam score on the Standard Aptitude Test 

the  dataset  summarizes 

from 

the  National  Center 

(SAT);  the  average  expenditure  per  pupil  (EXP);  the  average 
number of faculty per pupil (FAC); the average salary for teachers 
(SAL); and the percentage of students taking the SAT (PER). To 
increase  the  complexity  of  the  dataset  slightly,  we  added  two 
variables 
for  Education 
Statistics(www.http:nces.ed.gov):  the  number  of  high  school 
graduates  (HSG)  and  the  average  household  income  (INC). We 
hypothesize that states that spend more on education will cluster 
with states with high SAT averages.   
To assess the hypothesis and explore the data, we implement 
the  BaVA  process  using  PPCA.  Figure  1a),  displays  our  initial 
view of the data. Notice that the visualization does not present any 
structure in the data.  Analysts in the field of education, notice that 
two  states  with  different  expectations  for  SAT  scores  are 
displayed  close  to  one  another.  Thus,  we  select  the  appropriate 
observations and drag them apart as an expressive interaction to 
obtain an updated view that is displayed in Figure 1b). There are 
two clusters in 1b). These clusters correspond with SAT scores 
above and below the national median.  
Based  on  our  hypothesis,  we  suspect  that  the  clustering 
structure in SAT relates to EXP. However, when we re-plot 1b) 
and label the upper and lower EXP 50% quantiles in Figure 1c), 
EXP  does  not  explain  the  clusters.  Thus,  we  used  a  bi-plot  to 
identify  which  variables  explain  the  structure  we  see  in  Figure 
1b).  When  we  mark  the  observations  above  and  below  the 
empirical PER median in Figure 1d), we see that PER and SAT 
clearly  relate  to  the  formation  of  clusters  in  the  dataset.  Thus, 

(a)  Initial                            (b) SAT Scores 

 

 

              (c) EXP                                  (d) PER      

 
Figure  1.  After  injecting  expert  feedback  into  Figure  1a),  we 
obtain  Figures  b)-c).  For  frame  of  reference,  we  marked  the 
two  points  moved  to  inject  feedback  by  `x'  in  Figure  b).  The 
configuration  of  points  in  each  graph  are  identical,  but  the 
observations  are  labeled  differently.  In  Figure  b),  symbols  `●’ 
and  `○’  mark  the  upper  and  lower  50%  quantiles  for  SAT 
scores respectively; in Figure c), symbols `●’ and `○’ mark the 
upper  and  lower  50%  quantiles  for  EXP  scores  respectively; 
and in Figure d), symbols `●’ and `○’ mark the upper and lower 
50%  quantiles  for  the  percentage  of  students  taking  the  SAT 
the  clusters 
(PER) 
in  each  graph 
correspond with SAT and PER, but not EXP. 

respectively.  Notice 

further analyses of SAT and EXP must control for PER.   

 

4.2  MDS 

We  extend  our  framework  to  another  deterministic  method, 
which  forms  the  basis  for  a  large  number  of  visualization 
techniques: Multi-Dimensional Scaling (MDS).  
4.2.1  Overview 

All complex data visualizations are based on high-dimensional 
datasets, which contain features corresponding to dimensions, and 
the relative importance of such features through a set of weights 
(wi).  Classically  weighted  multidimensional  scaling  deals  with 
mapping  a  high  dimensional  dataset  d=[d1,…,dn]  into  a  low 
dimensional (in our case two-dimensional) space r, by preserving 
pairwise distances between observations in the low dimensional 
representation.  Let  w  represent  the  p-vector  of  feature  weights:  
w={w1,…,wp}. Given a set of feature weights, the low dimensional 
spatial coordinates are found by solving:  

 

where 

min
r
r i
,...,
1

n

, 

r
i

−

r δ
w
(
i j
j
,

−

)

∑

j n

<≤

)

w
(
δ
i j
,

w
k

dist(

d

ik

d

)

jk

 

p

=−∑

k

1
=

such  that  ∑k  wk=1.  dist()  represents  any  distance  function  for 
measuring  individual  features  in  the  high  dimensional  space. 
Because  it  is  not  possible  to  estimate  weights  and  the  set  r 
simultaneously,  we  provide  a  uniform  weighting  of  the  space 
wi=1/p for our first iteration. 
User Guided MDS 
4.2.2 

the  display  and 

learn  from  certain  aspects  of 

Once  a  visualization  is  generated,  the  user  may  either  agree 
with 
the 
visualization, or disagree, based on their domain expertise. Hence, 
the  user  may  wish  to  interact  and  rearrange  a  few  of  the 
observations in the visualization. Given a spatial interaction in the 
form  of  adjusting  the  relative  position  of  a  set  of  points,  we 
compute a set of feature weights, which are consistent with both, 
the  users  adjustment  and  the  underlying  mathematical  model. 
These are computed by inverting the optimization, by fixing the 
locations  of  the  adjusted  points  and  finding  an  optimal  set  of 
weights,  which  are  consistent  with  the  visualization.  Explicitly, 
we solve for w such that  

Figure  2.  Visualization  of  the  1990  census  dataset  using 
classical MDS. 
 
 

 

%  

r
( )
δ
−
i j
,

w
k

dist

d

(

i k
( )

−

r
j k
( )

)

min
w
w i
,...,
1

p

j

p

∑ ∑

l k
<≤ =

1

 
and  ∑k  wk=1,  where  d(i)k  represents  the  kth  element  in  the 
observation of d that maps to ri in the adjusted visualization and l 
is  the  total  number  of  manipulated  observations.  It  should  be 
noted  that  computing  the  new  weights  is  extremely  fast,  and  is 
then followed by a full MDS step. Thus, the entire generation of a 
new view can be performed in real time, depending on the size of 
the dataset and the specific hardware used. 
4.2.3 

Example 

inconsistencies  with 

their  mental  model 

Consider  for  example  a  visualization  produced  by  a  standard 
MDS  technique.  In  this  example  we  focus  on  the  1990  census 
dataset [27] under a Classical Metric Scaling (CMS) [28], using a 
Hamming distance (due to the categorical nature of the dataset) 
for  measuring  features  in  the  high  dimensional  space.  Figure  2 
illustrates  results  obtained  under  a  Classical  Metric  Scaling 
(CMS).  
Given  this  visualization,  a  user  may  distinguish  3-5  main 
clusters, and inquire what they mean. We see two major ways a 
user  can  interact  with  the  visualization,  in  order  to  explore  the 
space, and learn about the underlying dataset. The first of these is 
by highlighting a subset of the data, based on some question the 
user seeks to answer, and then rearranging the visualization based 
on 
(expressive 
interactions). 
The second approach is to hone in on visual structure, and move 
points  in  the  visual  space  in  order  to  learn  what  the  structure 
relates to in terms of the feature space (exploratory interactions). 
Both  of  these  interactions  are  nearly  identical,  however  the 
motivation for the interactions will differ. We will illustrate both 
types of visual reasoning through an example based on the 1990 
census dataset. 
The user may wish to interact expressively and identify points 
in the space that pertain to high and low income groups. The user 
highlights individuals with incomes below 15K and over 60K, as 
shown  by 
  in  leftmost  panel  of  Figure  3,  respectively. 
Because of the close proximity of the highlighted groups in the 
main clusters, the user drags (denoted by ⊗) a few representative 
low and high-income individuals into sets of groups in each of the 
3  main  sub-clusters.  The  system  reports  back  a  set  of  weights, 
which  explain  how  much  a  particular  feature  explains  the 
arrangement of points suggested by the user. High weights relate 
to 
their 

low  weights  suggest 

features,  while 

important 

and 

 

Figure 4. A user performing an exploratory interaction to learn 

to 

this 

information, 

the  system  updates 

    what distinguishes two clusters. 
corresponding  features  do  not  relate 
the  user's  visual 
rearrangement. For our example, we learn not only that income 
level  (29%),  but  also  by  their  means  of  transportation  to  work 
(20%), whether or not they worked the full year (25%), and their 
level of education (10%) are related to the user's repositioning of 
points.  Given 
the 
visualization, as shown in center panel of Figure 3. We notice that 
in  the  resulting  visualization,  the  income  groups  are  clearly 
separated.  The  resulting  visualization  displays  a  much  richer 
spatialization than simply showing clusters relating to the income 
groups.  For  example,  we  highlight  individuals  that  actually 
worked  in  the  right  most  panel  of  Figure  3,  and  notice  these 
individuals are shown in distinct sub-clusters. 2 of the 4 clusters in 
which  individuals  work  pertain  to  low-income  groups,  and  the 
other 2 pertain to high-income groups (as illustrated by the 
 and 
 symbols).  
Figure  4  shows  how  the  user  might  perform  an  exploratory 
interaction in order to learn what explains the clustering structure 
between the working/low income groups. To suggest the clusters 
could be moved further away from each other than they appear in 
the  current  visualization,  the  system  reports  back  the  weights, 
which explains the differences in the groups. For this example, the 
user learns that one of these clusters contains individuals that have 
a reliable mode of transportation to work (93% explained). The 
visualization could be updated based on this information, or the 
user could simply document this fact and proceed by explaining 
other areas of the spatialization. As always, there are an endless 

 
Figure 3. A sequence of visualizations derived through observation-level interaction with a modified MDS method. (Left) The user moves a 
set of points into new locations, communicating his intuition that there may be additional structure within each cluster. (Middle) The updated 
visualization showing new clusters. (Right) Highlighting showing the separation of income groups in the updated visualization. 
 

 

 

number  of  possibilities  for  learning  about  a  high  dimensional 
dataset via visual expression/exploration. Another example of an 
exploratory interaction with MDS is demonstrated by Buja et al. 
in  which  users  can  constrain  observations  to  specific  spatial 
locations [4]. 
GTM 
4.3 

4.3.1  Overview 

Introduced  by  Bishop  et  al,  [29]  Generative  Topographic 
Mapping (GTM) is a nonlinear latent variable modeling approach 
for  high-dimensional  data  clustering  and  visualization.  It  is 
considered  to  be  a  probabilistic  alternative  for  both  the  Self-
Organizing  Map  (SOM)  algorithm  [30]  and  Nonlinear  PCA. 
Similar  to  PPCA,  GTM  estimates  a  latent  variable  r=[r1,…,rn] 
(q×n  matrix)  that  is  a  low-dimensional  representation  of  high-
dimensional  data  d=[d1,…,dn]  (p×n  matrix  such  that  p>q). 
However, unlike PPCA, the q-dimensional coordinates r in GTM 
map  nonlinearly  to  a  complex  manifold  m=[m1,…,mn]  that  is 
embedded in the high-dimensional space.  This manifold, ideally, 
in  data  d  and  represents 
characterizes 
geometrically the expected value for d in the Gaussian model,   
                               (3) 

important  structure 

    

d N W r
( ),
i
i

Φ

(

I β−
1
p

)

:

 

j

2

)

=Φ

    

exp(

r
( )
Φ=−
i

µ
−
j
2
2
σ

m W r
( )
i
i
r
i

To estimate a coordinate mi, GTM takes a weighted average of J 
radial  basis  functions  {Φ1(),…,ΦJ()}  (Φj()  represents  a  radially 
symmetric Gaussian kernel) given ri and parameters there in,  
 

,                                      (4) 
,                            (5) 
 
where  W  is  a  p×J  transformation  matrix;  Φ(ri)  is  a  J×1  vector 
such that Φ(ri)=[Φ1(ri),Φ2(ri)…,ΦJ(ri)]ʹ′; and µj is a q×1vector that 
centers  the  basis  functions.  The  center  coordinates  µ=[µ1,…,µJ] 
cover the q-dimensional latent space uniformly. Model parameters 
are estimated using the EM algorithm [31]. 
One  advantage  of  GTM  is  that,  by  construction,  it  lacks 
sensitivity to outliers.  For tractability, the coordinates of each ri 
are  limited  a  priori  to  a  finite  set  g  of  K  possibilities,  ri 
∈g={g1,…,gK} 
latent  space 
uniformly.    To  decide  which  value  for  ri  generates  di,  GTM 
estimates the posterior probability, i.e., responsibility, that ri=gk. 
Given a prior probability that ri=gk is 1/K for all k ∈{1,…,K}, let 
Rik  represent  the  posterior  responsibility  that  latent  variable  ri 
generates di, when ri=gk, 
 

the  q-dimensional 

that  covers 

 ,                            (6) 

R
ik

=

d r
(
π
=Φ
i
i
K
d r
(
∑
π=
i
i
l
1

g W
,
,
k
g W
,
=Φ
l

())
,

())

 
In  turn,  GTM  plots  the  posterior  mode,  expectation,  or  any 
quantile of ri given specifications g and estimates for {Ri1,…RiK}.    
4.3.2 

User Guided GTM 

GTM  is  a  complex  modeling  approach  that  relies  on  many 
tunable parameters that are hard to interpret. User Guided GTM 
(ugGTM)  will  allow  analysts  to  both  take  advantage  of  the 
benefits  of  GTM 
complicated  GTM 
parameterization.  Specifically,  analysts  may 
tag 
clusters,  tag  regions  of  the  visualization  space,  and  query 
differences in documents.  

and  guide 

label,  i.e., 

the 

Here, we illustrate ugGTM within the context of an example. 
We have a collection of 54 abstracts from proposals funded by the 
National Institute for Health (NIH). After standard preprocessing, 
we apply a ranking system that we will call an Importance Index 
(ImpI),  which  is  based  on  the  Gini  coefficient.  ImpI  considers 
both the frequency and uniqueness of words that are shared across 
documents and assigns a metric between 0 and 1.  Entities that 
occur  equally  frequently  in  all  the  documents  have  ImpI=0  and 
entities that occur in only one document has ImpI=1.  We selected 
the 1000 entities with the highest ImpI. One advantage of ImpI is 
that we can measure document similarity using Euclidean distance 
between  proposals.  Pairs  of  documents  with  small  Euclidean 
distances have comparable terms with similar frequency; and pairs 
of  documents  with  large  Euclidean  distances  have  few,  if  any, 
words in common. 
We apply GTM for J=16 and K=400 to obtain an initial display 
of the proposals, shown in Figure 5. Notice four clusters appear in 
Figure 5 that we labeled A, B, C, and D.   
Tagging  the  Clusters  and  the  Space.  To  understand  the 
meaning of the clusters, we determine the words that both overlap 
the  least  within  each  cluster  and  have  the  highest  ImpI’s.  
Specifically, we apply k-means [32] to the low-dimensional data 
coordinates to determine cluster memberships.  For each cluster 
we  sum  the  ImpI  vectors  across  the  documents  and  rank  the 
entities based on the ImpI sum. Entities ranked highest are those 
that 1) have importance in the corpus (as determined by the ImpI) 
and 2) have occurred most frequently.  Given top rankings from 
each cluster, we delete those shared by all four clusters. Table 1 
lists  the  unique  key  words  that  describe  each  cluster.  Group  A 
represents proposals that include brain related cancer studies and 
their clinical applications. Group B represents proposals related to 
human neural systems. Group C represents proposals that address 
genomic  and  transcriptomic  research  problems. 
  Group  D 
represents  proposals  about 
such  as 
infectious  diseases, 
tuberculosis, and immunity.  

 
Table 1. Cluster tags (top 10 keywords) for NIH abstract groups. 
 

    
As  described  previously  in  Equation  (3),  GTM  characterizes 
high-dimensional  data  as  random  perturbations  from  a  complex 
manifold m; E[di] = mi for all i ∈ [1,…,n]. To tag the visualization 
space, we select any spot, r+, in the visualization and use Equation 
(4) to estimate its corresponding location on the manifold, m+. The 
estimate m+ will be a 1000×1 vector of ImpI’s that we may use to 
rank  the  entities.  We  report  the  top  ranked  entities  to  tag  the 
space. For example, in Figure 5, we pick up a spot r+ (represented 

Group A 

Group B 

Group C 

Group D 

Shared by All 

Groups 

tumors, brains, stem, treatments, patients, 
generations, drugs, ordering, controlling, 
therapeutics 
stem, neuronal, brains, proteins, deliveries, 
regulations, neural, patients, differentiation, 
expression, treatments 
stem, genetically, regulations, drugs, 
structurally, proteins, genomics, epigenetics, 
RNAs, complexities 
Infections, treatments, tuberculosis, 
expression, patients, drugs, strains, 
resistance, vaccination, immunity 
cells, functionalization, diseases, 
developments, genes, cancerous , studying, 
researchers, proposing, mechanisms, 
specification 

by a pink circle) that locates roughly at the center of cluster D.  
Several  of  the  tagged  top  keywords  overlap  with  the  words 
describing cluster D. 
    
Document-Based  Query  and  Cluster  Reorganization.  It  is 
common for users to assess documents by searching for keywords. 
However,  keyword  searching  may  be  a  tedious  task  and  fail  to 
reveal  document  clusters  of  interest.  For  example,  keyword 
searches may identify documents with similar keywords, but used 
in different contexts; miss documents that contain combinations of 
the  keywords;  or  prioritize  words  that  have  little  relative 
importance for the user.  In response to the challenges of keyword 
searching,  many  analysts  rely  on  document  matching.  For 
document  matching,  entire  documents  can  be  used  to  identify 
which of the remaining documents in the corpus are most similar 
(to the chosen document). Hence such a matching algorithm is a 
document-based query of a corpus. 
In our ugGTM, users may query documents in the corpus by 
dragging a document of interest directly in the visualization and 
watching  how  the  remaining  documents  respond;  e.g.,  similar 
documents will follow the document being dragged and dissimilar 
documents will repel. The behaviour of the documents is similar 
in spirit to Dust and Magnets (DnM) [17]. In DnM, analysts may 
drag or shake magnets that represent variables in the dataset and 
watch  as  relevant  documents  follow  the  magnets.  However,  a 
major  difference  between  DnM  and  ugGTM  is  that  when  users 
drag  documents  (not  variables)  and  watch  how  the  remaining 
react, they are comparing documents based on all of the variables 
in  the  dataset  simultaneously.  In  turn,  users  may  learn  which 
variables are important for comparisons, based on tags within the 
visualization space. 
The interaction is possible because ugGTM gives control to the 
users of some parameters in the model via the visualization.  Let 
r* represent the low-dimensional coordinates for a document that 
an  analyst  has  chosen  to  drag.  Given  r*,  we  add  to  the  model 
described in Equations (3)-(6) by expanding sets g and Φ so that 
g={g1,…,  gK,  g*}  and  Φ={Φ1,…,ΦJ,Φ*},  where  Φ*  =  exp{-||ri-
µ*||2/2σ2}  and  g*=µ*=r*.  In 
the  posterior 
responsibility (Equation 6) that r* generates d* via m* to 1 (where, 
m* is defined by Equation (4) so that the mapping between the 
low-  and  high-  dimensional  coordinates 
the  moving 
observations is deterministic. 
To  propagate  the  effect  of  moving  r*  to  the  remaining 
visualization,  we  take  a  local  regression  approach  [33]  to 
characterize high-dimensional data di |{ ri=g*,m*} in that we scale 
di-m* by the square-root of function V given scaled distance Δi = 
||d*-di||/c so that,  

turn,  we  assign 

for 

(
π

d r
=Φ=
i
i

g W
*

,

,
−

)

⎛
⎜
⎝

β
2
π

⎞
⎟
⎠

exp{

V
β

)

i

(
Δ
2

d m
i

*

 
}

2

p
/2
−
−
 

 

 

2 and c=0.5. In turn, both 
where c is user-defined; e.g., V(Δi)=Δi
posterior responsibility estimates (Equation 6) and estimates for m 
(Equation 4) change.  Let mi
(c) and mi
(u) represent the current and 
user-adjusted manifold estimates for observation i.  We define the 
BaVA-GTM estimate for the manifold, mi
 

(c+1), by 

 

m
(
i

c

1)
+ =

δ
i

m
c
( )
+−
i

(1

δ
i

)

m
u
( )
i

, 

where δi=||ri-r*||/b and b=max{||r1-r*||,…,||rn-r*||} so that δi∈ [0,1].  
(c+1) controls the visualization so that only the 
This definition for mi

regions  of  interest  respond  to  user  interactions;  areas  that  are 
distant from the dragged observations do not change.   
Parameters g*, Φ*, V(Δi), δ  and m(c+1) in ugGTM work together 
in the following way. When a data point di is far from d*, V(Δi) 
will  be  large  and  thus  decrease  the  posterior    responsibility 
(Equation 6) that ri=g* generates di. Similarly, when di is near d*, 
the  corresponding  responsibility  will  increase.  Increases  in  the 
responsibility  for  ri=g*  will  cause  the  coordinates  for  ri  to 
gravitate toward r*. Thus, analysts may specify constant c in our 
definition Δi, depending upon how many document matches they 
seek  for  the  moving  document.  Also,  the  degree  to  which  the 
observations  gravitate  toward  r*  is  determined  by  δ  and  m(c+1). 
When the manifold shifts from m(c) to m(c+1), the meaning of the 
visualization space changes, as we demonstrate in our example.  
4.3.3 

Example 

For our NIH example, we apply ugGTM. We display an initial 
GTM view of the documents (the 54×1000 dataset) in Figure 5. 
Suppose a user identifies a specific document of interest, e.g., Doc 
7 (highlighted in yellow in Figure 5) to investigate. A preliminary 
investigation might involve a sequence of non-spatial interactions, 
such as, searching of multiple keywords, reading all or part of the 
document  etc.  However,  a  comprehensive  assessment  of  the 
document may require spatial interactions as well. The user might 
explore  space  tags  across  the  screen  and  determine  a  more 
appropriate location for the document of interest. In this case, Doc 
7 is closer to Group A, and is about developing new brain tumor 
therapies  and  tumor  stem  cell  quiescence.  The  keywords  this 
document shares with group A include tumors, brains, cancerous, 
therapeutics and chemotherapy. However, since Doc 7 relates to 
therapy developments for disease, it shares some keywords with 
Group  D;  e.g.,  treatments,  strategies,  patients,  drugs,  resistance, 
clinically.   
As an exploratory spatial interaction, the user drags Doc 7 to 
the  lower  left  corner  of  the  display  and  watches  how  the 
remaining  documents  react.  By  repositioning  Doc  7,  the  user 
redefines the spatialization of the screen, i.e., modifies the space 
tag  corresponding  to  a  location.  For  example,  when  we  tag  the 
same  coordinates  r+  in  Figure  6  (r+  are  the  coordinates  of  the 
space tagged in Figure 5), we learn that the top keywords include 
treatments  and  tumors  as  well  as  those  that  were  there  earlier. 
Recall that ugGTM uses every variable in the dataset to compare 
documents.  For  this  reason,  documents  that  mention  stem  cells 
and  other  important  keywords  in  Doc  7  follow  Doc  7.  As 
expected, many documents in Group D gravitate toward Doc 7. 
However,  a  few  documents  in  Group  B  also  followed.  Future 
work will allow users to weight the keywords in Doc 7, if desired. 
Also  Documents  with  ID  20,  22,  32  and  39  change  locations. 
Important  keywords  for  these  documents  include  the  following: 
Doc 20 discusses diagnosis of HIV infection in patients who live 
with  limited  access  to  therapeutic  treatments;  Doc  22  discusses 
expression characteristics of a drug-resistant gene; Docs 32 relates 
to  varying  yeast  strains;  and  Doc  39  relates  to  Lymphocyte 
Homing.  Docs  20  and  22  repelled  against  Doc  7  because  the 
redefined-manifold down-weighted their important entities in the 
lower left corner and up-weighted the entity tumor. Thus, Doc 20 
and Doc 22 shifted to Groups A and C respectively. Docs 32 and 
39  are  separated  slightly  from  Group  D  and  gravitated  toward 
Group C because they have a few words in common with each 
group, but not enough to place them in either corner. 
An interesting note about the updated manifold is the change in 
shape or magnification factor [34]. The colour in the background 
is  plotted  based  on  the  logarithm  of  the  magnification  factor 

Figure  5.  GTM  display  of  the  NIH  abstracts.  Black  dots  mark 
documents and labeled by their document ID. 
 
evaluated on a fine grid that covers the visualization space. Due to 
the  nonlinear  mapping  from  ri  to  mi,  equal  distances  in  the 
visualization do not necessarily imply equal distances in the high-
dimensional space. The magnification factor describes the rate of 
change  between  distance  or  area  in  the  latent  space  and  the 
corresponding  distance  or  area  on  the  manifold  and  can  be 
interpreted  as  a  description  of  how  wiggly  the  manifold  is. 
Overall,  the  magnification  factor  is  lower  in  Figure  6  than  in 
Figure 5 and the clusters formed in Figure 6 are mainly in low 
magnification  areas.  This  means  the  clusters  in  Figure  6  are  in 
flat, stable regions of the estimated manifold. Thus, observations 
in these clusters are closer to one another than observations shown 
in clusters within Figure 5. 
5 

DISCUSSION 

We present a comparison of key characteristics of the methods 
used in this paper in Table 2. Again, the purpose of this work is 
not to make a direct comparison of these three methods, but rather 
to  present  how  to  apply  observation-level  interaction  to  each 
method, and summarise our findings in the table.  
Mappings. The three methods discussed in the paper provide 
us  with  a  spatialization  of  the  data  within  the  bounds  of  their 

 

 

 

Figure 6. The updated view after moving doc 7 from top left to 
bottom left. 
algorithmic  complexity.  Points  that  are  close  in  the  higher 
dimensional space remain close to each other in the visualization 
in  all  the  algorithms  although  the  concept  of  proximity  varies 
depending on the algorithm. As an artifact of the algorithms, in 
both PPCA and MDS, the high dimensional data is assumed to be 
a linear mapping of the visualized representation while GTM is a 
non-linear mapping of the same. Hence, the same dataset might 
provide  widely  disparate  visualizations  for  different  algorithms. 
Spatially  this  might  translate  to  the  fact  that  based  on  the 
algorithm, the user’s spatial interaction might target different sets 
of observations. Each algorithm can potentially have its own set 
of diagnostics overlaid with the visualization that might aid the 
user  in  understanding  the  proximity  of  the  data  in  the  higher 
dimensions;  e.g.  visualizing  the  magnification  factor  along  with 
the data in GTM indicates the level of distortion. The goal of the 
user is to obtain a view in multiple steps that matches with his 
mental model irrespective of the algorithm used to visualize the 
data.  The  specific  steps  that  the  user  goes  through  should  be 
immaterial in so far as the final visualization is concerned and all 
the algorithms discussed here have the flexibility to provide that.  
PPCA relies on the assumption that a single linear projection 
exists  that  can  reveal  useful  structure.  MDS  provides  a  two- 
dimensional representation of the observations via penalization of 
any  distance  distortion  that  happens  in  the  two-dimensional 

Table 2. Comparison of the methods used in this paper.  

  
Mapping Type 
Method Characterisation  
Distribution Assumption 
Scalability (Observations) 
Scalability (Dimensions) 
Conceptual Clarity 
Running Time 
Outlier Robustness 

PPCA 
Linear 
Variance 
Probabilistic 

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174) = Good(cid:1) (cid:174)(cid:174) = Average(cid:1)

MDS 
Linear 
Similarity 
Deterministic 

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174) = Poor 

GTM 

Non-linear 
Manifold 
Probabilistic 

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:1)

(cid:174)(cid:174)(cid:174)(cid:1)

representation  using  a  stress  function.  However,  the  linear 
projection assumption may not hold for complex datasets or, the 
visualization based on minimizing stress in MDS might not reveal 
all the information in the data. In PPCA, using variance to select 
the  direction  in  which  to  project  data  makes  sense  for  datasets 
with a global linear structure [23]; the projection will minimize 
the number of observations that overlap so that they are as visible 
as possible.  
However,  variance  estimates  and  hence  PPCA  visualizations 
are  sensitive  to  outliers  and  it  is  not  uncommon  for  PPCA  to 
display one or two outliers and a cloud of occluded points. Under 
Euclidean  distance,  MDS  is  algorithmically  the  same  as  PPCA 
and  will  suffer  from  the  same  sensitivity  to  outliers.  Assessing 
such  a  visualization  and  making  appropriate  adjustments  would 
be,  at  best,  challenging.  Thus,  a  more  complex  methodology  is 
often needed to summarize datasets, e.g., mixture PPCA or GTM. 
GTM being a topographic mapping places the outliers at one end 
of the screen or at a position that is distant from the region that 
has more structure. In our interactive framework, outliers can be 
brought closer to existing user defined clusters through redefining 
the principal components in PPCA, reweighting of the dimensions 
in MDS and constraining responsibilities in GTM; in all the cases 
the  user’s  observation-level  interaction  initiates  the  parameter 
update. 
Scalability. In terms of time complexity, GTM is O(KND) (K 
number  of  latent  points,  N  number  of  observations,  D  data 
dimensionality),  PPCA  is  O(qND)  (q  is  the  dimension  of  the 
latent space, usually equals 2) and MDS varies from O(qND) to 
O(N3).  The  effect  of  high  dimensionality  (i.e.  the  number  of 
columns for every observation) on the run-time will be similar for 
all three algorithms. The challenge in scalability (large N) is also 
of  the  same  order  for  the  three  algorithms  when  Euclidean 
distance is used.  
However  in  the  design  of  a  visual  analytic  system  that 
incorporates user interaction in the framework, the choice of the 
algorithm  should  be  based  not  only  on  the  run-time  of  the 
algorithm  but  also  on  the  cost  incurred  in  converting  the 
observation-level interaction or feedback to updated values of the 
parameters for the method.  In PPCA, it is the cost of evaluating 
the feedback matrix f(p); in MDS it is the cost of obtaining optimal 
feature weights w based on pair-wise distances of the observations 
that the user has moved; and in GTM, it is the cost of computing 
distances between data points and reference vectors. Under such 
considerations, we think MDS provides the quickest and easiest 
two-dimensional visualization of the data, followed by PPCA and 
GTM.  
We  maintain  a  probabilistic  framework  in  PPCA  and  GTM. 
Specifically  for  PPCA,  computation  is  quick  since  the  primary 
parameter  of  interest  Σd  has  a  posterior  distribution  and  a 
conjugate feedback distribution, and MAP(Σd) can be computed 
without MCMC.  Thus, analysts can explore the data in real time.  
GTM (although being most flexible in handling more complicated 
data occlusion issues that challenge MDS or PPCA) is based on 
an expectation-maximization algorithm and hence needs more run 
time to converge to the optimal parameter value. 
Sensitivity. The methods described in this paper will respond 
based on the interaction performed (i.e., number of observations 
moved, distance the observations were moved, etc.). For example, 
moving a single observation will generally result in a less drastic 
change in the layout compared to a similar interaction performed 
on a cluster of observations. Thus, the sensitivity of the models in 
terms of responding to the user’s intuition is dependent on how 
large the change or update is provided by the user’s interaction, 

the size of the dataset, as well as if the data supports the suggested 
updated  layout.  The  methods  will  attempt  to  find  the  “best  fit” 
given the user feedback, but will maintain mathematical validity 
(i.e., users cannot force the layout if the data does not support it). 
The  result  is  such  that  the  system  balances  the  user’s  intuition 
with  the  structure of  the  data  to  reduce  bias.  The  goal  of  these 
techniques is not to converge on a single structure or layout, but 
rather to allow exploration of many possible structures. 
Interaction. The examples of how observation-level interaction 
can occur within spatializations in this paper show only one form 
of interaction available to users within spatializations – movement 
of individual observations. The methods are expandable to allow 
more  complex 
interactions,  such  as  moving  clusters  of 
observations, annotating a region of the spatialization, and other 
interactions used for communicating the intuition of the user to 
the system. In a fully implemented visual analytics system, these 
interactions  may 
include  queries,  highlighting,  and  other 
interactions  from  which  analytical  reasoning  of  users  can  be 
interpreted.  
Implementation.  The  prototype  visualizations  shown  in  this 
paper are intended to provide working examples of the modified 
methods. Through the use cases, we highlighted how an end-user 
might  interact  with  such  systems.  We  plan  to  integrate  these 
methods  into  more  fully  functional  visual  analytics  tools.  That 
will allow us to perform a series of user studies to evaluate the 
usability  and  effectiveness  of  observation-level  interaction  in 
terms  of  providing 
the 
sensemaking process.  
6 

to  users,  and  supporting 

insight 

CONCLUSION 

In  this  paper,  we  described  how  modifications  of  powerful 
statistical methods allow user interaction at the observation-level. 
By  interacting  within  the  visualization  through  movement  of 
observations, users are able to perform exploratory and expressive 
interactions. Thus, users are able to perform sensemaking tasks, 
such as hypothesis validation, directly within the spatial metaphor. 
By keeping the interaction at the observation level, users are not 
required  to  transform  their  sensemaking  into  a  combination  of 
statistical parameter updates.  
In particular, we modified PPCA, MDS, and GTM using BaVA 
[18] and V2PI [19] approaches, so that users can focus on their 
spatial  analysis  of  data  rather  than  directly  updating  statistical 
parameters of models. We present three examples (one for each 
modified  method)  that  illustrate  the  effectiveness  of  these  new 
models. Based on the positive results in this paper, as well as the 
lessons  learned,  coupling  interaction  with  statistical  models 
provides  an  opportunity  to  explore  additional  forms  of  spatial 
interaction for visual analytic applications. 
ACKNOWLEDGEMENTS 
This research was funded by the National Science Foundation, 
Computer and Communications Foundations, grant #0937071. 
REFERENCES 
[1]  Thomas,  J.  J.,  Cook,  K.  A.,  National,  V.  and  Analytics,  C. 
Illuminating the path. IEEE Computer Society, 2005. 
[2] Pirolli, P. and Card, S. Sensemaking Processes of Intelligence 
Analysts  and  Possible  Leverage  Points  as  Identified  Though 
Cognitive  Task  Analysis  Proceedings  of  the  2005  International 
Conference on Intelligence Analysis, McLean, Virginia, 2005. 
[3] Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. and 
Chang,  R.  iPCA:  An  Interactive  System  for  PCA-based  Visual 
Analytics. Computer Graphics Forum, 28, 2009. 

[24]  Spiegelhalter,  D.  and  Lauritzen,  S.  Sequential  updating  of 
conditional  probabilities  on  directed  graphical 
structures. 
Networks, 20, 1990, 275-605. 
[25]  West,  M.  and  Harrison,  J.  Bayesian  Forecasting  and 
Dynamic Models (Springer Series in Statistics). Springer, 1997. 
[26]  Guber,  D.  Getting  What  You  Pay  For:  The  Debate  Over 
Equity  in  Public  School  Expenditures.  Journal  of  Statistics 
Education, 7, 2, 1999. 
[27]  Blake,  C.  and  Merz,  C.  J.  {UCI}  Repository  of  machine 
learning databases. 1998. 
[28] Schiffman, S., Reynolds, L. and Young, F. Introduction to 
Multidimensional  Scaling:  Theory,  Methods,  and  Applications. 
Academic Press, 1981. 
[29]  Christopher,  M.  B.  GTM:  The  generative  topographic 
mapping. 1998. 
[30] Kohonen, T., Kaski, S., Lagus, K., Salojarvi, J., Honkela, J., 
Paatero,  V.  and  Saarela,  A.  Self  Organization  of  a  Massive 
Document  Collection.  Transactions  on  Neural  Networks,  11,  3 
2000. 
[31] Dempster, A. P., Laird, N. M. and Rubin, D. B. Maximum 
likelihood from incomplete data via the EM algorithm. City, 1977. 
[32] MacQueen, J. Some methods for classification and analysis 
of  multivariate  observations.  Proceedings  of 
the  Berkeley 
Symposium  on  Mathematical  Statistics  and  Probability,  1,  281-
297, 1967, 14. 
[33] Hastie, T., Tibshirani, R. and Friedman, J. H. The Elements 
of Statistical Learning. Springer, 2003. 
[34]  Svensen,  J.  F.  M.  GTM:  the  generative  topographical 
mapping. Aston University, Birmingham, 1998. 
 

 

L. 

the  expert. 

in  Face  Verification.  In  Proceedings  of 

[4] Buja, A., Swayne, D. F., Littman, M., Dean, N., Hofmann, H. 
and  Chen, 
Interactive  Data  Visualization  with 
Multidimensional  Scaling.  Journal  of  Computational  and 
Graphical Statistics, 17, 2, 2008. 
[5]  Broekens,  J.,  Cocx,  T.  and  Kosters,  W.  A.  Object-centered 
interactive  multi-dimensional  scaling:  Ask 
In 
Proceedings  of  the  Eighteenth  Belgium-Netherlands  Conference 
on Artificial Intelligence, 2006. 
[6] Andrews, C., Endert, A. and North, C. Space to Think: Large, 
High-Resolution Displays for Sensemaking. In Proceedings of the 
CHI, 2010.  
[7] Conde, C., Ruiz, A. and Cabello, E. PCA vs Low Resolution 
Images 
the  12th 
International  Conference  on  Image  Analysis  and  Processing 
(2003). IEEE Computer Society.  
[8]  Gottumukkal,  R.  and  Asari,  V.  K.  An  improved  face 
recognition technique based on modular PCA approach. Pattern 
Recogn. Lett., 25, 4, 2004. 
[9]  Imran,  S.,  Bajwa,  S.  and  Hyder,  I.  PCA  based  Image 
Classification of Single-layered Cloud Types. Journal of Market 
Forces, 1, 2, 2005. 
[10]  Du,  Q.  and  Fowler,  J.  E.  Low-Complexity  Principal 
Component Analysis for Hyperspectral Image Compression. Int. 
J. High Perform. Comput. Appl., 22, 4, 2008. 
[11] Battista, D., Eades, P., Tamassia, R. and Tollis, I. Algorithms 
for Drawing Graphs: An Annotated Bibliography. Computational 
Geometry, 1994. 
[12] Zigelman, G., Kimmel, R. and Kiryati, N. Texture Mapping 
Using  Surface  Flattening  via  Multidimensional  Scaling.  IEEE 
Transactions  on  Visualization  and  Computer  Graphics,  8,  2, 
2002. 
[13]  Chen,  L.  Local  multidimensional  scaling  for  nonlinear 
dimension  reduction,  graph  layout  and  proximity  analysis. 
ScholarlyCommons@Penn, 2006. 
[14] Kaban, A. A Scalable Generative Topographic Mapping for 
Sparse Data Sequences. 2005. 
[15]  Olier,  I.,  Vellido,  A.  and  Giraldo,  J.  Kernel  generative 
topographic mapping. In Proceedings of the European Symposium 
on Artificial Neural Networks – Computational Intelligence and 
Machine Learning, 2010.  
[16] Cruz-Barbosa, R. and Vellido, A. Unfolding the Manifold in 
Generative  Topographic  Mapping.  In  Proceedings  of  the  3rd 
international workshop on Hybrid Artificial Intelligence Systems 
(Burgos, Spain, 2008). Springer-Verlag.  
[17]  Yi,  J.  S.,  Melton,  R.,  Stasko,  J.  and  Jacko,  J.  A.  Dust  & 
magnet:  multivariate  information  visualization  using  a  magnet 
metaphor. Information Visualization, 4, 4, 2005. 
[18]  House,  L.,  Leman,  S.  C.  and  Han,  C.  Bayesian  Visual 
Analytics  (BaVA).  In  revision,  Technical  Report:  FODAVA-10-
02, http://fodava.gatech.edu/node/342010). 
[19] Leman, S. C., House, L., Maiti, D., Endert, A. and North, C. 
A  Bi-directional  Visualization  Pipeline  that  Enables  Visual  to 
Parametric  Interation  (V2PI). NFS FODAVA Technical Report 
(FODAVA-10-41),  2011.  
[20] Pearson, K. On Lines and Planes of Closest Fit to Systems of 
Points in Space. City, 1901. 
[21]  Jolliffe,  I.  Principal  Component  Analysis.  John  Wiley  and 
Sons, Ltd, 2002. 
[22]  Torokhti,  A.  and  Friedland,  S.  Towards  theory  of  generic 
Principal Component Analysis. 
[23]  Tipping,  M.  E.  and  Bishop,  C.  M.  Probabilistic  Principal 
Component  Analysis.  Journal  of  the  Royal  Statistical  Society, 
SeriesB: Statistical Methodology, 61, 1999. 

",False,2011.0,{},False,False,conferencePaper,False,PRCPUX25,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/6102449/,,Observation-level interaction with statistical models for visual analytics,PRCPUX25,False,False
QGWC6ZS6,RI99Q2IA,Parsing Error,False,2010.0,{},False,False,journalArticle,False,QGWC6ZS6,[],self.user,False,False,False,False,,,"Recent Advances in Nonlinear Dimensionality Reduction, Manifold and Topological Learning",QGWC6ZS6,False,False
4DVV4NXW,WD6HIX8D,"Wang XM, Zhang TY, Ma YX et al. A survey of visual analytic pipelines. JOURNAL OF COMPUTER SCIENCE AND
TECHNOLOGY 31(4): 787–804 July 2016. DOI 10.1007/s11390-016-1663-1

A Survey of Visual Analytic Pipelines

Xu-Meng Wang, Tian-Ye Zhang, Yu-Xin Ma, Jing Xia, and Wei Chen ∗, Senior Member, IEEE

State Key Laboratory of Computer Aided Design and Computer Graphics, Zhejiang University, Hangzhou 310058, China

Innovation Joint Research Center for Cyber-Physical-Society System, Zhejiang University, Hangzhou 310058, China

E-mail: {wangxumeng, zhangtianye1026, mayuxin}@zju.edu.cn; jjane.summer@gmail.com
E-mail: chenwei@cad.zju.edu.cn

Received April 17, 2016; revised May 31, 2016.

Visual analytics has been widely studied in the past decade. One key to make visual analytics practical for
Abstract
both research and industrial applications is the appropriate deﬁnition and implementation of the visual analytics pipeline
which provides eﬀective abstractions for designing and implementing visual analytics systems. In this paper we review the
previous work on visual analytics pipelines and individual modules from multiple perspectives: data, visualization, model
and knowledge.
In each module we discuss various representations and descriptions of pipelines inside the module, and
compare the commonalities and the diﬀerences among them.

Keywords

visual analytics, pipeline, visualization, model, knowledge

1 Introduction

Nowadays the increasing availability of massive
datasets has raised a revolution of data gathering, sto-
rage and analysis. It becomes diﬃcult and gradually in-
feasible to apply standard tools for data analysis, which
are widely utilized during the past decades by business
analysts, scientists and government employees for in-
sight gaining and decision making.

In many ﬁelds such as biological computation, busi-
ness intelligence and online transaction analysis, auto-
mated data analysis approaches such as machine learn-
ing and data mining are commonly deployed to extract
patterns from existing data. The patterns are repre-
sented as the high-level abstraction of insights from the
data and then transformed into knowledge[1]. Visuali-
zation, from the perspective of human vision, provides
another scheme for analysts to enhance the ability of
understanding and exploring datasets. Usually visuali-
zation methods employ visual channels to represent
and transform raw datasets into various visual rep-

resentation forms, and thereby human intelligence is
incorporated into the data analysis process via intui-
tive interactive interface. In the past decade, the the-
ory of “visual analytics” (or visual analysis) has been
widely studied by combining automated data mining
techniques and visualization methods. Visual analytics
“integrates the capability of computer and the abilities
of the human analyst”[2] to empower the control of the
entire analysis and decision-making process.
In fact,
pioneers provide a few valuable literatures. Keim et
al.[2-3] gave a general introduction of visual analytics.
In addition, some scholars summarize the state-of-the-
art part in the ﬁeld of visual analytics, for instance,
Zhang et al.[4] focused on advanced commercial sys-
tems and Sun et al.[5] generalized cutting-edge research
and future challenges from the perspective of analytics
space.

The purpose of this paper is to bring visual analy-
tics into the limelight. We review a set of literatures on
visual analytics and propose a summarization of visual
analytics pipelines that cover automated data process-

Survey
The work was supported by the National Basic Research 973 Program of China under Grant No. 2015CB352503, the Major
Program of National Natural Science Foundation of China under Grant No. 61232012, the National Natural Science Foundation of
China under Grant Nos. 61422211, u1536118, and u1536119, Zhejiang Provincial Natural Science Foundation of China under Grant
No. LR13F020001, and Fundamental Research Funds for the Central Universities of China.

∗Corresponding Author
©2016 Springer Science + Business Media, LLC & Science Press, China

Wang XM, Zhang TY, Ma YX et al. A survey of visual analytic pipelines. JOURNAL OF COMPUTER SCIENCE AND
TECHNOLOGY 31(4): 787–804 July 2016. DOI 10.1007/s11390-016-1663-1

A Survey of Visual Analytic Pipelines

Xu-Meng Wang, Tian-Ye Zhang, Yu-Xin Ma, Jing Xia, and Wei Chen ∗, Senior Member, IEEE

State Key Laboratory of Computer Aided Design and Computer Graphics, Zhejiang University, Hangzhou 310058, China

Innovation Joint Research Center for Cyber-Physical-Society System, Zhejiang University, Hangzhou 310058, China

E-mail: {wangxumeng, zhangtianye1026, mayuxin}@zju.edu.cn; jjane.summer@gmail.com
E-mail: chenwei@cad.zju.edu.cn

Received April 17, 2016; revised May 31, 2016.

Visual analytics has been widely studied in the past decade. One key to make visual analytics practical for
Abstract
both research and industrial applications is the appropriate deﬁnition and implementation of the visual analytics pipeline
which provides eﬀective abstractions for designing and implementing visual analytics systems. In this paper we review the
previous work on visual analytics pipelines and individual modules from multiple perspectives: data, visualization, model
and knowledge.
In each module we discuss various representations and descriptions of pipelines inside the module, and
compare the commonalities and the diﬀerences among them.

Keywords

visual analytics, pipeline, visualization, model, knowledge

1 Introduction

Nowadays the increasing availability of massive
datasets has raised a revolution of data gathering, sto-
rage and analysis. It becomes diﬃcult and gradually in-
feasible to apply standard tools for data analysis, which
are widely utilized during the past decades by business
analysts, scientists and government employees for in-
sight gaining and decision making.

In many ﬁelds such as biological computation, busi-
ness intelligence and online transaction analysis, auto-
mated data analysis approaches such as machine learn-
ing and data mining are commonly deployed to extract
patterns from existing data. The patterns are repre-
sented as the high-level abstraction of insights from the
data and then transformed into knowledge[1]. Visuali-
zation, from the perspective of human vision, provides
another scheme for analysts to enhance the ability of
understanding and exploring datasets. Usually visuali-
zation methods employ visual channels to represent
and transform raw datasets into various visual rep-

resentation forms, and thereby human intelligence is
incorporated into the data analysis process via intui-
tive interactive interface. In the past decade, the the-
ory of “visual analytics” (or visual analysis) has been
widely studied by combining automated data mining
techniques and visualization methods. Visual analytics
“integrates the capability of computer and the abilities
of the human analyst”[2] to empower the control of the
entire analysis and decision-making process.
In fact,
pioneers provide a few valuable literatures. Keim et
al.[2-3] gave a general introduction of visual analytics.
In addition, some scholars summarize the state-of-the-
art part in the ﬁeld of visual analytics, for instance,
Zhang et al.[4] focused on advanced commercial sys-
tems and Sun et al.[5] generalized cutting-edge research
and future challenges from the perspective of analytics
space.

The purpose of this paper is to bring visual analy-
tics into the limelight. We review a set of literatures on
visual analytics and propose a summarization of visual
analytics pipelines that cover automated data process-

Survey
The work was supported by the National Basic Research 973 Program of China under Grant No. 2015CB352503, the Major
Program of National Natural Science Foundation of China under Grant No. 61232012, the National Natural Science Foundation of
China under Grant Nos. 61422211, u1536118, and u1536119, Zhejiang Provincial Natural Science Foundation of China under Grant
No. LR13F020001, and Fundamental Research Funds for the Central Universities of China.

∗Corresponding Author
©2016 Springer Science + Business Media, LLC & Science Press, China

",False,2016.0,{},False,False,journalArticle,False,4DVV4NXW,[],self.user,False,False,False,False,http://link.springer.com/10.1007/s11390-016-1663-1,,A Survey of Visual Analytic Pipelines,4DVV4NXW,False,False
3I937GAS,5H3LFLPI,"Collaborative Search Revisited 

  Meredith Ringel Morris 

 Microsoft Research 
 Redmond, WA, USA 

   merrie@microsoft.com 

ABSTRACT 
Despite  recent  innovations  in  technologies  supporting 
collaborative  web  search  [11,  13,  25,  34,  35,  37],  the 
features of the primary tools for digital information seeking 
(web  browsers  and  search  engines)  continue  to  reflect  a 
presumption  that  search  is  a  single-user  activity.  In  this 
paper,  we  present  the  findings  of  a  survey  of  167  diverse 
users’  collaborative  web  search  practices,  including  the 
prevalence and frequency of such activities, the information 
needs  motivating  collaboration,  the  methods  and  tools 
employed  in  such  tasks,  and  users’  satisfaction  with  the 
status quo. We find an increased prevalence and frequency 
of  collaborative  search,  particularly  by  younger  users,  and 
an appropriation of “old” technologies like e-mail as well as 
“new” technologies like smartphones and social networking 
sites,  rather  than  the  use  of  dedicated  collaborative  search 
tools.  We  reflect  on  how  and  why  collaborative  search 
practices have changed in the six years since the first survey 
detailing 
this  phenomenon  was  conducted  [22],  and 
synthesize  our  findings  to  offer  suggestions  for  the  design 
of future collaborative search technologies. 

Author Keywords 
Web search, collaborative search, social search, CSCW. 

ACM Classification Keywords 
H.5.m. Information interfaces and presentation (e.g., HCI): 
Miscellaneous.  

and 

technologies 

INTRODUCTION 
Information-seeking 
collaboration 
technologies are the two most popular online tools; a 2011 
Pew  Research  survey  [32]  found  that  92%  of  online 
American  adults  use  search  engines,  and  a  similar 
proportion  use  email.  However,  this  compartmentalization 
of  practices  as  either  search  or  collaboration  is  tenuous. 
Although  web  search  is  often  considered  a  de  facto  solo 
activity,  and  nearly  all  mainstream  search  technologies  are 
designed  for  single-user  scenarios,  a  growing  body  of 
research  suggests  that  active  collaboration  on  search  tasks 
among  users  with  shared  information  needs  is  relatively 
commonplace [13, 22, 25]. 

 
Permission  to  make  digital  or  hard  copies  of  all  or  part  of  this  work  for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CSCW ’13, February 23–27, 2013, San Antonio, Texas, USA. 
Copyright 2013 ACM  978-1-4503-1331-5/13/02...$15.00. 

 

Our  20061  survey  of  204  Microsoft  employees  [22] 
provided data regarding the prevalence of collaborative web 
search, the tasks motivating such practices, and the methods 
used to enable such collaborations. In the six years since we 
conducted  that  survey,  the  technology  landscape  has 
undergone significant changes, particularly the rise of social 
networking  sites  (only  16%  of  Americans  had  social 
networking  profiles  in  2006,  compared  with  66%  in  2012 
[4])  and  the  growing  ubiquity  of  smartphones  (46%  of 
American  adults  owned  smartphones  as  of  February  2012 
[36])  and  other  powerful  portable  technologies  (e.g.,  tablet 
computers).  The  intervening  six  years  have  also  seen  a 
technologies  for 
flurry  of  research  and  commercial 
collaborative  search  support, 
though  none  have  had 
mainstream success.  

In  light  of  this  changed  landscape,  we  reassess  status  quo 
collaborative  search  practices  through  a  survey  of  167 
American  adults.  Our  results  provide  insight  into  the 
evolution  of  collaborative  and  social  search  practices;  for 
instance,  we  find  an  increase  in  the  prevalence  and 
frequency  of  collaborative  web  search,  as  well  as 
appropriation  of  new  technologies  like  social  networking 
sites and smartphones to support this phenomenon. We also 
find  that  users  continue  to  piece  together  general  purpose 
technologies to facilitate collaborative information seeking, 
rather 
taking  advantage  of  systems  designed 
specifically for such experiences. In light of these findings, 
we  reflect  on  barriers 
to  adoption  of  collaborative 
information  seeking  tools,  and  identify  key  research 
directions moving forward.  

than 

RELATED WORK 
The term social search is used to refer to a broad spectrum 
of  information  seeking  behaviors,  ranging  from  behaviors 
which  are  implicitly  social  (e.g.,  search  over  socially-
generated  data  sets)  to  those  that  are  explicitly  social  (e.g., 
interacting  with  other  people  during  various  stages  of  the 
search process) [13]. Evans and Chi [7] described the types 

                                                           

1 Though published in 2008, the  survey data  was collected 
in  November  2006.  We  refer  to  it  (and  other  surveys 
discussed) by the year of data-gathering rather than the year 
of  publication,  when  known,  since  the  former  more 
accurately  represents  the  socio-technical  context  of  the 
findings  given  the  rapid  evolution  of  technologies  and 
practices. 

 

 

of  social  engagements  possible  at  different  stages  of  the 
Web search process.  

Collaborative  search  [11,  13,  25,  34]  is  a  subset  of  social 
search  in  which  participants  work  together  to  satisfy  an 
information  need.  The  collaborative  nature  of  search  tasks 
in  pre-web  scenarios  (e.g.,  in  libraries  and  paper-driven 
offices)  has  a  long  history  of  scholarship  (e.g.,  [9,  38]). 
Academic  investigation  of  the  challenges  and  practices 
associated  with  collaborative  web  search  is  a  more  recent 
phenomenon, usually associated with the 2007 introduction 
of  the  SearchTogether  [24]  system,  whose  design  was 
informed by a survey of collaborative web search practices 
conducted  in  2006  (but  not  published  until  2008)  [22]. 
Collaborative search  has many benefits, including enabling 
participants  to  achieve  synergic  effects  such  as  greater 
recall  [31,  35],  offering  the  potential  to  improve  search 
skills  through  exposure  to  others’  behavior  [20,  21],  and 
providing  an  opportunity  to  strengthen  social  connections 
[27,  28].  Note  that  the  investigation  of  collaborative  web 
search  differs  from  prior  work  on  collaborative  web 
browsers  (e.g.,  [12])  in  that  its  focus  is  not  on  general-
purpose  web  browsing,  but  specifically  on  the  use  of  the 
web for information-seeking tasks. 

It 

representative 

is  unclear  how 

In [22], we reported the results of a November 2006 survey 
of  204  Microsoft  employees’  collaborative  web  search 
practices. 
this 
demographic’s  behavior  was,  as  respondents  differed  in 
many  ways  from  the  general  population,  primarily  on  the 
basis  of  their  technical  expertise,  as  well  as  along  other 
demographic  dimensions  that  were  also  non-typical  (e.g.,  
80.4%  of  respondents  were  male).  However,  [22]  presents 
the  most  complete  picture  available  of  the  prevalence  and 
characteristics  of  the  collaborative  search  phenomenon;  it 
found  that,  despite  the  lack  of  any  tools  designed  to 
explicitly support collaborative web search, 53.4% of those 
surveyed  had  engaged  in  such  activities  by  using  the 
bottom-up 
[41]  approach  of  appropriating  existing 
technologies  (e.g.,  email, 
to 
supplement  the  web  browser.  A  smaller,  diary-based  study 
of 20 Microsoft employees [2] conducted in 20072 provided 
additional  insights  into  the  specific  scenario  of  co-located 
collaborative  search.  More  recently,  several  studies  have 
characterized  the  asymmetric  collaborative  search  scenario 
(in  which  participants  have  different 
roles  and/or 
motivations [25]) of using social networking sites to engage 
contacts in various stages of an information-seeking task [6, 
17, 27, 29]. 

instant  messaging,  etc.) 

In the intervening six years since our survey [22], a number 
of research prototypes supporting collaborative search have 
been 
innovations  have  proposed 
algorithmic  techniques  to  enhance  the  collaborative  search 
experience, including role-specific weightings of input [31], 

introduced.  Some 

[15].  Others  have  proposed  user 

group personalization of results [26], expertise-matching of 
potential  collaborators  [16],  and  agents  that  use  context 
from  social  network  Q&A  exchanges  to  suggest  relevant 
links 
interface 
enhancements,  such  as  enhancing  collaborators’  awareness 
of  each  other’s  search  process 
[20,  24],  enabling 
distribution of control in co-located settings [1], supporting 
collaborative search among users with asymmetric access to 
devices  [42],  and  supporting  collaborative  search  on 
emerging technologies such as large touch surfaces [23].  

Some commercial technologies have also included features 
supportive  of  specific  subsets  of  collaborative  search 
activities.  Examples  include  Aardvark  (a  service  to  match 
users  with  experts  to  support  their  information-seeking; 
2007),  Flock  (a  web  browser  that  incorporated  social 
networking  as  a  first-class  feature  in  its  design;  2008), 
HeyStaks (which uses feedback from communities of users 
with  common  interests  to  re-rank  search  results;  2008), 
Pinterest  (a  shared  bulletin  board  for  collections  of  web 
imagery;  2010),  SearchTeam  (a  tool  whose  features  are 
highly reminiscent of SearchTogether [24], 2011), and So.cl 
(a social network based around sharing collections of search 
results  [8];  2011).  None  of  these  commercial  tools  has 
achieved  mainstream  adoption  –  some,  like  Aardvark  and 
Flock,  have  already  become  defunct;  others,  like  Pinterest 
and So.cl, are still in limited, invitation-only beta stages.  

This paper adds to this body of work on collaborative web 
search by presenting survey results that give an updated and 
more  complete  view  of  the  current  state  of  practice.  Our 
survey  reports  on  the  collaborative  search  practices  of  a 
more  representative  sample  of  the  general  public  (as 
opposed to highly technical knowledge workers, e.g., [22]), 
and  reflects  recent  changes  in  the  technological  landscape 
(such  as  the  growing  prevalence  of  social  networking  [4] 
and smartphone use [36]).   

open-ended 

and  multiple-choice 

to  assess  current  practices 

SURVEY 
We conducted an online survey over a one  week period in 
March  2012 
regarding 
collaborative  information  seeking.  The  survey  consisted  of 
both 
questions. 
Respondents  were  asked  whether 
they  had  ever 
collaborated  with  other  people  to  search  the  Web;  if  they 
answered  affirmatively,  they  were  asked  to  describe  their 
most recent collaborative search experience and to answer a 
series  of  questions  about  that  specific  search  incident  (a 
critical-incident  approach  [7,  10]).  Additional  survey 
questions  addressed  demographics  and  use  of  specific 
search and collaboration technologies.  

The  survey  was  advertised  as  a  questionnaire  on 
“Information  Seeking  Practices”  via  the  Survey  Monkey 
Audience  recruiting  service3  to  1,025  adult  American 

                                                           

                                                           

2 Data gathered in 2007; published in 2009.  

3 http://www.surveymonkey.com/mp/audience/ 

 

 

participants;  167  completed  the  entire  survey,  yielding  a 
16% response rate.  

RESULTS 
We  first  characterize  the  demographic  details  of  our  167 
respondents.  We  then  report  our  findings  regarding  the 
prevalence  of  collaborative  search  and  the  nature  of  such 
searches.  We  also  report  findings  on  the  use  of  specific 
technologies 
seeking, 
including smartphones, social networks, and Q&A tools.  

collaborative 

information 

for 

Note  that  we  use  non-parametric  statistical  tests  when 
analyzing  Likert  responses,  due  to  the  subjective  and 
potentially  non-linear  interpretations  of  the  “spacing” 
between adjacent items on such scales. 

Demographics 
Our 167 survey respondents were all residents of the United 
States; 40 of the 50 states were covered by our sample. 56% 
of respondents were female.  38% of respondents were aged 
18 – 29; 24% were aged 30 – 44; 27% were aged 45 – 60; 
and  11%  were  older  than  60  years.  5%  had  a  high  school 
diploma or less, 57% had completed some college, 29% had 
a college degree, and 9% had a graduate degree. 

Occupations  were  varied,  with  students  making  up  the 
largest  single  group  at  25%  of  respondents.  An  additional 
8% of the  group  was comprised of retirees. The remaining 
67%  of  respondents  had  diverse  vocations  including  sales 
person,  customer  service  representative,  teacher,  nurse, 
school counselor, homemaker,  mortgage broker, physician, 
stock analyst, insurance adjuster, cosmetologist, accountant, 
software engineer, dentist, paralegal, copy editor, and heavy 
equipment operator.  

Respondents used search engines frequently;  most (79.9%) 
reported  using  a  major  search  engine  (Ask,  Bing,  Google, 
or  Yahoo!)  several  times  per  day,  and  nearly  all  (94.1%) 
reported doing so at least once per day. 

Collaborative Search 
All  respondents  were  asked  “Have  you  ever  collaborated 
with  other  people  to  search  the  Web?”  If  they  answered 
negatively,  they  skipped  ahead  to  the  questions  about 
specific technologies (see the “Beyond ‘Traditional’ Search 
Engines”  section)  and  demographics.  However,  the  109 
respondents  (65.3%)  who  answered  affirmatively  were 
asked  several  follow-up  questions  about  their  experiences 
with collaborative search.  

that  65.3%  of  respondents  had  engaged 

Finding 
in 
collaborative  search  indicates  an  increased  prevalence  of 
collaborative search behavior – our 2006 survey [22] found 
the  prevalence  of  collaborative  search  to  be  only  53.4% 
(and that was with a more “tech-savvy” audience, Microsoft 
employees, whom one would assume might be more likely 
to  appropriate  technologies  in  novel  ways  than  the  more 
diverse  audience  of  the  current  survey).  The  prevalence  of 
collaboration  we  found  differs  significantly  from  the 

 

 

 

Daily 

Weekly 

Monthly 

Less Often 

2006  

0.9% 

25.7% 

48.6% 

24.8% 

2012 

11.0% 

38.5% 

15.6% 

34.8% 

Table 1. Percent of respondents reporting collaboratively 
searching at various frequencies. 2006 numbers are taken 

from Table 2 of [22]. 

 

hypothesized proportions based on the 2006 survey, χ2(1, N 
= 167) = 9.62, p = .002). 

Age  was  significantly  negatively  correlated  with  the 
likelihood  of  engaging 
in  collaborative  search  (e.g., 
younger  respondents  were  more  likely  to  engage  in  this 
behavior), r = -.26, p = .001. 

Note that percentages given in the remainder of this section 
(“Collaborative Search”) and its sub-sections are out of the 
109  people  who 
they  had  searched 
collaboratively  rather  than  out  of  the  full  167  survey 
respondents. 

indicated 

that 

11%  of  respondents  who  had  searched  collaboratively 
reported doing so on a daily basis, and an additional 38.5% 
report searching collaboratively at least once per week. This 
is  a  marked  increase  over  the  self-reported  frequency  of 
collaborative searching in 2006 [22], as illustrated in Table 
1, χ2(3, N = 108) = 155.26, p < .001). 

After  indicating  whether  they  had  ever  searched  the  Web 
collaboratively and how often they did so, participants who 
had  searched  collaboratively  were  asked  to  engage  in  a 
recent critical-incident self-report [7, 10]. They were asked 
in a free-text question to “think about the most recent time 
you collaborated with others to search the web,” and then to 
“describe the nature of the information need that prompted 
this  incident.”  They  were  then  asked  several  follow-up 
questions  about  that  specific  incident.  The  following  four 
sub-sections (“Topics,” “Group Configurations,”  “Methods 
and Tools,” and “Satisfaction”)  are based on the follow-up 
questions  about  the  respondents’  most  recent  collaborative 
search incident. 

Topics 
In  addition  to  describing  the  information  need  prompting 
their  most  recent  collaborative  search  in  a  free-response 
question, we also asked respondents to classify the nature of 
the  information  need  they  investigated  collaboratively  by 
selecting  one  or  more  topics  from  a  list.  The  list  of  topic 
choices  was  created  by  combining  the  topics  reported  as 
prompting  collaborative  searches  in  [22]  and  the  list  of 
topics reported as likely and unlikely to prompt requests for 
search help from members of one’s social network in [27].  

# of Respondents 

Example Task Description 

26 

21 

19 

18 

16 

15 

14 

11 

11 

10 

10 

“we  split  up  research  for  software  development,  searching 
individually  for  coding  issues,  gui  design,  and  what  would  be 
appealing to our audience” 

“we  needed  to  find  information  about  iron  deficiency  and 
hypokalemia” 

“looking for reference footage and images for a school project” 

“looking for printer parts for our business operation” 

“we were planning a trip to Alaska and all the details that go into 
it for a group of 10 of us” 

“looking for a used car” 

“searching for music on YouTube and lyrics” 

“genealogy” 

“we were researching different kinds of e-portfolios” 

“find local restaurants” 

“planning a wedding” 

Topic 

professional 

health/medicine 

news/current events 

technology 

travel 

shopping  

entertainment 

home/family 

finance 

restaurants 

social events 

 

Table 2. The most common topics motivating respondents’ recent collaborative Web searches. 

respondents  said  described 

Table  2  shows  the  most  popular  topics  (those  which  10  or 
more 
recent 
collaboratively-investigated  information  need),  and  gives 
examples from respondents’ free-form descriptions of their 
most recent search incident. 

their  most 

Group Configurations 
Small-group  collaboration  was  more  common  than  larger 
groups.  Pairs  were 
the  most  common  configuration 
(31.2%). Triads were also fairly common (22.9%), as were 
quartets  (23.9%).  Groups  larger  than  four  members  were 
infrequent – 9.2% reported working in groups of five, 4.6% 
in groups of six, and 8.3% in groups having seven or more 
members.  

Our  2006  survey  [22]  also  found  that  smaller  group  sizes 
were  more  common  than  larger  ones,  though  that  survey 
found  much  smaller  group  sizes,  reporting  that  80.7% 
collaborated in pairs and 19.3% in  groups of three or four, 
with  no larger  groups at all.  Comparing our frequencies of 
pairs,  groups  of  three  or  four,  and  larger  groups  to  this 
earlier  finding  shows  a  significant  change  in  group  sizes, 
χ2(2, N = 109) = 610, p < .001. Our 2007 diary study of co-
located  collaborative  search  [2]  also  found  smaller  group 
sizes  than  our  current  study,  with  85.7%  collaborating  in 
pairs and 9.5% in groups of three or four. 

We also asked respondents to characterize their relationship 
with their collaborators on their search task. 55.0% reported 
collaborating with colleagues or classmates. Family was the 
next  most  common  type  of  collaborator  relationship,  at 
25.7%,  followed  by  close  friends  (19.3%),  and  then  casual 

 

 

acquaintances  (11.0%).  Collaboration  with  strangers  or 
professionals  (e.g.,  librarians)  was  rare,  at  5.5%.  Note  that 
these  values 
than  100%,  as  some 
respondents  indicated  that  they  worked  with  a  group 
comprised of multiple relationship types.  

to  greater 

total 

roughly 

two-thirds  of 

than 
Synchronous  collaboration  was  more  common 
asynchronous,  comprising 
the 
incidents (64.2%). Remote collaboration was more common 
than  co-located,  characterizing  61.5%  of  the  described 
searches. This is in contrast to the 2006 survey [22], which 
found 
search 
configurations,  although 
their  question  was  phrased 
differently  (asking  respondents  which  of  the  following 
behaviors they  had ever engaged in,  versus asking them to 
describe  a  single  recent  critical  incident  as  we  do  here), 
making direct comparisons difficult. 

slight  prevalence  of 

a 

co-located 

Methods and Tools 
Participants  used  a  checklist  to  indicate  what  tools  they 
employed in their most recent collaborative search incident 
(they  could  check  as  many  items  as  applied).  The  use  of 
search  engines  was  common  (67.9%  of  respondents  used 
them  in  their  most  recent  collaborative  search  task),  but 
other  methods  of  online  information-seeking  were  also 
employed, such as using a social networking site (19.3%) or 
Q&A  site  (6.4%).  We  explore  the  use  of  these  latter  two 
technologies  in  further  detail  in  the  “Beyond  ‘Traditional’ 
Search Engines” section.   

Devices:  “Traditional”  devices  like  laptops  (61.5%)  and 
PCs  (39.4%)  were  the  most  common  devices  used  in  the 

 

Very 
Dissatisfied 

Dissatisfied  Neutral 

Satisfied  Very 

Satisfied 

 

Daily  Weekly  Monthly 

Never 

Less  than 
once  per 
month 

quality 
of 
answer 
found 

ease  of 
working 
with 
others 

2.8% 

1.8% 

12.8%  55.0%  27.5% 

1.8% 

2.8% 

16.5%  42.1%  35.8% 

Table  3.  Respondents’  satisfaction  with  the  informational 
and  social  aspects  of  their  most  recent  collaborative 
search incident. 

 

course  of  collaborative  information  seeking.  Newer  device 
types  were  also  common,  with  30.3%  of  the  searches 
involving  a  smartphone  and  11%  involving  a  tablet. 
Technologies  that  might  facilitate  public  sharing  such  as 
TVs and projectors were rarely employed, in only 1.8% and 
0.9%  of  the  searches,  respectively.  Non-digital  tools  were 
also an important part of collaborative search processes; for 
instance,  11%  of  respondents  reported  using  paper  to 
support their collaborative search task.  

Communication:  Since  mainstream  web  browsers  and 
search  engines  don’t  incorporate  communication  tools 
(which  are  important  for  facilitating  remote  collaborative 
search  [24]),  respondents  often  employed  out-of-band 
communication  channels.  Email  was  the  most  common 
communication  tool,  involved  in  46.8%  of  the  searches. 
Other  communication  channels  used  were  talking  on  the 
phone  (27.5%),  text  messaging/SMS  (30.3%),  and  instant 
messaging  (12.8%).  Videoconferencing  was  rare;  only  one 
participant  reported  employing  it  as  a  communications 
channel during a collaborative search.  

Satisfaction 
Responents used a five-point Likert scale to rate their level 
of  satisfaction  with  both  the  informational  (quality  of 
answer found) and social (ease of working collaboratively) 
aspects  of  their  most  recent  collaborative  search.  Table  3 
summarizes  those  results.  82.5%  reported  satisfaction  with 
the  informational  outcome  and  77.9%  reported  satisfaction 
with  the  ease  of  collaboration.  Though  positive  overall, 
these  figures  still 
is  room  for 
improvement of both the informational and social aspects of 
the collaborative search experience.  

indicate 

there 

that 

Respondents were also given space to compose a free-form 
response  regarding  suggestions  for  how  their  collaborative 
search  experience  could  have  been  improved.  The  most 
common  response  (7  people)  was  for  facilities  to  make  it 
easier to share the products of  search  with  group  members 
and  increase  group  awareness  of  mutual  activities  (two 
issues  that  systems  like  SearchTogether  [24],  Coagmento 
[35],  and  WeSearch  [23]  sought  to  address).  For  example, 
one respondent said, “It might have helped if we had some 
sort  of  online  bulletin  board  on  which  to  post  our 
findings…”  Another  noted  that  he  would  have  liked  “an 

 

 

% of 
smartphone 
owners 

% of co-
located 
searchers 

36.1% 

24.7% 

21.6% 

10.3% 

7.2% 

38.9% 

26.7% 

23.3% 

11.1% 

N/A 

Table  4.  The  reported  frequency  of  engaging  co-located  multi-
party  smartphone  searches  among  all  smartphone  owners  in 
our  sample  (97  participants),  and  among  those  who  reported 
engaging in this behavior at least occasionally (90 participants). 

 

easier  way  for the rest of  the  group  members to access the 
information  each  of  us  found  separately,”  and  one  person 
desired the ability  to have  “real-time comparisons between 
mine  and  my  colleagues’  information.”  Redundant  work 
[22] remained problematic, with one respondent noting that 
“if  there  was  a  better  way  to  communicate  where  one 
person had already looked it would have prevented overlap  

of  seeking  for  information,”  and  another  observing  that  it 
would  be  helpful 
tracks 
collaborators’  searches  in  a  private  group,  so  that  you  can 
see which papers others have already found.”  

to  have  “a  database 

that 

Beyond “Traditional” Search Engines 
Although  the  questions  surrounding  respondents’  most 
recent collaborative search incident  were  asked only  to the 
109  respondents  who  indicated  that  they  had  engaged  in 
collaborative Web search, all 167 respondents were asked a 
series  of  questions  about  their  use  of  several  specific 
technologies, independent of the critical incident inquiry. In 
particular,  we  were 
the 
collaborative use of information-seeking technologies other 
than  the  traditional  “using  a  search  engine  in  a  PC  web 
browser.” Our questions focused on three kinds of tools that 
have experienced significant changes or growth since 2006 
– smartphones, social networking sites, and Q&A sites. 

investigating 

interested 

in 

Smartphones 
Though  smartphones  (which  permit  browsing  the  web  and 
running  third-party  applications)  existed  in  2006,  the 
capabilities  and  adoption  of 
smartphones  changed 
dramatically  in  2007  with  the  introduction  of  Apple’s 
iPhone,  whose  Safari  browser  provided  the  ability  to  view 
and  interact  with  “real”  web  pages  (rather  than  special 
mobile  versions).  By  early  2012,  46%  of  American  adults 
owned  a  smartphone  [36].  Recent  work  suggests  that 
mobile  local  searches  (i.e.,  searching  for  businesses  or 
services  near  a  user’s  current  geo-location)  are  often 
undertaken  in  a  social  setting,  but  does  not  offer  detailed 
insight into multi-phone collaborative search practices [39].  

In  our  survey  sample,  58.1%  of  respondents  reported 
owning  a  smartphone.  Of  these  97  smartphone-owning 
respondents,  48.5%  had  Android  devices,  34.0%  had  iOS 
(Apple) devices, and the remainder had devices running the 

 

Facebook  Twitter  Google+  LinkedIn 

have 
accounts 

have ever 
asked a 
question 

ask 
questions at 
least once 
per week 

lurk (read 
content but 
never post) 

139 

49 

42   

50  

(83.2%) 

(29.3%) 

(25.1%) 

(29.9%) 

50.0% 

33.3% 

24.6% 

24.6% 

15.4% 

9.5% 

9.8% 

4.6% 

4.3% 

8.8% 

12.5% 

15.9% 

Table  5.  The  first  row  reports  how  many  of  the  167 
respondents  had  accounts  on  each  social  networking  site. 
Additional  rows  report  the  percentage  of  those  account-
holders engaging in specific behaviors. 

 
Palm,  RIM  (Blackberry),  or  Windows  operating  systems. 
Smartphone ownership was not significantly correlated with 
any demographic factors. 

Although  they  may  not  have  previously  self-identified  as 
having  engaged  in  collaborative  search,  nearly  all  of  the 
smartphone owners (92.8%) reported using their phones  to 
engage  in  co-located  collaborative  searches  in  which 
several  people  simultaneously  used  their  smartphones  to 
look  up 
information  (Table  4).  This  behavior  was 
surprisingly frequent – of the 90 respondents  who reported 
engaging in this behavior, 38.9% reported doing so at least 
once  per  day,  and  65.6%  at  least  a  few  times  per  week. 
Younger  respondents  engaged  in  co-located  multi-phone 
searches more frequently than older respondents (r = -.26, p 
=  .01).  These  initial  findings  suggest  that  studying  co-
located collaborative smartphone search may be a rich area 
for  further  investigation  to  answer  questions  beyond  the 
scope  of  our  current  survey,  such  as  exploring  what  role 
specialized “apps” might play in such scenarios.   

Social Networking Sites 
Social  networking  sites  were  used  by  only  16%  of 
Americans  in  2006  (the  year  in  which  Facebook  opened 
enrollment  to  the  general  public  rather  than  merely  to 
students  at  selected  universities);  by  2012,  66%  had  an 
account  [4].  Social  networking  sites  have  taken  on  an 
increasingly  prominent  role  in  asymmetric  collaborative 
information  seeking  [25],  through  mechanisms  such  as 
posting search results directly to social network feeds (e.g., 
Bing  and  Ping  [5]  or  So.cl  [8]),  using  socially  embedded 
search  engines  (e.g.,  SearchBuddies  [15]),  or  asking 
questions via status messages [6, 17, 27, 29]. For instance, a 
20094  survey  of  Microsoft  employees  [27]  found  that 
50.6% had posted questions to Facebook or Twitter.  

                                                           

 

Figure 1. Self-reported frequency of posting a  question as  a 
status update, a form of asymmetric collaborative search, by 
account-holders on each social network. 

 
87.4%  of  our  survey  respondents  reported  having  social 
networking accounts,  with Facebook being by  far the  most 
popular,  distantly  followed  by  Twitter,  LinkedIn,  and 
Google+  (Table  5).  Other  networks  like  MySpace,  Orkut, 
Tumblr,  and  Yammer  had  negligible  representation. 
Younger  respondents  were  more  likely  to  have  social 
networking accounts than older ones (r = -.27, p < .001). 

Asking  questions  on  these  social  networking  sites  was 
common. 50.0% of those  with Facebook accounts reported 
having used that network to ask a question, as did 33.3% of 
those  with  Twitter  accounts,  and  24.6%  of  those  with 
LinkedIn  and  Google+  accounts.  Our  finding  that  half  of 
Facebook  users  have  engaged  in  status-message  question 
asking is similar to the findings of a survey we conducted in 
2009  [27],  despite  the  fact  that  the  2009  survey  audience 
was  comprised  of  Microsoft  employees  while  our  current 
survey draws  from a  more diverse demographic. However, 
it  conflicts  with  the  findings  of  a  survey  by  Lampe  et  al. 
conducted  in  20115  [17]  that  found  that  most  Facebook 
users  in  their  sample  did  not  view  Facebook  as  an 
appropriate  venue  for  information-seeking  (i.e.,  via  status 
message Q&A). The Lampe survey’s audience consisted of 
employees at a U.S. university, who were less diverse (e.g., 
more  educated,  more  female,  older)  than  our  sample 
population, which may explain this difference. 

While  prevalent,  this  behavior  appears  to  be  relatively 
infrequent  (frequency  of  social  network  Q&A  was  not 
reported  in  prior  surveys  such  as  [27],  which  focused 
primarily on prevalence, motivations, and topics associated 
with this phenomenon). Only 15.4% of the Facebook users, 
9.8% of Google+ users, 9.5% of Twitter users, and 4.6% of 
LinkedIn  users  reported  asking  questions  at  least  once  per 
week.  The  low  use  of  LinkedIn  for  question-asking  may 
reflect competition from other professional forums, such as 
internal  enterprise  SNS  sites  [40].  Figure  1  shows  the 
reported  frequency  of  question-asking  by  respondents 
holding accounts on each of those social networking sites. 
                                                           

4 Survey conducted in 2009; published in 2010. 

5 Survey conducted in 2011; published in 2012. 

 

 

 

 

 

 

Figure  2.  Frequencies  of  viewing  content,  posting  content,  and  posting  questions  by  account-holders  on  four  social  networks. 
The  higher  frequency  of  lurking  (users  who  view  but  never  post  content)  on  Google+  and  LinkedIn  may  contribute  to  their 
being viewed as less useful venues for getting questions answered. 

 

On  all  of  these  social  networks,  viewing  content  is  more 
common than posting content, which is more common than 
posting  questions.  “Lurking”  (having  an  account  and 
logging  in  to  view  content,  but  never  posting  any  content 
yourself)  was 
relatively  uncommon.  Only  4.3%  of 
respondents  with Facebook accounts  were lurkers. Lurking 
on Twitter was more common, at 8.8% (note that this figure 
only  includes  respondents  with  Twitter  accounts;  many 
additional  people  likely  read  Twitter  without  having 
accounts  at  all).  The  lurking  rates  for  Google+  and 
LinkedIn  were  higher  still,  at  12.5%  and  15.9%, 
respectively.  The 
is  strongly  negatively 
correlated  (r  =  -.94)  with  the  rate  of  question-asking  on 
each  service,  perhaps  because  it  relates  to  the  likelihood 
that  someone  who  views  a  question  will  chime  in  with  an 
answer.  Figure  2  shows 
the  frequency  of  different 
interactions on each social network. 

lurking  rate 

The frequency of question asking on the three less popular 
social  networks  was  significantly  correlated  (p  <  .01) 
(Twitter/Google+:  r  =  .36;  Twitter/LinkedIn,  r  =  .34, 
Google+/LinkedIn:  r  =  .44),  perhaps  representing  a  clique 
of “hard-core” askers who try many social venues in pursuit 
of an information need. In contrast, the frequency of asking 
on  Facebook  (a  more  popular  activity  overall),  was  not 
correlated  significantly  with  asking  on  LinkedIn  (the  least 
popular venue for question asking), and had relatively weak 

correlations with asking frequency on Google+ and Twitter 
(r = .19, p = .03). 

Demographic  factors  correlated  weakly  with  the  frequency 
of  question  asking  on  certain  social  networks.  Younger 
respondents were more likely to ask questions on LinkedIn 
frequently  (r  =  -.24,  p  <  .01),  as  were  respondents  with 
lower education levels (r = -.18, p = .03). Among Facebook 
users,  women  reported  asking  questions  more  often  than 
men (r = -.18, p - .03).  

Q&A Sites 
Q&A  sites  provide  an  alternative  method  of  online 
information  seeking  than  traditional  Web  search.  These 
forums  allow  users  to  post  questions  for  answering  by 
either  the  general  Web  population  (e.g.,  Yahoo!  Answers, 
Mahalo  Answers,  Ask  MetaFilter  [14]),  paid  staffers  (e.g., 
ChaCha, kgb), or self-identified topical experts (e.g., Quora 
[30]).  Such  sites  typically  archive  past  questions  and 
answers,  which  are  browseable  and/or  searchable  by  other 
users.  

The past few years have seen an increase in the prominence 
of  tools  that  form  social  structure  around  non-anonymous 
Q&A exchanges (e.g., Quora [33], founded in 2009) and of 
those  that  operate  on  a  paid-staffer  model,  which  is  a 
modern-day  analog  of 
librarian,  (e.g., 
ChaCha, founded in 2006, whose answer volume surpassed 

the  reference 

 

 

that  of  Yahoo!  Answers  in  2011  [18]).  Use  of  such  “next-
generation”  Q&A  sites  could  be  construed  as  a  form  of 
asymmetric collaborative search [25]. 

We  asked  all  respondents  how  often  they  posted  questions 
to  a  variety  of  Q&A  sites.  Most  respondents  reported  that 
they  had  never  posted  a  question  to  Ask  MetaFilter 
(97.3%),  ChaCha  (93.2%),  kgb  (98.6%),  Mahalo  Answers 
(100%),  or  Quora  (99.3%).  The  only  Q&A  site  in  our 
survey  that  was  occasionally  used  was  Yahoo!  Answers  – 
24%  of  respondents  had  posted  a  question  at  least  once, 
though  this  behavior  was  infrequent  (only  4.8%  reported 
posting a question at least once a week).  

The frequency of posting questions to Yahoo! Answers was 
not  significantly  correlated  with  demographic  factors  (age, 
gender,  or  education).  There  was,  however,  a  significant 
negative  correlation  between  the  frequency  of  posting  to 
Yahoo! Answers and the frequency of posting questions to 
some of the less popular social networking sites (Google+: r 
=  -.30,  p  <  .01;  LinkedIn:  r  =  -.17,  p  =  .04).  This  might 
indicate  that  users  employ  two  distinct  “backup”  strategies 
for seeking answers to difficult questions – either posting to 
a Q&A site or posting to a “secondary” social network. 

We also asked whether respondents perused the archives of 
these  Q&A  sites  for  answers,  even  if  they  did  not  post  a 
question  themselves.  48.6%  of  respondents  reported  using 
Yahoo!  Answers  in  this  manner  at  least  once,  and  11% 
reported using ChaCha in this manner. The use of archived 
answers from Ask Metafilter, kgb, Mahalo, and Quora was 
negligible.  Reusing  answers  was  a  more  frequent  behavior 
than  posting  new  questions;  1.4%  of  respondents  used  the 
ChaCha archives at least once per week, and 11.7% did so 
for  Yahoo!  Answers  (a  Wilcoxon  test  comparing  the 
frequency  of  posting  vs.  perusing  Yahoo!  Answers  found 
that the latter was significantly more frequent, z = -5.41, p < 
.001). Reusing existing answers on Yahoo and ChaCha was 
significantly  inversely  correlated  with  age  –  younger  users 
were  more  likely  to  engage  in  this  behavior  frequently 
(Yahoo!  Answers: r =  -.41, p < .01; ChaCha: r =  -.23, p < 
.01).  

DISCUSSION 
In this section,  we reflect on our survey findings. First, we 
compare and contrast our results with those of prior studies 
of  collaborative  search  behavior,  and  discuss  possible 
causes  of  differences.  We  then  discuss  the  implications  of 
our findings for the design of technical solutions supporting 
collaborative search. 

Comparison with Prior Findings 
Compared to six years ago [22], we found that more people 
are engaging in collaborative web search, and  that they are 
doing so with greater frequency.  65.3% of our respondents 
reported having collaborated on web search, with 49.5% of 
those  collaborative  searchers  engaging  in  such  activities  at 
least  once  per  week.  When  asked  about  specific  behaviors 
(such as engaging in co-located smartphone searches), even 

 

 

higher  prevalence  and 
reported, 
suggesting that the 65.3% number may be an underestimate, 
perhaps due to the generic nature of the question (“have you 
ever collaborated with other people to search the web?”). 

frequencies  were 

to 

likely  due 

The  typical  group  size  involved  in  such  collaborations  has 
increased,  as  well, 
the  adoption  of 
technologies  that  facilitate  simultaneous  interaction  of 
larger groups of users for remote collaboration (e.g., social 
networking  sites),  and  technologies  that  support  larger 
group engagement in co-located collaboration by providing 
each  group  member  with  their  own  input  device  (e.g., 
smartphones).  Despite  the  considerable  press  attention 
some  emerging  social  information  seeking  solutions  have 
received  (e.g.,  Quora  [33]),  community  Q&A  sites  do  not 
appear  to  be  part  of  a  typical  user’s  collaborative  search 
repertoire.  

We found that younger users were more likely to engage in 
collaborative  searches.  It  is  unclear  whether  the  observed 
increase in collaborative search activities is due primarily to 
the coming-of-age of a new generation of technology users 
who  are  more  comfortable  pushing  the  bounds  of  a  tool’s 
intended  use  and/or  have  differing  attitudes 
toward 
collaboration,  or  whether  it  is  due  to  the  invention  and 
adoption  of  new 
(smartphones,  social 
networking,  etc.).  It  is  likely  that  both  of  these  factors 
played a role in shaping our findings. 

technologies 

search 

create 

solutions 

tendency  of  respondents 
technologies 

to  appropriate  existing 
The 
facto 
to 
de 
communications 
using 
collaborative 
than 
increasingly  available  dedicated  collaborative 
tools) 
remains similar to six years ago. Consequently, respondents 
reported  many  of  the  same  frustrations  with  collaborative 
search  (lack  of  awareness,  wasted  duplication  of  effort)  as 
in our earlier survey [22].  

(rather 

Our  findings  suggest  that  the  meaning  of  the  term 
“collaborative search” has evolved (or should evolve!). Our 
initial conception of collaborative search in our survey [22] 
and  SearchTogether  prototype 
the 
synchronous  or  asynchronous  use  of  search  engines  by 
multiple  parties  with  a  shared  information  need.  However, 
our  survey  findings  indicate  that  collaborative  search  now 
occurs  beyond 
in  apps  on 
smartphones, in questions on social networking sites, etc.).  

the  search  engine  (e.g., 

involved 

[24] 

Limitations 
The  reader  should  note  that  differences  between  our 
findings  and  prior  work  (particularly  [22])  are  difficult  to 
attribute to a single cause. Differences may be due to social 
and  technological  changes  occurring  between  2006  and 
2012,  which  is  our  primary  hypothesis.  Other  sources  of 
differential findings may be in the survey audience (highly 
technical  respondents  in  [22]  versus  the  more  general 
population we reached with this survey), or in the nature of 
the  questions  themselves  (the  “have  you  ever”  approach 

employed 
approach [7, 10] employed in the current work). 

in  [22]  versus 

the  recent  critical-incident 

To  explore  whether  our  findings  were  due  to  audience 
background  rather  than  sociotechnical  changes  occurring 
between 2006 and 2012, we issued the same survey to 250 
randomly  selected U.S.-based  Microsoft  employees in July 
2012;  63  completed  the  survey  (25%  response  rate).  The 
results  from  this  group  were  very  similar  to  the  results  of 
the  more  diverse  2012  audience  discussed  in  this  paper  – 
for  instance,  61.9%  of  the  2012  Microsoft  employee 
respondents  reported  having  engaged  in  collaborative  Web 
search,  which  is  not  significantly  different  than  the  65.3% 
figure for the diverse group (χ2(1, N=63) = .279, p = .597). 
Similarly,  of  the  2012  Microsoft  employees  who  searched 
collaboratively,  49.5%  reported  doing  so  at  least  once  per 
week,  which  is  quite  similar  to  the  47.3%  of  our  more 
diverse  sample 
that  collaboratively  searched  at  least 
weekly.  The  similarity  between  the  diverse  and  tech 
audiences’ 
that 
differences in audience background between our survey and 
the  2006  survey  are  not  the  primary  source  of  the 
differences in our findings. 

increased  our  confidence 

responses 

Additionally,  the  reader  should  bear  in  mind  the  inherent 
limits  of  all  self-report  studies,  such  as  potential 
inaccuracies in participants’ memory or biases in what they 
choose  to  report  (for  more  detail  on  the  pros  and  cons  of 
retrospective  self-report  methods,  see  [19]).  It  is  also 
unclear  whether 
to  other 
demographics,  such  as  children  or  people  outside  the 
United  States  (e.g.,  staffed  Q&A  sites  are  reportedly  a 
popular  form  of  asymmetric  collaborative  search  amongst 
Korean  teenagers  [18]).  Combining  our  survey  findings 
with  other  approaches,  such  as  interview  or  observational 
methods,  would  provide  a  richer  understanding  of  this 
phenomenon, and is a suggested direction for further study. 

findings 

these 

extend 

the 

[24],  and  So.cl 

increasing  availability  of 

Challenges for Collaborative Search Solutions 
Despite 
tools  designed 
specifically  to  support  collaborative  web  search  (e.g.,  free 
online  tools  including  Coagmento  [35],  HeyStaks  [37], 
SearchTogether 
[8]),  none  of  our 
respondents  utilized  such  technologies.  General-purpose 
tools that could provide rich collaborative experiences, such 
as  videoconferencing  or  projection  technologies,  were  also 
rarely  used.  Instead,  respondents  repurposed  simpler 
communications 
that  were  part  of  their 
everyday  routines  (e-mail,  texting,  instant  messaging, 
phone calls, and social networking) as a way to supplement 
status quo web browser and search engine technologies and 
enable collaborative information seeking. This suggests that 
technologies 
for  collaborative  web  search  must  be 
sufficiently  lightweight  compared  with  status  quo  ad  hoc 
solutions.    One  of  our  survey  respondents  articulated  this 
well, observing, “It might have helped if we had some sort 
of online bulletin board on which to post our findings – but 
only  if  posting  something  to  the  bulletin  board  was  faster 

technologies 

 

 

and required fewer mouse clicks than copying a link into an 
e-mail  message.”  The  tension  between  dedicated,  “top 
down” solutions versus ad hoc “bottom up” solutions is not 
unique  to  collaborative  search;  lessons  learned  about 
similar issues in areas like cyberinfrastructure development 
[41]  may  be  applicable.  Reflecting  on  our  survey  findings 
in  light  of  this  related  work  suggests  that  rather  than 
creating  dedicated  tools  for  collaborative  search,  creating 
“glue”  systems  that  offer  integration,  tighter  coupling,  and 
functionality  between  existing  social  and 
symbiotic 
information-seeking 
technologies  might  be  a  more 
promising approach. 

Despite the challenge of striking a proper  balance between 
having a low barrier to entry and offering rich collaboration 
support, there appears to be an unmet need for technologies 
supporting  collaborative  web  search,  as  evidenced  by  the 
increasing prevalence and frequency of such activities. Our 
finding  that  collaborative  search  is  more  common  among 
younger  demographics  suggests  that  its  prevalence  might 
continue  to  increase  as  a  new  generation  of  users  with 
different  attitudes  about  collaboration  and  technology 
emerges into the marketplace.  

Our  results  suggest  that  systems  that  address  users’ 
frustrations  regarding  lack  of  awareness  of  collaborators’ 
activities  and  the  resulting  redundant  work  that  occurs 
would  be  particularly  valued;  these  findings  reinforce 
similar findings from prior work [22, 24], indicating a need 
that  has  continued  to  go  unmet  by  technical  advances. 
Solutions  that  can  enhance  common  scenarios,  such  as  the 
use  of  social  networks  for  Q&A  activities  or  the  use  of 
several  smartphones  for  synchronous  co-located  searching, 
may  be  a  particularly  promising  direction  for  research  and 
development.  

Shortly  after  the  completion  of  our  survey,  Microsoft 
introduced collaborative search support into its Bing search 
engine  with the “sidebar” feature [3], which enables a user 
to start a conversation with social network contacts around 
a query and a set of curated search results. The introduction 
of  collaborative  features  into  a  mainstream  search  engine 
could  potentially  significantly  alter  the  status  quo  reliance 
on bottom-up solutions. Revisiting the state of collaborative 
search practice in a few years seems prudent given the rapid 
evolution  of  technologies  and  attitudes  in  the  social  search 
space. 

CONCLUSION 
In this paper, we added to the growing body of knowledge 
about  collaborative  web  search  by  presenting  survey  data 
about  167  diverse  users’  status  quo  collaborative  search 
practices.  We  found  that  collaborative  search  has  become 
an 
information-seeking 
experience  (and  that  the  notion  of  what  constitutes  a 
“collaborative  search”  has  evolved  to  include  technologies 
beyond  search  engines,  such  as  smartphones  and  social 
networking sites). We also found that ad hoc combinations 
of  everyday 
to  support  such 

technologies  are  used 

increasingly  common 

type  of 

collaborations,  rather  than  dedicated  solutions  designed 
specifically for collaborative information seeking. 

By contrasting our findings with earlier work, we identified 
changes  in  the  prevalence  of  this  practice  and  in  the 
technologies  employed.  We  also 
important 
challenges  that  remain  to  be  addressed  by  designers  of 
collaborative  web search technologies. Our results indicate 
that  there  is  great  potential  for  technological  innovation  to 
enhance 
surprisingly  commonplace  practice  of 
collaborative information seeking in the digital era. 

identified 

the 

REFERENCES 
1.  Amershi, S. and Morris, M.R. CoSearch: A System for 
Co-located Collaborative Web Search. Proceedings of 
CHI 2008, 1647-1656. 

2.  Amershi, S. and Morris, M.R. Co-located Collaborative 
Web Search: Understanding Status Quo Practices. CHI 
2009 Extended Abstracts, 3637-3642. 

3.  Bing Team. Introducing the New Bing: Spend Less 

Time Searching, More Time Doing. Bing Search Blog, 
May 10, 2012. 

4.  Brenner, J. Social Networking. Pew Internet and 

American Life Project, March 29, 2012. 

5.  Dybwad, B. Bing and Ping: Share Search Results on 

Facebook and Twitter. Mashable, Sept. 3, 2009. 

6.  Efron, M. and Winget, M. Questions are Content: A 

Taxonomy of Questions in a Microblogging 
Environment. Proceedings of ASIS&T 2010. 

7.  Evans, B. and Chi, E. Towards a Model of 

Understanding Social Search. Proceedings of CSCW 
2008, 485-494. 

8.  Farnham, S.D., Lahav, M., Raskino, D., Cheng, L., 

Ickman, T., and Laird-McConnell, T. So.cl: An Interest 
Network for Informal Learning. Proceedings of ICWSM 
2012. 

9.  Fidel, R., Bruce, H., Pejtersen, A.M., Dumais, S.T., 

Grudin, J., and Poltrock, S. Collaborative Information 
Retrieval. The New Review of Information Behaviour 
Research, 2000, volume 1, 235-247. 

10. Flanagan, J. The critical incident technique. 

Psychological Bulletin, 51:327-358, 1954. 

11. Foster, J. Collaborative Information Behavior: User 

Engagement and Communication Sharing. IGI Global, 
June 2010. 

12. Greenberg, S. and Roseman, M. GroupWeb: A WWW 
Browser as Real Time Groupware. Proceedings of the 
CHI 1996 Conference Companion, 271-272. 

13. Golovchinksy, G., Morris, M.R., and Pickens, J. 

Introduction to the special issue. Information Processing 
& Management, Special Issue on Collaborative 
Information Seeking, 46(6), November 2010. 

 

 

14. Harper, F., Moy, D., and Konstan, J. Facts or friends? 

Distinguishing informational and conversational 
questions in social Q&A sites. Proceedings of CHI 
2009, 759-768. 

15. Hecht, B., Teevan, J., Morris, M.R., and Liebling, D. 

SearchBuddies: Bringing Search Engines into the 
Conversation. Proceedings of ICWSM 2012. 

16. Horowitz, D. and Kamvar, S.D. The Anatomy of a 
Large-Scale Social Search Engine. Proceedings of 
WWW 2010, 431-440. 

17. Lampe, C., Vitak, J., Gray, R., & Ellison, N. Perceptions 

of Facebook’s Value as an Information Source. 
Proceedings of CHI 2012, 3195-3204. 

18. Lee, U. Kang, H. Yi, E., Yi, M.Y., Kantola, J. 

Understanding Mobile Q&A Usage: An Exploratory 
Study. Proceedings of CHI 2012, 3215-3224. 

19. Metts, S., Sprecher, S., and Cupach, W.R. Retrospective 

Self-Reports. In Studying Interpersonal Interaction 
(Montgomery, B.M. and Duck, S., eds.). The Guilford 
Press: 1991. 

20. Moraveji, N., Morris, M.R., Morris, D., Czerwinski, M., 

and Riche, N. ClassSearch: Facilitating the 
Development of Web Search Skills through Social 
Learning. Proceedings of CHI 2011, 1797-1806. 

21. Morris, M.R. Interfaces for Collaborative Exploratory 

Web Search: Motivations and Directions for Multi-User 
Designs. CHI 2007 Workshop on Exploratory Search 
and HCI. 

22. Morris, M.R. A Survey of Collaborative Web Search 

Practices. Proceedings of CHI 2008, 1657-1660. 

23. Morris, M.R., Fisher, D., and Wigdor, D. Search on 

Surfaces: Exploring the Potential of Interactive 
Tabletops for Collaborative Search Tasks. Information 
Processing and Management, 46(6), November 2010. 

24. Morris, M.R. and Horvitz, E. SearchTogether; An 

Interface for Collaborative Web Search. Proceedings of 
UIST 2007, 3-12. 

25. Morris, M.R. and Teevan, J. Collaborative Web Search: 

Who, What, Where, When, and Why? Morgan & 
Claypool, 2010. 

26. Morris, M.R., Teevan, J., and Bush, S. Enhancing 

Collaborative Web Search with Personalization: 
Groupization, Smart Splitting, and Group Hit-
Highlighting. Proceedings of CSCW 2008, 481-484. 

27. Morris, M.R., Teevan, J., and Panovich, K. What Do 

People Ask Their Social Networks, and Why? A Survey 
Study of Status Message Q&A Behavior. Proceedings 
of CHI 2010, 1739-1748. 

28. Morris, M.R., Teevan, J., and Panovich, K. A 

Comparison of Information Seeking Using Search 
Engines and Social Networks. Proceedings of ICWSM 
2010. 

29. Paul, S.A., Hong, L., and Chi, E.H. Is Twitter a Good 

36. Smith, A. Nearly half of American adults are 

Place for Asking Questions? A Characterization Study. 
Proceedings of ICWSM 2011. 

smartphone owners. Pew Internet & American Life 
Project, March 1, 2012. 

30. Paul, S.A., Hong, L., and Chi, E.H. Who is 

37. Smyth, B., Briggs, P., Coyle, M., and O’Mahoney, M. 

Authoritative? Understanding Reputation Mechanisms 
in Quora. Proceedings of Collective Intelligence 2012. 

Google Shared: A Case Study in Social Search. 
Proceedings of UMAP 2009. 

31. Pickens, J., Golovchinsky, G., Shah, C., Qvarfordt, P., 
and Back, M. Algorithmic Mediation for Collaborative 
Exploratory Search. Proceedings of SIGIR 2008, 315-
322. 

32. Purcell, K. Search and email still top the list of most 

popular online activities. Pew Internet &American Life 
Project, August 9, 2011. 

38. Taylor, R.S. Question-Negotiation and Information-
Seeking in Libraries. College & Research Libraries, 
29(3), 1968, 178-194. 

39. Teevan, J., Karlson, A., Amini, S., Brush, A.J.B., and 
Krumm, J. Understanding the Importance of Location, 
Time, and People in Mobile Local Search Behavior. 
Proceedings of Mobile HCI 2011. 

33. Rivlin, G. Does Quora Really Have All the Answers? 

40. Thom, J., Helsley, S.Y., Matthews, T.L., Daly, E.M., 

Wired, May 2011. 

34. Shah, C. Collaborative Information Seeking: The Art 

and Science of Making the Whole Greater than the Sum 
of All. The Information Retrieval Series, Springer, 2012. 

35. Shah, C. and Gonzalez-Ibanez, R. Evaluating the 

Synergic Effect of Collaboration in Information 
Seeking. Proceedings of SIGIR 2011, 913-922. 

Millen, D.R. What Are You Working On? Status 
Message Q&A within an Enterprise SNS. Proceedings 
of ECSCW 2011. 

41. Twidale, M.B. and Floyd, I.R. Infrastructures from the 

Bottom-Up and Top-Down: Can They Meet in the 
Middle? Proceedings of PDC 2008, 238-241. 

42. Wiltse, H. and Nichols, J. PlayByPlay: Collaborative 

Web Browsing for Desktop and Mobile Devices. 
Proceedings of CHI 2009, 1781-1790.  

 

 

 

 

",False,2013.0,{},False,False,conferencePaper,False,3I937GAS,[],self.user,False,False,False,False,http://dl.acm.org/citation.cfm?doid=2441776.2441910,,Collaborative search revisited,3I937GAS,False,False
H4Y2RXSJ,KAV3IH93,"This paper is © IEEE and appears reformatted in IEEE Transactions on Visualization and Computer Graphics, 30(1), 
2016, DOI: 10.1109/TVCG.2015.2467552 

The Data Context Map:                                                                     

Fusing Data and Attributes into a Unified Display 

Shenghui Cheng, Student Member, IEEE and Klaus Mueller, Senior Member, IEEE 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

  

  

 
 

  

(a
) 

  

(b) 

(c) 

 
 

 
 

(d) 

C                                         

B                                         

A                                         

(e) 

Fig.1.  The  process  to  find  a  dream  university.  (a)  good  academics  region(>9).  (b)  good  athletic  region  (>9).  (c)  combined 
region by (a) and (b). (d) low tuition region(<$18,000). (e) combined region by region (c) and (d). 

Abstract— Numerous methods have been described that allow the visualization of the data matrix. But all suffer from a common 
problem – observing the data points in the context of the attributes is either impossible or inaccurate. We describe a method that 
allows  these  types  of  comprehensive  layouts.  We  achieve  it  by  combining  two  similarity  matrices  typically  used  in  isolation  –  the 
matrix encoding the similarity of the attributes and the matrix encoding the similarity of the data points. This combined matrix yields 
two of the four submatrices needed for a full multi-dimensional scaling type layout. The remaining two submatrices are obtained by 
creating a fused similarity matrix – one that measures the similarity of the data points with respect to the attributes, and vice versa. 
The resulting layout places the data objects in direct context of the attributes and hence we call it the data context map. It allows 
users  to  simultaneously  appreciate  (1)  the  similarity  of  data  objects,  (2)  the  similarity  of  attributes  in  the  specific  scope  of  the 
collection of data objects, and (3) the relationships of data objects with attributes and vice versa. The contextual layout also allows 
data regions to be segmented and labeled based on the locations of the attributes. This enables, for example, the map’s application 
in selection tasks where users seek to identify one or more data objects that best fit a certain configuration of factors, using the map 
to visually balance the tradeoffs.  
Index Terms— High Dimensional Data, Low-Dimensional Embedding, Visual Analytics, Decision Make, Tradeoffs  
 

1 

I NTRODUCTION 

The data matrix, DM, is one of the most fundamental structures in 
data analytics. It is the M×N rectangular array of N variables (often 
referred  to  as  attributes  or  labels)  and  M  samples  (also  frequently 
called  cases,  observations,  or  data  items).  The  N×N  or  M×M 
similarity (or co-occurrence, correlation) matrix S is another often 
used  structure  and  frequently  derived  from  DM.  Here  it  should  be 
noted that the roles of variables and samples can change – there are 
many  practical  settings  in  which  we  consider  the  ‘variables’  as 
outcomes we wish to predict (or use for prediction) using the set of 

 
  Shenghui Cheng and Klaus Mueller are with the Visual Analytics and 

Imaging Laboratory, Computer Science Department, Stony Brook 
University and SUNY Korea. Email: {shecheng, mueller}@cs.sunysb.edu 

Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of   
publication xx Aug. 2015; date of current version 25 Oct. 2015. 
For information on obtaining reprints of this article, please send  e-mail  to: 
tvcg@computer.org.  

 

1 

samples  acquired  before.  To  visualize  DM,  current  methods  either 
focus on spatially preserving the relations among the samples or on 
spatially preserving  the  relations among the variables, but they  are                    
typically  not  capable  to  do  both.  This  is  a  severe  limitation  when 
one  wishes  to  transform  DM  into  a  comprehensive  map  in  which 
the acquired samples are accurately presented in the context of the 
variables. Our paper describes new data and similarity matrices that 
overcome these deficiencies.   

to  consider  –  academic 

To  illustrate  these  points,  let’s  consider  a  parent  looking  for  a 
university  for their  child.  This  is  an  important  decision  with  many 
factors 
tuition,  athletics, 
teacher/student  ratio  and  many  others.  College  Prowler  [15]  is  a 
popular  website  that  allows  users  to  navigate  this  parameter  space 
by  filtering  –  using  slider  bars  and  menu  selections  for  each 
parameter to narrow down the search. But this is rather tedious and 
it  also  makes  it  difficult  to  recognize  tradeoffs.  Conversely,  a 
visualization  expert  would  use  interactive  parallel  coordinates  [17] 

score, 

plots  but  it  is  difficult  to  imagine  that  an  average  parent  would 
engage in such an advanced interface. There are other visualization 
methods,  such  as  biplots  or  interior  layouts  but  these  are  seldom 
found in the mainstream arena. The wide-spread familiarity of maps, 
on  the  other  hand,  makes  these  a  natural  canvas  to  overview  the 
landscape  of  universities  in  the  context  of  the  various  factors 
(attributes) to consider. Parents could simply sit back and examine 
this  illustration  like  an  infographic  and  then  decide  on  a  school. 
They could still use a filter to eliminate some schools from the map 
but they would never lose sight of the big picture. 

Methods  like  Multidimensional  scaling  (MDS)  [23][2],  self-
organizing  map  (SOM)  [6][22],  t-distributed  stochastic  neighbor 
embedding (t-SNE) [28], locally linear embedding (LLE) [31], etc. 
create  2D  map-like  data  layouts  computed  from  the  similarity 
matrix  S  of  schools  (in  this  case).  The  entries  of  S  are  derived  by 
assessing  the distances of pairs of the M schools in  the  N-D space 
spanned  by  the  N  attribute  axes.  The  maps  will  show  similar 
schools as clusters, and special schools as outliers. This is certainly 
useful,  but  parents  will  not  know  from  the  plot  alone  why  some 
schools are special and  others are  clustered. What is their ranking, 
tuition, athletics, etc.?  

It  is  important  to  note  that  S  could  just  as  well  hold  the 
similarities  of  attributes.  The  maps  mentioned  above  would  then 
allow  a  visual assessment  of  the  grouping  of  attributes.  So  instead 
of  finding  that  schools  A  and  B  are  very  similar  (or  dissimilar)  in 
terms of their attributes, one would find that attributes C and D are 
heavily  correlated  (or  not)  in  this  set  of  schools.  A  parent  might 
learn that the higher the academic score, the higher the tuition, and 
the  higher  the  number  of  students  per  faculty  (see  findings 
presented in [37] where such a map was presented). And so, if the 
parent is interested in smaller classes, schools with lower academic 
scores might be a better choice. Hence, while such a plot is useful 
in  explaining  the  relationships  of  the  different  features  of  the 
educational  landscape,  what  it  now  cannot  do  is  allow  anxious 
parents  pick  a  specific  school  for  their  child,  which  is  what  they 
really wish to do. 

We  propose  a  framework  that  overcomes  these  limitations  and 
combines  both  of  the  similarity  aspects  derived  from  DM  into  a 
single  comprehensive  map  which  we  call  the  data  context  map.  It 
requires  a  non-trivial  fusion  of  the  two  alternative  similarity 
matrices  S  discussed  above.  By  ways  of  this  fused  matrix  a 
mapping can be performed that allows users to faithfully appreciate 
all three types of relationships in a single display: (1) the patterns of 
the  collection  of  samples,  (2)  the  patterns  of  the  collection  of 
attributes,  and  (3)  the  relationships  of  samples  with  the  attributes 
and  vice  versa.  Further,  the  contextual  mapping  also  provides  the 
information needed to add semantic labelling of the samples as well 
as  the  regions  they  reside  in.  Iso-contouring  these  regions  then 
creates  decision  boundaries  by  which  one  can  easily  recognize 
trade-offs  among  different  samples  which  can  be  helpful  in 
complex decision making. Our paper demonstrates this by ways of 
a few practical examples. 

Our paper is structured as follows. Section 2 summarizes related 
work. Section 3 provides its theoretical aspects. Section 4 describes 
the  construction  of  our  data  context  map.  Section  5  presents  case 
studies. Section 6 concludes the paper and expands on future work.   

2  REL ATED W ORK 

The  visualization  of  high-dimensional  data  on  a  2D  canvas 
essentially follows three major paradigms – projective data displays, 
interior  displays,  and  space  embeddings.  However,  since  the 
visualization  of  high-dimensional  data  in  2D  is  inherently  an  ill-
posed problem, there is no method without drawbacks. It is simply 
impossible  to  preserve  all  variances  of  a  high-dimensional  point 
cloud in a 2D mapping. Hence the different methods that have been 
described  offer  different  strengths  and  weaknesses,  but  some  do 
better than others.  

2.1 

Projective and interior displays 

These  displays  typically  warp  the  data  in  some  way  to  emphasize 
certain  properties,  such  as  locality  or  similarity.  A  projective 
display  is  the  scatterplot  matrix  [12]  which  is  an  extension  of  the 
scatterplot.  It  reserves  a  scatterplot  tile  for  each  pair  of  variables 
and projects the data items into it. This distributes the data context 
into two variables per tile which makes it difficult to appreciate the 
overall  context  pertaining  to  all  variables  simultaneously.  In 
addition,  the  mapping  operation  can  lead  to  ambiguities  as  points 
located far way in high-dimensional space may project into similar 
2D  locations.  This  adds  to  the  difficulties  for  recognizing 
multivariate relationships.    

Parallel  coordinates  and  their  radial  version,  the  star  plot  [1], 
represent  the  variables  as  parallel  or  radial  axes,  respectively,  and 
map  the  data  as  polylines  across  the  axes.  However,  the  clutter  of 
polylines  can  become  a  significant  problem  once  the  number  of 
dimensions  and  data  points  increases.  In  order  to  decrease  the 
clutter of lines, star coordinates [20] arrange the attribute axes in a 
radial  fashion  but  instead  of  constructing  polylines,  they  plot  the 
data  points  as  a  vector  sum  of  the  individual  axis  coordinates. 
However, since a vector sum is an aggregation, it maps the data to 
locations  that  are  not  unique.  In  other  words,  points  that  map  to 
nearby  locations  may  not  be  close  in  high-dimensional  space,  and 
vice versa. To help users resolve these ambiguities, at least partially, 
an interactive interface is often provided that allows them to rotate 
and scale the data axes and so uncover false neighbors.  

In  fact,  there  are  number  of  displays  that  are  similar  to  star 
coordinates and share its shortcomings [32]. These are Radviz [13], 
Generalized Barycentric Coordinates (GBC) plot [29], and PolyViz 
[14].  We  call  them  interior  displays  since  they  all  lay  out  the 
variables  as  dimension  anchors  [14]  around  a  circle  and  map  the 
data  items  as  points  inside  it,  given  some  weighting  function  that 
relates  to  a  data  point’s  different  attribute  strengths.  All  of  these 
displays are useful in what they have been designed to convey, that 
is, the relation of data points with respect to the attributes. But since 
the  mapping  function  does  not  involve  the  similarity  of  the  data 
points, ambiguities result. 

Our research described here has been motivated by recent work 
presented  by  the  authors  [7]  which  proposes  an  optimization 
approach  to  reduce  the  data  mapping  ambiguities  in  Radviz-type 
displays.  The  current  framework  is  radically  different  in  that  it 
maps  the  attributes  not  in  the  periphery  along  a  circle,  but 
intersperses  them  into  the  data  distribution  which  reduces  all 
mapping errors significantly. It also enables the region labelling and 
decision boundaries discussed above.  

2.2 

Comparing the interior displays 

We have shown in [7] that the method of GBC [29] can serve as a 
standard  reference  framework  to  describe  most  interior  displays. 
The  GBC  plot  uses  the  dimension  values  of  an  N-D  point  as 
weights in a weighted sum of the anchor 2D locations to determine 
the point’s placement in the 2D polygon.   

Using  the GBC plots, we conducted  a controlled experiment  to 
compare  them  with  the  method  proposed  here.  For  this,  we 
generated  a  test  dataset  comprised  of  a  set  of  6  6-D  Gaussian 
distributions. We first randomized the 6 6-D center vectors and then 
randomized  600  data  points  following  these  distributions.  Fig.  5d 
visualizes  this  dataset  using  parallel  coordinates,  assigning  each 
Gaussian  a  unique  color.  In  addition,  we  also  colored  the  axes 
(representing  the  6  dimensions)  such  that  each  axis  color  matches 
that of the cluster with the highest value for that dimension. Fig. 2 
shows  how  (a)  standard  GBC  compares  with  (b)  the  optimized 
GBC  plot  [7],  and  (c)  the  method  proposed  in  this  paper  which 
allows  the attribute  nodes  to  intersperse  with  the  samples.  We  can 
show  our  method  is  more  flexible  and  can  preserve  the  pairwise 
distances well. We will describe more comparison in section 4.1.7. 

 
 

 

2 

affects  the  accuracy  of  the  decision  boundaries  computed 
from these regions [4].  

3  T HEORY  AND M ETHOD 

We  wish  to  create  a  mapping  in  which  all  three  types  of 
relationships  in  DM  are  preserved  –  the  relationships  among  the 
samples, among the attributes, and mutually among the samples and 
attributes. Here, the notion of relationship can be a distance, such as 
Euclidian  (across  space)  or  geodesic  (across  a  manifold),  or  a 
similarity, such as Pearson’s correlation [33], cosine, or pattern, or 
it  can  be  some  measure  of  significance,  such  as  value  or  feature. 
We  combine  these functions collectively into a  distance  metric, F, 
and note that, depending on the application, each relationship might 
be  expressed  in  a  different  F.  For  example,  the  similarity  of 
attributes might be measured by correlation, while the proximity of 
samples might be gauged via the Euclidian distance. We wish for a 
mapping that preserves this set of  simultaneous constraints as well 
as  possible.  It  calls  for  an  optimization  strategy  on  a  fused 
representation  of  three  types  of  relationships.  The  pipeline  of  this 
process is shown in Fig.3.  
 
 
 
 
 
 

Fig. 3 The fusion pipeline 

Mapping 

Distance 

Fusion 

Data 

 
 
In the following we outline the various steps of this pipeline in 
detail. The underlying primitive is a distance matrix, one for each of 
the three pairs, encoding the respective  F. The fusion process then 
merges 
into  a  single  distance  matrix 
emphasizing  certain  constituents  or  equalizing  them.  This  is 
followed  by  a  mapping  to  2D  using  an  optimization  process.  We 
use  an  MDS-type  strategy  because  it  is  well  tested  for  such 
mapping problems.    

three  matrices 

these 

Data Matrix 

3.1 
We begin with 𝐷𝑀, the data matrix, with 𝑚 rows and 𝑛 columns, 

𝑥11 ⋯ 𝑥1𝑛
⋮
⋮

⋱

] 

𝐷𝑀 = [

𝑥𝑚1 ⋯ 𝑥𝑚𝑛

Here,  the  rows  denote  the  data  samples,  the  columns  denote  the 
variables  and  xij  is  the  data  value  in  the  ith  row  and  jth  column. 
Without loss of generality, we assume 𝐷𝑀 is normalized to [0, 1].  

Depending  on  how  we  look  at  DM,  row-wise  or  column-wise, 
we  have  two  types  of  spaces  –  the  data  space  D  and  the  variable 
space  V,  respectively.  The  data  space  D  contains  all  m  data  items 
(samples): 

𝐷𝑖 = [𝑥𝑖1, 𝑥𝑖2, … , 𝑥𝑖𝑛]             (𝑖 = 1,2, … , 𝑚) 

 
 
 
 
 
 
 
 
 
 
 

(a)

                          (b)                              (c)                   
Fig.2.  Comparing  different  attribute/sample  layout  schemes:      
(a) standard GBC, (b) optimized GBC, and (c) our new method.  

 

2.3 

Embedded displays  

The  ambiguities  in  the  relations  of  the  data  points  are  often 
overcome  by  embedding  the  high-dimensional  space  into  the  2D 
canvas.  Principal  component  analysis  (PCA)  [19]  finds  the  two 
eigenvectors  associated  with  the  largest  variation  in  the  data 
(expressed by the largest positive eigenvalues) and then projects the 
data points into the plane spanned by these vectors. Other methods 
seek  to  create  a  mapping  from  high-dimensional  to  2D  space  that 
optimizes  for  some  measure  of  data  point  similarity.  MDS  [23] 
aims  to  preserve  some  distance  metric,  such  as  Euclidian  distance 
or  pattern  distance  [24].  Other  mappings,  such  as  ISOMAP  [34], 
LLE  [31],  SOM  [6][22],  t-SNE  [28],  LAMP  [18],  and  PLP  [30] 
optimize for geodesic distance, distribution distance, locality, etc.  

In  these  2D  embeddings,  the  viewer  can  easily  appreciate 
neighborhood  relations  and  obtain  a  good  overview  of  the  space 
quickly.  However,  these  methods  also  have  a  shortcoming  –  the 
mapped  data  points  no  longer  maintain  any  context  with  the 
attribute  space  as  this  information  is  typically not  preserved  in  the 
mapping. If users wish to see the relationships of both attributes and 
data  samples  then  two  separate  maps  need  to  be  created  using  the 
two alternative forms of the similarity matrix  S as presented in the 
introduction  –  one  for  the  samples  and  one  for  the  attributes.  But 
with two separately and independently created maps, it is difficult, 
if  not  impossible,  to  appreciate  the  mutual  relationships  of  the 
samples and their attributes – the context. The method we describe 
in this paper fuses the two alternative similarity matrices and so is 
able  to  create  an  embedding  in  which  the  relationships  among 
samples,  among  attributes,  and  among  the  two  of  them  is  equally 
well preserved.  We note that in practice we use a dissimilarity (or 
distance) matrix. Similarity is just the reverse of dissimilarity.   

2.4 

Fused displays 

The  work  on  fused  displays  is  relatively  rare.  One  recent 
implementation is by Broeksema  et  al.  [3] who similar  to us,  have 
also created a fused matrix of samples and attributes and used it for 
2D layouts. Our approach is different from theirs in multiple ways: 

Their  framework  is  primarily  designed  for  categorical  data. 
Numerical  data  are  binned  into  regular  intervals  which  can 
be 
inaccurate.  Conversely,  our  approach  starts  with 
numerical  data  by  default  and  could  use  the  approach  of 
Zhang et al. [37] to transform any categorical variables into 
numerical ones, taking into account the pairwise distribution 
relationships. 
They  use  a  linear  projection  approach  based  on  Multiple 
Correspondence Analysis (MCA) to create the 2D mapping. 
Our layouts are generated via numerical optimization  which 
can  support  a  variety  of  constraints  and  can  also  better 
preserve high-dimensional relationships. 
They compute a tiled Voronoi diagram to divide the domain 
into value regions which only accounts for the relationships 
among 
levels.  Our  approach 
generates  a  set  of  general  iso-contours  computed  from  a 
continuous  heat  map  of  the  data,  using  adaptive  kernel 
density  estimation.  The  extended  accuracy  this  affords  also 

the  attributes  and 

their 

3 

 

 

 

 

n 
V

m 

D
  

.... 

2 

D
  
  

  

V
3
  

D
  

1 
2 
V
  

m 

D
  

V1 
  

.... 

n 
V

2 
V
  

1 
V
  

(a)

                                                (b) 

 
Fig. 4. The two spaces: (a) data space D and (b) variable space V. 
  

3 

D
  

2 

D
  

1 

D
  

and is spanned by the n orthogonal attribute (or variable) axes (see 
Fig,  4a)  Conversely,  the  variable  space  V  contains  all  n  data 
attributes: 

𝑉𝑗 = [𝑥1𝑗, 𝑥2𝑗, … , 𝑥𝑚𝑗]′              (𝑗 = 1,2, … , 𝑛) 

and is spanned by a set of m orthogonal data item axes (Fig. 4b).  

The  data  space  D  is  the  more  familiar  of  the  two  but  there  are 
many  applications,  in  which  samples  can  turn  into  attributes  and 
vice versa depending on the focus of the analytics. For example, for 
a  data  matrix  storing  the  results  of  a  DNA  microarray  experiment 
for  multiple  specimens,  one  research  objective  might  consider  the 
genes  expressed  in  the  microarray  to  be  the  samples  and  the 
specimens to be the attributes, or vice versa.  

VD is not a transpose of DV, like Euclidian or correlation distance, 
we typically select one of DV or VD – using the one with the larger 
matrix  norm  –  and  computing  the  other  by  transposing  it.  In  this 
case, DV and VD become symmetric. 

Assembling the composite distance matrix (CM) 

3.2.2 
With all four constraint matrices in place, we can now assemble the 
composite  distance  matrix  CM  from  them.  The  fused  space 
composed  of  D  and  V  and  the  composite  distance  matrix  CM  are 
shown  in  Fig.  5.  We  can  now  use  it  within  an  MDS-like 
optimization  framework  to  achieve  the  2D  mapping  into  the  joint 
sample/attribute  display.  But  first  we  need 
to  make  some 
adjustments as is described in the following section.   

3.2 

The Composite Distance Matrix (CM) 

The  next  step  is  to  define  the  desired  distance  or  similarity  metric 
for  each  relationship.  Mapping  more  similar  items  into  closer 
proximity, we need to use (1-correlation), and (1-attribute value) etc, 
while the spatial distance metrics, such as Euclidian can be used as 
is. We have four different distance matrices: 

  DD to store the pairwise distance of data items 
 
VV to store the pairwise distance of attributes (variables) 
 
VD to store the pairwise distance of attributes to data items 
  DV to store the pairwise distance of data items to attributes  

DD is an n×n matrix with elements DDij=F(Di, Dj) and VV is a 
m×m  matrix  with  elements  VVij=F(Vi,Vj).  Fig.  6a  and  Fig.  6  b 
shows  an  MDS  layout  of  DD  and  VV  respectively  for  the  6  test 
Gaussians described in Section 2.2.  

The Data to Variables Distance Matrices (DV, VD)  

3.2.1 
The  DV  and  VD  matrices  are  new  types  of  matrices.  They  are 
required to enforce the distance/similarity constraints in the relation 
of the data samples with the attribute (dimension) anchors and vice 
versa. In the following, let us first consider DV – similar arguments 
also hold for VD. 

Referring  to  Fig.  4a  which  shows  the  data  space  D,  one  can 
make  the  argument that  an  attribute axis  is  essentially just  another 
data sample – a (fictional) data point with unit length, n dimensions, 
and  a  single  non-zero  component,  namely  a  value  of  1  for  the 
attribute’s dimension  j. So essentially, the attribute vector serves a 
dual role: (1) as a dimension axis and (2) as a data point. With this 
in  mind  we  can  then  impose  any  distance  metric  that  links  the  m 
data samples with the n attribute axes to fill the m×n matrix DV.    

The  derivation  of  the  matrix  VD  follows  a  similar  line  of 
thought. Just now we consider the variable space V depicted in Fig. 
4b where the axes are m-dimensional unit vectors each with exactly 
one dimension component set to 1. A point in that space is defined 
by the values a certain variable has for all of the data samples – one 
column of DM. For example, for a car dataset, if V1 is horsepower 
(hp)  and  V2  is  miles  per  gallons  (mpg)  and  we  have  two  cars  –  a 
VW  and  a  Ford  –  then  the  coordinates  for  V1  would  be  [hp(VW), 
hp(Ford)]  and  the  coordinates  for  V2  would  be  [mpg(VW), 
mpg(Ford)]. We can again impose any distance metric between the 
n V-points and the m points constituted by the D-vectors to fill the 
n×m matrix VD. 

We note that in order for CM to be a proper distance matrix, VD 
should be  a  transpose  of  DV. This, however, is not  necessarily the 
case,  even  when  normalizing  the  vectors  in  V  and  D  which  would 
place  all  distance  relationships  on  the  surface  of  a  hypersphere.  It 
occurs  because  V  and  D  have  different  dimensionalities  (and 
different  hyperspheres)  and  are  also  not  related  by  a  simple  scale 
factor.  The  only  similarity  metric  we  know  that  fulfils  this  matrix 
identity is (1-value), where ‘value’ is the value a space point SP has 
for  a  space  dimension  vector  SD’s  coordinate.  The  (1-value) 
distance can be thought of as a significance distance. It is small for 
a  given  data  point  when  the  value  of  a  point’s  attribute  is  large, 
encoding a notion of affinity that SP has for SD. We have used this 
distance  for  all  examples  shown  in  this  paper.  For  the  case  when 

 

4 

                   (a)                                                          (b) 

Fig.  5:  (a)  The  fused  space  composed  of  D  and  V  and  (b)  the 
composite distance matrix CM and the extents of its submatrices 
DD, DV, VD, and VV.  

3.3 

Fusion 

In order to merge or fuse the two spaces, V and D, in consideration 
of  the  four  distance  constraint  matrices,  VV,  DD,  DV  and  VD, 
defined on them we require a set of transformations – scale, rotation, 
translation. For the time being we have only implemented scaling.   
     The four matrices  VV, DD, DV and  VD that make up  CM  were 
not  created  equally.  They  have  been  calculated  from  vectors  with 
different lengths – n or m – and they may also have used different 
distance  metrics  F.  We  have  observed  that  this  inequality,  if  not 
compensated  for,  can  lead  to  cases  in  which  data  samples  and 
attributes may not mix well. That is, points due to the data samples 
and those due to the attributes may clump together into separate and 
disjoint communities.   

Thus, transformations are necessary to enlarge or shrink the data 

or variable spaces. Suppose, we have the transformation 𝜃: 

𝐷𝜃 = 𝜃𝐷(𝐷)                          𝑉𝜃 = 𝜃𝑉(𝑉)                        (1) 

where 𝐷𝜃 and 𝑉𝜃 are the transformed D and V, respectively. 

There are different ways to define the 𝜃. In order to mix the data 
and variables spaces well, we should balance the difference of each 
of the four matrices. One simple way to define 𝜃 or achieve this is 
to make the four sub-matrices (the entities in each submatrix) have 
equal  mean.  In  this  way,  the  two  spaces  have  equal  scale.  In 
addition, in order to keep the distance matrixes unite, we make the 
DV and VD also have these equal scales 

̅̅̅̅̅̅̅ = 𝐷𝜃𝑉𝜃
𝐷𝜃𝐷𝜃

̅̅̅̅̅̅̅ = 𝑉𝜃𝐷𝜃

̅̅̅̅̅̅̅ = 𝑉𝜃𝑉𝜃

̅̅̅̅̅̅                        (2) 

where  the  ¯  operator  denotes  the  mean  of  the  distance  matrix.        
There  are  different  options  to  make  these  four  distance  matrices 
have the same mean (or L1 norm) – we can use a linear, polynomial, 
or  kennel  function.  A  linear  function  has  the  advantage  that  it 
preserves the distribution, topology, etc. and thus, for this paper we 
apply  a  linear  weight  adjustment  for  each  submatrix.  In  this  way, 

the transform is a simple weight adjustment for each submatrix. The 
weights are obtained as: 

𝑊𝐷𝐷: 𝑊𝐷𝑉: 𝑊𝑉𝐷: 𝑊𝑉𝑉 =

𝑀𝑚𝑎𝑥
̅̅̅̅̅̅̅̅ :
𝐷𝜃𝐷𝜃

𝑀𝑚𝑎𝑥
̅̅̅̅̅̅̅̅ :
𝐷𝜃𝑉𝜃

𝑀𝑚𝑎𝑥
̅̅̅̅̅̅̅̅ :
𝑉𝜃𝐷𝜃

𝑀𝑚𝑎𝑥
̅̅̅̅̅̅̅̅         (3) 
𝑉𝜃𝑉𝜃

where 𝑊 is the weight for the submatrix and Mmax is the maximum 
mean of all the submatrices.  

3.4  Mapping 

With the composite distance matrix CM in hand, the final step is to 
create the joint map of samples and attribute points. We have opted 
to use an optimization approach for the map layout, as opposed to a 
linear  projection  with  PCA  or  biplots  since  it  gives  us  more 
freedom  in  choosing  the  constraints  governing  the  layout,  such  as 
mixed  distance  functions,  layout  schedules,  and  mapping  criteria. 
There are a number of distance-preserving optimization algorithms 
applicable for our purposes. LLE produces locally optimal layouts, 
while  MDS-type  schemes  create  globally  optimal  layouts  which 
have  become  more  popular  in  recent  years  since  they  provide  a 
consistent  overview  of 
linear 
discriminant  analysis  (LDA)  [9]  excel  in  their  ability  to  isolate 
individual  clusters,  but  they  have  a  reduced  ability  to  preserve  the 
statistical appearance of the clusters which we feel is important for 
visualization.  We  have  therefore  chosen  a  metric  MDS  approach. 
Particularly  useful  here  is  the  iterative  and  progressive  point 
insertion  schedule  of  Glimmer  MDS  [16].  We  have  adopted  this 
multi-level  scheme  for  our  framework  since  it  allows  us  to 
implement a variety of strategies for controlling the layout.    

the  data.  Finally, 

t-SNE  or 

One  of  these  strategies  makes  use  of  the  weighting  scheme  for 
handling  the  submatrices  of  CM,  as  proposed  in  the  previous 
section.  It  results  in  a  rather  general  framework  and  offers  much 
freedom to design a visualization that fits current criteria of interest. 
Users  can  simply  assign  the  default  weights  that  give  equal 
emphasis to all submatrices or they can increase the weight for one 
of more submatrices that influence those aspects they would like to 
focus  on.  For  example,  a  user  might  want  to  have  an  accurate 
representation  of  the  relationships  among  the  samples  and  of  the 
samples  to  the  variables  but  is  less  interested  in  an  accurate 
representation  of  the  relationships  the  attributes  have  with  one 
another.  So  he/she  would  increase  WDD,  WDV  and  at  the  same 
amount  WVD,  but  reduce  WVV.  Reducing  one  or  more  constraints 
will  enable  the  mapping  algorithm  to  trade  the  precision  losses 
incurred  for  these  unimportant  relations  in  favor  of  those  that  are 
less desirable. Essentially, it serves as a buffer of the errors that are 
incurred  with  the  necessarily  imperfect  space  embedding.  Section 
4.1.1 describes another mechanism by which users can express their 
emphases  –  the  scheduling  of  the  data/variable  primitives  in  the 
MDS-like layout.   

(a)
  

(b)
  

(c)
  

(d)
  

(d)
  

Fig. 6: Layout experiment for the 6  Gaussian test dataset. (a) MDS 
layout  of  the  data  samples  (Euclidian  distance);  (b)  MDS  layout  of 
the  attributes  (correlation  distance);  (c)  parallel  coordinate  display 
with node colors marked; (d) MDS layout of samples and attributes 
using the CM matrix (samples: Euclidian, attributes: correlation).    

 

5 

3.5 

A First Example 

Fig.  6  shows  a  first  result  achieved  with  this  mapping  using  the  6 
test Gaussians introduced in Section 2.2. Fig. 6a is the MDS layout 
for  just  the  data  samples  using  the  Euclidian  distance  metric;  Fig. 
6b  is  a  MDS  layout  for  the  attributes  using  Pearson’s  correlation 
distance  [33];  Fig.  6c  is  the  layout  created  with  MDS  using  the 
entire  CM matrix and  weights set to  not give  emphasis to  any  CM 
submatrix,  and  Fig.  6d  is  the  parallel  coordinate  display  for  this 
dataset  with  the  axes  marked  with  the  colors  used  for the  attribute 
nodes in  Fig. 6b and c. We  used the (1-value) distance for the  DV 
and VD submatrices.   

We  first  observe  that  the  layout  of  the  clusters  in  the  sample-
only  MDS  plots  has  been  well  preserved  in  the  CM-based  MDS 
layout. On the other hand, the locations of the attributes, while still 
largely  isolated  to  account  for  the  correlation  differences,  have 
changed  and  better  match  the  associations  they have  with  the  data 
clusters. This shows that the fusion of the two spaces D and V is not 
just a trivial superposition of the two plots.  

Some  more  specific  observations  we  make  are:  (1)  the  red 
cluster  has  a  clear  dominance  in  the  red  attribute  and  indeed  its 
dimension node gets mapped right into the red cluster’s center, (2) 
the green and the brown cluster both have high values in the green 
attribute  and  so  the  green  attribute’s  node  gets  mapped  between 
these two clusters, (3) similar is true for the brown attribute and the 
red  and  brown  data  clusters;  (4)  the  dark  blue  and  black  attributes 
have somewhat similar (but switched) relationships with respect to 
high  values  of  the  black  and  dark  blue  clusters  and  so  they  get 
mapped more closely to each other right between these two clusters.  
On closer inspection of Fig. 6 it appears that lower levels in the 
attributes are being taken into lesser or no account in CM’s layout. 
This  can  be  explained  by  the  distance  metrics  we  chose  for  this 
particular case. The preference of the algorithm in picking attribute 
locations  with  respect  to  high  values  of  the  data  clusters  is  due  to 
the (1-value) distance we selected for the DV and VD submatrices. 
The  behavior  would  change  had  we  chosen  a  different  distance. 
This  and  other  choices,  as  well  as  their  effects,  largely  depend  on 
the aspects in the data the analyst would like to emphasize. Here in 
this example the emphasis was on extreme values.  

4  CONSTRUCTI NG  THE D AT A C ONTEXT M AP (DCM) 

In this section we provide more details on the map construction and 
its segmentation into regions of similar properties.  

 Populating the map 

4.1.1 
The submatrices of CM can not only be weighted differently during 
the  MDS  layout,  we  can  also impose  different  MDS  schedules  for 
the  samples  and  the  attribute  points.  We  take  advantage  of  this 
concept to achieve layouts with different priorities.  

We  require  an  iterative  MDS  algorithm  to  achieve  this  goal. 
Iterative  MDS  algorithms  often  do  not  update  all  points 
simultaneously  at  each  step.  Rather,  they  select  a  subset  of  points 
that is allowed to move, while another stays put, either indefinitely 
after an initial layout or the point sets alternate. The point sets can 
also  be  transient  and  can  change  over  time.  A  particularly 
convenient algorithm in this regards is the Glimmer MDS (G-MDS) 
algorithm [16]. It has a stochastic force algorithm which iteratively 
moves  each  point  until  a  stable  state  is  reached.  The  forces  acting 
on a  point are based on a  Near Set of points and a  Random  Set of 
points.  The  Near  Set  contains  those  points  that  are  nearest  to  the 
point  being  updated.  The  Random  Set  contains  points  that  are 
randomly  chosen  from  the  set  of  available  points.  It  ensures  some 
global  control  in  the  update  process.  We  have  altered  the  standard 
Glimmer MDS framework in two ways. First, we manipulate which 
types of points – variables or data – are allowed to be chosen for the 
Random  Set.  Second,  we  manipulate  which  types  of  points  are 
allowed to be updated. Both change the local minimum of G-MDS 
as it is a metric MDS scheme using non-convex optimization. 

First use case for the car dataset 

4.1.2 
We  use  the  UCI  Auto  MPG  dataset  for  our  first  non-toy  example. 
This  dataset    has  392  cars  built  1983  or  older  with  7  attributes  – 
MPG,  #cylinders  (CYL),  horsepower,  weight,  acceleration,  year, 
and origin (US, Japan, Europe). Note that acceleration is the time a 
car requires to reach 60 mph and so slower cars have higher values. 
Fig. 8 shows a data context map generated via M-MDS. In this map, 
the  large  red  points  represent  the  attributes  while  the  small  blue 
points represent the cars. Cars that locate close to a given attribute 
node have high values for this attribute. On the other hand, cars that 
locate far away from a certain attribute node have a low value for it.  

Using  this  flexible  update  scheme  we  currently  provide  four 
MDS  schedules:  (1)  Update  the  variables  and  the  data  points 
simultaneously (M-MDS); (2) Map the variables first, then fix them 
and  only  map  the  data  (VF-MDS);  (3)  Map  the  data  first,  then  fix 
them,  and  only  map  the  variables  (DF-MDS);  (4)  the  user  defined 
order (U-MDS). We describe each of these in turn in more detail.  

(a) Update all types of points simultaneously (M-MDS) 

This  first  schedule  is  the  most  general.  It  only  runs  G-MDS  once 
and both types of points can be in the Random Set. See Fig. 7a. 

(b) Update variables first, then the data (VF-MDS) 

Here  the  goal  is  to  achieve  a  layout  that  prioritizes  the  fidelity  of 
the variable-variable (V-V) distances. It runs G-MDS two times. In 
the first run only the V-points are entered into the G-MDS point set. 
This  results  in  an  accurate  V-layout.  Then  we  run  G-MDS  the 
second  time  with  the  V-points  frozen.  Essentially,  we  add  a 
statement  that  disallows  the  selection  of  a  V-point  for  update,  that 
is,  only the  data  points  (D-points)  are  allowed  to  move.  Since  this 
has the tendency to drive the D-points away from the V-points we 
only  allow  V-points  in  the  Random  Set.  This  preserves  the 
influence the V-points have on the layout of the D-points. See Fig. 
7b. 

(c) Update data first, then the variables (DF-MDS) 

This  is  essentially  the  reverse  of  the  VF-MDS  scheduling  scheme 
and  prioritizes  the  fidelity  of  the  data-data  (D-D)  distances.  This 
schedule  also  has  two  stages.  First  G-MDS  is  run  in  the  D-points 
only. Next, G-MDS is run on the V-points with the D-points frozen 
and the D-points are only allowed in the Random Set. See Fig. 7c.  

(d) User-defined iteration schedule (U-MDS) 

Fig.8. The data context map for the car data.   

The  three  schemes  just  presented  are  very  basic  update  schedules 
and maybe there are better ones. For this purpose we allow users to 
draw a customized schedule via a timing (iteration) diagram editor. 
It first runs the VF-MDS schedule for a few iterations, then the DF-
MDS schedule, and finally the M-MDS schedule. See Fig. 7d. 

(e) Comparing the schedules 

Comparing  the  layouts  achieved  with  the  different  schedules  we 
observe  that  for  VF-MDS  the  variable  to  variable  error  is  lowest 
and for the DF-MDS the data to data error is lowest. It also appears 
that M-MDS and U-MDS are good compromises. It depends on the 
user’s priorities which method to choose. Considering the accuracy 
and  complexity,  we  normally  choose  the  first  schedule,  but  the 
others are also useful for different preferences.  

We  observe  that  there  are  two  main  populations  of  correlated 
attributes. On one side there are horsepower, weight, and CYL, and 
on  the  other  there  are  acceleration,  mpg,  and  year.  Origin  is 
somewhat  separate.  We  can  also  observe  four  distinct  clusters  of 
cars (with some sub-clusters) which are all heavily elongated in the 
vertical direction. Their relation with the attributes reveals that each 
cluster  has  a  fairly  large  diversity  in  car  attributes.  Using  the 
attribute  nodes  as  landmarks  we  can  now  gauge  the  types  of  cars 
these  clusters  contain.  For  example,  the  cluster  in  the  lower  left 
contains the large high-performance cars with high horsepower and 
weight. The other clusters are more difficult to judge since they are 
so elongated and span a large attribute interval.   

 The map can be readily used for informed selection tasks. The 
user would simply look for features he is most interested in (or not 
at  all),  observe how  many cars are  actually available that  have the 
desired  feature  constellation,  and  then  select  cars  near  these 
attributes (or far away depending on preference). For example, the 
user  may  be  interested  in  a  full-sized  car,  clicks  on  a  node  in  that 
region on the map, and uncovers a 1975 Pontiac Catalina which is 
an  entry-level  full-size  car  (red-circled  sample  node  in  bottom 
cluster).  Or  he  may  be  interested  in  a  newer  economic  car  and  so 
selects a node close to the year attribute and fairly close to the mph 
and  acceleration  attribute.  He  correctly  finds  a  newer  (for  the 
dataset) 1982 Chevy Cavalier which is an economy-grade compact 
car (red-circled sample node in top left cluster).   

(a)                                         

(c)                                         

(b)                                         

Error evaluation   

4.1.3 
Since  our  data  context  map  is  a  2D  optimized  layout,  there  is 
necessarily an error. As in every layout scheme we can estimate the 
error  by  comparing  the  distance  in  the  matrix  CM  with  the 
corresponding Euclidian distances in the 2D layout. We can use 𝐷𝐷̃ , 
𝐷𝑉̃ , 𝑉𝐷̃  and 𝑉𝑉̃  to  store  the  2D  layout  distances,  respectively.  A 
popular  metric  to  summarize  the  layout  error  is  stress  [23].  The 
error E in each sub-matrix is:  

(d)                                         

Fig.7.  MDS  layout  schedules.  (a)  M-MDS  layout.  (b)  VF-
MDS. (c) DF-MDS (d) U-MDS. 

                   𝐸𝐼𝐽 = √

∑

(𝑖∈𝐼,𝑗∈𝐽)

(𝐼𝐽𝑖𝑗−𝐼𝐽̃ 𝑖𝑗)2

∑

(𝑖∈𝐼,𝑗∈𝐽)

2
𝐼𝐽𝑖𝑗

      𝐼, 𝐽 ∈ {𝐷, 𝑉}           (4) 

 

6 

The overall error EA is also weighted based on different blocks, 

                                          𝐸𝐴 = ∑

𝐼,𝐽∈{𝐷,𝑉}

𝛽𝐼𝐽𝐸𝐼𝐽

                              (5) 

where βIJ is the weight. Typically, we set the βIJ to: 

𝛽𝐷𝐷: (𝛽𝐷𝑉 + 𝛽𝑉𝐷): 𝛽𝑉𝑉 = 1: 2: 4                    (6) 

As  mentioned  in  Section  2.2,  we  have  used  the  GBC  plot  as  the 
standard formulation to describe the set of interior displays, and we 
also presented an optimized plot to improve the GBC plot error [7], 
called DIFGBC. In this section we compare this error with the one 
we can now reach with the data context map (DCM).Table 1 below 
compares the error for three datasets we studied.  

We find that EVV improves greatly – this is because the interior 
layouts map the variables to the 1-dimensional space (the boundary 
of  the  enclosing  shape)  but  the  DCM  maps  them  into  2-
dimensional space which naturally incurs less error. The EDD error 
also  greatly  improved,  but  the  EDV  error  did  not  or  even  grew 
slightly for these examples, but this is also dependent on the update 
schedule,  the  distance  metric,  and  the  weighting.  Yet,  the  overall 
error  improved  greatly  and  this  quantitatively  shows  our  data 
context  map  is  more  accurate  than  the  competing  interior  layouts 
even when optimized. 

Table 1. Comparing the error of the optimized GBC plot and the DCM 

DataSet 

Car 

University 

Campaign 

DCM 

DIFGBC 

DCM 

Layout 
DIFGBC 

𝐸𝑉𝑉 
0.34 
0.16 
2.07 
0.38 

𝐸𝐴 
0.3 
0.19 
1.35 
0.39 
DIFGBC  0.33  0.26  0.31  0.31 
0.16  0.23 

𝐸𝐷𝐷 
0.23 
0.17 
0.49 
0.36 

𝐸𝐷𝑉 
0.25 
0.27 
0.32 
0.41 

DCM 

0.22 

0.3 

Up% 

36.7% 

71.1% 

25.8% 

4.2 

Segmenting the Map 

The  data  context  map  as  presented  so  far  already  allows  attribute-
informed  selection  of  data  objects,  as  we  have  demonstrated  in 
Section  4.1.6.  But  it  was  somewhat  difficult  to  judge  the  different 
value regions for combinations of attributes. This would be easy if 
the map could be somehow colored into distinct spatial areas which 
then  could  each  be  tagged  by  the  respective  attribute  value 
combinations.  To  achieve  this  goal  we  require  a  continuous 
representation  of  the  map.  We  have  used  adaptive  kernel  density 
estimation (AKDE) for this purpose.   

 Adaptive Kernel Density Estimation (AKDE) 

4.2.1 
The  AKDE  [21]  is  a  method  for  estimating  the  density  of  a  point 
cloud.  It  first  estimates  the  local  density  of  each  sample  and  then 
shrinks  or  enlarges  the  sample’s  bandwidth.  Suppose  we  have  N 
points and each point is marked as Pi with a fixed bandwidth H. For 
any point P, its local density f is obtained by: 

𝑓(𝑃) =

1

𝑁

∑ 𝐾𝐻(‖𝑃 − 𝑃𝑖‖)

𝑁
𝑖=1

                    (7) 

density estimation (KDE) (Fig. 9a) with AKDE (Fig. 9b) for the car 
dataset. In this figure the brighter values correspond to lower values 
and vice versa. Consider the regions pointed to by the yellow arrow 
where we can see two separate regions for the AKDE, while these 

  

  

(a)                                         
a)                                         

(b)                                         
b)                                         

Fig.9.The KDE (a) and AKDE (b) show the density of the data points 
respectively.    

regions  appear  mixed  together  for  the  KDE.  There  are  also  other 
examples in the map where AKDE gives a more accurate estimate 
of the local density.  

Creating the attribute distance field using AKDE 

4.2.2 
We  estimate  the  values  in  the  continuous  map  based  on  adaptive 
kernels – when the point has a higher density, it would have lower 
bandwidth to shrink its effect area, and vice versa. Then, based on 
the  adaptive  kernel  distance,  we  use  Nadaraya-Watson  kernel 
regression [27][35] to obtain the estimated value. Suppose the value 
at Pi is xi, then the value x at the estimated point P is 

𝑥 = ∑

N
i=1

KH(‖P−Pi‖)∙xi
N
∑
KH(‖P−Pj‖)
j=1

                           (9) 

where  KH  is  the  kennel  function.  Here  we  choose  Gaussian 
function. However, some areas on the 2D canvas are far away from 
the  samples  and  are  therefore  undefined.  Thus  it  is  important  to 
control  the  border  of  the  map  and  remove  these  undefined  areas. 
We set the threshold ε for the sum distance – if the estimated point 
is far away from all the samples, we ignore it. 

∑

𝑁
𝑗=1

𝐾𝐻(‖𝑃 − 𝑃𝑗‖) ≥ 𝜖

                          (10) 

Fig.  10  shows  how  we  convert  the  point  map  into  a  distance 
heatmap using AKDE. Here we first color the data points based on 
their  values  (here,  of  the  horsepower  attribute,  Fig.  10a)  and  then 
generate  the  heatmap  based  on  AKDE-based  interpolation  (Fig. 
10b). We can  see  that the  AKDE can estimate the  values well and 
the border of this heatmap is also well defined.  

where  ||.||  is  the  L2  distance.  We  can  then  estimate  the  local 
smoothing  parameter  𝜆𝑖  and  from  it,  the  new  bandwidth  Hi  for 
adaptive smoothing:  

λi = (G/f(Pi))2                𝐻𝑖 = 𝐻 × 𝜆𝑖              (8) 

 

 

where G is the geometric mean of all the samples local density.  

The  adaptive  bandwidth  of  the  AKDE  kernels  makes  sure  that 
small dense regions are preserved and not over-smoothed while less 
dense  regions  are  properly  fused.  Fig.  9  compares  fixed  kernel 

Hpower 

Hpower 

(a)                                         
a)                                         

Fig.10. The samples’ (a) “Horsepower” values  and (b) its heatmap.  

(b)                                         
b)                                         

 

7 

Creating the contour fields 

4.2.3 
Just by using the distance heatmap alone it is difficult to make out 
actual  values.  A  common  technique  to  visualize  distance  fields  is 
via topographic maps. Then if a point is within a certain pair of iso-
contours  we  can  easily  read  off  its  value.  We  generate  these 
contours  via  the  conrec  algorithm  [1].  Fig.  11a  shows  the  contour 
field of Fig. 10a, for the horsepower attribute. We observe that the 
contour  region’s  value  decreases  level  by  level  as  we  move  away 
from the attribute node.   

 The contour field can also compare the  layouts generated  with 
standard  MDS  and  our  DCM.  Fig.  11b  shows  the  contour  field 
generated from a distance heatmap based on a standard MDS layout 
(also  using  Glimmer  MDS  but  without  using  attribute  points).  We 
find that the contour field has a rather ragged appearance with many 
more  islands  than  the  one  generated  from  the  DCM.  In  the  data 
context  map,  on  the  other  hand,  the  attribute  nodes  attract  high-
valued  points  and  push  low-valued  points  away.  This  magnetic 
force  organizes  the  samples  and  so  a  smooth  distance  field  can  be 
created.       

 

 

(a)                                         (b)                                         

Fig.11. The data context map contour (a) and MDS plot contour (b). 

Hpower 

Hpower 

  

  

  

MPG 

  

(a)                                         

(b)                                         

MPG 

  

Origin 

(c)                                         
 

 

(d)                                         

MPG 

Origin 

Creating the decision regions 

4.2.4 
Each attribute gives rise to a set of contours, and a closed range of 
attribute  values  gives  rise  to  a  filled  region  between  the  two 
corresponding  contours.  Fig.  12a  shows  such  a  region  for  the 
horsepower  range  (120~230).  We  emphasize  that  this  region  has 
been  computed  from  the  value  field  generated  by  the  actual  data 
samples  and  so  any  sample  selection  that  is  based  on  it  will  be 
accurate. As such any of the cars that get mapped into the salmon-
colored region in Fig. 12a indeed has a horsepower value in it.  

Fig.  12b  shows  the  iso-region  for  the  (15-46)  mpg  value  range 
which  we  can  obtain  in  a  similar  fashion.  This  purple  region 
contains all cars that have a mpg rating in that range. Next we can 
superimpose these regions to create the joint map shown in Fig. 12c. 
This  joint  map  has  three  regions.  The  first  is  due  to  the  original 
horsepower range only, the second is due to the original mpg range, 
and  the  third,  overlapping  region  blending  into  a  darker  salmon 
color, contains cars that fit both value ranges. So if we wanted a car 
that fits both criteria we would pick a car from this overlap region.  
Finally,  we  add  a  third  constraint  –  origin.  Origin  is  a  discrete 
variable and we select the value 2  – the European cars. This gives 
rise  to  the  green  region  in  Fig.  12d.  Blending  it  with  the 
horsepower-mpg  joint  map  creates  the  triple-attribute  joint  map 
shown in Fig. 12e. Now if we wanted to buy a car that is European 
and fits the other two range constraints we would look into the olive 
green  region  on  the  lower  right.  There  are  still  some  choices.  We 
could pick a car on the upper boundary of that region which would 
be a more efficient car but with less horsepower. There is a car that 
fits the bill, which has been circled in the figure. Alternatively, we 
could  pick  a  car  from  the  left  region  boundary  which  would  be  a 
less efficient car but with a bit more muscle. There is also a car that 
fits  this  preference.  However,  if  we  sought  to  find  a  car  that 
represents  a  compromise  of  mpg  and  horsepower  –  one  that  falls 
right into center of the region – we learn from the map that there is 
no  such  car  in  the  database.  There  are  obviously  many  more 

 

8 

Hpower 

(e)                                         

Fig.12.  The  decision  of  (a)  “Hpower”  (120~230),  (b)  “MPG”  (15~46) 
and (d) “Origin” (“European”) and (c) (e) their merge process. 

explorations we can do with this map in hand. Since the interface is 
fully  interactive,  the  user  is  free  to  modify  his  preferences  in  real 
time and fit the map to these preferences.  

Creating a fully segmented and self-labeled map 

4.2.5 
Now suppose we have k attributes and each attribute can be divided 
into lk levels based on users’ preference. For example, these levels 
can be high level, middle level, low level etc. Then we can encode 
the  entire  area  and  see  the  combination  of  these  attributes.  Each 
region  can  then  be  encoded as [𝑅1, 𝑅2, … , 𝑅𝑘],  where  Ri  represents 
the level in the each factor i and 𝑅𝑖 ∈ [0, … , 𝑙𝑖]. We can divide the 
domain based on these codes and color the regions. However, it is 
important to maintain the color connection such that users can read 
the  combination  of  different  colors.  We  first  assign  the  color  for 
each attribute and let distances between colors are as big as possible. 
We set the intensity of each color based on the contour range level. 
Finally, when a region is composited we blend these colors. 
      Fig.13 shows an example for three attributes horsepower, mpg, 
and origin. We choose two levels - low or high (we set 40% as the 
threshold).  For  origin  we  split  the  set  between  Euro-Japanese  cars 
and  US  cars.  We  give  each  attribute  the  color  shown  inside  the 
attributes’  symbols  and  then  color  the  entire  domain  via  color 
blending.  We  can  now  color  each  of  the  regions  depending  on 
levels  of  the  participating  attributes.  The  legend  below  the  figure 
lists  a  human-created  annotation  for  the  regions.  However,  such  a 
labelling  could  also  be  done  automatically,  using  the  levels  of  the 
attributes in each region to support natural language generation.   

2 
 

3 
  

4 
  

5 
  

1 
 

6 
  

Euro-Japanese efficient compact cars 

  US efficient compact cars 
   US semi-efficient medium-power cars 
   US big block gas guzzlers 
  Euro-Japanese gas guzzlers 
   Euro-Japanese semi-efficient medium-power cars 

Fig.13.  The  fully  segmented  and  self-labeled  map  based  on 
“Horsepower”, “MPG” and “Origin”. 

5  C ASE STUDI ES 

5.1 

Selecting a College 

Let us now return to the scenario we mentioned in the introduction 
– selecting a college. Our database has 46 universities distinguished 
by 14 attributes of interest: academics, athletics, housing,  location, 
nightlife,  safety,  transportation,  weather,  score,  tuition,  dining, 
PhD/faculty,  population,  and  income.  Now  suppose  there  is  a 
prospective student, Tom, who is looking for a university. He aims 
for  a  school  that  has  high  athletics  (>9),  high  academics  (>9),  but 
low  tuition  (<$18,000).  He  searches  the  universities  with  a 
traditional browser, but sadly he cannot find one which can meet all 
three  requirements  at  the  same  time.  He  knows  that  he  needs  to 
make a compromise, trading  off a  few factors, and find  the  school 
that  offers  the  right  balance.  This,  however,  he  finds  hard  to  do 
because  he  does  not  even  know  what  his  personal  good  balance 
really is. He wants to see “what’s out there” and get inspired. So he 
calls  up  the  data  context  map  to  immersive  himself  into  the 
landscape of schools to find the elusive balance.  

He  begins  by  generating  the  decision  boundaries  based  on  his 
three criteria. This is shown in the teaser image, Fig.1a-d. Then he 
merges them and gets Fig. 1e. He (once more) recognizes that there 
is no university that can satisfy all three criteria at the same time – 
for example, the green tuition region does not overlap with the two 
other regions simultaneously. But now he sees in one view what his 
options  are.  He  notices  a  few  schools  that  meet  two  of  his 
conditions – those schools that fall into two-layer overlap areas. He 
picks  a  few  that  are  closest  to  the  third  layer  at  “just  the  right 
distance” as he describes it – the schools labelled A, B, and C in Fig. 
1e  He  says  that  he  likes  A  “because  it  has  good  athletics  and  low 
tuition, while the academics is not stellar but alright”. Similarly, B 
is good and he’d be “OK with paying a bit more tuition for the great 
value.” Finally, school C has good academics and low tuition which 
is great because he “could just use the savings to buy a big screen 
TV to watch the games of other schools”. Nevertheless, he picks A 
and lives happily ever after.   

 

9 

5.2 

Analyzing the business priorities 

Our second case study demonstrates the utility of the fused display 
of  samples  and  attributes.  We  peek  into  an  analysis  session  of  a 
group of top level managers of a multinational company with many 
subsidiaries  in  different  countries.  The  topic  is  to  determine  the 
different  priorities  these  companies  have  when  it  comes  to  sales 
strategy and long term goals. They have 600 samples of sales team 
data  with  10  attributes:  #Leads  (generated),  #Leads  Won  (LW), 
#Opportunities  (generated),  Pipeline  Revenue  (Rev),  Expected 
Return  on  Investment  (EROI),  Actual  Cost  (Cost),  Cost/WonLead 
(Cost/LW), Planned Revenue (Rev), and Planned ROI (PROI). The 
highly  paid  visual analytics  consultant  the  firm  has  hired  pulls  out 
the  data  context  map  and  with  a  few  mouse  clicks  produces  the 
visualization shown in Fig. 14. It is quickly seen that there are three 
clusters, call them red group, blue group, and green group. It turns 
out these three groups have rather different strategies and priorities. 
      The  red  group’s  focus  is  dominated  by  #Opp,  PROI, 
Cost/WL,  where  they  have  high  values  and  achievements.  At  the 
same time, however, they score very low in #Leads, LW, Rev, etc. 
The members of this group tend to focus on the individual leads and 
invest a lot in these, and as a result they have usually a high number 
of  opportunities.  The  blue  group,  in  contrast,  are  possibly  larger 
companies – they have high revenue and they can generate a large 
amount  of  Leads.  The  green  group  is  dominated  by  PRev 
and  %Comp.  Since 
revenue, 
their  %completed is high. But clearly all  groups have one thing in 
common  –  cost.  This  factor  has  equal  distance  for  all  of  them.  It 
means they all care about the cost with similar weights. 

they  have  high  expected 

  

Fig.14. The data context map for the business priority case study. 

6  CONCLUSI ON 

We have described the data context map – a framework and visual 
interface  that  enables  a  comprehensive  layout  of  both  data  points 
and variables. We achieve it by fusing two  distance matrices – the 
data  and  the  attribute  distance  matrix.  We  create  an  optimized 
layout  that  can  be  used  for  in  data-driven  decision  selection  and 
decision problems that require a mindful balancing of trade-offs. 

While  we  provide  several  parameters  for  experts  to  guide  the 
layout  for  their  goals,  they  are  not  essential  to  produce  usable 
results.  Casual  users  can  just  use  the  pre-set  weights,  upload  the 
data,  generate  the  initial  map,  and  interact  with  the  value  sliders. 
Future work will explore how casual users actually do this.  

AC KNOWLEDGEM ENT S 

This research was supported by NSF grant IIS 1117132 and by the 
MSIP (Ministry of Science, ICT and Future Planning), Korea, under 
the  “ICT  Consilience  Creative  Program”  supervised  by  the  IITP 
(Institute for Information & Communications Techn. Promotion)"". 

[29]  M.  Meyer,  A.  Barr,  H.  Lee,  M.  Desbrun,  “Generalized  Barycentric 
Coordinates  on  Irregular  Polygons,”  J.  Graphics  Tools,  7(1):13-22, 
2002. 

[30]  F. Paulovich, C. Silva, L. Nonato, “Two-Phase Mapping for Projecting 
Massive  Data  Sets”,  IEEE  Trans.  Vis.  Comput.  Graph.  16(6):  1281-
1290 (2010) 

[31]  L. Saul  ,  S. Roweis, “An Introduction to Locally Linear Embedding” 

IJPRAI 01/2009; 23:1739-1752. DOI: 10.1142/S0218001409007752. 

[32]  R. Spence, Information Visualization, Addison-Wesley 2000 ISBN: 0-

201-59626-1. 

[33]  B. Tabachnick and L. Fidell, Using Multivariate Statistics, New York: 

Harper & Row, 2001. 

[34]  J.  Tenenbaum,  V.  de  Silva,  J.  C.  Langford,  “A  Global  Geometric 
Framework  for  Nonlinear  Dimensionality  Reduction,”  Science  290,  
2319–2323, 2000. 

[35]  G.Watson. ""Smooth regression analysis"". Sankhyā: The Indian Journal 

of Statistics, Series A 26 (4): 359–372. JSTOR 25049340, 1964. 

[36]  J.  Yi,  R.  Melton,  J.  Stasko,  J.  Jacko,  ""Dust  &  Magnet:  Multivariate 
Information  Visualization  using  a  Magnet  Metaphor,""  Information 
Visualization, 4(4) : 239-256, 2005. 

[37]  Z. Zhang, K. T. McDonnell, E. Zadok, K. Mueller, ""Visual Correlation 
Analysis of Numerical and Categorical Data on the Correlation Map,""  
IEEE  Trans.  on  Visualization  and  Computer  Graphics,  21(2):  289-
303, 2015 

REFERENCES 

[1]  P.  Bourke,  “CONREC:  A  contouring  subroutine,” Byte:  The  Small 

[2] 

Systems Journal 12 (6): 143–150, 1987. 
I.  Borg  and  P.  Groenen  (2005).  Modern  multidimensional  scaling 
theory and applications. 2nd edition. New York: Springer. 

[3]  B.  Broeksema,  A.  Telea,  T.  Baudel,  “Visual  Analysis  of  Multi-
Dimensional  Categorical  Data  Sets,”  Computer  Graphics  Forum, 
32(8): 158-169, 2012. 

[4]  B.  Broeksema,  T.  Baudel,,  A.  Telea,  P.  Crisafulli,  “Decision 
exploration lab: A visual analytics solution for decision management,” 
IEEE Trans. on  Visualization and Computer Graphics, 19(12), 1972-
1981, 2013. 
J.  Chambers,  W.  Cleveland,  P.  Tukey,  Graphical  Methods  for  Data 
Analysis, Duxbury Press, 1983. 

[5] 

[6]  S. Chen, D. Amid, O. Shir, L. Limonad,D.  Boaz, A. Anaby-Tavor, T. 
Schreck,  ""Self-organizing  maps  for  multi-objective  Pareto  frontiers,"" 
Proc. IEEE Pacific Visualization Symposium, pp.153-160, 2013. 

[7]  S.  Cheng,  K.  Mueller,  ""Improving  the  Fidelity  of  Contextual  Data 
Layouts  Using  a  Generalized  Barycentric  Coordinates  Framework,"" 
Proc. Pacific Vis, pp. 295-302, 2015. 
J. Choo, S. Bohn, H. Park. “Two-stage framework for visualization of 
clustered high dimensional data,” Proc. IEEE Visual Analytics Science 
and Technology Conference (VAST), pp. 67-74, 2009. 

[8] 

[9]  R. Duda, P. Hart, and D. Stork. Pattern Classification. Wiley, 2000. 
[10]  K. Gabriel, “The Biplot Graphic Display of Matrices with Application 

to Principal Component Analysis,” Biometrika, 58(3): 453-467, 1997. 

[11]  G. Grinstein, M. Trutschl, U, Cvek, “High-dimensional visualizations,” 

Proc. Visual Data Mining Workshop, KDD, 2001. 

[12]  J.  Hartigan,  ""Printer  graphics  for  clustering,""  Journal  of  Statistical 

Computation and Simulation,4(3):187-213, 1975. 

[13]  P.  Hoffman,  G.  Grinstein,  K.  Marx,  I.  Grosse,    E.  Stanley,  ""DNA 
Visual and Analytic Data Mining"",  Proc. IEEE Visualization, pp. 437-
441,  1997. 

[14]  P.  Hoffman,  G.  Grinstein,  D.  Pinkney,  “Dimensional  anchors:  a 
graphic  primitive  for  multidimensional  multivariate 
information 
visualizations,”  Proc.  Workshop  on  New  Paradigms  in  Information 
Visualization and Manipulation, pp. 9-16, 1999. 

[15]  https://colleges.niche.com/ 
[16]  S. Ingram, T. Munzner, M. Olano, “Glimmer: Multilevel MDS on the 
GPU,”  IEEE  Trans.  Visualization  and  Computer  Graphics,  15(2): 
249–261, 2009. 

[17]  A.  Inselberg,  B.  Dimsdale,  “Parallel  Coordinates:  A  Tool  for 
Visualizing Multi-Dimensional Geometry,” Proc. IEEE Visualization, 
pp. 361-378, 1990. 

[18]  P.  Joia,  D.  Coimbra,  J.  Cuminato,  F.  Paulovich,  L.  Nonato,  “Local 
Affine  Multidimensional  Projection”,  IEEE  Trans.  Vis.  Comput. 
Graph. 17(12): 2563-2571 (2011). 

[19]  I. Jolliffe,  “Principal Component Analysis,” Series: Springer Series in 
Statistics, 2nd ed., Springer, NY, 2002, XXIX, 487  pp.28 illus. ISBN 
978-0-387-95442-4. 

[20]  E.  Kandogan,  “Star  Coordinates:  A  Multi-Dimensional  Visualization 
Technique  with  Uniform  Treatment  of  Dimensions,”  Proc.  IEEE 
Information Visualization, Late Breaking Topics, pp. 9-12, 2000. 

[21]  P.  Kerm,  “Adaptive  Kernel  Density  Estimation,”  The  Stata  Journal, 

2:148–156, 2002. 

[22]  T. Kohonen. Self-Organizing Maps. Springer, 3rd edition, 2001. 
[23]  J.  Kruskal.  M.  Wish,  Multidimensional  Scaling.  Sage  Publications, 

1977. 

[24]  J.  Lee,  K.  McDonnell,  A.  Zelenyuk,  D.  Imre,  K.  Mueller,  ""A 
Structure-Based  Distance  Metric 
for  High-Dimensional  Space 
Exploration  with  Multi-Dimensional  Scaling,""  IEEE  Trans.  on 
Visualization and Computer Graphics, 20(3): 351-364, 2014. 

[25]  G.  McLachlan.  Discriminant  Analysis  and  Statistical  Pattern 

Recognition, John Wiley & Sons, Aug 4, 2004. 

[26]  J.  Nam,  K.  Mueller,  ""TripAdvisorN-D:  A  Tourism-Inspired  High-
Dimensional  Space  Exploration  Framework  with  Overview  and 
IEEE  Trans.  Visualization  and  Computer  Graphics, 
Detail,"" 
19(2):291-305, 2013. 

[27]  A. Nadaraya, ""On Estimating Regression"".  Theory of Probability and 

its Applications 9 (1): 141–2. doi:10.1137/1109020, 1964. 

[28]  L.  van  der  Maaten,  G.  Hinton,  “Visualizing  data  using  t-SNE,” 

Journal of Machine Learning Research, 9:2579–2605, 2008. 

 

10 

",False,2016.0,{},False,False,journalArticle,False,H4Y2RXSJ,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7194836/,,The Data Context Map: Fusing Data and Attributes into a Unified Display,H4Y2RXSJ,False,False
CUB3R99J,AVKM2KCF,"Semantic Interaction for Visual Text Analytics 
Chris North 
Alex Endert 
Virginia Tech 
Virginia Tech 

Patrick Fiaux 
Virginia Tech 

Blacksburg, VA USA 

aendert@vt.edu 

Blacksburg, VA USA 

pfiaux@vt.edu 

 

Blacksburg, VA USA 

north@vt.edu 

by 

For 

through 

ABSTRACT 
Visual analytics emphasizes sensemaking of large, complex 
datasets 
interactively  exploring  visualizations 
generated 
example, 
statistical  models. 
dimensionality  reduction  methods  use  various  similarity 
metrics to visualize textual document collections in a spatial 
metaphor,  where  similarities  between  documents  are 
approximately  represented  through  their  relative  spatial 
distances  to  each  other  in  a  2D  layout.  This  metaphor  is 
designed to mimic analysts’ mental models of the document 
collection  and  support  their  analytic  processes,  such  as 
clustering similar documents together. However, in current 
methods, users must interact with such visualizations using 
controls  external  to  the  visual  metaphor,  such  as  sliders, 
menus, or text fields, to directly control underlying model 
parameters  that  they  do  not  understand  and  that  do  not 
relate  to  their  analytic  process  occurring  within  the  visual 
metaphor.  In  this  paper,  we  present  the  opportunity  for  a 
new  design  space  for  visual  analytic  interaction,  called 
semantic  interaction,  which  seeks  to  enable  analysts  to 
spatially interact with such models directly within the visual 
metaphor using interactions that derive from their analytic 
process,  such  as  searching,  highlighting,  annotating,  and 
repositioning  documents.  Further,  we  demonstrate  how 
semantic  interactions  can  be  implemented  using  machine 
learning 
tool,  called 
ForceSPIRE, for interactive analysis of textual data within 
a  spatial  visualization.    Analysts  can  express  their  expert 
domain knowledge about the documents by simply moving 
them,  which  guides  the  underlying  model  to  improve  the 
overall layout, taking the user’s feedback into account. 
Author Keywords 
Visualization; visual analytics; interaction 
ACM Classification Keywords 
H5.m.  Information  interfaces  and  presentation  (e.g.,  HCI): 
Miscellaneous.  
General Terms 
Design; Human Factors; Theory 

in  a  visual  analytic 

techniques 

 
Permission to  make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CHI’12, May 5–10, 2012, Austin, Texas, USA. 
Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00. 
 

 

INTRODUCTION 
Visual analytics bases its success on combining the abilities 
of statistical models, visualization, and human intuition for 
users to gain insight into large, complex datasets [23]. This 
success often hinges on the ability for users to interact with 
the  information,  manipulating  the  visualization  based  on 
their  domain  expertise,  interactively  exploring  possible 
connections, and investigating hypotheses. It is through this 
interactive exploration that users are able to make sense of 
complex  datasets,  a  process  referred  to  as  sensemaking 
[19].  
The  two  primary  parts  of  sensemaking  are  foraging  and 
synthesis. Foraging refers to the stages of the process where 
users filter and gather collections of interesting or relevant 
information.  Then,  using  that  information,  users  advance 
through  the  synthesis  stages  of  the  process,  where  they 
construct  and  test  hypotheses  about  how  the  foraged 
information  may  relate  to  the  larger  plot.  Tools  exist  that 
support users for either foraging or synthesis – but not both. 
In  this  paper  we  present  semantic  interaction,  combining 
the  foraging  abilities  of  statistical  models  with  the  spatial 
synthesis abilities of analysts. Semantic interaction is based 
on the following principles: 
1. Visual  “near=similar”  metaphor  supports  analysts’ 
spatial  cognition,  and  is  generated  by  statistical  models 
and similarity metrics. [22] 

2. Use  semantic  interactions  within  the  visual  metaphor, 
based  on  common  interactions  occurring  in  spatial 
analytic  processes  [4]  such  as  searching,  highlighting, 
annotating, and repositioning documents.  

3. Interpret  and  map  the  semantic  interactions  to  the 
underlying parameters of the model, by updating weights 
and adding information. 

4. Shield  the  users  from  the  complexity  of  the  underlying 

mathematical models and parameters. 

5. Models  learn  incrementally  by  taking  into  account 
interaction during the entire analytic process, supporting 
analysts’ process of incremental formalism [10]. 

6. Provide  visual  feedback  of  the  updated  model  and 

learned parameters within the visual metaphor. 

7. Reuse  learned  model  parameters  in  future  or  streaming 

data within the visual metaphor. 

To  demonstrate  the  concept  of  semantic  interaction,  we 
present  a  prototype  visual  analytics  tool,  ForceSPIRE,  for 
spatial analysis of textual information. In ForceSPIRE, the 
user  interaction  takes on  a deeper,  more  integrated role in 

the  exploratory  spatial  analytic  process.  This  is  done 
through capturing the semantic interaction, interpreting the 
analytical  reasoning  associated  with  the  interaction,  and 
updating the statistical model, and ultimately updating the 
spatialization.  Hence,  users  are  able  to  leverage  semantic 
interaction  to  explore  and  analyze  the  data  interactively, 
while  the  system  is  responsible  for  properly  updating  the 
underlying statistical model.  
RELATED WORK 
Foraging Tools 

 

Figure  1.  A  model  of  interaction  with  foraging  tools.  Users 
interact  directly  with  the  statistical  model  (red),  then  gain 
insight  through  observing  the  change  in  the  visualization 
(blue). 
We  categorize  foraging  tools  by  their  ability  to  pass  data 
through  complex  statistical  models  and  visualize  the 
computed structure of the dataset for the user to gain insight 
(Figure  1).  Thus,  users  interact  with  these  tools  primarily 
through directly manipulating the parameters of the model 
used  for  computing  the  structure.  As  such,  users  are 
required  to  translate  their  domain  expertise  and  semantics 
about  the  information  to  determine  which  (and  by  how 
much) to adjust these parameters. The following examples 
further describe this category of tools. 
Visualizations such as IN-SPIRE’s “Galaxy View” (shown 
in  Figure  3)  present  users  with  a  spatial  layout  of  textual 
information where similar documents are proximally close 
to  one  another  [25].  An  algorithm  creates  the  layout  by 
mapping the high-dimensional collection of text documents 
down  to  a  two-dimensional  view.  In  these  spatializations, 
the  spatial  metaphor  is  one  from  which  users  can  infer 
meaning  of  the  documents  based  on  their  location.  The 
notion  of  distance  between  documents  represents  how 
similar the two documents are (i.e., more similar documents 
are  placed  closer  together).  For  instance,  a  cluster  of 
documents  represents  a  group  of  similar  documents,  and 
documents  placed  between  two  clusters  implies  those 
documents are connected to both clusters. These views are 
beneficial  as  they  allow  users  to  visually  gain  a  quick 
overview  of  the  information,  such  as  what  key  themes  or 
groups  exist  within  the  dataset.  The  complex  statistical 
models  that  compute  similarity  between  documents  are 
based on the structure within the data, such as term or entity 
frequency. In order to interactively change the view, users 
are  required  to  directly  adjust  keyword  weights,  add  or 
remove documents/keywords, or provide more information 
on how to parse the documents for keywords/entities upon 
import. 

 

to  a 

to  understand 

the 

[15].  Through  adjusting 

Similarly, an interactive visualization tool called iPCA uses 
Principal  Component  Analysis  (PCA)  to  reduce  high-
dimensional  data  down 
two-dimensional  plot, 
providing  users  with  sliders  and  other  visual  controls  for 
directly  adjusting  numerous  parameters  of  the  algorithm, 
such  as  individual  eigenvalues,  eigenvectors,  and  other 
components  of  PCA 
the 
parameters,  the  user  can  observe  how  the  visualization 
changes.  This  allows  users  to  gain  insight  into  a  dataset, 
given  they  have  a  thorough  understanding  of  PCA, 
necessary 
the 
changes they are making to the model parameters. 
Alsakran  et  al.  presented  a  visualization 
system, 
STREAMIT,  capable  of  spatially  arranging  text  streams 
based  on  keyword  similarity  [3].  Again,  users  can 
interactively  explore  and  adjust  the  spatial  layout  through 
directly  changing  the  weight  of  keywords  that  they  find 
important.  In  addition,  STREAMIT  allows  for  users  to 
conduct  a  temporal  investigation  of  how  clusters  change 
over time. 
Synthesis Tools 

implications  behind 

 

Figure  2.  A  model  of  interaction  with  synthesis  tools.  Users 
manually  create  a  spatial  layout  of  the  information  to 
maintain and organize their insights about the data. 
Synthesis  tools  focus  on  allowing  users  to  organize  and 
maintain their hypotheses and insight regarding the data in 
a  spatial  medium.  In  large  part,  this  is  done  through 
presenting users with a flexible spatial workspace in which 
they  can  organize  information  through  creating  spatial 
structures,  such  as  clusters,  timelines,  stories,  etc.  (Figure 
2). In doing so, users externalize their thought processes (as 
well  as  their  insights)  into  a  spatial  layout  of  the 
information. 
For example, Analyst’s Notebook [2] provides users with a 
spatial workspace where information can be organized, and 
connections  between  specific  pieces  of  information  (e.g., 
entities, documents, events, etc.) can be created. Similarly, 
The Sandbox [26] enables users to create a series of cases 
(collections  of 
information)  which  can  be  organized 
spatially within the workspace.  
From  previous  studies,  we  found  cognitive  advantages 
associated  with  the  manual  creation  of  a  spatial  layout  of 
the  information  [4].  By  providing  users  a  workspace  in 
which  to  manually  create  spatial  representations  of  the 
information, users were able to externalize their semantics 
of the information into the workspace. That is, they created 
spatial  structures  (e.g.,  clusters,  timelines,  etc.),  and  both 
the structures as well as the locations relative to remaining 
layout  carried  meaning  to  the  users  with  regards  to  their 
sensemaking process. Marshall et al. have pointed out that 

this 

interaction  (and 

From  the  sensemaking  loop  presented  by  Pirolli  and  Card 
[19],  we  learn  that  in  intelligence  analysis,  that  analytic 
process  consists  not  only  of  the  information  that  is 
explicitly  within  the  dataset  being  analyzed,  but  also  the 
domain knowledge of the analyst performing the analysis. It 
is through this domain knowledge that analysts interact and 
explore  the  dataset  to  “make  sense”  of  the  information. 
Thus,  we  believe 
the  domain 
knowledge  associated  with  it)  is  equally  important  as  the 
raw data, and must be incorporated into the visualization by 
tightly coupling the model with the interaction. 
From this body of work, we most notably come away with 
an understanding that 1) analysts fundamentally understand 
the spatial metaphor used in many spatial visualizations, 2) 
many  of  these  systems  are  constructed  using  complex 
mathematical  algorithms  to  transform  high-dimensional 
data  to  two  dimensions,  and  3)  in  most  cases  these 
algorithms  can  be  controlled  by  analysts  largely  through 
visual  controls  (e.g.,  sliders,  knobs,  etc.)  to  directly  adjust 
parameters of the algorithms, updating the spatial layout. 
SEMANTIC INTERACTION 

 

Figure 4. A model of semantic interaction. Users are able to 
interact directly in the spatial metaphor. The system updates 
the corresponding parameters of the statistical model based on 
the analytic reasoning of the users. Finally, the model updates 

the visualization based on the changes, thus unifying the 
synthesis and foraging stages of the sensemaking loop. 

In the purest sense, semantic interaction refers to interaction 
occurring  within  a  spatial  visualization,  with  the  added 
benefit that it is tightly coupled to the model calculating the 
spatial layout (Figure 4). Given the previous work of what 
interaction  in  visual  analytic  tools  is,  semantic  interaction 
occupies a new design space for interaction. It merges the 
ability to change the statistical model while maintaining the 
flexibility  and  familiar  methods  for  interacting  within  the 
metaphor  of  spatial  visualizations.  Users  can  benefit  from 
semantic  interactions  in  that  they  can  interact  within  a 
metaphor  which 
they  are  familiar  with,  performing 
interactions  which  are  part  of  the  spatial  analytic  process 
[4], without having to focus on formal updates to the model.  
Semantic  interaction  leverages  the  cognitive  connection 
formed  between  the  user  and  the  spatial  layout.  The 
following intelligence analysis scenario is representative of 
the strategies and interactions of analysts when performing 
an  intelligence  analysis  task  of  textual  documents  in  a 
spatial visualization, as previously found by Andrews et al. 
[4],  and  further  motivates  and  explains  the  concept  of 
semantic interaction: 

 
Figure  3.  The  IN-SPIRE  Galaxy  View  showing  a 
spatializtiation  of  documents  represented  as  dots.  Each 
cluster of dots represents a group of similar documents.  
 
allowing users to create such informal relationships within 
information  is  beneficial,  as  it  does  not  require  users  to 
formalize these relationships [17].  
From this related work, we believe a trend is emerging in 
how interaction is currently handled in many visual analytic 
systems where complex statistical models are used – users 
are  required  to  go  outside  of  the  metaphor.  That  is,  while 
the  visual  representation  given  to  users  is  spatial,  the 
methods of interaction require users to step outside of that 
metaphor  and  interact  directly  with  the  parameters  of  the 
statistical model using visual controls, toolbars, etc.  
There  has  been  some  work  in  providing  more  easy  to  use 
interactions  for  updating  statistical  models.  For  example, 
relevance feedback has been used for content-based image 
retrieval, where users are able to move images towards or 
away  from  a  single  image  in  order  to  portray  pair-wise 
similarity  or  dissimilarity  [24].  From  there,  an  image 
retrieval algorithm determines the features and dimensions 
shared between the images that the user has determined as 
being  similar.  We  view  this  as  one  example  where  the 
interaction stays in the spatial metaphor of the visualization.  
Also, spatializations of document sets exist that allow users 
to place “points of interest” into the spatial layout. In VIBE, 
users are allowed to define multiple points of interest in the 
spatial  layout  that  correspond  to  a  series  of  keywords 
describing  a  subject  matter  of  interest  to  the  user  [18]. 
Similarly,  Dust  &  Magnet  [27]  allows  users  to  place  a 
series  of  “magnets”  representing  keywords  into  the  space 
and observe how documents are attracted or repelled from 
the  locations  of  these  magnets.  Through  both  of  these 
systems, users can interact in the spatial metaphor through 
these  placements  of  “nodes”  representing  keywords. 
However, the focus of semantic interaction is on interacting 
with  data  (i.e.,  documents),  an 
important  distinction 
discussed in the following section. 

 

 

 
Figure  5.  (top)  The  basic  version  of  the  “visualization 
pipeline”.  Interaction  can  be  performed  on  directly  the 
Algorithm  (blue  arrow)  or  the  data  (red  arrow).  (bottom) 
Our  modified  version  of 
for  semantic 
interaction,  where  the  user  interacts  within  the  spatial 
metaphor (purple arrow). 

the  pipeline 

During her analysis, an intelligence analyst finds a 
suspicious  and 
interesting  phrase  within  a 
document. While reading through the document, she 
highlights  the  phrase  “suspicious  individuals  were 
spotted  at  the  airport”,  in  order  to  more  easily 
recall  this  information  later.  After  she  finishes 
reading the document, she moves the document into 
the  bottom  right  corner  of  her  workspace,  in  the 
proximity of other documents related to an event at 
an airport. To remind herself of her hypothesis, she 
annotates  the  document  with  “might  be  related  to 
Revolution  Now  terrorist  group”.  Now,  with  the 
goal  of 
the 
“airport”, she searches for the term, continuing her 
investigation. 

further  examining 

the  events  at 

investigating 

that  each  of 

instead  point  out 

the  analytic  process  of 

In addition to the three forms of semantic interaction in the 
scenario,  Table  1  provides  a  list  of  various  forms  of 
semantic  interaction,  including  how  each  can  be  used 
within 
textual 
information  spatially.  We  do  not  claim  that  this  list  is 
complete,  but 
these 
interactions  can  relate  to  a  user’s  reasoning  within  the 
analytic process.  
Designing for Semantic Interaction 
In order for analysts to interact with information in a spatial 
metaphor, it must first be created. Following the model of 
the visualization pipeline [13], this creation calls for a series 
of  mathematical  transformations,  turning  raw  data  into  a 
spatial  layout  –  much  the  way  many  of  the  visualizations 
mentioned  previously  are  constructed.  However,  these 
visualizations  fit  this  model,  as  their  user  interactions  are 
primarily  focused  on  directly  modifying  the  statistical 
model  (as  well  as  other  attributes  of  the  visualization  or 
data  transformation).  Designing  for  semantic  interaction 
requires  a  fundamentally  different  model  for  how  tools 
integrate  user  interaction  –  one  that  can  capture  the 
interaction,  interpret  the  associated  analytical  reasoning, 
and update the appropriate mathematical parameters.  
Figure  5  illustrates  this  model,  where  the  spatialization  is 
treated  a  medium  through  which  the  user  can  perceive 

 

Figure 6. Overview of how nodes and edges in ForceSPIRE’s 
force-directed layout are created from documents (Doc) and 
entities (Ent), respectively.  

 

 

it 

interaction, 

information  and  gain  insight,  as  well  as  interact  and 
perform  his  analysis.  Through  expanding  the  pipeline  to 
accommodate  for  semantic 
is  a  more 
appropriate match to the user’s sensemaking process. 
Capturing the Semantic Interaction 
A  non-trivial  first  step  in  the  model  is  capturing  the  user 
interaction.  Much  research  has  been  done  in  this  area, 
primarily  for  the  purpose  of  maintaining  process  history 
(e.g., [5], [21], [12], etc.). When considering how to capture 
interaction,  one  decision  to  be  made  is  at  what  “level”  to 
capture  it.  For  example,  GlassBox  [6]  captures  interaction 
at a rudimentary level (i.e. mouse clicks and key strokes), 
while  Graphical  History  [14]  keeps  track  of  a  series  of 
previous  visualizations  as  a  user  changes  the  visualization 
during the exploration of the data.  
Semantic  interaction  is  captured  at  a  data  level,  as  the 
interactions  occur  on  the  data,  and  within  the  spatial 
metaphor.  Using 
the 
interaction being captured would be: 

the  earlier  analytic  scenario, 

•  The highlighted phrase 
•  When the highlighting occurs (timestamp) 
•  The color chosen for the highlight 
•  The document in which the highlight occurs 
•  The new document location 
•  The text of the annotation 

By  capturing  (and  storing)  the  interaction  history,  we  can 
interpret the analytical reasoning of the user. Thus, we not 
only capture the interaction, but also use it. 
Interpreting the Associated Analytical Reasoning 
In interpreting the interaction, the goal is for the system to 
determine  the  analytical  reasoning  associated  with  the 
interactions  and  update  the  model  accordingly.  From 
previous findings [4], we can associate analytical reasoning 
with  forms  of  semantic  interaction  (see  Table  1).  It  is 
essentially the model’s task to determine  why, in terms of 
the data, the interaction occurred. To answer this question, 
we do not propose that this model can accurately gauge user 
intent.  Instead,  the  goal  is  to  calculate,  based  on  the  data, 

Figure 7. Using ForceSPIRE on a 32 megapixel large, 
high-resolution display. 

 

 
what information is consistent with the captured interaction. 
For  instance,  we  associate  text  highlighting  with  adding 
importance to the text being highlighted. We do not claim 
that we can associate the interaction of highlighting to the 
intuition that spurred the analyst to highlight the text, which 
is far more challenging, and arguably impossible. 
We refer to the captured and interpreted interactions as soft 
data, in comparison to the hard data that is extracted from 
the raw textual information (e.g., term or entity frequency, 
titles,  document  length,  etc.).  We  define  soft  data  as  the 
stored result of user interaction as interpreted by the system. 
In  representing  interaction  as  soft  data,  the  algorithm  can 
calculate  and  reconfigure  the  spatial  layout  accordingly. 
Figure  5  illustrates  how  our  approach  differs  from  the 
traditional visualization pipeline. 
There has been previous work in capturing and interpreting 
reasoning from user interaction. For instance, Dou et al. [7] 
performed  a  study  where  financial  analysts  were  asked 
analyze  a  dataset  using  WireVis,  an  interactive  financial 
transaction visualization. The tool developers then analyzed 
the captured interaction, and assumptions were made about 
the  reasoning  of  the  analysts  at  specific  points  in  the 
investigation. These results were compared to the analysts’ 
self-recorded  reasoning,  and  found  to  be  accurate  up  to 
82%. While our work has similar goals (i.e., interpreting the 
analytical reasoning associated with the analysts through an 
evaluation  of  the  interaction)  our  model  does  so  through 
tightly  integrating  the  interaction  with  the  underlying 
mathematical model. In doing so, the interpretation can be 
done algorithmically. 
Updating the Underlying Model 
Through  metric  learning  of  distance  weights,  the  layout 
uses  the  soft  data  to  update  the  underlying  model. 
Depending  on  the  algorithm  used  to  compute  the  spatial 
layout,  the  precise  parameters  being  updated  will  vary.  In 
general,  this  will  refer  to  weighting  of  a  combination  of 
dimensions  that  will  help  guide  the  model  as  to  which 
dimensions the user finds important.  
FORCESPIRE: SYSTEM OVERVIEW 
ForceSPIRE  is  a  visual  analytics  prototype  designed  for 
specific 
(document 
movement,  text  highlighting,  search,  and  annotation)  for 

forms  of 

interaction 

semantic 

 

Figure  8.  Moving  the  document  shown  by  the  arrow, 
ForceSPIRE  adapts  the  layout  accordingly.  Documents 
sharing entities with the document being moved follow. 

 

interactively exploring textual data. The system has a single 
spatial  view  (shown  in  Figure  12),  where  a  collection  of 
documents is represented spatially based on similarity (i.e., 
documents closer together are more similar).  
ForceSPIRE is designed for large, high-resolution displays 
(such  as  the  one  shown  in  Figure  7).  As  semantic 
interaction emphasizes the importance of context in which 
the  interaction  takes  place  (e.g.,  highlighting  text  in  the 
context  of  the  document),  having  the  full  detail  text 
available  in  the  context  of  the  spatial  layout  is  beneficial 
over having a single document viewer. Further, the physical 

Table  1.  Forms  of  semantic  interaction.  Each  interaction 
corresponds  to  reasoning  of  users  within  the  analytic 
process. 

Form of Semantic 

Interaction 

Document Movement 

Text Highlighting 

Pinning  Document 
Location 
Annotation, “Sticky Note” 

to 

Document Coloring 

Level of Visual Detail 

Query Terms 
 

Associated Analytic Reasoning 

• Similarity/Dissimilarity 
• Create 

spatial  construct 

timeline, list, story, etc) 

• Test 

hypothesis, 

see 
document “fits” in region 

(.e.g 

how 

• Mark 

importance  of  phrase 

(collection of entities) 

• Augment  visual  appearance  of 

document for reference 

to 

in 

• Give 

semantic  meaning 

space/layout 

• Put 

semantic 

information 

workspace, within context 
• Create visual group/cluster 
• Mark group membership 
• Change 

ease 

of 

visually 
referencing  information  (e.g.  full 
detail = more important = easy to 
reference) 

• Expressive search for entity 

(and 

to  match 

is  positioning 

Semantic Interaction in ForceSPIRE 
The  semantic  interactions  in  ForceSPIRE  are:  placing 
information  at  specific  locations,  highlighting,  searching, 
and annotating in order to incrementally change the spatial 
layout 
their  mental  model.  The  primary 
parameters  of  the  force-directed  model  that  are  being 
updated  through  this  learning  model  are  the  importance 
values of the entities.  
Document  Movement.  The  predominant  interaction  in  a 
spatial  workspace 
repositioning) 
documents.  In  previous  work,  we  have  demonstrated  how 
users can perform both exploratory and expressive forms of 
this type of interaction [9]. In ForceSPIRE, we allow for the 
following  exploratory  interaction  (i.e.,  interaction  that 
allows users to explore the structure of the current model, 
but  does  not  change  it).  Users  are  able  to  interactively 
explore the information by dragging a document within the 
workspace, pinning a document to a particular location (see 
Figure  8),  as  well  as  linking  two  documents.  When 
dragging a document, the force-directed system responds by 
finding the lowest energy state of the remaining documents 
given  the  current  location  of  the  dragged  document. 
Mathematically, this adds a constraint to the stress function 
being  optimized  (in  this  case  the  force-directed  model). 
This  allows  users  to  explore  the  relationship  of  that 
document in comparison to the remaining documents.  
In addition to the exploratory dragging of a document, users 
have the ability to pin a document. By pinning a document, 
users  are  able  to  incrementally  add  semantic  meaning  to 
locations in their workspace. By specifying key documents 
to  user-defined  locations,  the  layout  of  the  remaining 
documents will adapt to these constraints. Thus, users can 
explore  how  documents  are  positioned  based  on  their 
similarity  (or  dissimilarity)  to  the  pinned  documents.  For 
instance,  if  the  layout  places  a  document  between  two 
pinned  documents, 
the  particular 
document holds a link between the two pinned documents, 
sharing entities that occur in both. 
Finally,  users  can  perform  an  expressive  form  of  this 
interaction  by  linking  two  documents,  performed  by 
dragging  one  document  onto  another  pinned  document.  In 
doing so, ForceSPIRE calculates the similarity between the 
documents,  and  increases  the  importance  value  of  the 
entities  shared  between  both  documents.  As  a  result,  the 
layout will place more emphasis on the characteristics that 
make those two documents similar. 
Highlighting.  When  highlighting  a  term,  ForceSPIRE 
creates an entity from the term (if not already one), and the 
importance  value  of  that  term  is  increased.  Similarly, 
highlighting  a  phrase  results  in  the  phrase  being  first 
parsed for entities, then increasing the importance value of 
each  of  those  entities.  For  example,  Figure  11  shows  the 
effect of highlighting the terms “Colorado” and “missiles” 
in the document pointed to with the arrow. As a result, the 

it  may 

imply 

that 

 
Figure  9.  The  Effect  of  adding  an  annotation  (“these 
individuals  may  be  related  to  Revolution  Now”)  to  the 
document shown with an arrow. As  a result,  the document 
becomes 
linked  with  other  documents  mentioning  the 
terrorist organization “Revolution Now”.  

presence of these displays creates an environment in which 
the  virtual  information  (in  this  case  the  documents)  can 
occupy  persistent  physical  space.  As  a  result,  users  are 
further  immersed  into  the  spatial  metaphor,  as  they  can 
point and quickly refer to information based on the physical 
locations.  
Constructing the Spatial Metaphor 
The spatial layout of the text documents is determined by a 
modified  version  a  force-directed  graph  model  [11].  This 
model  functions  on  the  principle  of  nodes  with  a  mass 
connected  by  springs  with  varying  strengths.  Thus,  each 
node has attributes of attraction and repulsion: nodes repel 
other  nodes,  and  two  nodes  attract  each  other  only  when 
connected  by  a  spring  (edge).  The  optimal  layout  is  then 
computed  by  iteratively  calculating  these  forces  until  the 
lowest energy state of all the nodes is reached. A complete 
description of this algorithm can be found in [11].  
We  apply  this  model  to  textual  information  by  treating 
documents  as  nodes  (an  overview  is  shown  in  Figure  6). 
The entire textual content of each document is parsed into a 
collection  of  entities  (i.e.,  keywords).  The  number  of 
entities corresponds to the mass of each document (heavier 
nodes  do  not  move  as  fast  as  lighter  nodes).  A spring  (or 
edge) represents one or more matching entities between two 
nodes.  Therefore,  the  initial  distance  metric  is  a  based  on 
co-occurrence  of  terms  between  documents.  For  example, 
two  documents  containing  the  term  “airport”  will  be 
connected  by  a  spring.  The  strength  of  a  spring  (i.e.  how 
close together it tries to place two nodes) is based on two 
factors:  the  number  of  entities  two  documents  have  in 
common,  and  the  importance  value  associated  with  each 
shared entity (initially, importance values are created using 
a  standard  tfidf  method  [16]).  The  sum  of  all  importance 
values add up to 1. 
The resulting spatial layout is one where similarity between 
documents  is  represented  by  distance  relative  to  other 
documents.  Similarity  in  this  system  is  defined  by  the 
strength of the spring between two documents. A stronger 
spring  (and  therefore  a  larger  amount  of  shared  entities) 
will pull two documents closer together, and thus represent 
two similar documents. 

 

 
Figure  10.  Searching  for  the  term  ”Atlanta”,  documents 
containing the term highlight green within the context of the 
spatial  layout.  Additionally,  the  importance  value  of  entity 
“Atlanta” is increased. 

other  documents  containing  that  term  are  clustered  more 
tightly. 
Searching.  When  coming  across  a  term  of  particular 
interest, analysts usually search on that term in order to find 
other  occurrences.  In  a  spatial  workspace,  this  is  of 
particular  importance,  because  the  answer  to  “where  the 
term  is  also  found”  is  not  only  given  in  terms  of  what 
documents,  but  also  where  in  the  layout  those  documents 
occur. The positions of documents containing the term are 
shown in context of the entire dataset, from which users can 
infer the importance of that term (as shown in Figure 10).  
ForceSPIRE  first  creates  an  entity  from  the  search  term 
(unless  it  is  already  one),  then  increases  the  importance 
value  of  the  search  term.  Figure  10  gives  an  example  of 
how a search result appears in ForceSPIRE. Searching for 
the  term  “Atlanta”,  documents  that  contain  the  term  are 
highlighted  green,  and  links  are  drawn  to  show  where  the 
resulting documents are in relation to the current document.  
Annotation.  Annotations  (i.e.,  “sticky  notes”)  are  also 
viewed as a form of semantic interaction, occurring within 
the analytic process, from which analytic reasoning can be 
inferred. When a user creates a note regarding a document, 
that semantic information should be added to the document. 
For example, if Document A refers to “Revolution Now” (a 
suspicious  terrorist  group),  and  Document  B  refers  to  “a 
group of suspicious individuals”, and the user has reason to 
believe  these  individuals  are  related  to  Revolution  Now, 
adding a note to Document B stating “these individuals may 
be  related  to  Revolution  Now”  is  one  way  for  the  user  to 
add semantic meaning to the document.  
ForceSPIRE  handles  the  addition  of  the  note  (shown  in 
Figure 9) by 1) parsing the note for any currently existing 
entities,  then  2)  increasing  the  importance  value  of  each, 
and 3) creating any new springs between other documents 
sharing these entities. In the example in Figure 9, edges are 
created between Document B and Document A (as well as 
any  other  documents  that  mention  “Revolution  Now”). 
Additionally,  if  the  note  contains  any  new  entities  not 
currently in the model, they are created, with the intent that 

 

 
Figure 11. The effect of highlighting a phrase containing the 
entites  “Colorado”  and  “missiles”.  Documents  containing 
these  entities  move  closer,  as  the  increase  in  importance 
value increases the edge strength.  

the 

importance  values  of 

any future entities that may match to that note can be linked 
at that time. ForceSPIRE also handles cases where notes are 
edited,  with  text  added  or  removed  from  the  note,  by 
updating  the  entities  associated  with  the  document,  and 
adjusting 
these  entities 
accordingly. 
Model Updates 
Each  of  the  semantic  interactions  in  ForceSPIRE  impacts 
the  model  by  updating  the  importance  values  of  entities, 
and  the  mass  of  each  document.  The  calculation  for 
updating the importance value of an entity is the same for 
each interaction. If an entity was “hit” (i.e., it was included 
in  a  highlight,  it  was  searched,  it  was  in  a  note,  etc.), 
ForceSPIRE increases its importance value by 10%. As the 
sum  of  all  importance  values  of  entities  adds  up  to  1, 
ForceSPIRE  subtracts  an  equal  amount  from  all  other 
entities’ importance values. As a result, importance values 
decay over time, and entities that are rarely used during the 
analysis  have  less  impact  on  the  layout.  The  mass  of  a 
document  uses  a  similar  calculation,  in  that  each  time  a 
document  is  “hit”  (i.e.,  text  was  highlighted,  it  was  the 
result of a search hit, etc.), it increases by 10%.  
When  undoing  an 
standard 
the 
“Control+Z”  keyboard  shortcut,  a  linear  history  of  the 
interactions will be reversed, and the importance values of 
affected  entities  will  be  returned  to  their  prior  values  (as 
well  as  document  masses).  As  for  the  locations  of  the 
documents,  the  reverted  importance  values  and  document 
masses  will  be  responsible  for  updating 
layout. 
However, this does not guarantee that the layout will return 
to  the  exact  previous  view,  and  the  user  may  find  it 
necessary to perform small adjustments. 
The model updates used in ForceSPIRE serve as an initial 
approach at how to couple semantic interactions with model 
updates. Other, more complex methods may exist, and we 
encourage  further  research  in  this  area.  Sensemaking  is  a 
complex exploratory process. As such, semantic interaction 

interaction  using 

the 

through 

more  central  documents.  While  reading 
the 
documents, he highlighted phrases of interest. For example, 
he highlighted the phrase “Nizar A. is now known to have 
spent six months in Afghanistan”. In doing so, ForceSPIRE 
increased  the  importance  value  of  the  entities  within  the 
phrase,  particularly  “Afghanistan”  and  “Nizar  A”.  As  a 
result, the layout forms more tightly around those entities. 
Each change incrementally changes the layout. 
Continuing  with  his  investigation,  he  began  searching  for 
words  of  interest  (e.g.,  “weapons”,  “Colorado”,  “Atlanta”, 
etc.). ForceSPIRE provided him with quick visual feedback 
on where in the dataset each terms showed up (the search 
result  for  “Atlanta”  is  shown  in Figure  10).  In  addition  to 
gaining an overview of the distribution of the term within 
the  dataset  (by  highlighting  each  document  containing  the 
term  green),  ForceSPIRE  treats  performing  a  search  as 
either  creating  a  new  entity  from  the  search  term,  or 
increasing the importance value if an entity corresponding 
to the search term already exists. As a result of the multiple 
search terms and highlights corresponding to locations (e.g., 
“Atlanta”,  “Los  Angeles”,  “Missouri”,  etc.),  ForceSPIRE 
adapts  the  spatialization  by  creating  a  more  geographic-
oriented layout (shown in the “Mid Stage” layout in Figure 
12).  
During  further  investigation,  he  began  opening  more 
documents and adding annotations to documents where he 
found  information  missing  that  he  knew.  For  example, 
Figure  9  shows  how  he  opened  one  document  where 
“suspicious individuals” were mentioned. Earlier, he read a 
document  containing 
terrorist 
organization  named  “Revolution  Now”.  While  reading 
about  the  suspicious  individuals,  the  other  information  in 
the document triggered him to make a connection between 
these  individuals  and  Revolution  Now.  He  made  added  a 
note  to  the  document  about  the  suspicious  individuals 
stating  “these  individuals  may  be  related  to  Revolution 
Now”. As a result, ForceSPIRE parsed the note for entities, 
added  them  to  the  document,  and  pulled  the  document 
closer to other documents containing the entity “Revolution 
Now”.  
After  continuing  his  investigation  in  this  manner,  he 
ultimately  made  the  connections  within  the  dataset  to 
uncover  the  terrorist  plot.  The  progression  of  the  spatial 
layout,  shown  in Figure 12, shows the final layout, where 
he  was  able  to  pinpoint  regions  of  the  layout  as  being 
important  in  his  finding.  Some  of  the  spatial  locations  of 
clusters  are  a  result  of  him  pinning  documents  to  that 
region (e.g., “Atlanta”, “Los Angeles”, etc.). These pinned 
documents are shown in red. Perhaps more interestingly is 
not the regions that were created as a result of him pinning 
documents  to  that  location,  but  rather  how  the  remaining 
documents respond in the layout. For example, in the final 
state  shown  in  Figure  12,  a  group  of  documents  began  to 
emerge  in  the  middle  of  all  the  pinned  locations.  Upon 
examining  these  documents,  he  discovered  that  these 

information  about  a 

the 

layout 

 

interaction, 

instances  during 

 
Figure 12. The incremental change of the spatial layout (main 
view  of  ForceSPIRE)  from  the  initial  to  the  final  state. 
Through  semantic 
incrementally 
changed  based  on the  semantic  input of the user. We labeled 
the regions based on what the user told us the regions meant to 
him at each stage. 
can  enable  analysts  to  explore  their  hypothesis  in-situ, 
while  the  provenance  of  their  insights  is  captured  and 
stored. An open area of research is what analyzing the soft 
data might reveal about the analytic process. For instance, if 
the  importance  values  of  entities  converge  on  a  small 
number  of  entities,  specific  biases  might  be  revealed. 
Similarly, 
the  analysis  when  new 
hypotheses  are  being  explored  may  be  indicated  by 
diverging importance values. 
Use Case 
We  demonstrate  the  functionality  of  ForceSPIRE  through 
the  following  use  case.  In  this  scenario,  we  simulate  an 
intelligence  analysis  scenario  where  the  task  is  to  find  a 
hidden terrorist plot in a pre-constructed, ficticious textual 
dataset.  The  dataset  consists  of  50 
text  documents, 
containing  a  complex  terrorist  plot  (explosives  are  being 
transported to various cities in the U.S. using trucks). The 
combination of the task of finding the hidden terrorist plot 
and  the  textual  dataset  is  representative  of  daily  work 
performed  by  professional  intelligence  analysts  [8].  The 
analysis  described  below  lasted  70  minutes,  and  was 
performed  by  an  individual  computer  science  graduate 
student.  
The user began the investigation by loading the collection 
of  documents  into  ForceSPIRE.  The  documents  were 
automatically  parsed  for  entities  using 
the  LingPipe 
keyword  extraction  library  [1].  From  these  entities,  an 
initial layout was generated, shown in Figure 12(top). From 
this  layout,  he  began  investigation  by  reading  through  the 

 

interpreting 

leverage 

interactions 

DISCUSSION 
Unifying the Sensemaking Loop 
With the fundamentally different role occupied by semantic 
interaction, we explore a new design space for interaction in 
visual analytic tools. With the addition of soft data, and a 
model  capable  of 
the  user’s  analytical 
reasoning,  we 
that  are  already 
occurring in the spatial analytic process to further aid users 
in their sensemaking process.  
With  semantic  interaction,  the  amount  of  formalization 
between foraging and sensemaking (Figure 13) on the part 
of the user is reduced. For instance, in moving a document, 
users  can  formulate  a  hypothesis  based  on  that  document, 
expecting  similar  documents 
to  follow.  ForceSPIRE 
attempts to update the layout based on the interaction, and 
gives the user feedback. Thus, the foraging stage occurs as 
a  result  of  the  hypothesis  being  formed  through  semantic 
interaction.  By  not  forcing  users  to  over-formalize  their 
analytic  reasoning  too  early  in  order  to  forage  for  the 
relevant  information,  semantic  interaction  creates  a  more 
seamless 
transition  between 
foraging  and  synthesis, 
unifying the sensemaking loop.  
Future Work 
Semantic 
interaction,  as  a  concept,  opens  up  many 
possibilities for further research, such as: what interactions 
to  capture  and  store,  which  parameters  of  the  model  to 
update,  how  to  store  the  soft  data,  and  which  models 
present a metaphor that can be extended upon.  
In  order  to  make  more  concrete  claims  regarding  the 
usability  and  effectiveness  of  ForceSPIRE  (and  thus,  of 
semantic  interaction),  a  formal  user  study  is  needed.  Our 
plan is to introduce ForceSPIRE to professional intelligence 
analysts  and  have  them  solve  scenarios  that  model  their 
daily  task,  such  as  one  of  the  VAST  datasets  [2020].  The 
observations  and  feedback  from  these  users  will  provide 
ecological validity for semantic interaction. 
CONCLUSION 
In  this  paper  we  have  discussed  how  the  concept  of 
semantic  interaction  leads  to  a  new  design  space  for 
interaction 
information. 
Semantic  interactions  occur  directly  within  the  spatial 
metaphor,  support  spatial  cognition,  and  exploit  spatial 
analytic  interactions.  We  describe  semantic  interaction, 
discussing  the  three  components  required  –  capturing  the 
interaction, 
the  analytical  reasoning,  and 
updating  the  mathematical  model.  Further,  we  present 
ForceSPIRE, designed for semantic interaction with textual 
information, discussing its functionality and demonstrating 
how it can be used through a use case. Lastly, we discuss 
how  semantic  interaction  has  the  opportunity  to  unify  the 
sensemaking  loop,  creating  a  more  seamless  analytic 
process.  In  allowing  users  to  interact  within  the  spatial 
metaphor, they can remain more focused on their analysis 
of  the  data,  without  having  to  become  experts  in  the 
underlying mathematical models of the system.  

in  spatializations  of 

interpreting 

textual 

 

Figure  13.  The  sensemaking  loop,  illustrating  the  complex 
sequence  of  steps  used  by  intelligence  analysts  in  order  to 
gain insight into data.  
 
documents  are  about  the  terrorist  organization  using  “U-
Haul”  or  “Ryder”  trucks  for  transportation  between  these 
locations. ForceSPIRE placing these documents in between 
these  cities  in  the  layout  was  helpful,  as  these  documents 
contain  information  “connecting”  the  events  in  these 
locations.  Immediately  after  noticing  this  event,  he  also 
made use of the expressive form of interaction, performed 
by dragging two of these documents together to determine 
what  made  them  similar.  After  seeing  that  it  was  indeed 
terms  such  as  “Ryder”  and  “U-Haul”,  the  layout  formed 
more tightly around these terms. 
ForceSPIRE interpreted the analytical reasoning of the user 
through the creation of new entities that were not found by 
the  initial  keyword  extraction,  as  well  as  the  increase  of 
importance values of existing entities. This is evidenced by 
the  creation  of  39  new  entities  during  the  course  of  the 
analysis.  LingPipe  extracted  89  initial  entities  from  this 
dataset,  and  at  the  time  of  completing  our  investigation 
ForceSPIRE  included  128.  Examples  of  newly  created 
entities  are  “big  event”,  “grenades”,  “Fisher  Island”, 
“weapons”,  and  others.  The  ability  for  new  entities  to  be 
created  via  semantic  interaction  did  not  interfere  with  the 
fluid sensemaking process of the user. Instead, it aided the 
process  by  creating  new  entities,  which  in  turn  created 
semantically relevant connections within the dataset. 
In  addition  to  creating  new  entities,  existing  entities 
dynamically  changed  their  importance  value  based  on  the 
semantic 
interpreted 
reasoning 
interactions.  Examples  of  entities 
their 
importance  values  are  “Atlanta”,  “Revolution  Now”, 
“Colorado”,  “L.A.”,  and  others.  As  a 
the 
ForceSPIRE incrementally adapted the layout based on the 
user  input.  This  shows  that  adjusting  importance  values, 
creating entities, and changing locations of key documents 
helped  the  user  discover  the  structure  of  the  dataset,  and 
ultimately make out the hidden terrorist plot.  

of 
that  changed 

analytical 

result, 

the 

 

ACKNOWLEDGEMENTS 
This research was funded by the NSF grant CCF-0937071 
and the DHS center of excellence. 
REFERENCES 
1.  Alias-i. 2008. LingPipe 4.0.1. City, 2008. 
2.  i2 Analyst's Notebook. City. 
3.  Alsakran, J., Chen, Y., Zhao, Y., Yang, J. and Luo, D. 

STREAMIT: Dynamic visualization and interactive 
exploration of text streams. In Proceedings of the IEEE 
Pacific Visualization Symposium, 2011.  

4.  Andrews, C., Endert, A. and North, C. Space to Think: 
Large, High-Resolution Displays for Sensemaking. In 
Proceedings of the CHI '10, 2010.  

5.  Callahan, S. P., Freire, J., Santos, E., Scheidegger, C. E., 

C, Silva, u. T. and Vo, H. T. VisTrails: visualization 
meets data management. In Proceedings of the 
SIGMOD international conference on Management of 
data (Chicago, IL, USA, 2006). ACM.  

6.  Cowley, P., Haack, J., Littlefield, R. and Hampson, E. 

Glass box: capturing, archiving, and retrieving 
workstation activities. In Proceedings of the workshop 
on Continuous archival and retrival of personal 
experences (Santa Barbara, California, USA, 2006). 
ACM.  

7.  Dou, W., Jeong, D. H., Stukes, F., Ribarsky, W., 

Lipford, H. R. and Chang, R. Recovering Reasoning 
Processes from User Interactions. IEEE Computer 
Graphics and Applications, 2009. 

8.  Endert, A., Andrews, C., Fink, G. A. and North, C. 

Professional Analysts using a Large, High-Resolution 
Display. In Proceedings of the IEEE VAST Extended 
Abstract (2009).  

9.  Endert, A., Han, C., Maiti, D., House, L., Leman, S. C. 

and North, C. Observation-level Interaction with 
Statistical Models for Visual Analytics. IEEE VAST, 
2011. 

10. Frank M. Shipman, I. and Marshall, C. C. Formality 

Considered Harmful: Experiences, Emerging Themes, 
and Directions on the Use of Formal Representations 
inInteractive Systems. ACM CSCW, 8, 4, 1999, 333-352. 

11. Fruchterman, T. M. J. and Reingold, E. M. Graph 

drawing by force-directed placement. Software: Practice 
and Experience, 21, 11 1991, 1129-1164. 

12. Gotz, D. Interactive Visual Synthesis of Analytic 

Knowledge. IEEE VAST, 2006. 
13. Heer, J. prefuse manual, 2006. 
14. Heer, J., Mackinlay, J., Stolte, C. and Agrawala, M. 

Graphical Histories for Visualization: Supporting 
Analysis, Communication, and Evaluation. IEEE 
Transactions on Visualization and Computer Graphics, 
14, 6 , 2008, 1189-1196. 

 

15. Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. 

and Chang, R. iPCA: An Interactive System for PCA-
based Visual Analytics. Computer Graphics Forum, 28, 
2009, 767-774. 

16. Karen A Statistical Interpretation of Term Specificity 

and its Application in Retrieval. Journal of 
Documentation, 28, 1972, 11-21. 

17. Marshall, C. C., Frank M. Shipman, I. and Coombs, J. 

H. VIKI: spatial hypertext supporting emergent 
structure. In Proceedings of the European conference on 
Hypermedia technology (Edinburgh, Scotland, 1994). 
ACM.  

18. Olsen, K. A., Korfhage, R. R., Sochats, K. M., Spring, 
M. B. and Williams, J. G. Visualization of a document 
collection: the vibe system. Information Process 
Management, 29, 1 1993, 69-81. 

19. Pirolli, P. and Card, S. Sensemaking Processes of 

Intelligence Analysts and Possible Leverage Points as 
Identified Though Cognitive Task Analysis Proceedings 
of the International Conference on Intelligence 
Analysis,2005, 6. 

20. Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., 

O'Connell, T., Laskowski, S., Chien, L., Tat, A., Wright, 
W., Gorg, C., Zhicheng, L., Parekh, N., Singhal, K. and 
Stasko, J. Evaluating Visual Analytics at the 2007 
VAST Symposium Contest. Computer Graphics and 
Applications, IEEE, 28, 2 2008, 12-21. 

21. Shrinivasan, Y. B. and Wijk, J. J. v. Supporting the 

analytical reasoning process in information 
visualization. In Proceedings of the CHI '08 (Florence, 
Italy, 2008). ACM.  

22. Skupin, A. A Cartographic Approach to Visualizing 
Conference Abstracts. IEEE Computer Graphics and 
Applications, pp. 50-58, January/February, 2002. 

23. Thomas, J. J., Cook, K. A., National, V. and Analytics, 
C. Illuminating the path. IEEE Computer Society, 2005. 
24. Torres, R. S., Silva, C. G., Medeiros, C. B. and Rocha, 

H. V. Visual structures for image browsing. In 
Proceedings of the conference on Information and 
knowledge management (New Orleans, LA, USA, 
2003). ACM.  

25. Wise, J. A., Thomas, J. J., Pennock, K., Lantrip, D., 

Pottier, M., Schur, A. and Crow, V. Visualizing the non-
visual: spatial analysis and interaction with information 
for text documents. Morgan Kaufmann Publishers, 1999. 

26. Wright, W., Schroh, D., Proulx, P., Skaburskis, A. and 

Cort, B. The Sandbox for analysis: concepts and 
methods. In Proceedings of the CHI '06 (New York, 
NY, 2006). ACM.  

27. Yi, J. S., Melton, R., Stasko, J. and Jacko, J. A. Dust & 
magnet: multivariate information visualization using a 
magnet metaphor. Information Visualization, 4, 4, 2005, 
239-256. 

",False,2012.0,{},False,False,conferencePaper,False,CUB3R99J,[],self.user,False,False,False,False,http://dl.acm.org/citation.cfm?doid=2207676.2207741,,Semantic interaction for visual text analytics,CUB3R99J,False,False
HGH83ZAU,X3PREN86,"Visualizing High-Dimensional Predictive Model Quality

Penny Rheingans

University of Maryland, Baltimore County

Marie desJardins
SRI International

Department of Computer Science and Electrical Engineering

Artiﬁcial Intelligence Center

rheingan@cs.umbc.edu

marie@ai.sri.com

Abstract

Using inductive learning techniques to construct classiﬁcation mod-
els from large, high-dimensional data sets is a useful way to make
predictions in complex domains. However, these models can be dif-
ﬁcult for users to understand. We have developed a set of visualiza-
tion methods that help users to understand and analyze the behavior
of learned models, including techniques for high-dimensional data
space projection, display of probabilistic predictions, variable/class
correlation, and instance mapping. We show the results of apply-
ing these techniques to models constructed from a benchmark data
set of census data, and draw conclusions about the utility of these
methods for model understanding.

1 Introduction

Discovering interesting information in large, high-dimensional data
spaces is a challenging problem. Using inductive machine learning
techniques to construct classiﬁcation models has proven to be one
useful approach for solving this problem. A typical machine learn-
ing application involves a great deal of manual effort to iteratively
construct a representation of the domain (feature engineering), set
the parameters of the learning algorithm, induce a set of models,
and analyze the resulting models. To support this process, we have
developed a set of visualization methods with the goal of improving
a user’s ability to evaluate the quality of learned models.

Traditional model analysis methods primarily consist of numeri-
cal and statistical tools for assessing the quality of a learned model.
These tools include classiﬁcation accuracy, confusion matrices, and
receiver operating characteristic (ROC) curves. Our visualization
techniques provide a richer representation of the information that
the statistical tools summarize by a single number or curve, and are
meant to augment, not replace, these statistical tools. To that end,
we discuss in this paper how the visualization methods can be used
to gain insights into how the behavior of the model varies across the
data space. These insights could be used to guide the application
development process by pinpointing, for example, regions of the
data space (groups of individuals) with high misclassiﬁcation rates,
thus helping the user to determine what additional data to gather, or
how to modify the set of features to improve differentiation.

2 Induced Predictive Models

A model is a description of how the world is expected to behave.
Typically a model describes the aspects of the world that are rele-
vant to a speciﬁc task: e.g., diagnosing a disease, predicting credit
risks, or classifying documents by topic area. Here we focus on
classiﬁcation tasks, which have the form “Given an object descrip-
tion, classify it into one of k classes.” Classiﬁcation methods can
be used for both prediction and diagnosis (e.g., “Given an appli-
cant’s characteristics, predict whether they will default on a loan,”
or “Given a patient’s symptoms, determine what disease is affecting
them”). Probabilistic classiﬁcation methods give the probability of
class membership, which is particularly useful in domains contain-
ing uncertainty, noisy data, or incomplete object descriptions.

In classiﬁcation problems, one of the variables is a distinguished
class variable; we refer to the other variables as input variables.
(The class variable can be thought of as the dependent variable;
the input variables as the independent variables.) The data space
is the n-dimensional space deﬁned by the n input variables. In a
classiﬁcation task, the goal is to derive the class probabilities, i.e.,
the marginal probabilities that an instance belongs to each class,
given values for some (or all) of the input variables.

The problem of accurately predicting class membership from
available information is a key challenge of knowledge discovery.
A wide variety of methods have been developed by machine learn-
ing and data mining researchers to solve this problem, ranging from
decision-tree learning algorithms to nearest-neighbor techniques to
Bayesian learning methods. The visualization techniques we have
developed are applicable to any learning methods whose output
makes predictions that can be interpreted as probabilities, such as
probabilistic decision trees or Bayesian networks. In the examples
given in this paper, we used the ADULT data set from the UCI
Machine Learning Repository [UCI 1999], which is derived from
U.S. Census data, to construct classiﬁcation models. We applied
Tree-Augmented Naive Bayes (TAN) [Friedman and Goldszmidt
1996], a Bayesian network learning system that is tailored for clas-
siﬁcation, to construct the models. Data instances contain fourteen
variables (six continuous and eight nominal) and a binary class la-
bel indicating income level (",False,2000.0,{},False,False,conferencePaper,False,HGH83ZAU,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/885740/,,Visualizing high-dimensional predictive model quality,HGH83ZAU,False,False
LIKTGL57,HN6EYPWT,"Visual Exploration of Classiﬁcation Models for Risk Assessment

Malgorzata Migut∗

Marcel Worring†

Intelligent Systems Lab Amsterdam

Intelligent Systems Lab Amsterdam

University of Amsterdam

University of Amsterdam

ABSTRACT
In risk assessment applications well informed decisions are made
based on huge amounts of multi-dimensional data.
In many do-
mains not only the risk of a wrong decision, but in particular the
trade-off between the costs of possible decisions are of utmost im-
portance. In this paper we describe a framework tightly integrating
interactive visual exploration with machine learning to support the
decision making process. The proposed approach uses a series of
interactive 2D visualizations of numeric and ordinal data combined
with visualization of classiﬁcation models. These series of visual
elements are further linked to the classiﬁer’s performance visual-
ized using an interactive performance curve. An interactive deci-
sion point on the performance curve allows the decision maker to
steer the classiﬁcation model and instantly identify the critical, cost
changing data elements, in the various linked visualizations. The
critical data elements are represented as images in order to trigger
associations related to the knowledge of the expert. In this context
the data visualization and classiﬁcation results are not only linked
together, but are also linked back to the classiﬁcation model. Such a
visual analytics framework allows the user to interactively explore
the costs of his decisions for different settings of the model and ac-
cordingly use the most suitable classiﬁcation model and make more
informed and reliable decisions. A case study on data from the
Forensic Psychiatry domain reveals the usefulness of the suggested
approach.
Keywords: Visual Analytics, Interactive Visual Exploration, De-
cision Boundary Visualization, Multi-dimensional Space, Classiﬁ-
cation

1 INTRODUCTION
Risk assessment is important in applications such as disaster man-
agement, security, medicine [1, 2] and forensics. Decisions have
to be made based on the analysis of multi-dimensional data. The
risk assessment expert has a difﬁcult task to understand such data,
make decisions based on it and moreover foresee the consequences
of those decisions. The crucial part of the decision making process
is determining the optimal balance between the cost of the different
decisions.

An example domain is the ﬁeld of Forensic Psychiatry where
the risk of criminal behavior of patients with psychiatric disorders
has to be predicted.
In this domain it is of high social impor-
tance that a suitable trade-off is achieved between incorrectly as-
sessed re-offending criminal patients and incorrectly assessed non
re-offending ones. Clearly, wrong prediction of re-offending has
a completely different impact than wrong prediction of non re-
offending, both for the society and for the patients themselves. An-
other example is from the medical domain, where a certain disorder,
for instance a genetic liver disorder, is to be diagnosed. The con-
sequences of both sort of mistakes, diagnosing the healthy patients

∗e-mail: M.A.Migut@uva.nl
†e-mail: M.Worring@uva.nl

IEEE Symposium on Visual Analytics Science and Technology 
October 24 - 28, Salt Lake City, Utah, USA 
978-1-4244-9486-6/10/$26.00 ©2010 IEEE 

and not diagnosing the ill patients, may be fatal.

Neither a fully automatic, nor a pure expert knowledge driven
assessment is an answer to the challenging requirements of risk as-
sessment. Combining automated data analysis, in particular ma-
chine learning and interactive visualization techniques with user
expertise in the speciﬁc domain, has been indicated to be useful
in various applications [1, 2, 3]. These techniques could help the
domain expert gain insight in the problem and make well-informed
decisions. This Visual Analytics approach, as formally described
by Keim [1], integrating the strength of the expert and the strength
of the machine is also promising for risk assessment.

In Visual Analytics, many different methods are used to visual-
ize multi-dimensional datasets such as scatterplots, heatmaps, par-
allel coordinates and parallel sets [4]. The visualizations should
relate to the domain speciﬁc understanding framework of the ex-
pert. Therefore, they should represent the data in the real attribute
values and not some (non-linear) projections. Furthermore, the dif-
ferent types of data, including numeric and ordinal, should be vi-
sualized in a consistent manner. To compensate for the potentially
limited expressiveness of such simple visualizations all visual el-
ements should be linked to each other revealing the relationships
between them [5]. Moreover, to utilize the knowledge of an expert
the data elements should be assigned a meaning. Images related to
the data elements could provide an expert with insightful associa-
tions. In Forensic Psychiatry the photographs of the patients could
be used to help an expert recall the information of the patient, ex-
panding the scope of the dataset. In the medical ﬁeld the exploration
of radiology images of the liver when diagnosing a certain disorder
allows the expert to directly relate measures derived from an image
to the actual content of the image.

Next to the visualization of the data, the core element in the
decision making process is an accurate and transparent predictive
model. Although machine learning techniques may exhibit excel-
lent performance, it is often difﬁcult to obtain intuitive understand-
ing of the induced model. The transparency of the model is essential
for the user to understand his decisions. Moreover, intuitive under-
standing of the classiﬁer obtained is important not only for the val-
idation of the model, but also to deepen the insight into the domain
and its underlying processes.
In order to help a user understand
domain intrinsics, it is favorable to obtain a visual comprehension
of what the classiﬁer looks like in the multi-dimensional space. To
enhance the understanding of the model, many visualization tech-
niques are used to present the results of the classiﬁer, such as ROC
curves or Precision and Recall graphs [6]. However, visualization
of the results alone does not give insight in the model.

One of the most informative characteristics of the classiﬁcation
model is the decision boundary. It can be easily visualized for two
and three dimensional datasets [6]. This allows a user to identify the
areas belonging to different classes for the displayed dimensions as
imposed by the classiﬁer. Thus it contributes to better comprehen-
sibility of the model. To allow better understanding of the model
in relation to the dataset we could also take advantage of expert
knowledge about data elements. Using the images representing the
data elements, as mentioned earlier, would trigger immediate asso-
ciations between the data and the model. Especially if visualizing
the critical data elements responsible for the cost change of the clas-
siﬁers output would contribute to awareness in cost selection.

11

In this work we propose an interactive risk assessment frame-
work. We interactively couple visualizations of different data types,
decision boundary and classiﬁer performance visualization and as-
sociative images representing the data to provide an expert with a
possibility to visually explore the classiﬁer and the costs of classi-
ﬁcation for different trade-offs.

This paper is organized as follows. In the next section we present
an overview of existing systems combining interactive visualiza-
tions with machine learning and their context of use. In the subse-
quent section we propose visualizations for different types of data
and the classiﬁers in relation to data elements. From there we show
how to make the visualization of the performance curve more in-
formative when linked with the visualizations of data. Following
the section describing classiﬁers performance visualization, we pro-
pose an interactive risk assessment system and show its application
in Forensic Psychiatry.

current decision tree is visualized and the user can proceed with
expanding the decision tree. A way of improving and analyzing a
classiﬁer is described in [3]. Starting from an initial hypothesis,
created with linking and brushing, the user steers a heuristic search
algorithm to look for alternative hypothesis generation.

Our work shows how the classiﬁers results (in terms of the cor-
rectly classiﬁed and misclassiﬁed data elements) can be visualized
and explored, with particular emphasis on the classiﬁcation costs
and the data elements with the highest risk factor. We show how
existing techniques can intelligently be integrated to support risk
assessment, by combining and structuring them in a highly interac-
tive way, as illustrated in ﬁgure 1.

3 VISUALIZATION OF CLASSIFICATION MODELS
The main visualization components constituting our system are the
visualizations of the data, the classiﬁer and the performance. The
choice of the techniques we use to facilitate these visualizations
is dictated by several requirements that emerge from the speciﬁcs
of the risk assessment problem. Therefore, we start this section
with the description of the problem and we motivate our choice of
techniques.

3.1 Problem description
Let us formalize the problem. Assume a training dataset with n
objects represented by feature vectors with numerical data in a p-
dimensional space. Here we consider the two class problem (with a
positive and negative class) and low dimensional spaces which are
typical for detection problems in risk assessment. Feature vectors
contain the variables of different data types. A classiﬁer is trained
on the dataset, resulting in a p-dimensional decision boundary. An
object classiﬁed as positive is called a true positive if the actual
value is also positive and is called a false positive if the actual value
is negative. The object classiﬁed as negative is called a true negative
if the actual value is negative and false negative if the actual value
is positive.

One of our tasks is to visualize the model. Several requirements
are imposed that are speciﬁc in a risk assessment context. First of
all the expert understands the data in the real feature values and can
therefore easily understand the model, only if related to the real fea-
ture values. The visualization of the data itself should be simple and
should preferably employ well known techniques, which the expert
is familiar with. To minimize the amount of visualized dimensions
and therefore make the exploration potentially easier for the expert
we could use one of many dimension reduction techniques. This
is however not desirable for several reasons. Those methods have
been designed to ﬁnd the most interesting 2D planes and to create
2D views that best summarize a multi-dimensional dataset. How-
ever their notion of ’interest’ has speciﬁc and restricted interpreta-
tion. They might not ﬁnd all the planes being interesting for the
expert in the context of risk assessment. Furthermore, and more
importantly for our purpose, methods that compute 2D projections
of multi-dimensional spaces are beyond the understanding of the
expert.

Moreover the expert should be stimulated to make use of his
knowledge about the data that is not contained in the data itself.
Therefore, a meaning has to be assigned to the data elements in
such a way that it has an associative role for the expert.

3.2 Numeric data
3.2.1 Data visualization
A taxonomy of multidimensional visualizations is provided by [11].
The categories listed include standard 2D/3D displays, geometri-
cally transformed displays, iconic displays, dense pixel displays,
and stacked displays.

Our requirement of simplicity, familiarity among users and vi-
sualizing the real values of dimensions accommodating high visual

Figure 1: The risk assessment framework proposed in this paper fol-
lowing the general Visual Analytics framework proposed by Keim[1].
The elements within the rectangle indicate the focus of this paper.
For privacy reasons we illustrate the usage of images in the frame-
work with publicly available photographs of celebrities as an example,
rather then the real data.

2 RELATED WORK
In recent years the idea of combining classiﬁcation with interactive
visualization has gained a lot of interest in the literature. A frame-
work for Visual Data Mining was formally described by Keim [1].
It proposes a tightly coupled system, with system ﬂows between
the automated data analysis and visualization elements. It more-
over employs user interaction to steer the visualization process and
to actively participate in the classiﬁcation process. Several of such
tightly integrated approaches have been proposed aiming at differ-
ent usage contexts. Building upon Keim’s framework, Yu et al. [7]
propose a smooth interface between data mining and visualization
for multimedia data in social and behavioral studies. All interme-
diate and ﬁnal results of data mining, in terms of found patterns,
are visualized allowing the user to obtain new insights and develop
more hypotheses about the data. Ankerst proposes the DataJewel
architecture [8] coupling a visual, an algorithmic and a database
approach for temporal data mining. The system focuses on the im-
provement of the discovery of useful patterns and not on how these
patterns are obtained. Interactive construction of decision tree clas-
siﬁers has been proposed in [9, 10]. The user can interactively se-
lect the splitting attribute from the dataset visualization. Then the

12

clarity, brings us to the scatterplot [12]. That implies trading in
some very good features of more complex visualization techniques,
but it does fulﬁll the requirements. As in a 2D scatterplot data el-
ements are drawn as points in the Cartesian space deﬁned by two
graphical axes deﬁned by the real attributes values, therefore ac-
commodating the understanding framework of the user. Scatter-
plots are also frequently used. They are basic building blocks in
statistical graphics and data visualization [13]. Multidimensional
visualization tools that feature scatterplots, such as Spotﬁre [14],
XmdvTool [15], Tableau/Polaris [16], GGobi [17], typically allow
mapping of data dimensions also to graphical properties such as
point color, shape, and size.

However, the number of dimensions that a single scatterplot can
reliably visualize is considerably less than many realistic datasets.
Therefore, a series of scatterplots should be visualized for all com-
binations of the dimensions, where all the dimensions can be ex-
plored by the user. However this approach yields little structure
to the visual exploration and provides no relation between the data
dimensions. Multiple plots can be arranged in a scatterplot matrix
[13] and we can link the multiple visualizations of combinations of
two dimensions in order to reveal relationships between them [5]
trough interactions.

3.2.2 Model visualization
We use scatterplots to visualize the results of a classiﬁcation model.
We use the color and the size to visually express the results of clas-
siﬁcation. Color expresses the original class membership of the
data. Size indicates whether the data element is misclassiﬁed by
the classiﬁer, if so it is assigned twice the regular size.

As mentioned in the introduction, the decision boundary is one
of the most informative characteristics of the classiﬁer. The vi-
sualization of multi-dimensional decision boundaries is however a
difﬁcult problem. Several attempts have been made to visualize
decision boundaries for multi-dimensional data [9, 18, 19]. How-
ever, those methods do not relate the visualization of the decision
boundary to the data projections of the axis that are meaningful
for the user. In [18] authors visualize the support vector machine
classiﬁer (SVM) using projection-based tour method. The authors
show visualizations of histograms of the data predicted class, visu-
alization of the data and the support vectors in 2d projections and
weighting the plane coordinates to choose the most important fea-
tures for the classiﬁcation. Poulet [9] displays histograms of the
data distribution according to the distances to the boundary and a
set of linked scatterplot matrices or parallel coordinates for SVM.
Further, Hamel [19] uses self-organizing maps to visualize results
of the SVM. These examples of methods to visualize the results of
the classiﬁers are all applied to SVM, with the exception of [9] who
proposes a method that can be applied to other classiﬁers like de-
cision trees or regression lines. The discussed approaches apply to
multi-dimensional data but are mostly speciﬁc to one type of clas-
siﬁcation model. What is needed is a uniform approach applicable
to any classiﬁcation method. Since we use scatterplots to visual-
ize the data we propose to visualize the multi-dimensional decision
boundary in series of scatterplots. However, the projection to 2D
of the multi-dimensional decision boundary onto this data projec-
tions would result in loss of separating information. Therefore, we
propose to use an approximation of the decision boundary, using
Voronoi tessellation.

3.2.3 Voronoi-based approximation of decision boundary
In short, a Voronoi diagram can be described as follows. Given a
set of points (referred to as nodes) a Voronoi diagram is a partition
of the space into regions, within which all points are closer to some
particular node than to any other node, see ﬁgure 2. Two Voronoi
regions that share a boundary are called Voronoi neighbors. We ap-
ply the Voronoi diagram to a combination of two dimensions used

by the classiﬁer. All the data objects are used as nodes to make
the Voronoi diagram. The boundaries of the Voronoi regions cor-
responding to neighbors belonging to different classes (according
to the labels assigned by a classiﬁer) form the decision boundary.
Such a representation of class separation for two given features is a
piecewise linear approximation of the actual decision boundary as
imposed by the multi-dimensional classiﬁer, see ﬁgure 2. The dis-
tances between the actual object and the decision boundary are not
preserved, but class membership is. To indicate the actual distances
to the decision boundary in the multi-dimensional space the poste-
rior probabilities for each data elements as returned by the classiﬁer
could be visualized.

(a) Scatterplot

(b) Mosaic plot

Figure 2: Visualizations of the results of a nearest mean classiﬁer for
a two-class dataset. (a) Scatterplot for the numerical data with the
Voronoi diagram together with an approximated decision boundary
following the Voronoi cells boundaries (thick solid line). For visual
clarity the classes are assigned different colors and the misclassiﬁed
data elements are assigned twice the size of the correctly classiﬁed
elements; (b) Mosaic plot for ordinal data. Each combination of the
ordinal value pairs gives rise to one large rectangle. The subrectan-
gles from left to right indicate the proportion of TN, FP, TP and FN
belonging to that block.

3.3 Ordinal data
3.3.1 Data visualization

There are several approaches to visualize ordinal data, where at-
tributes are ranked categories. Since we have chosen to visualize
numeric data using series of scatterplots, we visualize the ordinal
data in a consistent way. Therefore, we use mosaic plots which are
graphical displays to examine the relationship among two or more
categorical variables. First proposed by [20] they are now used
in visualization toolkits such as Mondriaan [21] and Manet [22].
Based on an analogy to the scatterplots we visualize two attributes
at the time, where ranked categories are assigned to the vertical and
horizontal axis. This results in a series of plots for all combinations
of categories.

In a mosaic plot each possible combination of categories is a cell
and is represented by a rectangle proportional in area to the num-
ber of data elements in it. A mosaic plot shows quickly whether
particular combinations of cases dominate the data set. The appear-
ance of a mosaic plot depends on the order in which the categorical
variables are drawn. The categories of the ﬁrst variable divide the
horizontal axis into columns whose widths are proportional to their
numbers. The categories of the second variable divide up the verti-
cal axis similarly. To simplify, we make an assumption that the two
variables are independent. Therefore, the separating gaps between
all levels are lined-up [23]. Another option would be to allow one
of the attributes to be dependent on the other, but this would yield
a visualization which would not be consistent with scatterplots for
numeric data.

13

erating Characteristics (ROC) curve, often used in medicine, vi-
sualizes the trade-offs between hit rate and false alarm rate [27].
The Precision and Recall curve often used in information retrieval
depicts the trade off between the fraction of retrieved documents
relevant to the search and the fraction of the documents relevant
to the query successfully retrieved. In risk assessment applications
the balance between the misclassiﬁcation in each class is of inter-
est. Therefore, the performance curve should depict the trade-off
between the classiﬁer’s errors for both classes. The curve, as we
use it, represents the trade-off between the False Positives and False
Negatives. Let FP be the number of incorrectly classiﬁed negative
data items and NP the total number of positive examples.
The false positive rate, FRr of a classiﬁer is:

F P r =

F P
N P

The false negative rate, FNr of a classiﬁer is:

F N r =

F N
N N

(1)

(2)

where FN are the incorrectly classiﬁed positive items and NN the
total number of negative items. On the performance graph FNr is
plotted on the Y axis and FPr is plotted on the X axis. These statis-
tics vary with a threshold on the classiﬁer’s continuous outputs. We
note several points on the curve. The upper left point (0,1) repre-
sents the classiﬁer that mis-classiﬁes all positive data elements, the
lower right point (1, 0) represents the classiﬁer that mis-classiﬁes
all negative data elements. The point (0,0) represents perfect clas-
siﬁcation and the line y=x represents the random guess. Such a
performance curve allows visual comparison of error trade-offs of
a set of classiﬁers. The trade-off of the current classiﬁer is visual-
ized by means of an operating point.

3.6 Images of critical data elements
An expert and his knowledge are an integral part of the visual an-
alytics based risk assessment process. To take advantage of his
knowledge in an effective way we suggest to trigger the associations
he/she has with the data elements. Purely from the visualization of
the data through scatterplots and mosaic plots, or any other graph-
ical displays, an expert is not always able to recall the meaning of
a particular data item. To assign a meaning to the data elements we
propose to use images that refer to the data in some way. For the
datasets consisting of features of patients, the photos of the patients
could be used. For the database consisting of the symptoms of a
certain liver disorder, the radiological images of an organ could be
used. Assigning images to the data elements might have a ”trig-
ger” effect on an experts mind, stirring conscious and unconscious
memories about certain data elements. In particular, for character-
istic data elements the associations can be strong. We distinguish
two cases where the images play a role in the exploration of data
and classiﬁcation results. First, when the images are not used as
data in the classiﬁcation, they are used to support the visual ex-
ploration and have merely an associative role. When viewing the
images in this case one should not focus on the physical proper-
ties of the object on the images, but on the associations that these
objects invoke. Secondly, measured characteristics of the images
might also be a part of a feature vector used to train the classiﬁer. In
the context of risk assessment we would like to focus the attention
of an expert on the most important data elements in the cost selec-
tion process. As most important we consider those elements which
are assigned a different label by the classiﬁer when changing the
classiﬁers trade-off. We call those critical elements and propose to
visualize them instantly whenever the classiﬁcation model changes.
Figure 4 shows how we visualize the images for the critical data ele-
ments, occurring in between adjacent operating points. In Forensic

Figure 3: Example of a matrix overview showing 6 dimensions of
a dataset. The overview matrix contains scatterplots for numerical
attributes and mosaic plots for ordinal attributes.

3.3.2 Model visualization
For each block of the mosaic plot we want to present information
about the classiﬁcation results. We use the following methodology.
In each block we visualize the proportion of TP, TN, FP and FN
according to the labels assigned by the classiﬁer. First we assign
a color to both classes, according to the original labels. Parts of
the block containing correctly classiﬁed elements are visualized ac-
cording to the original class color. Parts containing misclassiﬁed
elements are visualized in the darker tint of the original class color.
In this way we make an analogy to different sizes of the visualized
points in the scatterplots. To make the visualization of the classiﬁ-
cation results even more consistent with the scatter plot, we ﬁrst di-
vide each block into parts as divided by the classiﬁer. Therefore, in
each block a division occurs which could be considered a decision
boundary in the scatterplot. Further we show, as in a scatterplot, the
blocks containing misclassiﬁed elements on both sides of the deci-
sion boundary. Figure 2 shows a mosaic plot for two categorical
ranked features.

3.4 Matrix overview
Obviously the visualization of multi-dimensional data by means of
2D plots will result in many scatterplots which are neither easy not
clearly arrangeable. To deal with the large amount of visualized
plots we use the widely known scatterplot matrix and order the di-
mensions according to a suitable criteria. The dimensions could be
ordered according to some interestingness criterion. Ankerst [24]
proposes to place the similar dimensions next to each other, after
deriving the similarity metrics. In [12] Elmqvist et al. follow this
method, deﬁning the similarity measure as absolute correlation of
pairwise dimensions to order the rows and a dissimilarity measure
inverse of the absolute correlation to order the columns. Tatu [25]
developed methods to ﬁnd the best scatterplots showing the sepa-
rating classes.

We could also develop a criterion based on the model visual-
ization for the numeric data, where good views should show good
class separation. The minimal overlap of classes would then be the
the notion of importance of dimensions. In the risk assessment con-
text the dimensions could be ordered according to the order which
is natural for the expert. The order of rows and columns could
be changed manually by the user in an interactive way, as used in
Polaris [16], XmdvTool [15], ScatterDice[12]. A notion of interest-
ingness is then not included, however the understanding framework
of the expert is supported. To use the space of the scatterplot ma-
trix more efﬁciently we follow the example of [26]. We remove the
plots on the diagonal, which are the plots of the variable with itself,
and replace them with the names of variables plotted in the matrix.

The matrix overview is shown in ﬁgure 3.

3.5 Performance visualization
In general performance curves such as Precision and Recall graphs
or ROC curves capture the ranking performance of the binary clas-
siﬁer, as its discrimination threshold is varied. The Receiver Op-

14

Figure 4: Our framework for a visual risk assessment system, elaborating upon ﬁgure 1, with the proposed visualization solutions and interaction
techniques. Both system and user actions are indicated.

Psychiatry for example the expert examines his patients and there-
fore possesses extensive knowledge about them. Adding the images
of the patients to the data visualization could be helpful in the explo-
ration of the dataset and the classiﬁer. Since the data elements are
visualized in relation to the classiﬁer and the expert can associate
them with actual patients he exactly knows how particular patients
relate to the classiﬁcation model. He also can view the group of
patients that is in the direct neighborhood of the recognized patient
and therefore associate other patients with him. Therefore, the im-
age is a sort of orientation point for the general group of patients,
which might be unknown to the expert. One might argue that by
showing the images we introduce a bias, since we allow free as-
sociations of the expert that can not be justiﬁed and are deﬁnitely
not objective. However, the expert makes unconscious associations
all the time. Therefore, we can take advantage of it and structure
steering of the exploration process in this way. Moreover we put
constraints on expert actions concerning the change of the classi-
ﬁcation model based on image associations. This is described in
section 4.1.

4 INTERACTIVE RISK ASSESSMENT
The purpose of this work is to support risk assessment. The tech-
niques we described in the previous sections are well established
methods. Combining them into a highly interactive Visual Analyt-
ics framework, makes them suitable for the risk assessment task.
In this section we describe how we couple the elements in order to
design an interactive tool for data and classiﬁer exploration, partic-
ularly including the trade-off selection in the context of risk assess-
ment.

Therefore, several interaction techniques are integrated into our
method which form a minimum necessary set of interaction tech-
niques for the interactive risk assessment. We chose the techniques
from the 7-categories of interaction techniques proposed in the
study of [28]. They propose a well structured way to handle inter-
activity based on the users’ functional needs. We make a distinction
between the interaction techniques that are strictly user based and
interactions that are triggered by the user, but result in pre-deﬁned
system actions. Figure 4 shows the possible interaction techniques
in the system.

4.1.1 Select
The selection interaction technique aims at making the elements of
interest visually distinctive. An expert can highlight the element of
interest in the scatterplot or a rectangle in the mosaic plot, resulting
in a color change of the selected element. This allows to keep track
of elements of interest. The expert can also de-select the element if
he is no longer interested in it. In the overview matrix the currently
viewed plots are selected with the navigation lens.

4.1.2 Explore
Explore interaction techniques enable users to examine a different
subset of data cases. In the matrix overview an expert can explore
all the plots using the navigation lens. Direct-Walk allows users to
smoothly move the viewing focus from one position in the informa-
tion structure to another by ”a series of mouse movements”. That
occurs when moving the operating point on the performance curve
and by including certain data elements into a speciﬁc class. This is
described in detail in the subsequent section.

4.1 Interaction Techniques
Through the interactive data exploration the expert can not only
gain insight into the data but also into the classiﬁcation process.

4.1.3 Connect
Connect is used to highlight relationships between data items and
show hidden relations between relevant items. In our system we

15

Figure 5: Example of the connect interaction. Figure in the middle shows the initial state of the system, with initial operating point on the
performance curve and corresponding visualization of the model for scatter- and mosaic plot. Figure on the left shows the state of the system
when an expert manipulates the operating point to include more False Negatives and on the right to include more False Positives. The updated
model is visualized in the scatterplot and mosaic plot, together with the critical element represented as photos and highlighted in all visual
components.

connect all the visual displays. First of all the visualizations of data
elements are connected. All the elements selected on one scatter-
plot are instantly selected on all the other displayed plots. Thus
it allows to see the relations between the decision boundary and
selected elements for all the dimensions visualized. Moreover we
connect the plots with the performance curve, illustrated in ﬁgure 5.
That connection is twofold.

On the one hand we propose to use an interactive operating point
on the performance curve connected with the visualizations of the
model on the plots. An operating point on the performance curve
represents the current classiﬁer. Without knowledge of the domain
this is often chosen by optimizing the Equal Error rate(i.e. FP=FN).
In the context of risk assessment, the expert is not always interested
in the lowest combined classiﬁcation error. Therefore, we allow
the user to change the position of the operating point on the perfor-
mance curve. In this way the user can choose the suitable trade-off
for his application. The interactive operating point is then directly
connected to the classiﬁcation model. This means that the classi-
ﬁer is updated according to the new FP rate and FN rate. Since
the visual displays are connected we instantly observe what effect
the change of trade-off has on the classiﬁer. The updated classi-
ﬁer for the adjusted operating point is directly visible for all data
plots. Moving the operating point also triggers the display of the
images of the critical data elements. Those elements are moreover
highlighted on the scatterplots and the system highlights the com-
bination of categories they belong to on the mosaic plot.

On the other hand we allow the user to update the operating point
and therefore the classiﬁcation model, through selection of data el-
ements in the plots. The expert is allowed to choose a data ele-
ment and require the classiﬁer to assign it to a different class. The
model is then instantly updated, together with the corresponding

trade-off on the performance curve. The images representing criti-
cal elements are automatically displayed and critical data elements
are highlighted on all the plots. It is important to note that an expert
can only change the decision boundary as a whole by selecting a
different operating point. So forcing for example a False negative
to become posituive would also introduce new False Positives. In
such a way he can explore whether a change of the model for the
particular data element results in a serious cost change and which
other data elements are affected.

4.1.4 Elaborate

Elaborate allows the user to see more details, hence to adjust the
level of abstraction of the data representation. We use two tech-
niques belonging to this category.

Details-on-demand allows users to examine all the information
of a particular data point. The values of all attributes of that
data point are displayed together with the true and estimated la-
bel. Moreover the critical elements can be displayed, when the user
aims at placing that element on another side of the decision bound-
ary (”pre-deﬁned” is always triggered by the explore interaction
technique). Another technique that triggers details-on-demand is
exploration of the operating point on the performance curve, which
results in the visualization of images representing the critical data
elements.

Zooming in/out allows users to simply change the scale of the
representation so that the user can see a detailed view of a part of
the dataset.
In the matrix overview an expert can move the lens
which will directly result in displaying the selected choice of plots.

16

4.2 Visual design
The system we propose consists of several visual components. The
implementation of the system is shown in ﬁgure 6. The main com-
ponents of the system are:

• Overview Window: serves both as an overview and a naviga-
tion tool. Miniature versions of the individual plots are shown
as the cells of the matrix, and the lens that is used for naviga-
tion is placed above the plots that are currently displayed in
the main window. The plots are visualized together with the
decision boundary.

• Main Window: displays three plots currently chosen by the
expert using lens navigation in the matrix overview. The per-
formance curve is also part of the Main Window.

• Statistics Window: displays the statistical information on the
classiﬁcation model for the current interactive operating point,
such as the error, amount of TP and TN.

• Details Window: includes the information on a particular in-
teractively selected data element, including the photograph as-
sociated with this element.

5 APPLICATION OF INTERACTIVE RISK ASSESSMENT TO

FORENSIC PSYCHIATRY

As indicated earlier, one of the practical ﬁelds where interactive
risk assessment could be of great help is Forensic Psychiatry. In
the Dutch legal system forensic experts often have to advise the
court whether a sentenced mentally disordered criminal should be
released back to the society or should be treated in a closed men-
tal institution. In fact, the forensic expert has to assess the risk of
re-offending of such a patient. Obviously from the perspective of
the safety of the society releasing a potential recidivist has greater
consequences then keeping a potential non-recidivist imprisoned.
However, the release of the patient to the society is an integral
part of the treatment of mentally ill patients. Therefore informed
choices have to be made to minimize the risk of making severe mis-
takes but the costs of mistakes in disadvantage of the patient should
also be avoided.

5.1 Case study
As a case study we use a dataset of forensic psychiatric patients in
the Netherlands provided by the Expertise Center for Forensic Psy-
chiatry (EFP) [29]. The dataset consists of 100 male offenders who,
at the time of the alleged crime, suffered from mental disorder and
received what is called a ”disposal to be involuntarily admitted to
a forensic psychiatry hospital on behalf of the state” (TBS-order).
The TBS-orders have been terminated based exclusively on the pro-
fessional expertise of the clinicians. Each patient is assigned a class
label indicating whether he has been convicted for a new crime after
his TBS-order has been terminated. Of the 100 defendants, 37 were
convicted again, whereas 63 are non-recidivists. The 20 ordinal fea-
tures are the scores of the PCL-R (Psychopathic Checklist-Revised)
test. The 7 numeric features are the summed combinations of the
PCL-R outcomes. Patients were retrospectively scored with this
risk assessment measures and recidivism data was retrieved from
the documentation of the Dutch Ministry of Justice. To visualize
the patients we assign a red color to recidivists and blue to non-
recidivists. As we can not use the real photos for this dataset due to
privacy reasons. As an example we use a set of images of celebrities
downloaded from the internet. These images are used to illustrate
merely the design of the system and how the images are interac-
tively coupled to the other visual elements. To illustrate the usage
of the system we choose to focus only on the results of one clas-
siﬁer. Table 1 presents the 10-fold cross-validation error, over 3

Figure 6: Screenshot of the system.

repeats for selected classiﬁers trained on 27 features. The differ-
ence between them seems statistically insigniﬁcant. We choose to
visualize the nearest mean classiﬁer as it performs well and has a
low standard deviation.

Table 1: Performance of the selected classiﬁers for PCL-R dataset
obtained with 10-fold cross-validation, with 3 repeats.
Cross-val error %

Classiﬁer

Nearest Mean

Logistic Regression

Fisher

Support Vector Machine

Decision Tree

30.3
31.3
31.0
31.3
33.6

std
± 0.5
± 1.5
± 1.0
± 1.0
± 4.04

5.2 Usage scenario
We now demonstrate how our system can be used for visual explo-
ration of the nearest mean classiﬁer trained on the criminal patients
dataset. We describe a basic scenario. An expert, Analyst X, has
been provided with a newly developed classiﬁcation model. An-
alyst X is asked to explore the model applied to the patient data
which she is familiar with.

First the dataset is loaded and the model applied to it. All pos-
sible combinations of dimensions are visualized together with the
decision boundary for the current classiﬁer output in the matrix in
the Overview Window. The classiﬁcation error and the performance
curve with current operating point are displayed. Figure 6 shows
the screenshot of the system that is now available for the expert.

Analyst X ﬁrst looks at the Overview Window to get an overall
feeling of the visualization of the dataset. She moves to the elabo-
rate view (Main Window), where three currently selected plots from
the matrix (Overview Window) are displayed.

First she explores the connected plots. She selects two inter-
esting looking data elements on the scatterplot. Both elements are
patients who did not re-offend, but they are classiﬁed as recidivists.
On all the visible plots in the Main Window the patients are high-
lighted. They are located in the areas representing high values of
the attributes. She questions herself whether it can be explained
why those patients have such high attributes values and they still
did not re-offend. Analyst X elaborates on these two patients and

17

asks for their details. When she looks at the photos she realizes
that one of the patients had a very good re-socialization conditions
and strong network of people taking care of him, that could prevent
him from re-offending. For the other patient however, she does not
seem to have an explanation.

Then she starts exploring the performance curve. She moves
along the curve varying the operating point. The operating point is
connected to all the plots, so she observes the instant changes in the
model visualization in the Main Window. She is also observing the
pop-up photos of critical patients. While she moves the operating
point to include less False Negatives, as she is not satisﬁed with so
many recidivists being misclassiﬁed, she notices among the images
a patient that is familiar to her. She asks for the details. The scores
indicating the violence behavior of the patient are indeed not very
high indicating the average risk of re-offence. However she recog-
nizes Patient X and realizes he is very bad in taking his medication.
Therefore the risk of re-offending when he leaves the clinic and has
no one to control his medication use, would increase. She requires
the model to include that patient into True Positives. She explores
the result on the connected plots, where the model is updated and
the performance curve, where the operating point is updated. Based
on that limited exploration she ﬁnds the updated model suitable for
use in her clinic.

6 DISCUSSION AND FUTURE WORK
This paper proposes an interactive approach to risk assessment. The
main idea is to provide an expert a framework tightly integrating
interactive visual exploration with machine learning taking into ac-
count the demanding requirements of risk assessment applications.
We propose to couple visualizations of numeric and ordinal data,
classiﬁers visualization, performance visualization and associative
images representing meaningfully the data elements in order to sup-
port an expert in the visual exploration of the dataset and the clas-
siﬁcation model. In particular, we focus on the cost selection of the
classiﬁcation model. We have shown how the existing techniques
can be tightly coupled in a structured and highly interactive way.
The most important next step is to conduct systematic empirical
evaluation of the system. We plan to use multiple rounds of evalu-
ation with the end users to test the usefulness of the approach and
identify the areas of improvement. However user evaluation is dif-
ﬁcult for such broad tasks as visual exploration, so we anticipate
performing time consuming qualitative studies.

7 ACKNOWLEDGMENTS
This research is supported by the Expertise center for Forensic Psy-
chiatry, The Netherlands.

REFERENCES
[1] D. A. Keim, F. Mansmann, J. Schneidewind, J. Thomas, and
H. Ziegler. Visual analytics: Scope and challenges. pages 76–90,
2008.

[2] J.J. Thomas and K.A. Cook. Illuminating the Path: The Research and

Development Agenda for Visual Analytics. IEEE CS Press, 2005.

[3] R. Fuchs, J. Waser, and M.E. Gr¨oller. Visual human+machine learn-

ing. IEEE TVCG, 15(6):1327–1334, October 2009.

[4] F. Bendix, R. Kosara, and H. Hauser. Parallel sets: Visual analysis of
categorical data. In INFOVIS ’05: Proceedings of the Proceedings of
the 2005 IEEE Symposium on Information Visualization, 2005.

[5] C. Collins and S. Carpendale. VisLink: Revealing relationships
amongst visualizations. IEEE Transactions on Visualization and Com-
puter Graphics, 13(6), 2007.

[6] R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. Wiley-

Interscience Publication, 2000.

[7] C. Yu, Y. Zhong, T. Smith, I.n Park, and W. Huang. Visual data min-
ing of multimedia data for social and behavioral studies. Information
Visualization, 8(1):56–70, 2009.

18

[8] M. Ankerst, D. H. Jones, A. Kao, and C. Wang. Datajewel: Tightly
integrating visualization with temporal data mining. ICDM Workshop
on Visual Data Mining, 2003.

[9] F. Poulet. Towards Effective Visual Mining with Cooperative Ap-

proaches. Springer-Verlag, Berlin, Heidelberg, 2008.

[10] M. Ankerst, M. Ester, and H. P. Kriegel. Towards an effective coop-
eration of the user and the computer for classiﬁcation. In Knowledge
Discovery and Data Mining, pages 179–188, 2000.

[11] D. A. Keim.

Information visualization and visual data mining.
IEEE Transactions on Visualization and Computer Graphics, 8(1):1–
8, 2002.

[12] N. Elmqvist, P. Dragicevic, and J.D. Fekete. Rolling the dice: Mul-
tidimensional visual exploration using scatterplot matrix navigation.
IEEE Transactions on Visualization and Computer Graphics (Proc.
InfoVis 2008), 14:1141–1148, 2008.

[13] W.A. Cleveland and M. E. McGill. Dynamic graphics for statistics.

Statistics/Probability Series, 1988.

[14] Spotﬁre Inc. Spotﬁre. http://www.spotﬁre.com, 2007.
[15] M. O. Ward. Xmdvtool: integrating multiple methods for visualizing
multivariate data. In VIS ’94: Proceedings of the conference on Visu-
alization ’94, pages 326–333. IEEE Computer Society Press, 1994.

[16] C. Stolte, D. Tang, and P. Hanrahan. Polaris: A system for query,
analysis, and visualization of multidimensional relational databases.
IEEE Transactions on Visualization and Computer Graphics, 8(1):52–
65, 2002.

[17] D. F. Swayne, D. T. Lang, A. Buja, and D. Cook. Ggobi: evolving
from xgobi into an extensible framework for interactive data visual-
ization. Computational Statistics and Data Analysis, 43(4):423–444,
2003.

[18] D. Caragea, D. Cook, and V. G. Honavar. Gaining insights into support
vector machine pattern classiﬁers using projection-based tour meth-
ods. In KDD ’01: Proceedings of the seventh ACM SIGKDD inter-
national conference on Knowledge discovery and data mining, pages
251–256, 2001.

[19] L. Hamel. Visualization of support vector machines with unsupervised
learning. In Proceedings of 2006 IEEE Symposium on Computational
Intelligence in Bioinformatics and Computational Biology, 2006.

[20] A. Unwin, G. Hawkins, H. Hofmann, and B. Siegl. Mosaic for con-
tingency tables. Computer Science and Statistics: Proceedings of the
13th Symposium on the interface.

[21] M. Theus and S. Urbanek.

Interactive Graphics for Data Analy-
sis: Principles and Examples (Computer Science and Data Analysis).
Chapman & Hall/CRC, 2008.

[22] A. Unwin, G. Hawkins, H. Hofmann, and B. Siegl. Interactive graph-
ics for data sets with missing values: Manet. Journal of Computa-
tional and Graphical Statistics, 5(2):113–122, 1996.

[23] M. Theus and S.R.W. Lauer. Visualizing of loglinear models. Journal

of Computational and Graphical Statistics, 8(3):396–412, 1999.

[24] M. Ankerst, S. Berchtold, and D.A. Keim. Similarity clustering of
dimensions for an enhanced visualization of multidimensional data.
In INFOVIS ’98: Proceedings of the 1998 IEEE Symposium on In-
formation Visualization, page 52, Washington, DC, USA, 1998. IEEE
Computer Society.

[25] A. Tatu, G. Albuquerque, M. Eisemann, J. Schneidewind, H. Theisel,
M. Magnor, and D. Keim. Combining automated analysis and visual-
ization techniques for effective exploration of high-dimensional data.
In Proceedings of the IEEE Symposium on Visual Analytics Science
and Technology (IEEE VAST), pages 59–66, Atlantic City, New Jer-
sey, USA, 10 2009.

[26] J. Heer, M. Bostock, and V. Ogievetsky. A tour through the visualiza-

tion zoo. Commun. ACM, 53(6):59–67, 2010.

[27] T. Fawcett. An introduction to ROC analysis. Pattern Recogn. Lett.,

27(8):861–874, 2006.

[28] J.S. Yi, J. Kang, J. Stasko, and J. Jacko. Toward a deeper under-
standing of the role of interaction in information visualization. IEEE
Transactions on Visualization and Computer Graphics, 13(6):1224–
1231, 2007.

[29] H. Hildebrand, B.L. Hesper, M. Spreen, and H.L.I. Nijman. The value
of structured sisk assessment and the diagnosis of Psychopathy (in
Dutch). Expertise Center for Forensic Psychiatry, 2005.

",False,2010.0,{},False,False,conferencePaper,False,LIKTGL57,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/5652398/,,Visual exploration of classification models for risk assessment,LIKTGL57,False,False
494B5SDV,8CJ4C9Z3,"PaintingClass: Interactive Construction, Visualization and

Exploration of Decision Trees

Soon Tee Teoh

Department of Computer Science

University of California, Davis

Kwan-Liu Ma

Department of Computer Science

University of California, Davis

teoh@cs.ucdavis.edu

ma@cs.ucdavis.edu

ABSTRACT
Decision trees are commonly used for classiﬁcation. We propose
to use decision trees not just for classiﬁcation but also for the wider
purpose of knowledge discovery, because visualizing the decision
tree can reveal much valuable information in the data. We introduce
PaintingClass, a system for interactive construction, visualization
and exploration of decision trees. PaintingClass provides an intu-
itive layout and convenient navigation of the decision tree. Paint-
ingClass also provides the user the means to interactively construct
the decision tree. Each node in the decision tree is displayed as a
visual projection of the data. Through actual examples and com-
parison with other classiﬁcation methods, we show that the user
can effectively use PaintingClass to construct a decision tree and
explore the decision tree to gain additional knowledge.

Categories and Subject Descriptors
H.1.2 [Information Systems]: User/Machine Systems—Human
Information Processing; H.2.8 [Database Management]: Database
Applications—Data Mining; I.3.6 [Computer Graphics]: Method-
ology and Techniques

Keywords
visual data mining, classiﬁcation, decision trees, information visu-
alization, interactive visualization

1.

INTRODUCTION

Classiﬁcation of multi-dimensional data is one of the major chal-
lenges in data mining. In a classiﬁcation problem, each object is
deﬁned by its attribute values in multi-dimensional space, and fur-
thermore each object belongs to one class among a set of classes.
The task is to predict, for each object whose attribute values are
known but whose class is unknown, which class the object belongs
to. Typically, a classiﬁcation system is ﬁrst trained with a set of
data whose attribute values and classes are both known. Once the
system has built a model based on the training, it is used to assign a
class to each object. For example, neural networks have been used
effectively for this purpose in algorithms such as [17] and [21].

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGKDD ’03, August 24-27, 2003, Washington, DC, USA
Copyright 2003 ACM 1-58113-737-0/03/0008 ...$5.00.

A decision tree classiﬁer ﬁrst constructs a decision tree by re-
peatedly partitioning the dataset into disjoint subsets. One class
is assigned to each leaf of the decision tree. Most classiﬁcation
systems, including most decision tree classiﬁers, are designed for
minimal user intervention. More recently, a few classiﬁers have
incorporated visualization and user interaction to guide the classi-
ﬁcation process. On one hand, visual classiﬁcation makes use of
human pattern recognition and domain knowledge. On the other
hand, visualization gives the user increased conﬁdence and under-
standing of the data [2, 3, 8].

A representative visual classiﬁcation systems include Ankerst et
al.’s PBC (Perception-Based Classiﬁer) [2]. In [20], we proposed
StarClass, a new interactive visual classiﬁcation technique. Star-
Class allows users to visualize multi-dimensional data by project-
ing each data object to a point on 2-D display space using Star
Coordinates [11]. When a satisfactory projection has been found,
the user then partitions the display into disjoint regions; each re-
gion becomes a node on the decision tree. This process is repeated
for each node in the tree until the user is satisﬁed with the tree and
wishes to perform no more partitioning.

In this paper, we introduce PaintingClass, a complete user-directed

decision tree construction and exploration system. PaintingClass
uses some ideas from StarClass, but has two main features as its
contributions:

(cid:15) PaintingClass introduces a new decision tree exploration mech-

anism, to give users understanding of the decision tree as
well as the underlying multi-dimensional data. This is im-
portant to the user-directed decision tree construction process
as users need to efﬁciently navigate the decision tree to grow
the tree.

(cid:15) PaintingClass extends the technique proposed to StarClass so
that datasets with categorical attributes can also be classiﬁed.
Many real-world applications use data containing both nu-
merical and categorical attributes; therefore PaintingClass is
much more useful than StarClass. We show the effectiveness
of PaintingClass in classifying some benchmark datasets by
comparing accuracy with other classiﬁcation methods.

These features make PaintingClass an effective data mining tool.
PaintingClass extends the traditional role of decision trees in classi-
ﬁcation to take on the additional role of identifying patterns, struc-
ture and characteristics of the dataset via visualization and explo-
ration. This paradigm is a major contribution of PaintingClass. In
this paper, we show some examples of knowledge gained from the
visual exploration of decision trees.

2. PAINTINGCLASS OVERVIEW

667

As is typical of decision tree methods, PaintingClass starts by
accepting a set of training data. The attribute values and class of
each object in the training set is known. In the root of the Paint-
ingClass decision tree, every object in the training set is projected
and displayed visually. In PaintingClass, each non-terminal node
in the decision tree is associated with a projection, which is a def-
inite mapping from multi-dimensional space into two-dimensional
display. The user creates a projection that best separates the data
objects belonging to different classes.

Each projection is then partitioned by the user into regions by
painting. Next, for each region in the projection, the user can
choose to re-project it, forming a new node. In other words, the user
creates a projection for this new node in a way that best separates
the data objects in the region leading to this node.

For every new node formed, the user has the option of partition-
ing its associated projection into regions. The user recursively cre-
ates new projection/nodes until a satisfactory decision tree has been
constructed. Each projection thus corresponds to a non-terminal
node in the decision tree, and each un-projected region thus cor-
responds to a terminal node. In this way, for each non-root node,
only the objects projecting onto the chain of regions leading to the
node are projected and displayed.

In the classiﬁcation step, each object to be classiﬁed is projected
starting from the root of the decision tree, following the region-
projection edges down to an un-projected region, which is a ter-
minal node (ie. a leaf) of the decision tree. The class which has
the most training set objects projecting to this terminal region is
predicted for the object.

The following sections will explain in more detail how projec-

tions are created and how regions are speciﬁed.

3. VISUAL PROJECTIONS USED IN PAINT-

INGCLASS

In StarClass [20], we used Star Coordinates [11] to project data
deﬁned in higher-dimensional to two-dimensional display. In Paint-
ingClass, we use a similar but less restrictive paradigm. Instead of
using only Star Coordinates, we conceptually can use any sensible
projection method. This is important because each different pro-
jection method has its own advantages and drawbacks. Since each
dataset has its unique characteristics, choosing the most appropri-
ate method for a dataset or a subset of a dataset can better reveal
the underlying structure of the data. In our current implementation,
we allow the user to choose between Star Coordinates and Parallel
Coordinates [9] projection methods. Star Coordinates is better at
showing dimensions with numerical attributes and Parallel Coor-
dinates is better at showing dimensions with categorical attributes.
With just this simple choice between Star and Parallel Coordinates,
PaintingClass can be used to classify datasets with both numerical
and categorical attributes, giving it much wider applicability than
StarClass.
3.1 Star Coordinates

Star Coordinates was ﬁrst proposed as a method for visualizing

multi-dimensional clusters, trends, and outliers. The two-dimensional
screen position of an n-dimensional object is given by the vector
sum of all unit vectors on each coordinate multiplied by the value of
the data element in that coordinate. Each unit vector corresponds to
one dimension and is shown by a line on the display. To edit a Star
Coordinates projection, the user manually moves an axis around by
clicking on the end-point of the selected axis and dragging it to the
desired position.
3.2 Parallel Coordinates

668

Figure 1: Parallel coordinates projection of the Australian
dataset using the categorical dimensions. The categorical at-
tribute values are enumerated. Since in some dimensions, the
categorical attribute values can have only a few discrete values,
many lines overlap. The resulting image does not convey much
useful information.

Figure 2: Modiﬁed parallel coordinates: Each integer value is
mapped to an interval of length half the distance between one
integer value and the next, and the mapping of an object to a
parallel axis within this interval is determined by the ID num-
ber of the object. In this way, more lines are visible. More
information is conveyed, for example the reader can verify that
between the ﬁfth and sixth axes, most of the objects in the pur-
ple class are projected to the lower half.

Parallel Coordinates [9] is a widely-used multi-dimensional data
visualization method. Each dimension is represented by a vertical
line, whose end-points represent the maximum and minimum value
of the dimensionl. Each object therefore maps to one point on each
of these lines, according to its attribute value in that dimension. The
object is then displayed as a poly-line connecting all the points.

In PaintingClass, we slightly modify the standard Parallel Coor-
dinates to better display the categorical dimensions. First, for each
categorical dimension, all the possible attribute values are enumer-
ated. Since the enumerated values are all integer values, and in
some dimensions the number of distinct values is small, the re-
sulting parallel projection contains many lines completely obsur-
ing other lines. The information conveyed is therefore not useful.
Since the enumerated categorical values are all integer values, in
PaintingClass, each integer value is mapped to an interval of length
half the distance between one integer value and the next, and the
mapping of an object to a parallel axis within this interval is de-
termined by the ID number of the object. Furthermore, lines are
rendered in a semi-transparent manner. In this way, lines do not
completely obscure each other. Figures 1 and 2 show the contrast
between the original and the modiﬁed methods.

Another problem encountered in Parallel Coordinates visualiza-
tion of categorical data is that when there are too many distinct val-
ues in a dimension, the interval of each value becomes too small,
making it hard to select. Therefore, PaintingClass allows the user
to specify a section on any axis to zoom in on.

Figure 3: Painting regions in a parallel coordinates projection.
The user clicks on an interval to toggle it between blue and red.
In this ﬁgure, the top interval in the ﬁfth dimension has been
set to red. The objects belonging to the red region are shown
in the left picture, and the objects belonging to the blue region
are shown in the right picture. This separates the data shown
in Figure 2 into two regions. In the blue region, nearly all the
objects belong to the purple class, whereas in the red region,
the majority of the objects belong to the green class, though
there are still some purple class objects, so further separation
is necessary.

4. PAINTING REGIONS

The main idea behind the decision tree construction method is
to identify regions in the projected two-dimensional display that
would separate the objects of different classes as much as possible.
In a Star Coordinates projection, this is done by moving the axes
around until a satisfactory projection is found. For example, in
the middle image, the blue class is rather well-separated from the
rest. When the user is satisﬁed with a projection, the user speciﬁes
regions by “painting” over the display with the mouse icon in the
same way as in StarClass.

The “painting” paradigm can be extended to paint regions in the
Parallel Coordinates projection. Initially, in a Parallel Coordinates
projection, all intervals are set to belong to the blue region. The
user clicks on an interval to change it to red. An object belongs
to the red region if for every dimension with at least one red in-
terval, the object has attribute value equal to a red interval. The
object belongs to the blue class otherwise. An example is shown in
Figure 3.

5. DECISION TREE VISUALIZATION AND

EXPLORATION

Decision tree visualization and exploration is important for two
mutually-complimentary reasons. First, to effectively and efﬁciently
build a decision tree, it is crucial to be able to navigate through the
decision tree quickly to ﬁnd nodes that need to be further parti-
tioned. Second, exploration of the decision tree aids the under-
standing of the tree and the data being classiﬁed. From the vi-
sualization, the user gains helpful knowledge about the particular
dataset, and can more effectively decide how to further partition the
tree.

The ultimate goal of PaintingClass is to maximize user under-
standing and knowledge. It is thus not the goal to display as much
information as possible in the available screen space, because clut-
ter may be detrimental to user understanding. The challenge then is
to utilize the available display to let the user absorb as much useful
information as possible. This is achieved through space-efﬁcient
visual metaphors which take advantage of human intuition. A prob-
lem encountered in attempting to visualize a PaintingClass decision
tree is that there is too much data to be displayed coherently on a
single screen. The visualization and exploration mechanism must

Figure 4: PaintingClass decision tree layout schematic. The
current projection (ie. the projection in focus) is drawn as the
largest square in the upper right corner of the display. The par-
ent of the current projection is drawn to its immediate left, and
the line of ancestors up to the root is drawn in this way. For ev-
ery projection, its children are drawn directly below it. In this
ﬁgure, each projection is labelled with the number representing
its distance from the root.

therefore not only convey as much information as possible in a dis-
play but also allow the user to quickly navigate the tree to gain even
more knowledge.

Numerous tree visualization techniques including some speciﬁ-
cally designed for visualizing decision trees [3, 6], have been pro-
posed in the past. However, these methods are not appropriate
for visualization decision trees generated by PaintingClass. This
is because each node in PaintingClass is itself a picture requiring
space. Furthermore, the display of the decision tree needs to con-
vey clearly the relationship between regions and their correspond-
ing nodes.

Figure 4 shows a schematic of the decision tree layout we de-
signed for PaintingClass. It makes use of the focus+context con-
cept, focusing on one particular projection, called the “current pro-
jection”, which is given the most screen space and shown in the
upper right corner. The rest of the tree is shown as context to give
the user a sense of where the current node is within the decision
tree. Nodes that are close to the node in focus are allocated more
space, because they are more relevant and the user is more likely to
be interested in their contents. The ancestors (up to and including
the root) of the node in focus are drawn in a line towards the left.
The line including the focus node and all its ancestors is called the
ancestor line. Except with both parent and child are in the an-
cestor line, the children of each node are drawn directly below it,
in accordance with traditional tree drawing conventions. This lay-
out is simple to understand, intuitive, and immediately portrays the
shape and structure of the decision tree being visualized.

For exploration purposes, interactivity is of utmost importance.
PaintingClass allows the user to easily navigate the tree by chang-
ing the node in focus. This is done either by clicking on the arrow
on the upper right corner of a projection to bring it into focus, or by
clicking on the arrow on the lower left corner of a projection in the
ancestor line to bring it out of focus. In this case, the parent of the

669

projection brought out of focus will be the new focus.

5.1 Auxiliary display

In the PaintingClass interface, the auxiliary display is shown in
the lower left corner of the window. This portion of the window is
not utilized in the display of the decision tree. PaintingClass thus
makes use of this space to display some very important information
that can aid the user’s understanding of the data and the construc-
tion of the decision tree.

While decision tree visualization gives a good overview of the
data, it is sometimes necessary for the user to see additional in-
formation. For example, in the Star Coordinates projection, many
objects often map to the same pixel or nearby pixels.
In fact,
two objects with large Euclidean distance between them in high-
dimensional space may map to nearbly pixels, and it is hard to dis-
tinguish them. Therefore, as in StarClass, PaintingClass allows the
user to zoom in on a region in the current projection, showing the
region as auxiliary display using the “sticks” feature found in Star
Coordiates.

When visualizing datasets with both numerical and categorical
attributes, PaintingClass allows the user the option to use Star Co-
ordinates or Parallel Coordinates. If the user chooses Star Coordi-
nates, only the numerical attributes are used to determine the pro-
jected position of each point. Likewise, if the user chooses Parallel
Coordinates, only the categorical attributes are used. This gives an
incomplete picture of the data. Therefore, the auxiliary display can
also be used to display the “dual” projection. In other words, when
Star Coordinates is used to display the numerical attributes in the
current projection, Parallel Coordinates can be used to display the
categorical attributes in the auxiliary display, and vice versa. An
example is shown in Figure 5.

The user clicks on the labels above the auxiliary display to choose
between whether to show “sticks” zoom or to show the “dual” pro-
jection.

5.2 User interface for building the decision

tree

As mentioned in Section 2, the decision tree construction pro-
cess involves the repetition of three steps: (1) edit a projection, (2)
specify regions, and (3) create a new node/projection for some re-
gion. In PaintingClass, interaction tools are provided so that users
can easily perform each of these steps.

In Step 1, the PaintingClass control panel provides a check box
for users to choose between Star Coordinates and Parallel Coordi-
nates projection.

In Star Coordinates projection, the way the user moves the axes
and paints a region is the same: holding down the left mouse but-
ton and dragging the mouse icon across the screen. To resolve the
ambiguity, a control bar is displayed at the left edge of the display.
The user clicks on the “edit axes” box, or one of the colored boxes
to determine if the user wants to edit the axes or paint the region
represented by the selected color.

In Parallel Coordinates projection, the user deﬁnes the red and
blue regions simply by clicking on an interval to toggle it between
red and blue. There are two additional boxes shown below the Par-
allel Coordinates display, one box is red and the other is blue. The
user clicks on either box to toggle the display between showing the
red or the blue region.

To create a new node from a region, the user needs to specify
which region in the current projection to re-project. In Star Coor-
dinates mode, this is simply done by clicking on one of the colored
boxes in the same control bar at the left of the display. In Paral-
lel Coordinates mode, the region selected by the two colored boxes

Figure 5: An auxiliary display is shown on the un-utilized space
at the lower left of the display. This auxiliary display is used to
supplement the user’s exploration of the data. It can be used to
show a detailed view of part of the data or the “dual” projec-
tion, which is the case in this ﬁgure. Since the current projec-
tion uses Star Coordinates to show the objects based on their
numerical attributes, the “dual” projection uses Parallel Coor-
dinates to show the categorical attributes.

below the display is used. The user then clicks on the “Create Pro-
jection” button in the control panel, and a new projection is created.
5.3 Classiﬁcation

PaintingClass counts the number of objects belonging to each
class mapping to each terminal region (i.e., the leaf of the deci-
sion tree). The class with the most number of objects mapping to
a terminal region is elected as the region’s “expected class”. Dur-
ing classiﬁcation, any object which ﬁnally projects to the region is
predicted that class.

6. RESULTS

The dual objectives of PaintingClass are (1) to create a user-
directed decision tree classiﬁcation system, and (2) to enable users
to explore and visualize multi-dimensional data and their corre-
sponding decision trees to improve user understanding and to gain
knowledge. We evaluate the success of PaintingClass in meeting
the ﬁrst goal by running experiments on well-known benchmark
datasets and testing its accuracy, comparing it to other classiﬁcation
techniques. We also show some examples of how PaintingClass vi-
sualization is able to reveal important information in the datasets
used.
6.1 Experimental Evaluation of accuracy

The design objective of PaintingClass is to create a decision-tree
construction, visualization and exploration system so that the user
can gain knowledge of the data being analyzed. Therefore, it is not
the goal of PaintingClass to achieve superior classiﬁcation accu-
racy. However, for PaintingClass to be a viable data mining tool,
the decision trees generated by PaintingClass have to be “good”. A
good decision tree is deﬁned as one that is able to effectively par-

670

Table 2: Accuracy of PaintingClass compared with algorithmic
approaches and visual approach PBC.

Algorithmic

Visual

Satimage
Segment
Shuttle

Australian

CART
85.3
84.9
99.9
85.3

C4
85.2
95.9
99.9
84.4

SLIQ PBC PaintingClass
86.3
94.6
99.9
84.9

85:3
95:2
99:9
84:7

83.5
94.8
99.9
82.7

Table 3: Accuracy of PaintingClass compared with other clas-
siﬁcation methods.

Australian

adult

diabetes

CBA C4.5
85.0
82.6
85.4
84.2
74.4
73.8

FID Fuzzy PaintingClass
58.0
23.6
62.0

84:7
85:1
74:6

88.9
85.9
77.6

tition the domain of the data, so to accurately predict the classes
of objects. The structure of a good decision tree therefore reveals
some underlying information about the data. Such a decision tree
is worth visualization and exploration in PaintingClass. Therefore,
we need to show that the decision trees generated by PaintingClass
are good, and we do that by experimenting on well-known bench-
mark datasets and comparing accuracy results with popular existing
classiﬁcation systems.

We used the Satimage, Segment, Shuttle and Australian datasets

from the benchmark Statlog database [15] for evaluating the accu-
racy of PaintingClass classiﬁcation. We also use two additional
well-known benchmark datasets, diabetes [19] and adult [12] in
our evaluation. The description of these datasets is presented in
Table 1.

Table 2 compares the accuracy of PaintingClass against the ac-
curacy of popular algorithmic decision tree classiﬁers CART and
C4 from the IND package [16], as well as SLIQ [14], and also vi-
sual classiﬁer PBC. The results are taken from [3]. Table 3 shows
how PaintingClass compares with CBA [13], C4.5 [18], FID [10],
and Fuzzy [5]. The accuracy ﬁgures for these methods are taken
from [5].

From the experimental results, PaintingClass performs well com-
pared with the other methods. In particular, it appears that Paint-
ingClass is slightly more accurate than PBC. Since the classiﬁca-
tion task is performed by the user in PaintingClass, the accuracy is
highly dependent on the skill and patience of the user, and even the
same user can produce some small variations in accuracy for dif-
ferent attempts. However, the accuracy produced by a competent
user as shown in the results is sufﬁcient to establish PaintingClass
as a viable method for constructing decision tree. Small variations
in accuracy do not change the conclusion of this evaluation. More
importantly, the accuracy achieved shows that the decision trees
generated by PaintingClass are effective in partioning the data, and
therefore their visualization and exploration can yield valid and im-
portant information. The knowledge gained from the exploration is
discussed in the next section.
6.2 Knowledge Gained

In [7], Buja and Lee mention that data mining, unlike traditional
statistics, is not concerned with modeling all of the data, but with
the search for interesting parts of the data. Therefore, the goal is
not to achieve optimal performance in terms of global performance
measures such as classiﬁcation accuracy. In our design of Paint-
ingClass, we follow the same principles. As a result, PaintingClass
decision tree is a powerful tool in the discovery of knowledge in
datasets for the following reasons:

(cid:15) Exploration of the decision tree allows the user to see the

671

hierarchy of split points. These split points reveal which
dimensions, combinations of dimensions, and which values
with each dimension are most correlated to different classes.

(cid:15) The shape of the decision tree indicates certain characteris-
tics of the data. For example, the unbalanced tree constructed
for the Australian dataset suggests that certain large clusters
are “pure”, meaning that they contain only objects belong-
ing to one class and therefore need no further partitioning,
whereas certain clusters are “mixed”, meaning that they con-
tain objects of different classes and therefore need to be par-
titioned further.

(cid:15) The conﬁdence in the prediction of the class of a data object
can vary widely. This can be visualized clearly in Painting-
Class, sometimes objects belonging to a single class appear
clustered in the projection chosen by the user, far away from
objects belonging to other classes. In this case, the user can
have great conﬁdence that an object projected to that area
would belong to this class. This is contrasted with a “mixed”
region, where the user is unable to edit the projection to sep-
arate objects belonging to different classes. Objects of differ-
ent classes fall into the same region, and they are all predicted
to belong to the majority class of the training-set objects pro-
jecting to this region. However, in this case, there is much
less conﬁdence in the prediction.

(cid:15) Each node in the decision tree is itself a visual projection of a
subset of the data. The projection used, whether Star Coordi-
nates or Parallel Coordinates or any other method which may
be incorporated into PaintingClass in the future, can reveal
patterns, clusters (and their shapes) and outliers, as shown
through various examples in this paper. These displays pro-
vide the user with rich information, not just statistical mea-
sures expressed as single numbers.

(cid:15) Following the hierarchy of nodes in the decision tree allows
the user to focus on subsets of the data. For example, the
adult dataset split by the married-civ-spouse category in the
marital status dimension clearly indicates a strong correla-
tion between married-civ-spouse and high income. To fo-
cus attention on only people who are married-civ-spouse, the
user simply explores this subtree. Within this subtree, Paint-
ingClass visualization reveals a strong correlation between
higher education and high income.

7. FUTURE WORK

Although experimental evaluation has indicated the effectiveness
of PaintingClass, we believe there are some areas where Painting-
Class can be further improved. For example, just as Ankerst et al.
have incorporated algorithmic support into PBC with signiﬁcant
success [3], we are also extending PaintingClass to include options
for using automatic algorithms in cases where human judegment
is difﬁcult and also to aid in searching for optimal splits to relieve
user tedium. We believe that with some algorithmic support, the
accuracy and usability of PaintingClass can be further improved.
We would like to apply PaintingClass to more datasets to further
investigate its effectiveness and to conduct an extensive user study
to ﬁnd out how effectively different users perform the classiﬁca-
tion task with this tool. Finally, we plan to extend PaintingClass
to handle very large datasets. This could be done, for example, by
random sampling.

Table 1: Description of the datasets.

num
classes

numerical
dimensions

Dataset
Satimage
Segment
Shuttle
diabetes
Australian

adult

Training
Set Size

4435
2310
43500
768
690
32561

Testing
Set Size

2000
10-fold
14500
3-fold
10-fold
16281

6
7
7
2
2
2

4
19
9
8
6
6

categorical
dimensions

0
0
0
0
8
8

8. CONCLUSIONS

We have presented PaintingClass, a decision tree construction,
visualization and exploration system. In PaintingClass, the user in-
teractively edits projections of multi-dimensional data and paints
regions to build a decision tree. We have experimentally veriﬁed
the effectiveness of PaintingClass by applying it to the classiﬁca-
tion of actual data with up to 19 dimensions, and comparing its
performance to that of well-known algorithmic and visual classiﬁ-
cation methods. With further improvements such as the incorpo-
ration of algorithmic techniques, PaintingClass can achieve even
better results in terms of accuracy.

Yet it is not the sole purpose of PaintingClass to facilitate inter-
active construction of decision trees. The decision tree layout and
navigation method introduced by PaintingClass allows users to ex-
plore decision trees. Exploration of decision trees fulﬁlls some gen-
eral goals of data mining beyond classiﬁcation. We have shown that
visualizing the structure of the decision tree, the individual nodes,
and exploring through the hierarchy can reveal valuable knowledge
about the dataset.

Besides introducing the decision tree exploration mechanism,
PaintingClass also provides several useful extensions to StarClass,
the most important of which is the use of Parallel Coordinates to
display categorical values so that even datasets with categorical di-
mensions can be classiﬁed and visualized.

We believe that PaintingClass is a practical and effective data ex-
ploration and classiﬁcation tool, and the idea of using decision tree
visualization for knowledge discovery is an important contribution.
Further features built on the PaintingClass platform may make it an
even more powerful system.

9. REFERENCES
[1] R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami.

An Interval Classiﬁer for Database Mining Applications.
Proc. 18th Intl. Conf. on Very Large Databases (VLDB ’92),
pp. 560–573, Vancouver, B.C., Canada, August 1992.

[2] M. Ankerst, C. Elsen, M. Ester, and H.-P. Kriegel. Visual

classiﬁcation: An interactive approach to decision tree
construction. Proc. 5th Intl. Conf. on Knowledge Discovery
and Data Mining (KDD ’99), pp. 392–396, 1999.

[3] M. Ankerst, M. Ester, and H.-P. Kriegel. Towards an
effective cooperation of the user and the computer for
classiﬁcation. Proc. 6th Intl. Conf. on Knowledge Discovery
and Data Mining (KDD ’00), 2000.

[4] K. Alsabti, S. Ranka, and V. Singh. CLOUDS: A Decision
Tree Classiﬁer for Large Datasets. Proc. 4th Intl. Conf. on
Knowledge Discovery and Data Mining (KDD ’98), New
York, 1998, pp. 2–8.

[5] W.-H. Au and K.C.C. Chan. Classiﬁcation with Degree of

Membership: A Fuzzy Approach. Proc. 2nd IEEE Intl. Conf.
on Data Mining (ICDM ’02), 2002.

[6] T. Barlow and P. Neville. Case Study: Visualization for

Decision Tree Analysis in Data Mining. Proc. IEEE

Symposium on Information Visualization, 2001.

[7] A. Buja and Y-S. Lee. Data Mining Criteria for Tree-Based

Regression and Classiﬁcation. Proc. 7th Intl. Conf. on
Knowledge Discovery and Data Mining (KDD ’01), 2001.
[8] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. The KDD
Process for Extracting Useful Knowledge from Volumes of
Data Communications of the ACM 39, 11, 1996.

[9] A. Inselberg. The Plane with Parallel Coordinates. Special
Issue on Computational Geometry: The Visual Computer,
vol. 1, pp. 69–91, 1985.

[10] C.Z. Janikow. Fuzzy Decision Trees: Issues and Methods.
IEEE Trans. on Systems, Man, and Cybernetics - Part B:
Cybernetics, vol. 28, no. 1, pp 1-14, 1998.

[11] E. Kandogan. Visualizing Multi-Dimensional Clusters,
Trends, and Outliers using Star Coordinates. Proc. ACM
SIGKDD ’01, pp. 107-116, 2001.

[12] R. Kohavi. Scaling Up the Accuracy of Naive-Bayes

Classiﬁers: A Decision Tree Hybrid. Proc. 2nd Intl. Conf. on
Knowledge Discovery and Data Mining (KDD ’96),
Portland, Oregon, 1998.

[13] B. Liu, W. Hsu, and Y. Man. Integrating Classiﬁcation and

Association Rule Mining. Proc. 4th Intl. Conf. on Knowledge
Discovery and Data Mining (KDD ’98), New York, 1998.

[14] M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: A Fast
Scalable Classiﬁer for Data Mining Proc. Intl. Conf. on
Extending Database Technology (EDBT ’96), Avignon,
France, 1996.

[15] D. Michie, D.J. Spiegelhalter, and C.C. Taylor. Machine

Learning, Neural and Statistical Classiﬁcation. Ellis
Horwood, 1994.

[16] NASA Ames Research Center. Introduction to IND Version

2.1, 1992.

[17] R. Parekh, J. Yang, and V. Honavar. Constructive
Neural-Network Learning Algorithms for Pattern
Classiﬁcation. IEEE Trans. on Neural Networks, vol.11,
no.2, 2000.

[18] J.R. Quinlan. C4.5: Programs for Machine Learning.

Morgan Kaufman, 1993.

[19] J.W. Smith, J.E. Everhart, W.C. Dickson, W.C. Knowler, and

R.S. Johannes. Using the ADAP Learning Algorithm to
Forcast the Onset of Diabetes Mellitus. Proc. Symp. on
Computer Applications and Medical Cares, pp. 422–425,
1983.

[20] S.T. Teoh and K.L. Ma. StarClass: Interactive Visual

Classiﬁcation Using Star Coordinates. Proc. 3rd SIAM Intl.
Conf. on Data Mining (SDM ’03), 2003.

[21] Z.-H. Zhou, Y. Jiang, and S.F. Chen. A General Neural

Framework for Classiﬁcation Rule Mining. Intl. Journal of
Computers, Systems, and Signals, vol.1, no.2, pp.154–168,
2000.

672

",False,0.0,{},False,False,journalArticle,False,494B5SDV,[],self.user,False,False,False,False,,,"PaintingClass: Interactive Construction, Visualization and Exploration of Decision Trees",494B5SDV,False,False
7KGAT6TB,ACFDV7UY,"“Why Should I Trust You?”

Explaining the Predictions of Any Classiﬁer

Marco Tulio Ribeiro
University of Washington
Seattle, WA 98105, USA
marcotcr@cs.uw.edu

Sameer Singh

University of Washington
Seattle, WA 98105, USA
sameer@cs.uw.edu

Carlos Guestrin

University of Washington
Seattle, WA 98105, USA
guestrin@cs.uw.edu

6
1
0
2

 

g
u
A
9

 

 
 
]

G
L
.
s
c
[
 
 

3
v
8
3
9
4
0

.

2
0
6
1
:
v
i
X
r
a

ABSTRACT
Despite widespread adoption, machine learning models re-
main mostly black boxes. Understanding the reasons behind
predictions is, however, quite important in assessing trust,
which is fundamental if one plans to take action based on a
prediction, or when choosing whether to deploy a new model.
Such understanding also provides insights into the model,
which can be used to transform an untrustworthy model or
prediction into a trustworthy one.

In this work, we propose LIME, a novel explanation tech-
nique that explains the predictions of any classiﬁer in an in-
terpretable and faithful manner, by learning an interpretable
model locally around the prediction. We also propose a
method to explain models by presenting representative indi-
vidual predictions and their explanations in a non-redundant
way, framing the task as a submodular optimization prob-
lem. We demonstrate the ﬂexibility of these methods by
explaining diﬀerent models for text (e.g. random forests)
and image classiﬁcation (e.g. neural networks). We show the
utility of explanations via novel experiments, both simulated
and with human subjects, on various scenarios that require
trust: deciding if one should trust a prediction, choosing
between models, improving an untrustworthy classiﬁer, and
identifying why a classiﬁer should not be trusted.

INTRODUCTION

1.
Machine learning is at the core of many recent advances in
science and technology. Unfortunately, the important role
of humans is an oft-overlooked aspect in the ﬁeld. Whether
humans are directly using machine learning classiﬁers as tools,
or are deploying models within other products, a vital concern
remains: if the users do not trust a model or a prediction,
they will not use it. It is important to diﬀerentiate between
two diﬀerent (but related) deﬁnitions of trust: (1) trusting a
prediction, i.e. whether a user trusts an individual prediction
suﬃciently to take some action based on it, and (2) trusting
a model, i.e. whether the user trusts a model to behave in
reasonable ways if deployed. Both are directly impacted by

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
KDD 2016 San Francisco, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4232-2/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2939672.2939778

how much the human understands a model’s behaviour, as
opposed to seeing it as a black box.

Determining trust in individual predictions is an important
problem when the model is used for decision making. When
using machine learning for medical diagnosis [6] or terrorism
detection, for example, predictions cannot be acted upon on
blind faith, as the consequences may be catastrophic.

Apart from trusting individual predictions, there is also a
need to evaluate the model as a whole before deploying it “in
the wild”. To make this decision, users need to be conﬁdent
that the model will perform well on real-world data, according
to the metrics of interest. Currently, models are evaluated
using accuracy metrics on an available validation dataset.
However, real-world data is often signiﬁcantly diﬀerent, and
further, the evaluation metric may not be indicative of the
product’s goal. Inspecting individual predictions and their
explanations is a worthwhile solution, in addition to such
metrics. In this case, it is important to aid users by suggesting
which instances to inspect, especially for large datasets.

In this paper, we propose providing explanations for indi-
vidual predictions as a solution to the “trusting a prediction”
problem, and selecting multiple such predictions (and expla-
nations) as a solution to the “trusting the model” problem.
Our main contributions are summarized as follows.
• LIME, an algorithm that can explain the predictions of any
classiﬁer or regressor in a faithful way, by approximating
it locally with an interpretable model.

• SP-LIME, a method that selects a set of representative
instances with explanations to address the “trusting the
model” problem, via submodular optimization.

• Comprehensive evaluation with simulated and human sub-
jects, where we measure the impact of explanations on
trust and associated tasks. In our experiments, non-experts
using LIME are able to pick which classiﬁer from a pair
generalizes better in the real world. Further, they are able
to greatly improve an untrustworthy classiﬁer trained on
20 newsgroups, by doing feature engineering using LIME.
We also show how understanding the predictions of a neu-
ral network on images helps practitioners know when and
why they should not trust a model.

2. THE CASE FOR EXPLANATIONS

By “explaining a prediction”, we mean presenting textual or
visual artifacts that provide qualitative understanding of the
relationship between the instance’s components (e.g. words
in text, patches in an image) and the model’s prediction. We
argue that explaining predictions is an important aspect in

Figure 1: Explaining individual predictions. A model predicts that a patient has the ﬂu, and LIME highlights
the symptoms in the patient’s history that led to the prediction. Sneeze and headache are portrayed as
contributing to the “ﬂu” prediction, while “no fatigue” is evidence against it. With these, a doctor can make
an informed decision about whether to trust the model’s prediction.

getting humans to trust and use machine learning eﬀectively,
if the explanations are faithful and intelligible.

The process of explaining individual predictions is illus-
trated in Figure 1. It is clear that a doctor is much better
positioned to make a decision with the help of a model if
intelligible explanations are provided. In this case, an ex-
planation is a small list of symptoms with relative weights –
symptoms that either contribute to the prediction (in green)
or are evidence against it (in red). Humans usually have prior
knowledge about the application domain, which they can use
to accept (trust) or reject a prediction if they understand the
reasoning behind it. It has been observed, for example, that
providing explanations can increase the acceptance of movie
recommendations [12] and other automated systems [8].

Every machine learning application also requires a certain
measure of overall trust in the model. Development and
evaluation of a classiﬁcation model often consists of collect-
ing annotated data, of which a held-out subset is used for
automated evaluation. Although this is a useful pipeline for
many applications, evaluation on validation data may not
correspond to performance “in the wild”, as practitioners
often overestimate the accuracy of their models [20], and
thus trust cannot rely solely on it. Looking at examples
oﬀers an alternative method to assess truth in the model,
especially if the examples are explained. We thus propose
explaining several representative individual predictions of a
model as a way to provide a global understanding.

There are several ways a model or its evaluation can go
wrong. Data leakage, for example, deﬁned as the uninten-
tional leakage of signal into the training (and validation)
data that would not appear when deployed [14], potentially
increases accuracy. A challenging example cited by Kauf-
man et al. [14] is one where the patient ID was found to be
heavily correlated with the target class in the training and
validation data. This issue would be incredibly challenging
to identify just by observing the predictions and the raw
data, but much easier if explanations such as the one in
Figure 1 are provided, as patient ID would be listed as an
explanation for predictions. Another particularly hard to
detect problem is dataset shift [5], where training data is
diﬀerent than test data (we give an example in the famous
20 newsgroups dataset later on). The insights given by expla-
nations are particularly helpful in identifying what must be
done to convert an untrustworthy model into a trustworthy
one – for example, removing leaked data or changing the
training data to avoid dataset shift.

Machine learning practitioners often have to select a model
from a number of alternatives, requiring them to assess
the relative trust between two or more models. In Figure

Figure 2: Explaining individual predictions of com-
peting classiﬁers trying to determine if a document
is about “Christianity” or “Atheism”. The bar chart
represents the importance given to the most rele-
vant words, also highlighted in the text. Color indi-
cates which class the word contributes to (green for
“Christianity”, magenta for “Atheism”).

2, we show how individual prediction explanations can be
used to select between models, in conjunction with accuracy.
In this case, the algorithm with higher accuracy on the
validation set is actually much worse, a fact that is easy to see
when explanations are provided (again, due to human prior
knowledge), but hard otherwise. Further, there is frequently
a mismatch between the metrics that we can compute and
optimize (e.g. accuracy) and the actual metrics of interest
such as user engagement and retention. While we may not
be able to measure such metrics, we have knowledge about
how certain model behaviors can inﬂuence them. Therefore,
a practitioner may wish to choose a less accurate model for
content recommendation that does not place high importance
in features related to “clickbait” articles (which may hurt
user retention), even if exploiting such features increases
the accuracy of the model in cross validation. We note
that explanations are particularly useful in these (and other)
scenarios if a method can produce them for any model, so
that a variety of models can be compared.
Desired Characteristics for Explainers
We now outline a number of desired characteristics from
explanation methods.

An essential criterion for explanations is that they must
be interpretable, i.e., provide qualitative understanding
between the input variables and the response. We note that
interpretability must take into account the user’s limitations.
Thus, a linear model [24], a gradient vector [2] or an additive
model [6] may or may not be interpretable. For example, if

sneezeweightheadacheno fatigueageFlusneezeheadacheModelData and PredictionExplainer (LIME)ExplanationExplainer (LIME)Human makes decisionExplanationno fatiguesneezeheadacheactiveHuman makes decisionhundreds or thousands of features signiﬁcantly contribute
to a prediction, it is not reasonable to expect any user to
comprehend why the prediction was made, even if individual
weights can be inspected. This requirement further implies
that explanations should be easy to understand, which is
not necessarily true of the features used by the model, and
thus the “input variables” in the explanations may need
to be diﬀerent than the features. Finally, we note that the
notion of interpretability also depends on the target audience.
Machine learning practitioners may be able to interpret small
Bayesian networks, but laymen may be more comfortable
with a small number of weighted features as an explanation.
Another essential criterion is local ﬁdelity. Although it is
often impossible for an explanation to be completely faithful
unless it is the complete description of the model itself, for
an explanation to be meaningful it must at least be locally
faithful, i.e. it must correspond to how the model behaves in
the vicinity of the instance being predicted. We note that
local ﬁdelity does not imply global ﬁdelity: features that
are globally important may not be important in the local
context, and vice versa. While global ﬁdelity would imply
local ﬁdelity, identifying globally faithful explanations that
are interpretable remains a challenge for complex models.

While there are models that are inherently interpretable [6,
17, 26, 27], an explainer should be able to explain any model,
and thus be model-agnostic (i.e. treat the original model
as a black box). Apart from the fact that many state-of-
the-art classiﬁers are not currently interpretable, this also
provides ﬂexibility to explain future classiﬁers.

In addition to explaining predictions, providing a global
perspective is important to ascertain trust in the model.
As mentioned before, accuracy may often not be a suitable
metric to evaluate the model, and thus we want to explain
the model. Building upon the explanations for individual
predictions, we select a few explanations to present to the
user, such that they are representative of the model.

3. LOCAL INTERPRETABLE

MODEL-AGNOSTIC EXPLANATIONS

We now present Local Interpretable Model-agnostic Expla-
nations (LIME). The overall goal of LIME is to identify an
interpretable model over the interpretable representation
that is locally faithful to the classiﬁer.

3.1 Interpretable Data Representations

Before we present the explanation system, it is impor-
tant to distinguish between features and interpretable data
representations. As mentioned before, interpretable expla-
nations need to use a representation that is understandable
to humans, regardless of the actual features used by the
model. For example, a possible interpretable representation
for text classiﬁcation is a binary vector indicating the pres-
ence or absence of a word, even though the classiﬁer may
use more complex (and incomprehensible) features such as
word embeddings. Likewise for image classiﬁcation, an in-
terpretable representation may be a binary vector indicating
the “presence” or “absence” of a contiguous patch of similar
pixels (a super-pixel), while the classiﬁer may represent the
image as a tensor with three color channels per pixel. We
denote x ∈ Rd be the original representation of an instance
being explained, and we use x(cid:48) ∈ {0, 1}d(cid:48)
to denote a binary
vector for its interpretable representation.

3.2 Fidelity-Interpretability Trade-off

Formally, we deﬁne an explanation as a model g ∈ G,
where G is a class of potentially interpretable models, such
as linear models, decision trees, or falling rule lists [27], i.e. a
model g ∈ G can be readily presented to the user with visual
or textual artifacts. The domain of g is {0, 1}d(cid:48)
, i.e. g acts
over absence/presence of the interpretable components. As
not every g ∈ G may be simple enough to be interpretable -
thus we let Ω(g) be a measure of complexity (as opposed to
interpretability) of the explanation g ∈ G. For example, for
decision trees Ω(g) may be the depth of the tree, while for
linear models, Ω(g) may be the number of non-zero weights.
Let the model being explained be denoted f : Rd → R. In
classiﬁcation, f (x) is the probability (or a binary indicator)
that x belongs to a certain class1. We further use πx(z) as a
proximity measure between an instance z to x, so as to deﬁne
locality around x. Finally, let L(f, g, πx) be a measure of
how unfaithful g is in approximating f in the locality deﬁned
by πx. In order to ensure both interpretability and local
ﬁdelity, we must minimize L(f, g, πx) while having Ω(g) be
low enough to be interpretable by humans. The explanation
produced by LIME is obtained by the following:

ξ(x) = argmin

g∈G

L(f, g, πx) + Ω(g)

(1)

This formulation can be used with diﬀerent explanation
families G, ﬁdelity functions L, and complexity measures Ω.
Here we focus on sparse linear models as explanations, and
on performing the search using perturbations.
3.3 Sampling for Local Exploration

We want to minimize the locality-aware loss L(f, g, πx)
without making any assumptions about f , since we want the
explainer to be model-agnostic. Thus, in order to learn
the local behavior of f as the interpretable inputs vary, we
approximate L(f, g, πx) by drawing samples, weighted by
πx. We sample instances around x(cid:48) by drawing nonzero
elements of x(cid:48) uniformly at random (where the number of
such draws is also uniformly sampled). Given a perturbed
sample z(cid:48) ∈ {0, 1}d(cid:48)
(which contains a fraction of the nonzero
elements of x(cid:48)), we recover the sample in the original repre-
sentation z ∈ Rd and obtain f (z), which is used as a label for
the explanation model. Given this dataset Z of perturbed
samples with the associated labels, we optimize Eq. (1) to
get an explanation ξ(x). The primary intuition behind LIME
is presented in Figure 3, where we sample instances both
in the vicinity of x (which have a high weight due to πx)
and far away from x (low weight from πx). Even though
the original model may be too complex to explain globally,
LIME presents an explanation that is locally faithful (linear
in this case), where the locality is captured by πx. It is worth
noting that our method is fairly robust to sampling noise
since the samples are weighted by πx in Eq. (1). We now
present a concrete instance of this general framework.
3.4 Sparse Linear Explanations
For the rest of this paper, we let G be the class of linear
models, such that g(z(cid:48)) = wg · z(cid:48). We use the locally weighted
square loss as L, as deﬁned in Eq. (2), where we let πx(z) =
exp(−D(x, z)2/σ2) be an exponential kernel deﬁned on some

1For multiple classes, we explain each class separately, thus
f (x) is the prediction of the relevant class.

Algorithm 1 Sparse Linear Explanations using LIME
Require: Classiﬁer f , Number of samples N
Require: Instance x, and its interpretable version x(cid:48)
Require: Similarity kernel πx, Length of explanation K

Z ← {}
for i ∈ {1, 2, 3, ..., N} do

i ← sample around(x(cid:48))
z(cid:48)
Z ← Z ∪ (cid:104)z(cid:48)
i, f (zi), πx(zi)(cid:105)
end for
w ← K-Lasso(Z, K) (cid:46) with z(cid:48)
return w

i as features, f (z) as target

Figure 3: Toy example to present intuition for LIME.
The black-box model’s complex decision function f
(unknown to LIME) is represented by the blue/pink
background, which cannot be approximated well by
a linear model. The bold red cross is the instance
being explained. LIME samples instances, gets pre-
dictions using f , and weighs them by the proximity
to the instance being explained (represented here
by size). The dashed line is the learned explanation
that is locally (but not globally) faithful.

distance function D (e.g. cosine distance for text, L2 distance
for images) with width σ.

πx(z)(cid:0)f (z) − g(z

(cid:48)

)(cid:1)2

(cid:88)

z,z(cid:48)∈Z

L(f, g, πx) =

(2)

For text classiﬁcation, we ensure that the explanation is
interpretable by letting the interpretable representation be
a bag of words, and by setting a limit K on the number of
words, i.e. Ω(g) = ∞1[(cid:107)wg(cid:107)0 > K]. Potentially, K can be
adapted to be as big as the user can handle, or we could
have diﬀerent values of K for diﬀerent instances. In this
paper we use a constant value for K, leaving the exploration
of diﬀerent values to future work. We use the same Ω for
image classiﬁcation, using “super-pixels” (computed using
any standard algorithm) instead of words, such that the
interpretable representation of an image is a binary vector
where 1 indicates the original super-pixel and 0 indicates a
grayed out super-pixel. This particular choice of Ω makes
directly solving Eq. (1) intractable, but we approximate it by
ﬁrst selecting K features with Lasso (using the regularization
path [9]) and then learning the weights via least squares (a
procedure we call K-LASSO in Algorithm 1). Since Algo-
rithm 1 produces an explanation for an individual prediction,
its complexity does not depend on the size of the dataset,
but instead on time to compute f (x) and on the number
of samples N . In practice, explaining random forests with
1000 trees using scikit-learn (http://scikit-learn.org) on a
laptop with N = 5000 takes under 3 seconds without any
optimizations such as using gpus or parallelization. Explain-
ing each prediction of the Inception network [25] for image
classiﬁcation takes around 10 minutes.

Any choice of interpretable representations and G will
have some inherent drawbacks. First, while the underlying
model can be treated as a black-box, certain interpretable
representations will not be powerful enough to explain certain
behaviors. For example, a model that predicts sepia-toned
images to be retro cannot be explained by presence of absence
of super pixels. Second, our choice of G (sparse linear models)
means that if the underlying model is highly non-linear even
in the locality of the prediction, there may not be a faithful
explanation. However, we can estimate the faithfulness of

the explanation on Z, and present this information to the
user. This estimate of faithfulness can also be used for
selecting an appropriate family of explanations from a set of
multiple interpretable model classes, thus adapting to the
given dataset and the classiﬁer. We leave such exploration
for future work, as linear explanations work quite well for
multiple black-box models in our experiments.

3.5 Example 1: Text classiﬁcation with SVMs
In Figure 2 (right side), we explain the predictions of a
support vector machine with RBF kernel trained on uni-
grams to diﬀerentiate “Christianity” from “Atheism” (on a
subset of the 20 newsgroup dataset). Although this classiﬁer
achieves 94% held-out accuracy, and one would be tempted
to trust it based on this, the explanation for an instance
shows that predictions are made for quite arbitrary reasons
(words “Posting”, “Host”, and “Re” have no connection to
either Christianity or Atheism). The word “Posting” appears
in 22% of examples in the training set, 99% of them in the
class “Atheism”. Even if headers are removed, proper names
of proliﬁc posters in the original newsgroups are selected by
the classiﬁer, which would also not generalize.

After getting such insights from explanations, it is clear
that this dataset has serious issues (which are not evident
just by studying the raw data or predictions), and that this
classiﬁer, or held-out evaluation, cannot be trusted. It is also
clear what the problems are, and the steps that can be taken
to ﬁx these issues and train a more trustworthy classiﬁer.

3.6 Example 2: Deep networks for images
When using sparse linear explanations for image classiﬁers,
one may wish to just highlight the super-pixels with posi-
tive weight towards a speciﬁc class, as they give intuition
as to why the model would think that class may be present.
We explain the prediction of Google’s pre-trained Inception
neural network [25] in this fashion on an arbitrary image
(Figure 4a). Figures 4b, 4c, 4d show the superpixels expla-
nations for the top 3 predicted classes (with the rest of the
image grayed out), having set K = 10. What the neural
network picks up on for each of the classes is quite natural
to humans - Figure 4b in particular provides insight as to
why acoustic guitar was predicted to be electric: due to the
fretboard. This kind of explanation enhances trust in the
classiﬁer (even if the top predicted class is wrong), as it shows
that it is not acting in an unreasonable manner.

(a) Original Image

(b) Explaining Electric guitar (c) Explaining Acoustic guitar

(d) Explaining Labrador

Figure 4: Explaining an image classiﬁcation prediction made by Google’s Inception neural network. The top
3 classes predicted are “Electric Guitar” (p = 0.32), “Acoustic guitar” (p = 0.24) and “Labrador” (p = 0.21)

4. SUBMODULAR PICK FOR

EXPLAINING MODELS

Although an explanation of a single prediction provides
some understanding into the reliability of the classiﬁer to the
user, it is not suﬃcient to evaluate and assess trust in the
model as a whole. We propose to give a global understanding
of the model by explaining a set of individual instances. This
approach is still model agnostic, and is complementary to
computing summary statistics such as held-out accuracy.

Even though explanations of multiple instances can be
insightful, these instances need to be selected judiciously,
since users may not have the time to examine a large number
of explanations. We represent the time/patience that humans
have by a budget B that denotes the number of explanations
they are willing to look at in order to understand a model.
Given a set of instances X, we deﬁne the pick step as the
task of selecting B instances for the user to inspect.

The pick step is not dependent on the existence of explana-
tions - one of the main purpose of tools like Modeltracker [1]
and others [11] is to assist users in selecting instances them-
selves, and examining the raw data and predictions. However,
since looking at raw data is not enough to understand predic-
tions and get insights, the pick step should take into account
the explanations that accompany each prediction. Moreover,
this method should pick a diverse, representative set of expla-
nations to show the user – i.e. non-redundant explanations
that represent how the model behaves globally.
Given the explanations for a set of instances X (|X| = n),
we construct an n× d(cid:48) explanation matrix W that represents
the local importance of the interpretable components for
each instance. When using linear models as explanations,
for an instance xi and explanation gi = ξ(xi), we set Wij =
|wgij|. Further, for each component (column) j in W, we
let Ij denote the global importance of that component in
the explanation space.
Intuitively, we want I such that
features that explain many diﬀerent instances have higher
importance scores. In Figure 5, we show a toy example W,
with n = d(cid:48) = 5, where W is binary (for simplicity). The
importance function I should score feature f2 higher than
feature f1, i.e. I2 > I1, since feature f2 is used to explain
more instances. Concretely for the text applications, we set
i=1 Wij. For images, I must measure something
that is comparable across the super-pixels in diﬀerent images,

Ij =(cid:112)(cid:80)n

Figure 5: Toy example W. Rows represent in-
stances (documents) and columns represent features
(words). Feature f2 (dotted blue) has the highest im-
portance. Rows 2 and 5 (in red) would be selected
by the pick procedure, covering all but feature f1.

Algorithm 2 Submodular pick (SP) algorithm
Require: Instances X, Budget B

for all xi ∈ X do

Wi ← explain(xi, x(cid:48)
i)

end for
for j ∈ {1 . . . d(cid:48)} do

Ij ←(cid:112)(cid:80)n

i=1 |Wij|

(cid:46) Using Algorithm 1

(cid:46) Compute feature importances

end for
V ← {}
while |V | < B do

end while
return V

V ← V ∪ argmaxi c(V ∪ {i},W, I)

(cid:46) Greedy optimization of Eq (4)

such as color histograms or other features of super-pixels; we
leave further exploration of these ideas for future work.

While we want to pick instances that cover the important
components, the set of explanations must not be redundant
in the components they show the users, i.e. avoid selecting
instances with similar explanations. In Figure 5, after the
second row is picked, the third row adds no value, as the
user has already seen features f2 and f3 - while the last row
exposes the user to completely new features. Selecting the
second and last row results in the coverage of almost all the
features. We formalize this non-redundant coverage intuition
in Eq. (3), where we deﬁne coverage as the set function c
that, given W and I, computes the total importance of the
features that appear in at least one instance in a set V .

f1f2f3f4f5Covered Featuresc(V,W, I) =

d(cid:48)(cid:88)

j=1

1[∃i∈V :Wij >0]Ij

(3)

The pick problem, deﬁned in Eq. (4), consists of ﬁnding the
set V,|V | ≤ B that achieves highest coverage.
c(V,W, I)

P ick(W, I) = argmax
V,|V |≤B

(4)

The problem in Eq. (4) is maximizing a weighted coverage
function, and is NP-hard [10]. Let c(V ∪{i},W, I)−c(V,W, I)
be the marginal coverage gain of adding an instance i to a set
V . Due to submodularity, a greedy algorithm that iteratively
adds the instance with the highest marginal coverage gain to
the solution oﬀers a constant-factor approximation guarantee
of 1−1/e to the optimum [15]. We outline this approximation
in Algorithm 2, and call it submodular pick.
5. SIMULATED USER EXPERIMENTS

In this section, we present simulated user experiments to
evaluate the utility of explanations in trust-related tasks. In
particular, we address the following questions: (1) Are the
explanations faithful to the model, (2) Can the explanations
aid users in ascertaining trust in predictions, and (3) Are
the explanations useful for evaluating the model as a whole.
Code and data for replicating our experiments are available
at https://github.com/marcotcr/lime-experiments.
5.1 Experiment Setup

We use two sentiment analysis datasets (books and DVDs,
2000 instances each) where the task is to classify prod-
uct reviews as positive or negative [4]. We train decision
trees (DT), logistic regression with L2 regularization (LR),
nearest neighbors (NN), and support vector machines with
RBF kernel (SVM), all using bag of words as features. We
also include random forests (with 1000 trees) trained with
the average word2vec embedding [19] (RF), a model that is
impossible to interpret without a technique like LIME. We
use the implementations and default parameters of scikit-
learn, unless noted otherwise. We divide each dataset into
train (1600 instances) and test (400 instances).

To explain individual predictions, we compare our pro-
posed approach (LIME), with parzen [2], a method that
approximates the black box classiﬁer globally with Parzen
windows, and explains individual predictions by taking the
gradient of the prediction probability function. For parzen,
we take the K features with the highest absolute gradients
as explanations. We set the hyper-parameters for parzen and
LIME using cross validation, and set N = 15, 000. We also
compare against a greedy procedure (similar to Martens
and Provost [18]) in which we greedily remove features that
contribute the most to the predicted class until the prediction
changes (or we reach the maximum of K features), and a
random procedure that randomly picks K features as an
explanation. We set K to 10 for our experiments.

For experiments where the pick procedure applies, we either
do random selection (random pick, RP) or the procedure
described in §4 (submodular pick, SP). We refer to pick-
explainer combinations by adding RP or SP as a preﬁx.
5.2 Are explanations faithful to the model?

We measure faithfulness of explanations on classiﬁers that
are by themselves interpretable (sparse logistic regression

(a) Sparse LR

(b) Decision Tree

Figure 6: Recall on truly important features for two
interpretable classiﬁers on the books dataset.

(a) Sparse LR

(b) Decision Tree

Figure 7: Recall on truly important features for two
interpretable classiﬁers on the DVDs dataset.

and decision trees). In particular, we train both classiﬁers
such that the maximum number of features they use for any
instance is 10, and thus we know the gold set of features
that the are considered important by these models. For
each prediction on the test set, we generate explanations and
compute the fraction of these gold features that are recovered
by the explanations. We report this recall averaged over all
the test instances in Figures 6 and 7. We observe that
the greedy approach is comparable to parzen on logistic
regression, but is substantially worse on decision trees since
changing a single feature at a time often does not have an
eﬀect on the prediction. The overall recall by parzen is low,
likely due to the diﬃculty in approximating the original high-
dimensional classiﬁer. LIME consistently provides > 90%
recall for both classiﬁers on both datasets, demonstrating
that LIME explanations are faithful to the models.
5.3 Should I trust this prediction?

In order to simulate trust in individual predictions, we ﬁrst
randomly select 25% of the features to be “untrustworthy”,
and assume that the users can identify and would not want
to trust these features (such as the headers in 20 newsgroups,
leaked data, etc). We thus develop oracle “trustworthiness”
by labeling test set predictions from a black box classiﬁer as
“untrustworthy” if the prediction changes when untrustworthy
features are removed from the instance, and “trustworthy”
otherwise. In order to simulate users, we assume that users
deem predictions untrustworthy from LIME and parzen ex-
planations if the prediction from the linear approximation
changes when all untrustworthy features that appear in the
explanations are removed (the simulated human “discounts”
the eﬀect of untrustworthy features). For greedy and random,
the prediction is mistrusted if any untrustworthy features
are present in the explanation, since these methods do not
provide a notion of the contribution of each feature to the
prediction. Thus for each test set prediction, we can evaluate
whether the simulated user trusts it using each explanation
method, and compare it to the trustworthiness oracle.

Using this setup, we report the F1 on the trustworthy

randomparzengreedyLIME0255075100Recall (%)17.472.864.392.1randomparzengreedyLIME0255075100Recall (%)20.678.937.097.0randomparzengreedyLIME0255075100Recall (%)19.260.863.490.2randomparzengreedyLIME0255075100Recall (%)17.480.647.697.8Table 1: Average F1 of trustworthiness for diﬀerent
explainers on a collection of classiﬁers and datasets.

Books

DVDs

LR NN RF SVM LR NN RF SVM

Random 14.6 14.8 14.7 14.7
84.0 87.6 94.3 92.3
Parzen
Greedy
53.7 47.4 45.0 53.3
96.6 94.5 96.2 96.7
LIME

14.2 14.3 14.5 14.4
87.0 81.7 94.2 87.3
52.4 58.1 46.6 55.1
96.6 91.8 96.1 95.6

(a) Books dataset

(b) DVDs dataset

Figure 8: Choosing between two classiﬁers, as the
number of instances shown to a simulated user is
varied. Averages and standard errors from 800 runs.

predictions for each explanation method, averaged over 100
runs, in Table 1. The results indicate that LIME dominates
others (all results are signiﬁcant at p = 0.01) on both datasets,
and for all of the black box models. The other methods either
achieve a lower recall (i.e. they mistrust predictions more
than they should) or lower precision (i.e. they trust too many
predictions), while LIME maintains both high precision and
high recall. Even though we artiﬁcially select which features
are untrustworthy, these results indicate that LIME is helpful
in assessing trust in individual predictions.
5.4 Can I trust this model?

In the ﬁnal simulated user experiment, we evaluate whether
the explanations can be used for model selection, simulating
the case where a human has to decide between two competing
models with similar accuracy on validation data. For this
purpose, we add 10 artiﬁcially “noisy” features. Speciﬁcally,
on training and validation sets (80/20 split of the original
training data), each artiﬁcial feature appears in 10% of the
examples in one class, and 20% of the other, while on the
test instances, each artiﬁcial feature appears in 10% of the
examples in each class. This recreates the situation where the
models use not only features that are informative in the real
world, but also ones that introduce spurious correlations. We
create pairs of competing classiﬁers by repeatedly training
pairs of random forests with 30 trees until their validation
accuracy is within 0.1% of each other, but their test accuracy
diﬀers by at least 5%. Thus, it is not possible to identify the
better classiﬁer (the one with higher test accuracy) from the
accuracy on the validation data.

The goal of this experiment is to evaluate whether a user
can identify the better classiﬁer based on the explanations of
B instances from the validation set. The simulated human
marks the set of artiﬁcial features that appear in the B
explanations as untrustworthy, following which we evaluate
how many total predictions in the validation set should be
trusted (as in the previous section, treating only marked
features as untrustworthy). Then, we select the classiﬁer with

fewer untrustworthy predictions, and compare this choice to
the classiﬁer with higher held-out test set accuracy.

We present the accuracy of picking the correct classiﬁer
as B varies, averaged over 800 runs, in Figure 8. We omit
SP-parzen and RP-parzen from the ﬁgure since they did not
produce useful explanations, performing only slightly better
than random. LIME is consistently better than greedy, irre-
spective of the pick method. Further, combining submodular
pick with LIME outperforms all other methods, in particular
it is much better than RP-LIME when only a few examples
are shown to the users. These results demonstrate that the
trust assessments provided by SP-selected LIME explana-
tions are good indicators of generalization, which we validate
with human experiments in the next section.
6. EVALUATION WITH HUMAN SUBJECTS
In this section, we recreate three scenarios in machine
learning that require trust and understanding of predictions
and models. In particular, we evaluate LIME and SP-LIME
in the following settings: (1) Can users choose which of two
classiﬁers generalizes better (§ 6.2), (2) based on the explana-
tions, can users perform feature engineering to improve the
model (§ 6.3), and (3) are users able to identify and describe
classiﬁer irregularities by looking at explanations (§ 6.4).
6.1 Experiment setup

For experiments in §6.2 and §6.3, we use the “Christianity”
and “Atheism” documents from the 20 newsgroups dataset
mentioned beforehand. This dataset is problematic since it
contains features that do not generalize (e.g. very informative
header information and author names), and thus validation
accuracy considerably overestimates real-world performance.
In order to estimate the real world performance, we create
a new religion dataset for evaluation. We download Atheism
and Christianity websites from the DMOZ directory and
human curated lists, yielding 819 webpages in each class.
High accuracy on this dataset by a classiﬁer trained on 20
newsgroups indicates that the classiﬁer is generalizing using
semantic content, instead of placing importance on the data
speciﬁc issues outlined above. Unless noted otherwise, we
use SVM with RBF kernel, trained on the 20 newsgroups
data with hyper-parameters tuned via the cross-validation.
6.2 Can users select the best classiﬁer?

In this section, we want to evaluate whether explanations
can help users decide which classiﬁer generalizes better, i.e.,
which classiﬁer would the user deploy “in the wild”. Specif-
ically, users have to decide between two classiﬁers: SVM
trained on the original 20 newsgroups dataset, and a version
of the same classiﬁer trained on a “cleaned” dataset where
many of the features that do not generalize have been man-
ually removed. The original classiﬁer achieves an accuracy
score of 57.3% on the religion dataset, while the “cleaned”
classiﬁer achieves a score of 69.0%. In contrast, the test accu-
racy on the original 20 newsgroups split is 94.0% and 88.6%,
respectively – suggesting that the worse classiﬁer would be
selected if accuracy alone is used as a measure of trust.

We recruit human subjects on Amazon Mechanical Turk –
by no means machine learning experts, but instead people
with basic knowledge about religion. We measure their
ability to choose the better algorithm by seeing side-by-
side explanations with the associated raw data (as shown
in Figure 2). We restrict both the number of words in each
explanation (K) and the number of documents that each

0102030# of instances seen by the user456585% correct choiceSP-LIMERP-LIMESP-greedyRP-greedy0102030# of instances seen by the user456585% correct choiceSP-LIMERP-LIMESP-greedyRP-greedyB = 10 instances with K = 10 words in each explanation (an
interface similar to Figure 2, but with a single algorithm).
As a reminder, the users here are not experts in machine
learning and are unfamiliar with feature engineering, thus
are only identifying words based on their semantic content.
Further, users do not have any access to the religion dataset
– they do not even know of its existence. We start the experi-
ment with 10 subjects. After they mark words for deletion,
we train 10 diﬀerent classiﬁers, one for each subject (with the
corresponding words removed). The explanations for each
classiﬁer are then presented to a set of 5 users in a new round
of interaction, which results in 50 new classiﬁers. We do a
ﬁnal round, after which we have 250 classiﬁers, each with a
path of interaction tracing back to the ﬁrst 10 subjects.

The explanations and instances shown to each user are
produced by SP-LIME or RP-LIME. We show the average
accuracy on the religion dataset at each interaction round
for the paths originating from each of the original 10 subjects
(shaded lines), and the average across all paths (solid lines)
in Figure 10.
It is clear from the ﬁgure that the crowd
workers are able to improve the model by removing features
they deem unimportant for the task. Further, SP-LIME
outperforms RP-LIME, indicating selection of the instances
to show the users is crucial for eﬃcient feature engineering.
Each subject took an average of 3.6 minutes per round
of cleaning, resulting in just under 11 minutes to produce
a classiﬁer that generalizes much better to real world data.
Each path had on average 200 words removed with SP,
and 157 with RP, indicating that incorporating coverage of
important features is useful for feature engineering. Further,
out of an average of 200 words selected with SP, 174 were
selected by at least half of the users, while 68 by all the
users. Along with the fact that the variance in the accuracy
decreases across rounds, this high agreement demonstrates
that the users are converging to similar correct models. This
evaluation is an example of how explanations make it easy
to improve an untrustworthy classiﬁer – in this case easy
enough that machine learning knowledge is not required.
6.4 Do explanations lead to insights?

Often artifacts of data collection can induce undesirable
correlations that the classiﬁers pick up during training. These
issues can be very diﬃcult to identify just by looking at
the raw data and predictions.
In an eﬀort to reproduce
such a setting, we take the task of distinguishing between
photos of Wolves and Eskimo Dogs (huskies). We train a
logistic regression classiﬁer on a training set of 20 images,
hand selected such that all pictures of wolves had snow in
the background, while pictures of huskies did not. As the
features for the images, we use the ﬁrst max-pooling layer
of Google’s pre-trained Inception neural network [25]. On
a collection of additional 60 images, the classiﬁer predicts
“Wolf” if there is snow (or light background at the bottom),
and “Husky” otherwise, regardless of animal color, position,
pose, etc. We trained this bad classiﬁer intentionally, to
evaluate whether subjects are able to detect it.

The experiment proceeds as follows: we ﬁrst present a
balanced set of 10 test predictions (without explanations),
where one wolf is not in a snowy background (and thus the
prediction is “Husky”) and one husky is (and is thus predicted
as “Wolf”). We show the “Husky” mistake in Figure 11a. The
other 8 examples are classiﬁed correctly. We then ask the
subject three questions: (1) Do they trust this algorithm

Figure 9: Average accuracy of human subject (with
standard errors) in choosing between two classiﬁers.

Figure 10: Feature engineering experiment. Each
shaded line represents the average accuracy of sub-
jects in a path starting from one of the initial 10 sub-
jects. Each solid line represents the average across
all paths per round of interaction.

person inspects (B) to 6. The position of each algorithm
and the order of the instances seen are randomized between
subjects. After examining the explanations, users are asked
to select which algorithm will perform best in the real world.
The explanations are produced by either greedy (chosen
as a baseline due to its performance in the simulated user
experiment) or LIME, and the instances are selected either
by random (RP) or submodular pick (SP). We modify the
greedy step in Algorithm 2 slightly so it alternates between
explanations of the two classiﬁers. For each setting, we repeat
the experiment with 100 users.

The results are presented in Figure 9. Note that all of
the methods are good at identifying the better classiﬁer,
demonstrating that the explanations are useful in determining
which classiﬁer to trust, while using test set accuracy would
result in the selection of the wrong classiﬁer. Further, we see
that the submodular pick (SP) greatly improves the user’s
ability to select the best classiﬁer when compared to random
pick (RP), with LIME outperforming greedy in both cases.
6.3 Can non-experts improve a classiﬁer?

If one notes that a classiﬁer is untrustworthy, a common
task in machine learning is feature engineering, i.e. modifying
the set of features and retraining in order to improve gener-
alization. Explanations can aid in this process by presenting
the important features, particularly for removing features
that the users feel do not generalize.

We use the 20 newsgroups data here as well, and ask Ama-
zon Mechanical Turk users to identify which words from the
explanations should be removed from subsequent training, for
the worse classiﬁer from the previous section (§6.2). In each
round, the subject marks words for deletion after observing

greedyLIME406080100% correct choice68.075.080.089.0Random Pick (RP)Submodular Pick (RP)0123Rounds of interaction0.50.60.70.8Real world accuracySP-LIMERP-LIMENo cleaninglearning, speciﬁcally for vision tasks [3, 29]. Letting users
know when the systems are likely to fail can lead to an
increase in trust, by avoiding “silly mistakes” [8]. These
solutions either require additional annotations and feature
engineering that is speciﬁc to vision tasks or do not provide
insight into why a decision should not be trusted. Further-
more, they assume that the current evaluation metrics are
reliable, which may not be the case if problems such as data
leakage are present. Other recent work [11] focuses on ex-
posing users to diﬀerent kinds of mistakes (our pick step).
Interestingly, the subjects in their study did not notice the
serious problems in the 20 newsgroups data even after look-
ing at many mistakes, suggesting that examining raw data
is not suﬃcient. Note that Groce et al. [11] are not alone in
this regard, many researchers in the ﬁeld have unwittingly
published classiﬁers that would not generalize for this task.
Using LIME, we show that even non-experts are able to
identify these irregularities when explanations are present.
Further, LIME can complement these existing systems, and
allow users to assess trust even when a prediction seems
“correct” but is made for the wrong reasons.

Recognizing the utility of explanations in assessing trust,
many have proposed using interpretable models [27], espe-
cially for the medical domain [6, 17, 26]. While such models
may be appropriate for some domains, they may not apply
equally well to others (e.g. a supersparse linear model [26]
with 5 − 10 features is unsuitable for text applications). In-
terpretability, in these cases, comes at the cost of ﬂexibility,
accuracy, or eﬃciency. For text, EluciDebug [16] is a full
human-in-the-loop system that shares many of our goals
(interpretability, faithfulness, etc). However, they focus on
an already interpretable model (Naive Bayes). In computer
vision, systems that rely on object detection to produce
candidate alignments [13] or attention [28] are able to pro-
duce explanations for their predictions. These are, however,
constrained to speciﬁc neural network architectures or inca-
pable of detecting “non object” parts of the images. Here we
focus on general, model-agnostic explanations that can be
applied to any classiﬁer or regressor that is appropriate for
the domain - even ones that are yet to be proposed.

A common approach to model-agnostic explanation is learn-
ing a potentially interpretable model on the predictions of
the original model [2, 7, 22]. Having the explanation be a
gradient vector [2] captures a similar locality intuition to
that of LIME. However, interpreting the coeﬃcients on the
gradient is diﬃcult, particularly for conﬁdent predictions
(where gradient is near zero). Further, these explanations ap-
proximate the original model globally, thus maintaining local
ﬁdelity becomes a signiﬁcant challenge, as our experiments
demonstrate. In contrast, LIME solves the much more feasi-
ble task of ﬁnding a model that approximates the original
model locally. The idea of perturbing inputs for explanations
has been explored before [24], where the authors focus on
learning a speciﬁc contribution model, as opposed to our
general framework. None of these approaches explicitly take
cognitive limitations into account, and thus may produce
non-interpretable explanations, such as a gradients or linear
models with thousands of non-zero weights. The problem
becomes worse if the original features are nonsensical to
humans (e.g. word embeddings). In contrast, LIME incor-
porates interpretability both in the optimization and in our
notion of interpretable representation, such that domain and
task speciﬁc interpretability criteria can be accommodated.

(a) Husky classiﬁed as wolf

(b) Explanation

Figure 11: Raw data and explanation of a bad
model’s prediction in the “Husky vs Wolf ” task.

Trusted the bad model
Snow as a potential feature

10 out of 27
12 out of 27

3 out of 27
25 out of 27

Before

After

Table 2: “Husky vs Wolf ” experiment results.

to work well in the real world, (2) why, and (3) how do
they think the algorithm is able to distinguish between these
photos of wolves and huskies. After getting these responses,
we show the same images with the associated explanations,
such as in Figure 11b, and ask the same questions.

Since this task requires some familiarity with the notion of
spurious correlations and generalization, the set of subjects
for this experiment were graduate students who have taken at
least one graduate machine learning course. After gathering
the responses, we had 3 independent evaluators read their
reasoning and determine if each subject mentioned snow,
background, or equivalent as a feature the model may be
using. We pick the majority to decide whether the subject
was correct about the insight, and report these numbers
before and after showing the explanations in Table 2.

Before observing the explanations, more than a third
trusted the classiﬁer, and a little less than half mentioned
the snow pattern as something the neural network was using
– although all speculated on other patterns. After examining
the explanations, however, almost all of the subjects identi-
ﬁed the correct insight, with much more certainty that it was
a determining factor. Further, the trust in the classiﬁer also
dropped substantially. Although our sample size is small,
this experiment demonstrates the utility of explaining indi-
vidual predictions for getting insights into classiﬁers knowing
when not to trust them and why.
7. RELATED WORK

The problems with relying on validation set accuracy as
the primary measure of trust have been well studied. Practi-
tioners consistently overestimate their model’s accuracy [20],
propagate feedback loops [23], or fail to notice data leaks [14].
In order to address these issues, researchers have proposed
tools like Gestalt [21] and Modeltracker [1], which help users
navigate individual instances. These tools are complemen-
tary to LIME in terms of explaining models, since they do
not address the problem of explaining individual predictions.
Further, our submodular pick procedure can be incorporated
in such tools to aid users in navigating larger datasets.

Some recent work aims to anticipate failures in machine

8. CONCLUSION AND FUTURE WORK
In this paper, we argued that trust is crucial for eﬀective
human interaction with machine learning systems, and that
explaining individual predictions is important in assessing
trust. We proposed LIME, a modular and extensible ap-
proach to faithfully explain the predictions of any model in
an interpretable manner. We also introduced SP-LIME, a
method to select representative and non-redundant predic-
tions, providing a global view of the model to users. Our
experiments demonstrated that explanations are useful for a
variety of models in trust-related tasks in the text and image
domains, with both expert and non-expert users: deciding
between models, assessing trust, improving untrustworthy
models, and getting insights into predictions.

There are a number of avenues of future work that we
would like to explore. Although we describe only sparse
linear models as explanations, our framework supports the
exploration of a variety of explanation families, such as de-
cision trees; it would be interesting to see a comparative
study on these with real users. One issue that we do not
mention in this work was how to perform the pick step for
images, and we would like to address this limitation in the
future. The domain and model agnosticism enables us to
explore a variety of applications, and we would like to inves-
tigate potential uses in speech, video, and medical domains,
as well as recommendation systems. Finally, we would like
to explore theoretical properties (such as the appropriate
number of samples) and computational optimizations (such
as using parallelization and GPU processing), in order to
provide the accurate, real-time explanations that are critical
for any human-in-the-loop machine learning system.
Acknowledgements
We would like to thank Scott Lundberg, Tianqi Chen, and
Tyler Johnson for helpful discussions and feedback. This
work was supported in part by ONR awards #W911NF-13-
1-0246 and #N00014-13-1-0023, and in part by TerraSwarm,
one of six centers of STARnet, a Semiconductor Research
Corporation program sponsored by MARCO and DARPA.

9. REFERENCES

[1] S. Amershi, M. Chickering, S. M. Drucker, B. Lee,
P. Simard, and J. Suh. Modeltracker: Redesigning
performance analysis tools for machine learning. In Human
Factors in Computing Systems (CHI), 2015.

[2] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe,
K. Hansen, and K.-R. M¨uller. How to explain individual
classiﬁcation decisions. Journal of Machine Learning
Research, 11, 2010.

[3] A. Bansal, A. Farhadi, and D. Parikh. Towards transparent

systems: Semantic characterization of failure modes. In
European Conference on Computer Vision (ECCV), 2014.

[4] J. Blitzer, M. Dredze, and F. Pereira. Biographies,

bollywood, boom-boxes and blenders: Domain adaptation
for sentiment classiﬁcation. In Association for
Computational Linguistics (ACL), 2007.

[5] J. Q. Candela, M. Sugiyama, A. Schwaighofer, and N. D.

Lawrence. Dataset Shift in Machine Learning. MIT, 2009.

[6] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and

N. Elhadad. Intelligible models for healthcare: Predicting
pneumonia risk and hospital 30-day readmission. In
Knowledge Discovery and Data Mining (KDD), 2015.

[7] M. W. Craven and J. W. Shavlik. Extracting tree-structured

representations of trained networks. Neural information
processing systems (NIPS), pages 24–30, 1996.

[8] M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G.
Pierce, and H. P. Beck. The role of trust in automation
reliance. Int. J. Hum.-Comput. Stud., 58(6), 2003.

[9] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least

angle regression. Annals of Statistics, 32:407–499, 2004.

[10] U. Feige. A threshold of ln n for approximating set cover. J.

ACM, 45(4), July 1998.

[11] A. Groce, T. Kulesza, C. Zhang, S. Shamasunder,

M. Burnett, W.-K. Wong, S. Stumpf, S. Das, A. Shinsel,
F. Bice, and K. McIntosh. You are the only possible oracle:
Eﬀective test selection for end users of interactive machine
learning systems. IEEE Trans. Softw. Eng., 40(3), 2014.
[12] J. L. Herlocker, J. A. Konstan, and J. Riedl. Explaining

collaborative ﬁltering recommendations. In Conference on
Computer Supported Cooperative Work (CSCW), 2000.

[13] A. Karpathy and F. Li. Deep visual-semantic alignments for

generating image descriptions. In Computer Vision and
Pattern Recognition (CVPR), 2015.

[14] S. Kaufman, S. Rosset, and C. Perlich. Leakage in data

mining: Formulation, detection, and avoidance. In
Knowledge Discovery and Data Mining (KDD), 2011.

[15] A. Krause and D. Golovin. Submodular function

maximization. In Tractability: Practical Approaches to Hard
Problems. Cambridge University Press, February 2014.
[16] T. Kulesza, M. Burnett, W.-K. Wong, and S. Stumpf.

Principles of explanatory debugging to personalize
interactive machine learning. In Intelligent User Interfaces
(IUI), 2015.

[17] B. Letham, C. Rudin, T. H. McCormick, and D. Madigan.

Interpretable classiﬁers using rules and bayesian analysis:
Building a better stroke prediction model. Annals of Applied
Statistics, 2015.

[18] D. Martens and F. Provost. Explaining data-driven

document classiﬁcations. MIS Q., 38(1), 2014.

[19] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and

J. Dean. Distributed representations of words and phrases
and their compositionality. In Neural Information
Processing Systems (NIPS). 2013.

[20] K. Patel, J. Fogarty, J. A. Landay, and B. Harrison.

Investigating statistical machine learning as a tool for
software development. In Human Factors in Computing
Systems (CHI), 2008.

[21] K. Patel, N. Bancroft, S. M. Drucker, J. Fogarty, A. J. Ko,

and J. Landay. Gestalt: Integrated support for
implementation and analysis in machine learning. In User
Interface Software and Technology (UIST), 2010.

[22] I. Sanchez, T. Rocktaschel, S. Riedel, and S. Singh. Towards
extracting faithful and descriptive representations of latent
variable models. In AAAI Spring Syposium on Knowledge
Representation and Reasoning (KRR): Integrating Symbolic
and Neural Approaches, 2015.

[23] D. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips,

D. Ebner, V. Chaudhary, M. Young, and J.-F. Crespo.
Hidden technical debt in machine learning systems. In
Neural Information Processing Systems (NIPS). 2015.

[24] E. Strumbelj and I. Kononenko. An eﬃcient explanation of

individual classiﬁcations using game theory. Journal of
Machine Learning Research, 11, 2010.

[25] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,

D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich.
Going deeper with convolutions. In Computer Vision and
Pattern Recognition (CVPR), 2015.

[26] B. Ustun and C. Rudin. Supersparse linear integer models
for optimized medical scoring systems. Machine Learning,
2015.

[27] F. Wang and C. Rudin. Falling rule lists. In Artiﬁcial

Intelligence and Statistics (AISTATS), 2015.
[28] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville,

R. Salakhutdinov, R. Zemel, and Y. Bengio. Show, attend
and tell: Neural image caption generation with visual
attention. In International Conference on Machine Learning
(ICML), 2015.

[29] P. Zhang, J. Wang, A. Farhadi, M. Hebert, and D. Parikh.

Predicting failures of vision systems. In Computer Vision
and Pattern Recognition (CVPR), 2014.

",False,2016.0,{},False,False,journalArticle,False,7KGAT6TB,[],self.user,False,False,False,False,http://arxiv.org/abs/1602.04938,,"""Why Should I Trust You?"": Explaining the Predictions of Any Classifier",7KGAT6TB,False,False
5W3MIPZH,IXDKDM3P,"IEEE Conference on Visual Analytics Science and Technology (VAST), Salt Lake City, October, 2010 

A Visual Analytics Approach to Model Learning 

Supriya Garg, I.V. Ramakrishnan, and Klaus Mueller 

Computer Science Department, Stony Brook University 

tags  those  data  elements  that  the  system  cannot  easily  resolve 
itself. 

One  crucial  idea  behind  our  system  is  that  given  good  feature 
vectors to represent each data point, points that are similar will be 
close-by  in  the  feature  vector  space.  Here,  we  mean  data-points 
which  though  rich  in  semantics,  do  not  have  an  explicit  high-
dimensional feature vector automatically attached to them. In such 
cases we need to design feature vectors to represent the semantics 
and  structure  of  the  data-points.  We  aim  to  achieve  this  in  our 
system  by  designing  feature  vectors  which  encompass  a  data 
point’s structure, context, and location in the dataset. If some sort 
of  semantic  information  is  available,  that  can  be  added  to  the 
feature vectors as well. 

Based on the above feature vectors, we design a visual interface 
where data is displayed in 2D space based on their feature vector 
similarity.  This  gives  the  users  an  overview  of  the  dataset,  and 
they  can  easily  observe  sets  of  data  points  which  form  spatial 
clusters.  At  this  stage,  we  can  apply  clustering  algorithms  which 
arrange similar points closer together. 

Clustering  as  we  know  is  a  general  approach  which  can  help 
users  explore  and  analyze  large  data  sets  since  it  lets  users  work 
with groups of objects, rather than individual objects which can be 
large in number. Clustering associates objects in groups such that 
the  objects  in  each  group  share  some  properties  (similar  feature 
vectors) that hold to a lesser degree  for the other objects. Spatial 
clustering  builds  clusters  from  objects  being  spatially  close  or 
having  similar  spatial  properties.  However,  clustering  methods 
when  run  automatically  can  give  non-intuitive  results.  Therefore, 
we  allow  the  user  to  tweak  the  parameters  of  both  the  clustering 
algorithm and the data’s feature vector to improve results.  

in 

the 

We  can  use  any  well-known  clustering  algorithm  to  subdivide 
the  points  into  clusters.  This  can  help  provide  the  initial  biasing 
for  any  classification  algorithm.  However,  the  model  learnt 
initially is a very rough estimation of the actual model, and needs 
fine-tuning to improve performance. For this purpose, we keep the 
user 
to  resolve 
inconsistencies.  Overall,  our  visualization  system  allows  the 
analyst  to:  (1)  identify  the  classes  in  the  data  and  communicate 
these  to  the  model  learning  system;  (2)  assess  the  fitness  of  the 
generated  model  by  the  structure  it  imposes  on  the  data;  and  (3) 
refine the model by communicating misrepresented patterns back 
to the model learning system. 

loop  and  utilize  human 

insight 

The model learning system uses the visualization to explain the 
model to the analyst. While most communication of the analyst is 
gesture-driven, editing facilities are available to directly specify or 
refine  the  model.  We  call  this  iterative  model  building  process 
data-driven, user-assisted model sculpting or model debugging.  

After we have learnt an initial model, we apply it to the dataset 
to  segment  it.  The  user  can  then  examine  the  visualization  for 
possible  misclassifications,  and  reclassify  the  data  points.  To 
guide the refinement of the model, we develop an interface similar 
to  a  step  debugger 
in  a  common  software  development 
environment.  Here,  the  model’s  structural  graph  represents  the 
program  script  while  the  visualization  is  the  program  output. 
Users  can  step  through  the  model’s  graph,  examine  the  visual 
model explanations, and refine the corresponding rules if required.  
The  main  contribution  of  our  work  is  that  it  lightens  the  huge 
burden of individually hand tagging data, and allows users to  tag 

 

 

ABSTRACT 

The process of learning models from raw data typically requires a 
substantial  amount  of  user  input  during  the  model  initialization 
phase. We present an assistive visualization system which greatly 
reduces  the  load  on  the  users  and  makes  the  process  of  model 
initialization  and  refinement  more  efficient,  problem-driven,  and 
engaging.  Utilizing  a  sequence  segmentation  task  with  a  Hidden 
Markov  Model  as  an  example,  we  assign  each  token  in  the 
sequence  a  feature  vector  based  on  its  various  properties  within 
the  sequence.  These  vectors  are  then  clustered  according  to 
similarity, generating a layout of the individual tokens in form of 
a node link diagram where the length of the links is determined by 
the  feature  vector  similarity.  Users  may  then  tune  the  weights  of 
the  feature  vector  components  to  improve  the  segmentation, 
which is visualized as a better separation of the clusters. Also, as 
individual  clusters  represent  different  classes,  the  user  can  now 
work  at  the  cluster  level  to  define  token  classes,  instead  of 
labelling  one  entry  at  time.  Inconsistent  entries  visually  identify 
themselves  by  locating  at  the  periphery  of  clusters,  and  the  user 
then  helps  refine  the  model  by  resolving  these  inconsistencies. 
Our system therefore  makes efficient use of the knowledge of its 
users, only requesting user assistance for non-trivial data items. It 
so allows users to visually analyze data at a higher, more abstract 
level, improving scalability. 
 
KEYWORDS:  Visual  Knowledge  Discovery,  Visual  Knowledge 
Representation, Data Clustering, Human-Computer Interaction. 
 
INDEX  TERMS:  H.5.2  [Information  Interfaces  and  Presentation]: 
interfaces;  I.2.6  [Artificial 
User  Interfaces—Graphical  user 
Intelligence]:  Learning—Concept  Learning; 
[Pattern 
Recognition]: Clustering—Similarity Measures. 

I.5.3 

1 

INTRODUCTION 

With the tremendous growth in physical and online data collection 
technology,  we  are  now  experiencing  an  explosion  of  digital 
information. Since a large  amount of these data are unstructured, 
various  machine  learning  techniques  have  been  developed  to 
assign  structure  to  these  data  to  make  them  machine  readable. 
This  process  can  allow  the  machine  to  reason  with  and  draw 
insight  from  data  almost  automatically.  However,  all  such  tasks 
depend  heavily  on  large  amounts  of  user-tagged  data  as  the 
starting  point,  and  use  various  semi-supervised  learning  methods 
[19].  Due  to  the  high  user  input  required,  such  tagged  data  is 
difficult  to  construct.  Further,  data  is  dynamic,  and  as  a  dataset 
grows and changes, we might need to supplement the tagged data 
from  time  to  time.  We  propose  to  make  this  task  simpler  and 
interactive  by  designing  a  system  where  the  user  can  obtain  a 
visual  overview  of  the  dataset,  and  in  that  visual  interface  only 

Email: {sgarg, ram, mueller}@cs.sunysb.edu 
 

 

1 

data at a higher level, with greater interactivity. Our approach can 
provide  a  solution  to  a  large  range  of  segmentation  and 
classification problems in the presence of complex and ambiguous 
data.  This  includes  the  audio/video  domain  where  we  want  to 
segment the input by speakers, scenes, moods, etc, and the image 
domain where we classify images by content type, or segment an 
image into objects.  

On the other end of the spectrum are methods which try to learn 
important  feature  vectors  for  data  classification  automatically 
[16].  However,  they  are  highly  time  intensive.  By  using  our 
approach, we can let the user into the loop, and allow him to guide 
the system making the process much more efficient. 

Our  paper  is  structured  as  follows.  Section  2  presents  related 
work,  Section  3  describes  relevant  theoretical  aspects,  Section  4 
provides details on implementation, Section 5 presents results, and 
Section 6 ends with conclusions and pointers for future work. 

2 

RELATED WORK 

This work follows the general Visual Analytics idea of combining 
human  domain  knowledge  with  automatic  data  analysis 
techniques  by  providing  users  with  interactive  visual  interfaces, 
whereby ‘interactive’  means that users can actively  participate in 
the analytical process as it evolves. Though our main focus is on 
allowing users to facilitate the process of model learning, visually 
guided  data  clustering  is  an  integral  part  of  it.  There  has  been 
some  work  related  to  this  field,  including  some  in  Visual 
Analytics.  The  approach  described  by  Schreck  et  al.  [17]  allows 
users 
leverage  existing  domain  knowledge  and  user 
preferences,  arriving  at  improved  cluster  maps.  Zhang  et  al.  [19] 
present a paradigm for visual exploration of clusters. Further, 
to 
make  sense  of  the  cluster  results,  visual  representations  are 
necessary.  Projection-based  approaches  as  presented  in  [8,  4]  are 
common. We use a Multidimensional Scaling (MDS) approach in 
this paper. 

to 

Learning  models  from  patterns  is  an  active  research  topic  in 
various  branches  of  computer  vision.  But  there  the  pattern 
examples  in  most  cases  originate  directly  from  image  analysis, 
promoting  unsupervised  learning  where  subtle  anomalies  (the 
unexpected  data)  or  new  families  of  patterns  which  the  model 
parameters  cannot  capture  often  go  undetected.  Visual  analytics, 
on  the  other  hand,  aims  to  be  more  flexible  in  the  data 
constellations  encountered,  appealing  to  the  complex  pattern 
recognition  apparatus  of  humans  and  their  intuition,  creativity, 
and  expert  knowledge  to  point  out  unusual  configurations  for 
further testing and  model refinement. Papers recognizing this are 
currently  emerging.  Janoos  et  al.  [9]  used  a  visual  analytics 
approach  to  learn  models  of  pedestrian  motion  patterns  from 
video  surveillance  data,  in  order  to  distinguish  typical  from 
unusual  behavior  in  order  to  flag  security  breaches  in  outdoor 
environments.  Their  semi-supervised  learning  approach  in  which 
users interact with video stream data improves upon the standard 
unsupervised  learning  schemes  that  are  typically  used  in  these 
scenarios.  

There  has  been  little  work  in  the  field  of  visually  assisted 
machine  learning.  The  last  few  years  at  VAST  has  seen  some 
papers  in  this  domain.  Our  own  work  on  Model-Driven  Visual 
Analytics  [6]  describes  a  visual  analytics  system  for  high-
dimensional  data  analysis.  In  this  system,  users  visually  explore 
the data in a high-dimensional space, and  mark  example patterns 
to iteratively learn rules using Logic Programming. The paper on 
LSAView [3] provides a visual analytic framework to analyze the 
model  parameters  in  Latent  Semantic  Analysis,  hence  promoting 
model  learning  and  debugging.  Andrienko  et  al.  [1]  present  an 
approach  to  extracting  meaningful  clusters  from  large  databases 

by combining clustering and classification, which are  driven by a 
human analyst through a visual interface. 

3 

THEORY 

We  shall  use  a  text  segmentation  application  as  an  example  to 
illustrate  our  work.  Important  here  are  the  concepts  of  document 
and  token.  In  text  segmentation  we  then  have  a  collection  of 
documents  that  contain  the  text  and  the  tokens  are  the  words 
appearing  in  these  text  items  (each  token  is  composed  of  a 
collection  of  letters).  But  we  may  just  as  well  perceive  a  set  of 
images (or even videos) as a collection of documents and coherent 
image  regions  as  tokens  (which  are  then  further  composed  into 
individual pixels). So we see that these concepts are quite general.  
The  data  segmentation  task  involves  working  on  multiple 
documents,  which  are  further  divided  into  tokens  to  do  any 
processing  and  calculations.  A  document  is  the  unit  which  is 
subject to either classification or segmentation. For text, it can be 
a single string (an address), a story (news entry), or an entire web 
page. A token is usually the smallest semantically meaningful unit 
in  the  document,  and  can  vary  depending  on  your  approach.  For 
text,  it  is  usually  a  word;  for  images,  it  can  be  a  pixel,  or  a 
contiguous iso-value region. For video, the natural unit is a frame. 
We observe that if there is a well-known boundary, then it is easy 
to  extract  tokens.  In  cases  where  such  boundaries  are  missing,  a 
multi-scale approach can work well. 
  Given the tokenized dataset, we often need to start with a coarse 
segmentation.  This  segmentation  gets  refined  as  we  apply  the 
learning algorithm, and possibly involve the user in the loop. This 
windowing  approach  has  been  used  in  the  segmentation  of  audio 
broadcast news into stories [18]. In the absence of a numeric way 
to  define  data  tokens,  these  windows  provide  a  simple  way  to 
define a feature vector. 

Segmentation  is  the  process  of  converting  the  data  in  a  raw 
stream  of  information  into  structured  records.  Given  a  schema 
consisting  of  n  attributes  and  an  input  string,  the  problem  of 
segmenting  the  input  string  involves  partitioning  the  string  into 
contiguous  sub-strings  and  assigning  each  sub-string  a  unique 
attribute  from  the  n  attributes.  For  example,  given  the  address 
schema  consisting  of  the  five  attributes  <COMPANY,  STREET, 
CITY, STATE, PHONE> and the input string “Adieu Travel 117 
Franklin  St  Dansville  NY  (716)  335-2222”, 
task  of 
segmentation  is  to  convert  the  string  into  the  address  record: 
<Adieu Travel, 117 Franklin St, Dansville, NY, (716) 335-2222>. 
Further  difficulties  may  emerge  when  the  records  appear  in 
different  order  in  different  strings.  The  running  example  in  our 
paper  uses  the  BigBook  business  address  dataset  [3]  which 
contains  approximately  3000  business  addresses  from  New  York 
State. 

the 

Information  on  the  web  (such  as  product  listings,  audio/video 
collections,  or  newscast)  exists  in  an  unstructured  format. 
Segmentation  into  structured  records  is  necessary  to  facilitate 
efficient query processing and analysis. The same goes for images 
and video for content-based image retrieval.   

Segmentation  techniques  either  use  rules  for  identifying 
attributes  or  employ  statistical  models.  Rule-based  approaches 
require  domain  experts  to  create  and  maintain  a  set  of  rules  for 
each  application  domain.  It  is  difficult  to  anticipate  all  possible 
variations  in  the  documents  to  be  segmented  and  design  rules 
accordingly.  Further,  noise  in  the  data  can  compound  the 
difficulty.  Therefore  rule-based  approaches  are  neither  scalable 
nor robust. In contrast, statistical approaches automatically learn a 
statistical model for each application domain. The variability and 
noise  in  the  input  text  data  are  elegantly  dealt  with  by  the 
statistical characteristics inherent in such approaches. 

 

2 

Table  1:  Feature  vector  table.  Each  matrix  entry  represents 
the presence or absence of a token in a document. Each row 
represents the feature vector for the corresponding token. 

  

3.1 

Hidden Markov Models 

Without  loss  of  generality,  we  use  a  Hidden  Markov  Model 
(HMM)  [15]  to  learn  the  segmentation  model.  The  HMM  is  a 
widely  used  statistical  model  used  for  data  segmentation.  It  is  a 
generative  model  since  it  captures  the  probability  distribution  of 
observations  (e.g.  the  input  strings  in  ase  of  text  segmentation). 
HMMs  are  commonly  used  to  represent  a  wide  range  of 
phenomena  in  text,  speech,  and  even  images  (using  2D  HMM 
[10,12].).  An  HMM  consists  of  a  set  of  states  S,  a  set  of 
observations  (in our  case  words  or  tokens)  W,  a  transition  model 
specifying P(st|st−1), the probability of transitioning from state st−1 
to  state  st,  and  an  emission  model  specifying  P(w|s)  the 
probability of emitting word w while in state s.  

To  compute  hidden  state  expectations  efficiently,  we  use  the 
Baum Welch algorithm. Emission models are initialized using the 
approximate  classification  done  using  the  visual  interface.  The 
transition  model  consists  of  a  completely  connected  graph  with 
uniform  probabilities.  Finally,  we  use  the  Viterbi  algorithm  with 
the learned parameters to label the test data. 

3.2  Overall Concept 

The  main  idea  behind  this  project  is  that  humans  with  specific 
domain knowledge can  easily spot the pattern in data, something 
which is very difficult for a machine to do. However, to make use 
of  this  expert  knowledge,  the  data  should  be  displayed  such  that 
the  task  for  the  user  becomes  easier.  A  good  approach  is  to 
identify  terms  that  exhibit  similar  characteristics,  and  display 
them  together.  This  essentially  means  that  we  need  to  find 
appropriate  feature  vectors  for  each  term  and  then  cluster  them 
together.  In  data  segmentation  and  classification  tasks,  the 
location  and  neighborhood  of  a  word  has  a  great  bearing  on  its 
classification. We observe two distinct cases: 

I.  Data  belonging  to  the  same  class  appears  together  within 
documents (e.g. when they belong to the same news topic), 
or 

II.  Data  belonging  to  the 
same  class  appears  in 
similar locations across 
(e.g.  city 
documents 
names  appearing 
in 
addresses). 

We  propose  to  treat  each 
document  as  a  sequence  of 
tokens.  For  the  case  where 
each  document  contains  one 
topic,  the  feature  vector  for 
the tokens is simply  a binary 
vector  with  a  1  representing 
presence,  and  a  0  absence. 

 

An  example  is  shown  in  Table  1  where  each  column  is  a 
document, and each row is a token. Here our vocabulary contains 
the letters (A,B,C,D,E,F), and we can clearly see the three clusters 
(A,B),  (C,D)  and  (E,F).  A  better  way  to  see  patterns  in  a  large 
dataset is to calculate the similarity between the feature vector of 
the  tokens  (using  the  dot  product,  or  some  other  well-known 
method),  and  laying  them  out  in  2D  using  Multidimensional 
Scaling (MDS) [11]. When tokens belong to exactly one topic, the 
classification  task  is  quite  simple.  However  the  fact  that  most 
tokens belong to multiple topics makes the task harder, and often 
impossible to solve for the machine. 
 
In Figure 1, we show an example to illustrate the value of this 
approach. In Figure 1(a), the tokens from our dataset are laid out 
randomly.  Briefly  looking  at  it  makes  it  clear  that  these  nodes 
represent male first names. However, in Figure 1(b) which shows 
the  points  laid  out  using  the  windowed  approach,  a  user  can  see 
the  further  pattern 
to  different 
nationalities,  and  in  fact  this  property  (same  nationality)  causes 
nodes to be laid out close by. Figure 1(c) shows the words colored 
and circled according to their classification by the user. This task 
could  not  be  done  automatically  since  even  though  tokens  in  the 
same  class  occur  close  by,  tokens  belonging  to  different  classes 
are  also  at  a  similar  distance  in  multiple  instances.  Only  the 
domain knowledge of a user can help resolve such ambiguities. 
  For case II, i.e. when tokens in the same class appear in similar 
locations across documents, we need to capture the locations they 
appear  within  all  documents.  To  do  so,  we  divide  the  tokens  in 
each  document  into  equal  numbers  of  windows  (Nw),  i.e.  each 
token  is  assigned  a  window  according  to  its  relative  position 
within  the  document.  Given  document  di  ,  token  tokj,  number  of 
tokens  in  the  document  num(di)  and  the  location  of  the  token 
loc(tokj), its window number is: 
 

these  names  belong 

that 

(1) 

 

 

 

(e.g.)      

 
  The  above  example  shows  how  a  document  with  6  tokens  is 
divided  into  3  windows.  We  construct  the  feature  vector  of  each 
token  as  the  histogram  summarizing  the  frequency  with  which  a 
token has appeared in different windows. An ideal window size is 
data and task dependent, and is best chosen while interacting with 
the  visual  layout  of  the  data.  For  visualization,  we  can  again 
calculate token similarity by taking a dot product between feature 
vectors, and display them in 2D using MDS. 
  An  example  of  using  windows  for  the  initial  segmentation  of 

 

  

  

  

  

   (a) Random layout                                 (b) Window-based layout                         (c) Clustered layout 

Figure 1: This image shows the layout of points representing people’s names (a) randomly, and (b) based 
on the window based  approach. The user interacts with the graph in (b), and marks people who belong 
together (here based on nationality), giving us (c). 

 

3 

(a) Random layout(b) Window based layout(c) Clustered Layout(a) Initial 

segmentation 

(b) After 1st 
iteration 

(c) Final 

segmentation 

Figure  2:  An  example  of  using  a  coarse  segmentation 
(windowing) to initialize the image segmentation process. 

data is shown in Figure 2.  Here our task is to segment the colored 
image into its constituent blocks (Fig 2(c)). Initially we divide the 
image into 4 4 sub-images. This starts off the learning algorithm, 
and  after  a  few  iterations  gives  the  required  result.  Note  that  the 
window  sizes  are  at  a  similar  scale  to  the  final  segments.  If  we 
start off with windows too large (larger than the largest segment), 

then the results might not be even close to expected. On the other 
hand,  windows  that  are  too  small  increase  the  problem  size, 
making it longer (and harder) to solve. 

This  windowing  approach  is  similar  to  the  one  used  in  [14] 
which  use  bigrams  and  trigrams  as  basic  units  for  document 
visualization.  

4 

IMPLEMENTATION  

The basic approach of our system is as follows: we take tokenized 
data  documents,  calculate  the  relevant  feature  vectors,  and  allow 
the user to help initialize and refine models to cluster the data. As 
mentioned, we use HMMs as an example of a learning approach. 
An overview of our system is shown in the flowchart in Figure 3. 
   Initially  the  data  is  preprocessed,  and  tokenized.  So  each 
document  will  contain  a  sequence  of  tokens  which  need  to  be 
labeled.  We  consider  all  identical  tokens  to  be  instances  of  the 
same entity, and our feature vector calculations are based on these 
entities. 

Figure 3: Overall system. (Left): Flowchart outlining our approach. The red boxes represent user-interaction steps. (Right): An overview of 
the system. (c) The entities are laid out in a node-link diagram, along with the clusters; (a) and (b) show sliders used by the user in step 4.   
(a)  Sliders  to  filter  the  graph,  (b) Sliders to  adjust the  weights  of the  feature  vector components,  (d)  The  interface  showing  segmentation 
results that fulfil certain inconsistency criteria – the user then interacts with the system (step 9) to resolve these inconsistencies. 

 

4 

weights wi due to the three properties, the final similarity between 
them is: 

 

 

 

 

(2) 

4.1 

Initialization Stage 

The  entities  are  displayed  as  a  node-link  diagram  using  a  force-
directed  layout  algorithm  [7].  This  algorithm  considers  a  spring-
like force for every pair of nodes (x, y) where the ideal length δxy 
of  each  spring  is  proportional  to  the  graph-theoretic  distance 
between  nodes  x  and  y.  Minimizing  the  difference  between 
Euclidean  and  ideal  distances  between  nodes  is  equivalent  to  a 
metric  multidimensional  scaling  problem.  Here  we  use  the 
dissimilarity  between  nodes  as  the  ideal  distance  between  two 
nodes. The dissimilarity is simply the inverse of similarity:  

 

 

 

 
 

(3) 
 
Figure 4 shows the graph evolution, and finally patterns emerge 
showing the overall structure of the dataset. At this stage, the user 
can  modify  the  weights  of  the  feature  vectors.  This  updates  the 
node similarities as well as the graph layout.  

Note that any pair of nodes will have a similarity measure, but 
keeping all the edges will give us an extremely dense graph with 
no discernible structure. Removing edges between nodes with low 
similarity  (<  0.5)  helps  this  structure  to  emerge  better.  The 
optimal  cutoff  value  depends  on  the  data  density  and  the  feature 
vector  selection  (i.e.  weights).  We  allow  the  user  to  select  this 
cutoff value to find the appropriate optimal value. Figure 5 shows 
an illustration of how the graph layout changes  with the removal 
of edges representing low similarity. The one disadvantage is that 
some  nodes  become  isolated,  i.e.  are  not  connected  to  any  other 
node.  We  can  handle  this  by  either  allowing  the  user  to  assign 
them  to  a  cluster  after  we  call  the  clustering  algorithm.  Else,  the 
HMM  can  classify  them  during  the  learning  stage,  and  they  get 
displayed  in  the  corresponding  cluster  during  the  refinement 
stage.  

Figure  4:  Graph  evolution  based  on  node  dissimilarities  (as 
shown along the edges) 

  Observing  the  datasets  led  us  to  the  realization  that  there  are 
three important properties which can help us classify a token – 

(a)  Its  structure.  For  text  this  includes  –  is  it  a  word,  is  it  a 

number, its length etc.  

(b)  Its context, e.g. the tokens that appear before and after it,  
(c)  Its location in the document. 

Structure  and  context  can  also  include  information  beyond  the 
information  contained  in  the  tokens  themselves.  This  includes 
meta-data  which  comes  attached.  Webpage  data  though  not 
necessarily  machine  readable  already  exhibits  some  structure  via 
html meta-data.   The  non-visual  web  browser  presented  in  [13] 
uses the meta-data to segment webpages into semantically related 
regions. 

We  numerically  capture  the  above  properties  in  each  entity’s 
feature vector. This means that a feature vector contains multiple 
high-dimensional  vectors.  When  we  calculate  the  similarity 
between  any  two  entities,  we  need  to  calculate  similarities  based 
on  each  property  separately,  and  then  combine  them  into  a  final 
value. 

A survey on cluster data mining techniques   [2]  concludes  that 
data  specific  attribute  selection  has  yet  to  be  invented.  Recent 
work  on  unsupervised  feature  learning  [16],  confirms  that  user 
interaction can greatly reduce the learning time. To help alleviate 
this  problem  to  some  extent,  we  give  the  power  of  assigning  the 
weights of the feature vector elements to the user. The similarities 
due to different elements are all normalized to ranges between [0, 
1]. Given two entities x and y, and similarity values simi(x, y) and 

Reduce  edges  with  low 
similarity 
to 
reveal more structure 

values 

Figure 5: Graph structure: As we change the cutoff values for node-pair similarity from 0.5 to 0.85, the graph goes from being 
almost a single mass of nodes on the left, to one displaying more clusters on the right 

 

5 

Users  can  affect  the  graph  layout,  and  the  clustering  using  the 

following inputs: 

(a)  Modifying the weights of the feature vector elements 
(b)  Modifying  the  similarity  cutoff  –  this  removes  the  edges 
representing  low  similarity,  and  also  reveals  a  clearer 
structure of the dataset. 

(c)  Modifying  the  frequency  cutoff  of  the  data  points.  This 
leaves only the most prominent nodes behind, which act as 
representative nodes, and give  the users a good idea of the 
classes present in the dataset. 

(d)  Modifying the “fineness” level of clustering – this controls 
how many clusters the data gets split into. Getting the most 
appropriate  number  of  clusters  will  reduce  the  load  on  the 
user during the sculpting stage. (Step 6 in Figure 3). 

Further, we notice that the visualization reveals  more  structure 
when we display entities from the entire dataset, rather than from 
a  smaller  subset.  This  is  because  in  the  smaller  dataset  most  low 
frequency entities do not cluster well. As we increase the number 
of documents, some of these frequencies improve, and the entities 
become better defined.  

When  the  various  settings  work  together  to  produce  a 
semantically meaningful structure, the user can call the clustering 
algorithm.  A  good  choice  of  weights  at  this  early  stage  will 
minimize  the  work  required  from  the  user  at  later  stages  – 
especially at the cluster sculpting stage, and relabeling the tokens 
at  the  refinement  stage.  In  most  cases,  a  range  of  values  gives 
decent results, hence carefully updating the weights and observing 
the visualization can take care of this problem. 

We use the Markov Cluster Algorithm (MCL), an unsupervised 
cluster algorithm for graphs based on the simulation of stochastic 
flow in graphs [5]. Only those similarity values that are above the 

cutoff specified by the user are passed to the clustering algorithm. 
This  has  a  similar  effect  as  it  has  on  the  visualization  –  it  helps 
segment the data better, i.e. divides them into more classes. Once 
the  clusters  are  calculated,  they  are  displayed  by  forming  a 
convex-hull  boundary  around  the  data  points  they  contain. 
Further,  we  relax  the  strength  of  edges  across  clusters  to  reduce 
their spatial overlap. (See Figure 6) 

 The physical boundaries between clusters, might lead the user 
to discover some semantic discrepancies – this usually occurs due 
to entities which are ambiguous and can occur in multiple classes, 
or in cases where the feature vector is not able to classify an entity 
correctly.  In this case, a user can visually sculpt the clusters – this 
involves splitting and merging  clusters, dragging nodes from one 
cluster to another, or duplicating nodes into different clusters (for 
example, the token ‘York’ may appear in ‘Street’, ‘State’, ‘City’, 
and  even  ‘Company’  depending  on  context).  The  visualization 
assists  the  user  at  this  stage  by  highlighting  the  nodes  that  have 
more than 99% of edges to nodes within the same cluster. At the 
end of this stage, the number of states in the HMM is assumed to 
be  the  number  of  clusters  in  our  visualization.  Further,  the 
emission  probabilities  are  calculated  based  on  these  clusters  – 
entities  with  a  higher  frequency  in  the  original  dataset  have  a 
higher  emission  probability.  To  allow  the  model  learning 
algorithm  to  assign  a  word  to  a  different  class  from  that  in  the 
above cluster, we assign a small emission probability value to all 
the words in all the states other than the one they appear in. This 
approach  also  allows  the  Baum-Welch  algorithm  to  learn  fairly 
accurately  for  words  which  appear  in  multiple  classes.  We  go 
through the first round of HMM training to learn our initial model, 
and then use Viterbi algorithm to segment the strings. 

Figure 6: Result. The four panels show the evolution of the clusters as the user reclassifies the segmentation of some address entries, and 
the HMM relearns the model taking this into account 

 

6 

4.2 

Refinement Stage 

At  this  stage  the  user  can  help  resolve  inconsistencies  in  the 
segmentation,  and  help  debug  the  model.  To  make  this  more 
intuitive,  we  first  need  the  user  to  give  the  clusters  semantic 
names. This is done by highlighting the nodes in all clusters which 
has  a  very  low  similarity  to  nodes  in  other  clusters.  In  case  the 
presence  of  some  entity  makes  the  identity  of  the  class 
ambiguous,  the  user  can  choose  to  see  some  strings  where  the 
entity  appears.  For  e.g.  if  Washington  appears  in  a  cluster  with 
what  mostly  appears  to  be  street  names,  seeing  it  in  context  will 
help the user be sure that  the given cluster  indeed contains street 
names, not cities or states. Two examples are given below: 

1.  D  Sacilotto  399  Washington  St  New  York  NY  (212)  966-

7274 

2.  Burke  &  Casserly  PC  255  Washington  Avenue  Ext  Albany 

NY (518) 452-1961 

At  this  stage,  we  have  named  clusters  and  a  trained  HMM. 
Now,  we  can involve  multiple users in the refinement stage. Just 
as  interaction  with  users  can  quickly  help  define  models, 
misclassifications at any stage can slow the process down. Hence 
involving multiple users at the refinement stage can help remove 
mistakes  made  by  a  user  due  to  either  lack  of  knowledge,  or 
uncertainty. 

 We pick documents that contain tokens that did not cluster well 
– i.e. had a lot of highly similar nodes in other clusters. These are 
the entities which belong to multiple classes in a dataset. The user 
resolves this inconsistency, and if such a token is indeed classified 
into more than one class by the user, we duplicate it to represent 
various  identities  of  a  single  entity.  This  also  reduces  the  edges 
across clusters, making the model better defined. We ask the user 
to reclassify a few documents at a time, and then retrain the HMM 
to reflect these changes.  

During  this  debugging  stage,  the  user  can  highlight  the 
consecutive  tokens  in  the  graph  layout.  When  a  specific  token  is 
selected,  the  corresponding  node(s)  in  the  graph  are  highlighted. 
Also, the cluster to which it has been classified is also highlighted. 
This helps the user see the possible classes the token could belong 
to.  The  user  can  traverse  forwards  and  backwards  along  a  given 
document to establish if there is any misclassification. 

The visualization where the clusters are marked often excludes 
entities with low frequency – either because they were filtered out 
by  the  user,  or  they  were  not  assigned  a  cluster  due  to  low 
similarity  to  any  cluster.  However,  after  the  HMM  is  trained,  all 
the  entities  in  the  dataset  will  be  assigned  emission  probability 
values.  Further,  if  we  want  to  display  duplicates  for  entities 
belonging  to  multiple  clusters,  the  visualization  will  become  too 
dense. To overcome this problem, after the initialization stage, we 
only  display  the  entities  which  have  very  few  high  similarity 
edges to nodes in other clusters. These are called the inner nodes, 
and are the representatives of their classes.  When a certain token 
is  selected  during  debugging,  its  corresponding  node(s)  are  also 
visualized.  The  user  can  still  use  our  interface  to  display  more 
nodes if required. 

5 

RESULTS 

The  system  we  described  can  help  in  the  classification  of  varied 
types  of  datasets.  As  a  starting  step,  we  worked  with  text-based 
datasets  since  they  require  minimal  preprocessing,  and  can  show 
the  utility  of  our  method.  In  this  section  we  will  see  a  running 
example  of  learning  a  model  (HMM)  for  the  business  address 
dataset.  The  BigBook  business  address  dataset  contains 
approximately  3000  business  addresses  from  New  York  State. 
This dataset contains a large number of distinct integers, and each 

 

7 

separate integer does not have a special meaning, but usually just 
represents  a  street  or  building  number  or  is  a  part  of  a  business 
name. To make sure that the HMM is able to learn well, and apply 
the model to addresses  with new tokens, we replace each integer 
with the term DIGIT-<i> where “i” is the length of the integer.  

After  the  initial  preprocessing,  we  calculate  feature  vectors 

based on the three properties of structure, context and location: 
 

 

 

 

 

 

The  feature  vector  on  structure  contains  the  following 
information:  does  it  contain  a  letter,  does  it  contain  a  digit, 
does  it  contain  a  non-alphanumeric  symbol,  does  it  begin 
with  a  capital  letter,  is  it  all  caps,  and  its  length.  Except  for 
the last element, the others are binary.  

The  feature  vector  on  context  contains  a  summary  (i.e. 
histogram)  of  the  structure  based  feature  vectors  of  the 
tokens  that  appear  immediately  before  and  after  the  given 
entity in all the documents.  

Finally,  for  the  feature  vector  on  location,  we  use  the 
windowing approach as presented in Section 3.2. Initially, if 
we know the number of classes (Nc) in the dataset, we use its 
multiples  as  the  number  of  windows.  In  the  absence  of  this 
information,  the  window  size  is  decided  by  the  document 
lengths (i.e. number of tokens in the document). 

 

Now the data is displayed on the screen, and as the user plays with 
the  similarity  cutoff  slider,  the  graph  goes  from  being  one  main 
cluster  with  all  nodes  interconnected,  to  one  which  shows  the 
inbuilt  structure  of  the  data.  At  this  point,  the  user  clusters  the 
dataset.  Since  there  is  a  lot  of  overlap  between  the  terms  in 
ADDRESS  and  CITY  (see  Figures  4  and  6),  there  is  an  extra 
cluster  which  contains  these  common  tokens.  The  user  organizes 
and splits this cluster to the best of his knowledge, and merges the 
corresponding halves with the two clusters.  
  Next  we highlight the  inner nodes in the dataset, which shows 
the user the representative terms for each cluster, and he is able to 
name them. Figure 7 shows an example of these inner nodes. Now 
we initialize the emission probabilities for all states based on the 
clusters. If an entity belongs to a cluster, its emission probability 

Figure  7:  Graph  showing  the  inner  entities/nodes  in  green. 
These  nodes  help  the  user  both  during  the  cluster  sculpting 
stage, and during the cluster naming stage. 

REFERENCES  

[1] 

[2] 

[3] 

[4] 

[5] 

[6] 

[7] 

[8] 

[9] 

[10] 

[11] 

[12] 

[13] 

G.  Andrienko,  N.  Andrienko,  S.  Rinzivillo,  M.  Nanni,  D. 
Pedreschi  and  F.  Giannotti.  Interactive  visual  clustering  of  large 
collections  of  trajectories.  Proc.  IEEE  Symp.  Visual  Analytics 
Science and Technology (VAST), pp. 3-10, 2009. 
P.  Berkhin.  A  survey  of  clustering  data  mining  techniques. 
Grouping Multidimensional Data. pp. 25–71, 2006. 
P.  Crossno,  D.  Dunlavy,  T.Shead.  LSAView:  A  Tool  for  Visual 
Exploration  of  Latent  Semantic  Modeling.  Proc.  IEEE  Symp. 
Visual Analytics Science and Technology (VAST), pp. 83-90, 2009. 
I.S.  Dhillon,  D.S.  Modha  and  W.S.  Spangler.  Class  visualization 
of  high-dimensional  data  with  applications.  Computational 
Statistics and Data Analysis. 41(1): 59–90,  2002. 
S.V.  Dongen.  Graph  clustering  by  flow  simulation.  PhD  Thesis, 
University of Utrecht, The Netherlands. 2000. 
S. Garg, J.E. Nam, I.V. Ramakrishnan, K. Mueller. Model-Driven 
Visual Analytics. Proc. IEEE Symp. Visual Analytics Science and 
Technology (VAST), pp. 19-26, 2008. 
J.  Heer,  S.  Card,  J.A.  Landay.  Prefuse:  a  toolkit  for  interactive 
information  visualization.  Proc.  SIGCHI  Conference  on  Human 
Factors in Computing Systems, pp. 421–430, 2005. 
A.  Hinneburg,  D.  Keim,  M. Wawryniuk.  HD-Eye:  Visual  mining 
IEEE  Computer  Graphics  and 
of  high-dimensional  data. 
Applications. 19(5): 22–31,  1999. 
F.  Janoos,  S.  Singh,  O.  Irfanoglu,  R.  Machiraju,  R.  Parent. 
Activity  Analysis  Using  Spatio-Temporal  Trajectory  Volumes  in 
Surveillance  Applications.  Proc.  IEEE  Symp.  Visual  Analytics 
Science and Technology (VAST), pp. 3-10, 2007. 
J.  Jiten,  B.  Merialdo.  Semantic  Image  Segmentation  with  a 
Multidimensional Hidden Markov Model. Advances in Multimedia 
Modeling. 4351: 616–624, 2007. 
J.B.  Kruskal  and  M.  Wish.  Multidimensional  scaling.  Sage 
Publications, Inc. 1978. 
J.  Li,  A.  Najmi,  R.  Gray.  Image  classification  by  a  two-
dimensional  hidden  Markov  model.  IEEE  Trans.  on  Signal 
Processing. 48(2): 517–533,  2000. 
J.U.  Mahmud,  Y.  Borodin,  I.V.  Ramakrishnan.  Csurf:  a  context-
driven  non-visual  web-browser.  Proc.  16th 
International 
Conference on World Wide Web, pp. 31-40, 2007. 

[14]  Y.  Mao, 

J.  Dillon.  G.  Lebanon.  Sequential  Document 
Visualization.  IEEE  Trans.  on  Visualization  and  Computer 
Graphics. 13(6): 1208-1215,  2007. 
L.R.  Rabiner.  A  tutorial  on  hidden  Markov  models  and  selected 
applications  in  speech  recognition.  Proceedings  of  the  IEEE. 
77(2): 257–286,  1989. 

[15] 

[16]  R.  Raina,  A.  Battle,  H.  Lee,  B.  Packer,  A.  Ng.  Self-taught 
learning:  transfer  learning  from  unlabeled  data.  Proc.  24th 
International Conference on Machine Learning, pp. 766, 2007. 
T.  Schreck,  J.  Bernard,  T.  Tekusova,  J.  Kohlhammer.  Visual 
cluster analysis of trajectory data with interactive Kohonen maps. 
Proc.  IEEE  Symp.  Visual  Analytics  Science  and  Technology 
(VAST), pp. 3–10, 2008. 

[17] 

[18]  A.  Vinciarelli,  S.  Favre.  Broadcast  News  Story  Segmentation 
using Social Network Analysis and Hidden Markov Models. Proc. 
15th International Conference on Multimedia, pp. 264-267, 2007. 

[19]  K.B. Zhang, M. Huang, M. Orgun, Q. Nguyen. A  Visual Method 
for  High-Dimensional  Data  Cluster  Exploration.  Proc.16th 
International  Conference  on  Neural  Information  Processing,  pp. 
699–709, 2009. 

[20]  X.  Zhu.  Semi-Supervised  Learning  Literature  Survey.  Technical 

Report 1530, 2005. 

 

is the ratio between frequency of the entity and the total frequency 
of  the  entities  in  the  cluster.  As  mentioned  in  Section  4.1,  we 
assign a small probability value to all the entities not belonging to 
the  cluster.  At  this  stage  the  Baum-Welch  algorithm  is  called  to 
learn an initial model. 
  Now  the  user  is  shown  strings  with  one  token  that  has  high 
similarity  to  nodes  in  multiple  clusters.  Given  that  the  token  has 
high similarity to entities in classes c1 and c2, we: 

(a)  Assign a positive value if the classification was correct 
(b)  If  the  classification  in  class  ci  is  deemed  incorrect,  and 
reclassified to class cj, then we assign a heavy penalty to ci 
and a heavy bonus value to cj. 

The accumulated values at the end of a reclassifying round are 
used  to  modify  the  emission  tables  learned  by  the  Baum-Welch. 
Further, based on this, we also split the nodes if they are classified 
into  different  classes  in  the  cluster.  The  user  helps  in  the 
debugging  process  till  the  clusters  become  better  defined,  as 
shown  in Figure  6.  After  four  rounds  of  refinement  based  on  the 
tokens  with  high  presence  in  multiple  clusters,  we  get  a  good 
model  for  classification.  The  time  taken  between  iterations 
depends on the complexity and implementation of the algorithms 
used to learn the model, and segment the strings – in this case the 
Baum Welch, and Viterbi algorithm respectively.  

6 

CONCLUSIONS  

this  paper  we  have  presented  a  general  approach 

In 
to 
visualization-assisted  model  learning  for  data  segmentation  and 
classification  tasks.  The  driving  motivation  for  our  approach  is 
that the typical manual tagging of data is very resource intensive. 
Further, even if one uses just a  small dataset  for tagging to boot-
strap  the  learning  process,  if  the  chosen  subset  is  not  a  good 
representative  of  the  entire  dataset,  then  the  models  learnt  might 
not be robust. On the other hand, completely  automated  methods 
which  take  a  brute  force  approach  by  using  large  feature  vectors 
for classification have the drawback of being very time intensive, 
and small misclassifications can cause the model learnt to be less 
than ideal. In this case  user interaction at various stages can help 
the machine stay on track, and so will improve the speed as well.  
As  part  of  our  future  work,  we  aim  to  extend  this  approach  to 
varied  data  types  and  with  different  clustering  and  classification 
algorithms.  This  promises  to  give  further  insight  into  semi-
automatic  design  of  feature  vectors  for  other  domains.  In  this 
paper,  we  have  demonstrated  the  use  of  our  system  for  a  fairly 
constrained  problem  –  the  segmentation  of  business  addresses 
using HMMs. Work is currently underway that extends the use of 
our system to other model building tasks, such as classifiers, using 
the  categorization  of  large  image  collections  and  documents  as  a 
driving application. 

In future work, we plan to optimize the usability aspects if the 
system  by  ways  of  focused  user  studies.  In  particular,  we  would 
like  to  study  and  precisely  quantify  how  much  faster  a  model  of 
equal  quality  can  be  derived  with  our  framework,  as  opposed  to 
more traditional non-interactive approaches. Further, we also wish 
to  study  if  the  visual  analytics  approach  we  have  proposed  here 
allows  domain  experts  to  produce  models  that  are  more  accurate 
than  those  derived  automatically  since  the  user  is  actively 
involved in the model-sculpting task.   

ACKNOWLEDGEMENTS 

This  research  was  supported  by  NSF  grants  CCF-0702699  and 
CNS-0627447.  We  also  thank  all  reviewers  for  their  useful 
suggestions. 

 

8 

",False,2010.0,{},False,False,conferencePaper,False,5W3MIPZH,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/5652484/,,A visual analytics approach to model learning,5W3MIPZH,False,False
ITGJM9CV,6XW9NVA7,"Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Opening the Black Box –

Data Driven Visualizaion of Neural Networks

Fan-Yin Tzeng & Kwan-Liu Ma

September 20, 2006

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Aritiﬁcial Neural Networks
Limitations of ANNs
Use of Visualization

Artiﬁcial Neural Networks

I Artiﬁcial Neural Networks (ANNs) mimic the processes found

in biological neural networks.

I The brain is a massively parallel information processing system

formed by about ten billion nerve cells (neurons) and their
synapses.

I Used for predicting and learning from a given set of data.
I Based on the combination of neurons, connections, transfer
functions, learning algorithms, and neuron layout methods.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Aritiﬁcial Neural Networks
Limitations of ANNs
Use of Visualization

Limitations of ANNs

I ANNs act as a black box.

I Information stored is a set of weights and connections that

provides no insight as to how a task is performed.

I The relationship between the input and output is not clear.
I ANNs do not provide a solution that is easily understood or

validated.

I Parameter selection is tedious, and often done in a

trial-and-error process.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Aritiﬁcial Neural Networks
Limitations of ANNs
Use of Visualization

Use of Visualization

I Visualizations of ANNs give insight to how the NN works.
I Viz also used in network structuring and parameter selection,

saving time and cost.

I Previous ANN viz did not focus on data used by network.

I This paper takes a data driven approach.
I In addition to ANN viz, the user is allowed to interact with

data and see the eﬀect on the network.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

ANN Basics
Speciﬁcs for this paper

In General

I Each node (neuron) is connected to all other nodes in the

adjacent layer.

I Each connection between nodes has a weight, with the

weights modulating the value across the connection.

I Notation:

I Nodes in input layer: I1, I2, I3, . . . , Im
I Nodes in Hidden layer: H1, H2, H3, . . . , Hn
I Weight of connection between Ii and Hj : Wij

I The value of a node in the hidden layer is given by:

Hj = TF (Pn

i=1 Wij × Ii )

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

ANN Basics
Speciﬁcs for this paper

(Cont.)

Ok = TF (Pm

i=1 Wjk × Hj )

I The value of a node in the output layer (Ok ):

I TF (x) is a non-linear transfer function.

I Transfer function is what converts NN to a non-linear system.
I Non-linear needed since classifying NNs make terminal

desicions.

I When calculating the value of an output node, the same

transfer function used on the previous layer is applied to the
summed results from that (previous) layer.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

ANN Basics
Speciﬁcs for this paper

Training

I At the beginning....

I We need a set of training inputs and desired outputs.
I Weights are set at random.

I Weights are then iteratively modiﬁed to obtain minimum error.
I The criterion for ”minimum” is some arbitrary threshold

(maybe a small one).

I After training, the network may be used for data similar to the

training set.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

ANN Basics
Speciﬁcs for this paper

Speciﬁcs

I NN is feed-forward.

I Input always moves from left to right, no loops.

I Back-propogation training algorithm used.

I Mean squared error calculated ﬁrst calculated at output level

for a given input.

I Error is then minimized, by changing weights, in a

back-to-front manner at the next level.

I Error minimization iteratively continues until threshold

reached.

I Repeat steps 1-3 on the other test data.
I Start all over again.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

ANN Basics
Speciﬁcs for this paper

(Cont.)

I The standard sigmoid function is used for the transfer

function.

1

(1+e−x )

I f (x) =
I A continuous diﬀerentiable version of a stair-stepping function.
I Is the most common transfer fucntion used for classiﬁcation.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

Volume Classiﬁcation

I Trained by a small set of input data including the

corresponding class IDs.

I Input vector includes a voxel’s scalar value, gradient

magnitude, its neighbors’ scalar values, and its position.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

Spam Classiﬁcation

I Trained by a set of pre-classiﬁed spam and non-spam email

messages (117, 283 respectively).

I Words and phrases from emails form input vectors.
I This NN used because of its large size (82 input nodes and

100 hidden nodes).

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

Single Data

I Probe is provided to select from data domain.
I NN is then shown as data are passing through the network.
I Input and output nodes are colored based on the selected

voxel’s value.

I A connection’s width is based on its importance.

I The focus is on the input-hidden layer, so results can be

mapped to input data domain.

I Importance based on weight.
I The layer’s inﬂuence is propogated by multiplying each weight

between both layers that connect to the same hidden node.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

Set of Data

I A region of data is now selected.
I Now, input node size is also set, also based on importance, or

contribution of the node to the result.

I For large NNs, the nodes can be arranged by order of

importance.

I Relative importance is calculated by a ratio of weights

associated with that node to that of all nodes (in the same
layer).

I The user can (maybe) set a threshold that will hide less

important connections.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

(Cont.)

I The assumption is made that selected data are similar (values

are close to the mean of values), so weights between
input-hidden layer are divided by those means.

I The standard deviation is also calculated, and nodes are

assigned color based on the similarity of mean and standard
deviation.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Datasets
Visualizing Weights – With Volume Classiﬁcation NN
Visualizing Uncertainty

Visualizing Uncertainty

I NN outputs a value representing uncertainty.
I High and low values indicate low uncertainty.
I Middle values indicate hight uncertainty.

I Parallel coordinates are used to show the inputs and outputs

when training or classifying.

I Colors correspond to how well the NN classiﬁed certain sets of

data.

I This can be used to understand what type of test data is

needed.

I Boundaries between materials, due to interpolation will appear

(by color) to have a high level of uncertainty.

I The ﬁx for this is to assign a voxel’s color based on the value
of the neighboring voxel along the opposite normal direction.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

Introduction
Artiﬁcial Neural Networks
ANN Visualization
Results

Results, Uses, Rendering Times, etc.

I None listed!
I The authors do note that based on their rendered

node/connection weights for a particular NN, 4 nodes were
removed from the hidden layer.

I Cost of classiﬁcation was reduced by 15%
I Result 0.5% diﬀerent from original result.

Fan-Yin Tzeng & Kwan-Liu Ma

Opening the Black Box – Data Driven Visualizaion of Neural Networks

",False,0.0,{},False,False,journalArticle,False,ITGJM9CV,[],self.user,False,False,False,False,,,Opening the Black Box --   Data Driven Visualizaion of Neural Networks,ITGJM9CV,False,False
99KNJDE9,KWHZ5Y9E,"Surveying the complementary role of automatic data 

analysis and visualization in knowledge discovery 

Enrico Bertini 

Université de Fribourg 

Bd de Pérolles 90 

Fribourg, Switzerland 

Denis Lalanne 

Université de Fribourg 

Bd de Pérolles 90 

Fribourg, Switzerland 

enrico.bertini@unifr.ch 

denis.lalanne@unifr.ch

ABSTRACT
The aim of this work is to survey and reflect on the various ways 
to  integrate  visualization  and  data  mining  techniques  toward  a 
mixed-initiative  knowledge  discovery  taking  the  best  of  human 
and  machine  capabilities.  Following  a  bottom-up  bibliographic 
research approach, the article categorizes the observed techniques 
in  classes,  highlighting  current  trends,  gaps,  and  potential  future 
directions  for  research.  In  particular  it  looks  at  strengths  and 
weaknesses of information visualization and data mining, and for 
which purposes researchers in infovis use data mining techniques 
and  reversely  how  researchers  in  data  mining  employ  infovis 
techniques. The article further uses this information to analyze the 
discovery  process  by  comparing  the  analysis  steps  from  the 
perspective  of  information  visualization  and  data  mining.  The 
comparison  permits  to  bring  to  light  new  perspectives  on  how 
mining  and  visualization  can  best  employ  human  and  machine 
skills. 

Categories and Subject Descriptors
H.5.2  [User  Interfaces]:  Graphical  user  interfaces  (GUI).  H.1.2 
[User/Machine  Systems]:  Human  information  processing.  H.2.8 
[Database applications]: Data mining. 

General Terms
Survey, Human Factors, Human-Machine Interaction. 

Keywords
Visualization,  Data  Mining,  Visual  Data  Mining,  Knowledge 
Discovery, Visual Analytics. 

1. INTRODUCTION 
While  information  visualization  (infovis)  targets  the  visual 
representation  of  large-scale  data  collections  to  help  people 
understand  and  analyze  information,  data  mining,  on  the  other 
hand,  aims  at  extracting  hidden  patterns  and  models  from  data, 

Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 

VAKD'09, June 28, 2009, Paris, France. 
Copyright 2009 ACM 978-1-60558-670-0...$5.00. 

automatically or semi-automatically. 

In  its  most  extreme  representation,  infovis  can  be  seen  as  a 
human-centered  approach  to  knowledge  discovery,  whereas  data 
mining  is  generally  purely  machine-driven,  using  computational 
tools  to  extract  automatically  models  or  patterns  out  of  data,  to 
devise information and ultimately knowledge. 

Interactive Machine Learning [1][2] is an area of research where 
the  integration  of  human  and  machine  capabilities  is  advocated, 
beyond  scope  of  visual  data  analysis,  as  a  way  to  build  better 
computational  models  out  of  data.  It  suggests  and  promotes  an 
approach where the user can interactively influence the decisions 
taken by learning algorithms and make refinements where needed. 

Visual  analytics  is  an  outgrowth  of  infovis  and  focuses  on 
analytical reasoning facilitated by interactive visual interfaces [3]. 
Often,  it  is  presented  as  being  the  combination  of  infovis 
techniques with data mining capabilities to make it more powerful 
and interactive. According to Keim et al., visual analytics is more 
than  just  visualization  and  can  rather  be  seen  as  an  integrated 
approach  combining  visualization,  human  factors  and  data 
analysis [4]. 

At  the  time  of  writing,  it  is  not  clear  how  this  human-
machine  integration  should  happen.  In  our  view, visual analytics 
should  enable  the  collaboration  between  the  natural  abilities  of 
humans  and  the  powerfulness  of  data  mining  tools,  thus 
combining in a synergetic way natural and artificial intelligences. 

Despite the growing interests on this integration, however, we still 
lack  a  detailed  analysis  of:  1)  how  currently  the  existing 
techniques  integrate  and  to  what  extent;  2)  what  other  kinds  of 
integrations might be achieved. 

to  categorize 

The  purpose  of  this  work  is  start  shedding  some  light  on  this 
issue. To this end we have performed a literature review of papers 
from  premier  conferences  in  data  mining  and  information 
visualization, extracting those in which some form of integration 
exists.  The  analysis  permitted 
the  observed 
techniques in  classes.  For each class  we  provide a description of 
the  main  observed  patterns followed by  a discussion  of  potential 
extensions  we  deem  feasible  and  important  to  realize.  The 
analysis  is  then  followed  by  a  comparison  of  the  analytical 
processes as they happen in data mining and in visualization. This 
comparison,  together  with  the knowledge  gained in  the literature 
review,  permits  to  clarify  some  commonalities  and  differences 
between  the  automatic  and  visual  approaches.  We  believe  this 
kind of reasoning can help framing the problem of automatic and 

12

interactive  analysis  and  better  understand  the  role  of  human  and 
machine.

The  paper  is  organized  as  follows.  Section  2  introduces  some 
terminology  to  clarify  the  meaning  of  some  word  that  often 
appear when talking about automatic or interactive data analysis. 
Section  3  introduces  the  literature  review  and  its  methodology. 
Section  4  illustrates  the  result  of  the  review.  It  describes  the 
observed  patterns  and  the  potential  enhancements  we  suggest. 
Section  5  dissects  commonalities  and  differences  between  the 
analysis  processes  in  data  mining  and  visualization.  Finally, 
Section 6 discusses the limitations of this work, and thus provides 
ideas for its future extension, and Section 7 closes the paper with 
conclusions. 

x

x

2. TERMINOLOGY 
The  common  goal  of  information  visualization  and  data  mining 
domains  is  to  extract  knowledge  from  raw  data.  Before  going 
further  in  our  inspection  of  this  process,  we  thought  useful  to 
agree on the definitions of basic concepts that are commonly used 
in  this  context  such  as  data,  information,  knowledge,  model, 
pattern and hypothesis:  
x

Data  refer  to  a  collection  of  facts  usually  collected  by 
observations,  measures  or  experiments.  Data  consist  of 
numbers,  words,  or  images.  It  is  generally  called  abstract 
data  in  infovis,  since  it  refers  to  data  that  has  no  inherent 
spatial structure enabling further mapping to any geometry.  
A model  in  science  is  a  physical,  mathematical,  or  logical 
representation  of  a  system  of  entities,  phenomena,  or 
processes. Basically a model is a simplified abstract view of 
the  complex  reality.  Models  are  meant  to  augment  and 
support  humans  reasoning,  and  further  can  be  simulated, 
visualized and manipulated.  
A pattern  is  made  of  recurring  events  or  objects  that  repeat 
in  a  predictable  manner.  The  most  basic  patterns  are  based 
on repetition and periodicity.  
A hypothesis  consists  either  of  a  suggested  explanation  for 
an  observable  phenomenon  or  of  a  reasoned  proposal 
predicting  a  possible  causal  correlation  among  multiple 
phenomena. The scientific method requires that one can test 
a scientific hypothesis. A hypothesis is never to be stated as 
a  question,  but  always  as  a  statement  with  an  explanation 
following it.
Information, in its earliest historical meaning, corresponds to 
the act of informing, or to the act of giving form or shape to 
the  mind,  according  to  the  Oxford  English  Dictionary. 
Inform  itself  comes  (via  French)  from  the  Latin  verb 
“informare”, to give form to, to form an idea of.   
Knowledge  is  the  ""justified  true  belief""  according  to  Plato. 
According to the Oxford English Dictionary, knowledge can 
be  defined  as  (i)  expertise,  and  skills  acquired  by  a  person 
through  experience  or  education;  (ii)  what  is  known  in  a 
particular  field  or  in  total;  or  (iii)  awareness  or  familiarity 
gained by experience of a fact or situation. 
In  the  context  of  knowledge  discovery,  we  believe  these 
concepts  can  be  linked  as  follow:  Data  are  the  lowest  level  of 

x

x

x

13

abstraction; researchers often speak about raw data to emphasize 
this fact. From data, models and patterns can be extracted, either 
automatically  using  data  mining  techniques  or  by  humans  using 
their conceptual, perceptual or visual skills respectively. The use 
of human intuition to come up with observations about the data is 
generally  called  insight,  i.e.,  the  act  or  outcome  of  grasping  the 
inward or hidden nature of things or of perceiving in an intuitive 
manner.  Patterns  and  models  are  not  necessarily  linked,  even 
though  some  authors  consider  them  as  synonyms.  One  way  to 
distinguish  these  two  concepts  is  the  following:  patterns  are 
directly attached to data or a sub-set of data; whereas models are 
more conceptual and are extra information that cannot necessarily 
be observed visually in the data.  Further, the observation of some 
patterns  can  result  in  a  model  and  inversely,  the  simulation  of  a 
model can result in a pattern. Hypotheses are derived from models 
and  patterns.  A  validated  hypothesis  becomes  information  that 
can be communicated. Finally, information reaches the solid state 
of  knowledge  when  it  is  crystallized,  i.e.,  it  reaches  the  most 
compact description possible for a set of data relative to some task 
without removing information critical to its execution. 

3. LITERATURE REVIEW 
We started our analysis with a literature review in order to ground 
our  reasoning  on  observed  facts  and  limit  the  degree  of 
subjectivity.  We followed a  mixed approach  in  which bottom-up 
and top-down analyses have been mixed to let the data speak for 
themselves  and  suggest  new  ideas  or  use  the  literature  to 
investigate our assumptions or formulated hypotheses. 

We  included  in  the  literature  papers  from  major  conferences  in 
information visualization,  data mining, knowledge discovery and 
visual  analytics.  In  the  current  state  of  our  analysis  the  papers 
have  been  selected  from  the  ACM  SIGKDD  International 
Conference  on  Knowledge  Discovery  and  Data  Mining  (KDD),
IEEE International Conference on Data Mining (ICDM)  and  the 
IEEE  Symposium  on  Information  Visualization  (InfoVis).  We 
selected  infovis  candidate  papers  searching  in  the  IEEE  Explore 
library  using  keywords 
like:  “data  mining”,  “clustering”, 
“classification”,  etc.  Reversely,  in  data  mining  conferences  we 
looked  for  the  keywords  like:  “visualization”,  “interaction”,  etc. 
Manual  skimming  followed  paper  extraction.  The  final  set  of 
papers retained counts 55 items. Table 1 shows the distribution of 
the  retained  papers  according  to  the  paper  source  and  the 
classification of papers presented below. 

SOURCE 

NUM. 

VIS 

V++  M++ 

VM 

OF 

PAPERS 

KDD 

ICDM 

INFOVIS 

23 

16 

16 

3 

2 

1 

7 

5 

9 

9 

5 

5 

4 

4 

0 

Table 1 - Distrubution of the final list of retained papers 

according to source (conference) and paper type. 

The  whole  list  of  reviewed  papers  with  attached  notes  and 
categories 
address: 
http://diuf.unifr.ch/people/bertinie/ivdm-review.

following 

found 

can 

the 

be 

at 

4. PAPER CATEGORIES 
We  used  various  dimensions  in  order  to  classify  the  chosen 
papers:  the  knowledge  discovery  step  it  supports,  whether  it  is 
interactive  or  not,  the  major  mining  and  visualization  techniques 
used,  etc.  In  particular,  in  regards  to  the  aim  of  this  paper,  we 
classified the paper according to four major categories indicating 
which approach drives the research: 

x

x

x

x

(VIS)  contains 

techniques  based 
Pure  Visualization 
exclusively on visualization without any type of algorithmic 
support; 
Computationally  enhanced  Visualization  (V++)  contains 
techniques which are fundamentally visual but contain some 
form of automatic computation to support the visualization; 
Visually  enhanced  Mining  (M++)  contains  techniques  in 
which automatic data mining algorithms are the primary data 
analysis  means  and  visualization  provides  support 
in 
understanding and validating the result; 
Integrated  Visualization  and  Mining  (VM)  contains 
techniques  in  which  visualization  and  mining  are  integrated 
in  a  way  that  it’s  not  possible  to  distinguish  a  predominant 
role of any of the two in the process. 

Since  the focus  of this paper is on how visualization and mining 
can  cooperate  in  knowledge  discovery,  in  the  following  we  will 
not  take  into  account  the  VIS  category  of  pure  visualization 
techniques.

4.1 Enhanced Visualization (V++) 
This category pertains to techniques in which visualization is the 
primary  data  analysis  means  and  automatic  computation  (that  is 
the  “++”  in  the  name)  provides  additional  features  to  make  the 
tool  more  effective.  In  other  words,  when  the  “++”  part  is 
removed the technique becomes a “pure” visualization technique. 
4.1.1 Observed enhancements with mining  
As illustrated by black boxes on figure 1, the techniques collected 
in  our  literature  review  can  be  organized  around  three  main 
patterns  (Projection,  Data  Reduction,  Pattern  Disclosure)  that 
represent  different  benefits  brought  by  automatic  computation  to 
the  information  visualization  process.  Interestingly,  as  one  can 
notice, the three patterns occur at the beginning of the knowledge 
discovery process: 
x

Projection.  Automatic  analysis  methods  often  take  place  in 
the  inner  workings  of  visualization,  by  creating  a  mapping 

x

x

between  data  items  and  their  graphical  objects’  position  on 
the  screen.  The  most  traditional  type  of  this  method  is 
Multidimensional  Scaling  (MDS),  but  in  the  literature  it  is 
possible  to  find  many  variations  and  alternatives.  They  all 
share  the  idea  that  the  position  assumed  by  a  data  point  on 
the screen is not the result of a direct and fixed mapping rule 
between  some  data  dimensions  and  screen  coordinates  but 
rather  on  a  more  complex  computation  that  takes  into 
account  all  data  dimensions  and  cases.  Ward  refers  to  this 
kind  of  placement  techniques  in  [5]  as  “Derived  Data 
Placement Strategies” in his glyph placement taxonomy. 

Data  Reduction.  Data  reduction  is  another  area  where 
computation  can  support  visualization.  Visualization  has 
very  well  known  scalability  problems  that  limit  the  number 
of  data  cases  or  dimensions  that  can  be  shown  at  once. 
Automatic  methods  can  reduce  data  complexity,  with 
controlled information loss, and at the same time allow for a 
more  efficient  use  of  screen  space.  Pattern  matching 
techniques can replace data overviews with visualizations of 
selected  data  cases 
that  match  a  user-defined  query. 
Sampling  can  reduce  the  number  of  data  cases  with 
controlled information loss. Feature selection can reduce the 
number of data dimensions by retaining subsets that carry the 
large  majority  of  useful  information  contained  in  the  data 
(and thus are most likely to show interesting patterns). 

Pattern  Disclosure.  In  several  visualization  techniques  the 
effectiveness  with  which  useful  patterns  can  be  extracted 
depends  on  how  the  visualization  is  configured.  Automatic 
methods  can  help  configure  the  visualization  in  a  way  that 
useful  patterns  more  easily  emerge  from  the  screen.  Axes-
reordering in parallel coordinates is one instance of such case 
[6]. Similarly, in visualizations where the degrees of freedom 
in  visual  configuration  are 
limited,  pattern  detection 
algorithms  can  help  make  some  visual  patterns  more 
prominent  and thus  readily  visible. For instance, Vizster [7] 
organizes 
in 
automatically  detected  clusters  enclosed  within  colored 
areas. Johansson et al. in [8] describe an enhanced version of 
Parallel  Coordinates  where  clustering  and  a  series  of  user-
controlled  transfer  functions  help  the  user  reveal  complex 
structures  that  would  be  hard,  if  not  impossible,  to  capture 
otherwise.

the  nodes  of  a  social  network  graph 

V

Data

Visualization

Knowledge

V++

+

+

++

Projection Data 

Reduction

Pattern 
Disclosure

+ Visual

+ Verification

Modeling

& Refinement

Figure 1 – Computationally enhanced Visualization (V++) benefit from mining techniques to improve information visualization 
standard process. Black boxes represent enhancements found in the literature survey; grey boxes (with “+”) are extra benefits 
that could bring mining to visualization. 

14

4.1.2 Other potential enhancements 
All the automatic data analysis methods described above share the 
common goal of helping the user more easily extract information 
from  the  visualization.  But,  if  we  take  into  account  the  broader 
picture  of  data  analysis  and  analytical  reasoning,  we  see  that 
automatic techniques could also be employed to go beyond simple 
pattern  detection,  and  intervene  at  later  stages  of  the  knowledge 
discovery process, as illustrated in figure 1 (grey boxes with “+”). 
Here we list some of the function we deem important: 
x

implies  a  grouping  model,  where  data 

Visual  Model  Building.  One 
limitation  of  current 
visualization  systems  is  their  inability  to  go  beyond  simple 
pattern  detection  and  frame  the  problem  around  a  scheme. 
Ideally,  the  user  should  be  able  to  find  connections  among 
the  extracted  patterns  to  build  higher  level  hypotheses  and 
complex models. This is another area where data mining has 
an advantage over visualization in that in the large majority 
of  the  existing  methods  a  specific  conceptual  model  is 
inherent  in  the  technique.  Classification  and  regression
imply  a  functional  model:  any  instantiation  of  the  set  of 
target  value. 
predictive  variables  returns  a  predicted 
Clustering 
is 
aggregated  in  groups  of  items  that  share  similar  properties. 
Rules  imply  an  inductive  model  where  if-then  associations 
are  used.  This  kind  of  mental  scaffold  is  absent  in 
visualization,  nonetheless  there’s  no  inherent  reason  why 
future  systems  might  not  be  provided  with  visual  modeling 
tools  that  permit,  on  the  one  hand  to  keep  the  level  of 
flexibility  of  visualization  tools,  on  the  other  hand  to 
structure  the  visualization  around  a  specific  model  building 
paradigm. Two rare examples of systems that go towards this 
direction  are  PaintingClass  [9]  and  the  Perception  Bases 
Classification (PBC) system [10] in which classification can 
be  carried  out  interactively  by  means  of  purely  visual 
systems. 

x

Verification  and  Refinement.  One  notable  feature  of 
automatic data mining methods over data visualization is its 
ability to communicate not only patterns and models but also 
the  level  of  trust  a  user  can  assign  to  the  extracted 
knowledge.  Similar  functions  are  usually  not  present  in 
standard visualization tools and surprisingly little research as 
been  carried  out  towards  this  direction  so  far.  Automatic 
algorithms could be run on extracted patterns to help the user 
assess their quality once they are detected. To date, the only 
systems  we  are  aware  of  where  a  similar  idea  has  been 
respectively  data 
implemented 

[11][12],  where 

are 

abstraction  quality  is  measured  and  progressive  automatic 
refinement of visual clusters is performed. 

Another  related  area  of  investigation  is  the  use  of  the 
traditional  split  in  training  data  and  test data  used  in 
supervised  learning  as  a  novel  paradigm  to  use  in  data 
visualization.  There  is  no  reason  in  principle  not  to  use  the 
same  technique  in  information  visualization  to  allow  for 
verification  of  extracted  patterns.  Some  few  studies  on 
sampling  for  data  visualization  slightly  touch  this  issue 
[13][14] but none of them focuses on the use of sampling or 
data segmentation for verification purposes. 

Worthy of special remark is also the almost complete absence of 
predictive modeling in visualization, as highlighted by Amar and 
Stasko  in  their  analysis  of  “analytic  gaps”  in  information 
visualization  [15].  While  it  is  fairly  simple  to  isolate  data 
segments and spot correlations, even in multidimensional spaces, 
current  information  visualization  tools  lack  the  right  affordances 
and  interactive  tools  to  structure  a  problem  around  prediction. 
Questions 
the  highest 
predictive power?”, “what combination of data values are needed 
to  obtain  a  target  result?”  are  not  commonly  in  the  scope  of 
traditional visualization tools. 

like:  “which  data  dimensions  have 

4.2 Enhanced Mining (M++) 
This  category  pertains  to  techniques  in  which data mining  is the 
primary data analysis means and visualization (that is the “++” in 
the  name)  provides  an  advanced  interactive  interface  to  present 
the  results.  In  other  words,  when  the  “++”  part  is  removed  it 
becomes a “pure” data mining technique.  
4.2.1 Observed enhancements with visualization 
As illustrated by black boxes on figure 2, the techniques collected 
in  our  literature  review  can  be  organized  around  two  major 
patterns  (Model  presentation  and  pattern  exploration  &  filtering) 
that  represent  different  benefits  brought  by  visualization  to  data 
mining.  Interestingly,  reversely  to  the  previous  category  (V++), 
the  two  patterns  occur  at  the  end  of  the  knowledge  discovery 
process: 
x Model  Presentation.  Visualization  is  used  to  facilitate  the 
interpretation  of 
the  mining 
technique.  According  to  the  method  used,  the  ease  with 
which  the  model  is  interpreted  can  vary.  Some  models 
naturally 
to  visual  abstraction  (e.g., 
dendrogram  in  hierarchical  clustering)  whereas  some  others 
require  more  sophisticated  designs  (e.g.,  neural  networks  or 

the  model  extracted  by 

themselves 

lend 

M

Data

M++

+

Mining

+

Knowledge

++

+ Visualizing
Alternatives

+ Model-Data 

Linking

Model 
Presentation

Patterns Exploration 
and Filtering

Figure 2 – Visually enhanced Mining (M++): benefits of visualization over data mining standard process. Black boxes represent 
potential enhancements found in the literature; grey boxes (with “+”) are extra benefits that could bring visualization to mining.

15

x

vector  machines).  Beyond 

support 
interpretation, 
visualization  also  works  as  a  way  to  visually  convey  the 
level  of  trust  a  user  can  assign  to  the  model  or  parts  of  it. 
Interactions associated to the visualization permits to “play” 
with  the  model  allowing  for  deeper  understanding  of  the 
model and its underlying data. 

in  a  compact 

Patterns Exploration and Filtering. Some mining methods 
generate complex and numerous patterns which are difficult 
to  summarize 
representation;  notably 
association  rules.  In  this  case  visualization  often  adopts 
techniques similar to plain data visualization and the patterns 
are managed like raw data. Visualization here helps gaining 
and  overview  of  the  distribution  of  these  patterns  and  to 
make  sense  of  their  nature.  Interactive  filtering  and  direct 
manipulation tools have a prominent role in that finding the 
interesting  pattern  out  of  numerous  uninteresting  is  the  key 
goal.

4.2.2 Other potential enhancements 
Visualization  applied  to  data  mining  output,  as  shown  above, 
provides great benefits in terms of model interpretation and trust-
building.  We  believe  that  visualization,  however,  can  provide 
additional benefits that have not been fully addressed so far, and 
enable  users  to  intervene  in  early  stages  of  the  knowledge 
discovery process, as illustrated in figure 2 (grey boxes with “+”): 
x

Visualizing  Alternatives.  One  of  the  characteristic  features 
of data mining is the capability of generating different results 
and models by manipulating a limited set of parameters. This 
is  common  to  all  methods  and  can  be  seen  as  both  an 
advantage  and  a  limitation.  It  is  an  advantage  in  that  the 
necessary flexibility is given to create alternatives and adapt 
to  different  analytic  goals.  But,  it  is  also  a  big  limitation  in 
that  setting  the  parameters  of  a  mining  algorithm  is  often 
perceived  by  the  user  as  an  “esoteric”  activity  in  which  the 
relation  between  actions  and  results  is  blurred.  Even  more 
problematic,  when  alternative  models  are  constructed  it  is 
extremely  complicated  to  compare  them  in  the  space  of  a 
single  user  interface.  Visualization  in  our  opinion  has  the 
power  to  bridge  this  gap  by:  1)  providing  means  to  more 
directly represent the connection between the parameters and 
the  results;  2)  allow  for  visualization  structures  that  permit 
the  comparison  of  alternative  results.  This  last  point  is 
particularly interesting in that visualization has the power to 
provide 
to  compare  alternative  visual 
abstractions,  as  demonstrated  for  instance  by  the  success  of 
the    systems  presented  at  the  InfoVis  2003  contest  on  Pair 
Wise Comparison of Trees [16]. One system in our literature 
review  partially  supports  this  kind  of  comparison  by 
generating  different  alternative  results  of  a  subspace 
clustering  algorithm  [17].  The  user  can  see  the  results 
obtained  through  the  variation  of  various  parameters  and 
choose  the  most  interesting  one  among  the  set  of  available 
results. 

the  right 

tools 

x Model-Data  Linking.  The  models  that  mining  algorithms 
create  out  of  data  are  higher  level  data  abstractions  that 
permits to summarize complex relations out of large data. If 
from  the  one  hand  these  abstractions  facilitate  data  analysis 
and  reduce  the  complexity  of  the  original  problem  space, 
from  the  other  hand  the  abstraction  process  often  makes  it 

difficult  to  interpret  the  observed  relations  in  terms  of  the 
original  data  space.  Most  systems  in  our  literature  survey 
provide model representation, but very rarely they permit to 
drill down to the data level to link an observed relation to its 
underlying  data.  In  some  cases  such  a  lack  of  connection 
between  model  and  data  can  create  relevant  limitations  in 
model  understanding  and  trust  building  and  visualization 
seems  to  be  the  right  tool  to  bridge  this  gap.  One  notable 
example  is  data  clustering.  Besides  the  large  provision  of 
visual  and  interactive  techniques  to  represent  clustering 
results  it  is  very  rare  to  find  systems  where  the  linkage 
between  extracted  clusters  and  data  instances  is  made 
explicit by the visualization. And this is somewhat surprising 
in that the goal of data clustering is not only to partition data 
in  a  set  of  homogeneous  groups  but  also,  and  potentially 
more  important,  to  characterize  them  in  a  way  that  their 
content can be described in terms of few data dimensions and 
values.  A  better  connection  between  model  and  raw  data  is 
then  useful  also  to  spot  relevant  outliers,  which  can  often 
triggers  new  analyses  and  lines  of  thought.  Without  such  a 
capability the analyst is forced to base his reasoning only on 
abstractions, thus limiting the opportunities for serendipitous 
discoveries and trust building. 

4.3 Integrated Visualization & Mining (VM) 
This  category  combines  visualization  and  mining  approaches.  
None  of  them  predominate  the  other  and  ideally  they  are 
combined in a synergic way. In the literature we found two kinds 
of  integration  strategies  that  we  describe  below.  Following  their 
description  we  speculate  on  a  mixed-initiative  approach  to  the 
KDD process. 
4.3.1 Integration strategies 
There  are  two  extreme  approached  to  integrate  mining  and 
visualization, as described below: 
x White-Box  Integration.  In  this  kind  of  integration  the 
human and the machine cooperate during the model building 
process in a way that intermediary steps in the algorithm can 
be visualized and decisions can be taken by the user on how 
to direct the model building process. This kind of systems is 
quite rare. There are examples of cooperative construction of 
classification trees, like the one presented in [18], where the 
user steers the construction process and at any stage can ask 
the  computer  to  make  one  step  in  his  or  her  place  like 
splitting  a  node  or  expanding  a  sub-tree.  This  kind  of 
systems  shows  the  highest  degree  of  collaboration  between 
the  user  and  the  machine  and  goes  beyond  the  creation 
accurate models. They help building trust and understanding, 
because the whole process is visible, and also they permit to 
directly  exploit  the  user’s  domain  knowledge  in  the  model 
construction process. 

loop). 

(feedback 

Integration 

Integration
Black-Box 
between mining and visualization can also happen indirectly 
using  the  algorithm  as  a  black  box,  but  giving  the  user  the 
possibility to “play” with parameters setting in a tight visual 
loop  environment  where  changes  in  the  parameters  are 
automatically  reflected  in  the  visualization.  In  this  way  the 
connection  between  parameters  and  model,  even  if  not 
explicit,  could  be  intuitively  understood.  Alternatively,  the 
same  integration  can  be  obtained  in  a  sort  of  “relevance 

x

16

feedback”  fashion,  where  the  system  generates  a  set  of 
alternative  solutions  and  the  user  instructs  the  system  on 
which  are  the  most  interesting  ones  and  gives  hints  on  how 
to generate a new set. 

4.3.2 A mixed-initiative KDD process 
Having  analyzed  a  wide  spectrum  of  integrations  between 
automatic  and  interactive  methods,  we  believe  that  one  of  the 
most  interesting  and  promising direction for future research is to 
achieve a full mixed-initiative KDD process where the human and 
the machine can cooperate on the same level.  

Humans and machines are complementary, and visualization and 
data mining should make use of the specificities of each. Humans 
are intuitive and have good skills at interpretation according to the 
context and their domain knowledge. They are good at getting the 
“big  picture”  and  at  performing  high  level  reasoning  towards 
knowledge.  Machine  on  the  other  side  are  fast  and  reliable  at 
computing data, and they do not make errors.  

to  use  visualization 

In the early 90’s already, Colgan & Spence et al. had already the 
vision 
to  enhance  human-machine 
collaboration  in  electronic  circuit  design  through  the  cockpit  of 
their  Coco  system.  Their  approach  highlighted  the  need  for  an 
effective interface to blend the complementary capabilities of the 
human designer and computer algorithms [22, 23]. More recently, 
Pu  &  Lalanne  proposed  a  mixed-initiative  system  to  support 
problem  solving  via  algorithm  visualization  and  visual  trade-off 
analysis [20, 21]. Through visual interaction, the Comind system 
enables designers to select and control the solving algorithm they 
want  to  use,  i.e.  they  can  visualize  it  while  it  is  processing  the 
data,  stop  it  at  anytime  and  modify  the  problem  definition  or 
select another mining or solving algorithm. Finally they can select 
the  visualization  techniques  they  want  to  view  the  results,  while 
still  being  able  to  tune  parameters.  In  the  context  of  sequential 
pattern  detection  for  text  mining,  [19]  proposes  to  combine 
computational and statistical efforts through data mining with the 
human participation through visualization for the ultimate goal of 
knowledge  discovery.  In  their  application,  visualization  helps 
humans  quickly obtain an  overall structural  view  of  patterns and 
complementary,  data  mining  provides 
support 
information for all patterns. 

accurate 

from our literature review. 

Human  

Machine  

Select strategies 

Project & Reduce data 

Observe, derive knowledge 

Select optimal solution, best 

configuration 

Interpretation, explanation 

Build models 

Measure interestingness 

Extract patterns, models 

Generating hypothesis 

Verification 

Table 2 – Complementary strengths of human and machine in 

the knowledge discovery process. 

by 

humans 

brought 

strengths 

Figure 3 is the result of the benefits brought by visualization and 
mining  independently  to  the  knowledge  discovery  process  as 
described in section 4.1 and 4.2 respectively. It is inline with the 
complementary 
through 
visualization and by machines through data mining. For example, 
while  humans  are  good  at  choosing  modeling  strategies  through 
visualization,  the  machine  is  good  at  computing  large  amount  of 
data for projecting and reducing data. Further, while machines can 
disclose  and  highlight  all  the  patterns  found  automatically  over 
the  data,  human  can  explore  them  and  keep  only  the  most 
interesting ones, according to their knowledge of the data set and 
its  associated  domain.  Later  on,  human  and  machine  can 
collaborate to build models, either coming from mining models or 
alternatively  derived  by  humans  through  their  perceptive  and 
cognitive  systems.  At  this  stage  visualization  techniques  can  be 
particularly  useful  to  bridge  the  gap  between  data  and  the 
extracted models. Finally, data mining techniques can be useful to 
support  the  validation  of  observed  model  or  knowledge  that 
humans can ultimately refine through interaction. 

To date, the only system that comes closer to the idea of a mixed-
initiative KDD process is the one we mentioned above in White-
Box Integration [18], where a decision tree can be constructed by 
alternating  steps  of  human-based  decisions  and  machine-based 
algorithmic steps. 

 Table  2  summarizes  the  major  complementary  strengths  of 
human and machine in the knowledge discovery process, derived 

Data

Visualization // Mining

Knowledge

V

M

M

V//M

V//M

M

V

+V

+M

Visualizing
Model
Alternatives

Visual Patterns 
Exploration 

Projection Data 

Reduction

Pattern 
Disclosure

Model-Data 
Linking & 
Presentation

Visual
Modeling

Refinement

Verification

Figure 3 – Integrated visualization and Mining (VM): towards a white box for the full KDD process with benefits coming from 
visualization (+V) and benefits from mining (+M). 

17

5. ANALYZING THE ANALYSIS PROCESS 
Both  visualization  and  data  mining  are  alternative  methods  to 
transform  data  into  knowledge.  Having  said  that,  a  legitimate 
question  remains:  are  they  just  different  recipe  that  work  in  the 
same  manner  or  do  they  differ  in  any  substantial  manner?  We 
believe  that  posing  this  question  is  becoming  of  increasing 
importance as we attempt to get the most out of the two and create 
successful integrations like the one advocated in Visual Analytics. 

Here  we  provide  reflections  on  this  subject,  based  on  an  initial 
schematization  of  the  analysis  process  in  data  mining  and 
visualization, highlighting notable differences and commonalities 
between them. 

Figure  4  -  Comparison  between  mining  and  visualization 
analytics processes. 

5.1 Processes versus products 
Looking  at  Figure  4  we  can  see  that  both  in  visualization  and 
mining we have products (boxes) and processes (arrows). What is 
interesting to note, at least from the terminological point of view, 
is  that  visualization  and  data  mining  are  not  on  the  same  level. 
More  precisely, the  word  “visualization”  is often intended as the 
product  of  the  visual  mapping  between  data  and  a  visual 
representation; the word “mining”, on the other hand, commonly 
refers  to  the  process  that  transforms  data  into  a  data  mining 
model.  This  distinction  is  important  because  in  Visual  Data 
Mining  and  Visual  Analytics  often  mining  and  visualization  are 
considered  as  alternatives.  Even  more 
to 
acknowledge  the  fact  that  in  data  mining  there  are  necessarily 
always  some  tasks  performed  by  the  human  and,  likewise,  in 
information  visualization  there  are  always  some  tasks  performed 
by  the machine.  The machine, in particular, is responsible of the 
mining  process,  in  data  mining,  and  of  the  visual  mapping 
process, in visualization. Moreover, the mining process produces 
a mining model,  whereas  the  visual  mapping  process  produces  a 
visualization.

important 

is 

If  we  adopt  this  perspective  it  is  easy  to  see  for  instance  how in 
visual  mapping  and  mining  process  similar  human  tasks  are 
involved,  like  the  definition  of  an  appropriate  schema  (visual  or 
functional)  that  fits  the  user’s  mental  model  and  goal.  Similarly, 
we realize that in terms of perceptive and cognitive processes it is 
the  comparison  of  the  activities  that  go  from  visualization  to 
hypothesis generation, in visualization, and from mining model to 

18

hypothesis  generation,  in  mining  that  matters.  We  believe  that  a 
deeper  analysis  and  comparison  of  what  happens  at  this  stage, 
where  the  human  interfaces  with  the  machine,  might  lead  to 
relevant advancements in Visual Analytics. 

5.2 Mental models and problem instantiation 
Again,  comparing  the  two  processes  in  Figure  4,  it  is  interesting 
to  note  a  key  difference  between  them.  In  visualization  the 
formation  of  a  mental  model  and  its  formalization  happen  “in 
sequence” when the mapping has already been performed and the 
data  is  already  visualized.  In  other  terms,  the  visualization  by 
itself is a vehicle to aid the formation of a mental schema. In data 
mining instead the human has to first mentally formulate a mental 
schema  in  a  way  that  it  can  fit  with  one  of  the  existing  input-
output mappings provided by data mining. 

A  clarifying  example  comes  from  the  comparison  of  how 
knowledge building happens on Parallel Coordinates visualization 
or  a  Decision  Tree  algorithm.  In  the  first  case,  the  user  most 
probably approaches the  problem with a limited formalization of 
the  problem  space  and  an  opportunistic  approach.  Usually  he  or 
she just wants to look at the data and see what’s there. Moreover, 
the  kind  of  extracted  patterns  can  cover  a  quite  broad  range  of 
data  models,  e.g.,  correlations  among  two  or  more  dimensions, 
groupings (clusters), outliers, etc. In the case of decision trees, the 
user  has  to  first  formulate  the  problem  in  terms  of  a  definite 
mental  schema  that  matches  the  particular  input-output  mapping 
enforces  by 
technique.  Specifically,  the  data  will  be 
transformed  in  a  series  of  IF-THEN  rules  that  segment  the  input 
space  in  groups  characterized  by  their  relations.  For  any 
additional  data  record,  once  the  model  is  built,  the  model  will 
provide  a  specific  output  (label).  It  is  worth  to  note  in  this 
example that some of the conclusions to which the user might end 
up in one case might easily overlap with those extracted from the 
other.  The  question  of  how  these  processes  compare,  when  and 
how  it  is  more  preferable  to  use  one  or  another,  or  if  a  synergy 
between the two can be found is in our opinion one of the central 
issues to study in Visual Analytics. 

the 

5.3 The Human-Machine Interface 
Another  important  aspect  illustrated  in  figure  4  is  that  in  both 
processes  there  is  a  stage  in  which  necessarily  the  human  has  to 
acquire  some  information  from  the  machine,  that  is,  what  we 
called the human boundary.

In  traditional  data  mining,  systems  are  not  without  an  interface, 
they  just  provide  simple  and  minimalistic  interfaces  like  results 
organized in tabular data. Visualization systems on the other hand 
provide  visually  rich  and  highly  interactive  tools  for  data 
exploration. 

More  importantly,  in  data  visualization  the  interface  has  the 
primary  goal  to  let  the  user detect  and  correctly  extract  relevant 
patterns  from  the  screen.  In  data  mining  the  interface  has  the 
primary goal to let the user understand the model produced by the 
machine  and  its  relation  to  data.  From  the  visualization  design 
point  of  view  it  is  important  to  recognize  this  difference  and 
acknowledge that not necessarily what we have learned from data 
visualization is enough to build effective model visualizations. 

Model visualization seems to be a more complex task, where we 
are confronted with novel design challenges like: finding effective 
metaphors  to  represent  the  model,  finding  ways  to  represent  the 

model  in  relation  to  the  data  and  vice  versa,  and  finding 
convenient  interaction  methods to manipulate  the model. Further 
research is still needed to advance towards this direction. 
5.3.1 The feedback loop 
So  far  we  have  only  discussed  one  direction  of  the  human-
machine  interface,  that  is,  from  the  machine  to  the  human.  The 
opposite  direction  is  often  neglected  but  it  is  equally  important 
because  it  permits  to  close  the  feedback  loop.  It  is  in  fact  the 
possibility  to  iterate  over  alternate  phases  of  human  perception 
and  understanding  of  the  current  state  and  human  actions  to 
change  this  state  and  devise  alternatives  that  fuel  the  discovery 
and learning process.  

On  a  higher  level  this  is  also  supported  by  the  Sensemaking 
Theory that describes how people make sense of information. As 
Pirolli and Card note in [19], the process revolves around “one set 
of  activities  that  cycle  around  finding  information  and  another 
that cycles around making sense of the information”. 

Data

Data + Model

Mining

Changing the 

scheme

Mapping

Manipulating

the scheme

Model

Visualization

Figure  5  –  The  feedback  loop  in  Knowledge  Discovery.  The 
grey  boxes  represents  the  two  major  stages  at which humans 
can intervene.

5.3.2 User intervention levels 
In our literature review, almost half of the papers do not propose 
means for users to interact with the system and as such intervene 
on  the  knowledge  discovery  process.  In  the  55  papers  reviewed, 
the  major  interaction  techniques  found  can  be  grouped  in  two 
major  categories  depending  on  the  knowledge  discovery  step  at 
which users can intervene, i.e. pre or post model interventions, to 
change the scheme or manipulate it respectively as illustrated on 
figure 5: 
x

Changing  the  scheme.  Both  in  visualization  and  in  data 
mining  at  any  stage  the  user  can  decide  to  change  the 
schema.  In  visualization  changing 
the  schema  means 
changing the visual mapping in a way that data can be seen 
under a new perspective. In data mining it means reframing 
the  problem  so  that  it  is  represented  under  a  new  model,  as 
an example, moving the analysis from the generation of rules 
to  finding  data  clusters.  This  kind  of  activities  is  often 
neglected  and  yet  it  is  very  important  because  as  the  user’s 
mental  model  changes  the  tools  must  adapt  in  a  way  to 
reflect  this  change.  The  goodness  of  a  data  analysis  system 
should  be  measured  also  in  terms  of  this  flexibility.  This 
need of reframing problems under different schemes uncover 
a  relevant  gap  in  current  tools;  especially  those  found  in 
information  visualization.  One  of  the  biggest  challenges  is 
yet  to  find  an appropriate visualization  for the  task  at hand. 
Despite numerous efforts towards this direction, especially at 
the  early  stage  of  information  visualization  (e.g.,  in  Jock 

MacKinlay’s  work  [20]),  current  tools  offer  very  limited 
support.  Automatic  or  semi-automatic  methods  should  be 
employed  to  help  users  find  appropriate  visual  mappings  or 
yet suggest possible alternatives. 

x Manipulating  and  tuning  the  scheme.  Another  option  the 
user has to create alternatives is to change parameters within 
the context of a given scheme. In visualization this comprises 
interactions like: dynamic filtering, axes reordering, zoom & 
pan, etc. In data mining it involves some form of parameter 
tuning, as when using different distance functions or number 
of  desired  groups  in  data  clustering.  This  last  function  is  of 
special interest in that visualization can be a powerful means 
to  help  users  tune  up  their  mining  models.  As  we  have 
already  discussed 
in  “Visualizing 
Alternatives”, 
the  use  of  powerful  visualization  and 
interaction  schemes  could  greatly  improve  the  state  of 
current  tools.  Of  special  interest  is  the  study  of  efficient 
techniques  that  permit  to  understand  how  a  model  changes 
when  one  or  more  parameters  change.  In  current  tools  it  is 
almost  impossible  to  achieve  this  level  of  interaction.  Not 
only the large majority of parameters are difficult to interpret 
but  also  the  user  is  forced  to  go  through  a  series  of  “blind” 
trial-and-error 
some 
parameters,  waits  for  the  construction  of  the  new  model, 
evaluates  the  result  and  iterates  over  until  he  or  she  is 
satisfied.  

the  user  changes 

in  Section  4.2.2 

steps  where 

6. LIMITATIONS AND FUTURE WORK 
Despite  our  effort  to  produce  a  meaningful  literature  survey  and 
to extract useful indication out of it, we believe it is important to 
highlight and acknowledge some limitations of this work. 

The literature we have analyzed, though useful, is far from being 
complete.  We  decided  to  use  a  number  of  papers  that  could  be 
analyzed  in  a  relative  short  time  (by  the  two  authors)  and  at  the 
same time capture most of the relevant trends. 

As a consequence we decided not to draw any statistics out of our 
study.  The  literature  contains  some  hand-made  categorizations 
that could have been used to further categorize the techniques and 
depict  some  general  trends  out  of  it.  We  postpone  this  task  to  a 
later version of our work, where the number and kind of collected 
papers  will  provide  us  with  a  more  solid  base  on  which  to  draw 
relevant statistics. 

Finally, it’s important to take into account that a large part of this 
paper is the product of subjective indications stemming from what 
we believed worth to extract from the literature. Nonetheless, we 
believe  that  our  analysis  and  guidelines  can  highlight  hidden 
patterns and stimulate further research on important issues in this 
cross-disciplinary topic. 

We  plan  to  advance  this  work  after  having  received  sufficient 
feedback  from  the  community.  Specifically,  we  want  to  extend 
the  literature,  further  categorize  the  techniques,  and  draw  some 
general  statistics  on  research  trends  that  could  help  suggesting 
additional future research directions. 

7. CONCLUSIONS 
We have presented a literature review on the role of visualization 
and  data  mining  in  the  knowledge  discovery  process.  From  the 
review  we  have  generated  a  series  of  classes  through  which  we 

19

[8]  J. Johansson, P. Ljung, M. Jern, and M. Cooper, “Revealing 
Structure within Clustered Parallel Coordinates Displays,” 
Proceedings of the Proceedings of the 2005 IEEE 
Symposium on Information Visualization, IEEE Computer 
Society, 2005, p. 17. 

[9]  S.T. Teoh and K. Ma, “PaintingClass: interactive 

construction, visualization and exploration of decision 
trees,” Proceedings of the ninth ACM SIGKDD 
international conference on Knowledge discovery and data 
mining,  Washington, D.C.: ACM, 2003, pp. 667-672. 

[10]  M. Ankerst, C. Elsen, M. Ester, and H. Kriegel, “Visual 
classification: an interactive approach to decision tree 
construction,” Proceedings of the fifth ACM SIGKDD 
international conference on Knowledge discovery and data 
mining, ACM, 1999, pp. 392-396. 

[11]  Q. Cui and J. Yang, “Measuring Data Abstraction Quality in 

Multiresolution Visualizations,” IEEE Transactions on 
Visualization and Computer Graphics,  vol. 12, 2006, pp. 
709-716.

[12]  D. Yang, Z. Xie, E.A. Rundensteiner, and M.O. Ward, 

“Managing discoveries in the visual analytics process,” 
SIGKDD Explor. Newsl.,  vol. 9, 2007, pp. 22-29. 

[13]  G. Ellis and A. Dix, “Density control through random 
sampling: an architectural perspective,” Information
Visualisation, IV 2002., 2002, pp. 82–90. 

[14]  E. Bertini and G. Santucci, “Give chance a chance: modeling 
density to enhance scatter plot quality through random data 
sampling,” Information Visualization,  vol. 5, 2006, pp. 
95–110. 

[15]  R.A. Amar, “Knowledge Precepts for Design and Evaluation 

of Information Visualizations,” IEEE Transactions on 
Visualization and Computer Graphics,  vol. 11, 2005, pp. 
432-442.

[16]  C. Plaisant, J. Fekete, and G. Grinstein, “Promoting Insight-

Based Evaluation of Visualizations: From Contest to 
Benchmark Repository,” Visualization and Computer 
Graphics, IEEE Transactions on,  vol. 14, 2008, pp. 120-
134.

[17]  E. Müller, I. Assent, R. Krieger, T. Jansen, and T. Seidl, 

“Morpheus: interactive exploration of subspace 
clustering,” Proceeding of the 14th ACM SIGKDD 
international conference on Knowledge discovery and data 
mining, ACM, 2008, pp. 1089-1092. 

[18]  M. Ankerst, M. Ester, and H. Kriegel, “Towards an effective 

cooperation of the user and the computer for 
classification,” Proc. ACM SIGKDD International 
Conference on Knowledge Discovery and Data Mining,
ACM, 2000, pp. 179-188. 

[19]  P. Pirolli and S. Card, “The sensemaking process and 
leverage points for analyst technology as identified 
through cognitive task analysis,” Proceedings of 
International Conference on Intelligence Analysis, 2005. 

[20]  J. Mackinlay, “Automating the design of graphical 

presentations of relational information,” ACM 
Transactions on Graphics,  vol. 5, 1986.

have  categorized  the  collected  papers:  the  knowledge  discovery 
step it supports, whether it is interactive or not, the major mining 
and visualization techniques used, etc. In particular, in regards to 
the  aim  of  this  paper,  we  classified  the  paper  according  to  three 
major categories indicating which approach drives the knowledge 
discovery:  computationally  enhanced  visualization  systems, 
visually enhanced data mining systems, and integrated visual and 
mining systems.  

This  categorization  highlights  some  observed  patterns  and 
suggests  potential  extensions  which  are  not  present  in  the 
considered  literature.  For  instance,  in  order  to  enhance  the 
standard visualization process, we believe data mining techniques 
could support visual model building to go beyond simple pattern 
detection. Further, mining techniques could be also used to verify 
and  assess  the  quality  of  patterns  detected  by  users.  Reversely, 
visualization  could  enhance  the  data  mining  process  to  visualize 
modeling alternatives, and to understand modeling results through 
a better model-data linking and presentation.  

In  addition  to  these  suggestions,  the  article  provides  a  series  of 
higher  level  reflections  on  the  analysis  process  as  it  happens  in 
visualization  and  data  mining.  These  reflections  suggest  new 
perspective  on  the  role  of  visualization  and  mining  in  the  data 
analysis  process  and  potential  areas  of  investigation  towards  a 
better 
this 
integration  of  both 
preliminary  study  suggests 
the  human  machine 
interaction through a better consideration of the feedback loop so 
that  users  can  intervene  at  different  levels  of  the  knowledge 
discovery  process, 
the  scheme 
respectively.   

to  change  and  manipulate 

techniques. 
improving 

In  particular, 

8. REFERENCES
[1]  J.A. Fails and J. Olsen, “Interactive machine learning,” IUI 

'03: Proceedings of the 8th international conference on 
Intelligent user interfaces,  New York, NY, USA: ACM, 
2003, pp. 39–45. 

[2]  M. Ware, E. Frank, G. Holmes, M. Hall, and I.H. Witten, 

“Interactive machine learning: letting users build 
classifiers,” International Journal of Human Computer 
Studies,  vol. 55, 2001, pp. 281–292. 

[3]  J.J. Thomas and K.A. Cook, Illuminating the path: The 
research and development agenda for visual analytics,
IEEE, 2005. 

[4]  D.A. Keim, F. Mansmann, J. Schneidewind, J. Thomas, and 

H. Ziegler, “Visual analytics: Scope and challenges,” 
Visual Data Mining: Theory, Techniques and Tools for 
Visual Analytics, Springer, 2008, pp. 76–90. 

[5]  M.O. Ward, “A taxonomy of glyph placement strategies for 

multidimensional data visualization,” Information 
Visualization,  vol. 1, 2002, pp. 194–210. 

[6]  W. Peng, M.O. Ward, and E.A. Rundensteiner, “Clutter 
reduction in multi-dimensional data visualization using 
dimension reordering,” IEEE Symposium on Information 
Visualization, 2004. INFOVIS 2004, pp. 89–96. 

[7]  J. Heer and D. Boyd, “Vizster: Visualizing online social 

networks,” Proceedings of the 2005 IEEE Symposium on 
Information Visualization, 2005, pp. 33–40. 

20

",False,2009.0,{},False,False,conferencePaper,False,99KNJDE9,[],self.user,False,False,False,False,http://portal.acm.org/citation.cfm?doid=1562849.1562851,,Surveying the complementary role of automatic data analysis and visualization in knowledge discovery,99KNJDE9,False,False
9PFBNDT7,LB3DSJBQ,"Kwan-Liu Ma
University of
California 
at Davis

Visualization Viewpoints

Editor: 
Theresa-Marie Rhyne 

Machine Learning to Boost the Next Generation of
Visualization Technology ____________________________

V isualization has become an indispensable tool in

many areas of science and engineering. In partic-
ular, the advances made in the ﬁeld of visualization over
the past 20 years have turned visualization from a pre-
sentation tool to a discovery tool. Besides our improved
understanding of perceptual aspects of visualization
design, our ability to create and integrate novel inter-
action metaphors and hardware-accelerated rendering
algorithms that enable interactive visualization has
been key to this success. If you asked different visual-
ization researchers about their opinions on the next key
advance to boost the capability of the next generation
of visualization technology, you’d likely get a variety of
answers. The responses might include coupling quan-
titative analysis, integrating visualization into the over-
all workﬂow of scientiﬁc study, or extensive support for
collaborative visualization. I would like to add another:
incorporation of machine learning into the visualiza-
tion process. As the large data problem drives many of
the remaining challenges in visualization, we often ﬁnd
ourselves buried in data mining tasks. Machine learn-
ing has received great success in both data mining1 and
computer graphics;2 surprisingly, the study of system-
atic ways to employ machine learning in making visu-
alization is meager. 

Machine learning is a well-established ﬁeld of study.
Like human learning, we can make a computer program
learn from previous input data to optimize its perfor-
mance on processing new data. In the context of visu-
alization, the use of machine learning can potentially
free  us  from  manually  sifting  through  all  the  data.
However, if we would treat challenges that we face sim-
ply as conventional data mining jobs, then we might as
well hand our problems to the data mining research
community. Visual-based data mining uses visualiza-
tion to guide data mining and has demonstrated suc-
cesses in several application areas. Here, I discuss a new
approach to making future visualization systems that
go beyond visual-based data mining. 

In addition to the large data challenges, sophisticat-
ed visualization tasks and algorithms require a mastery
of the algorithm’s details, the data’s properties, and the
hardware’s capabilities. These hurdles often discourage
those most knowledgeable of the underlying problem
from driving the visual exploration process. As a result,
the visualization’s potential is limited, possibly reduc-
ing the extent of scientiﬁc discovery. If we can abstract

away sophisticated algorithms and hardware behind a
simple and intuitive user interface, then the user is left
with only high-level decision making for guiding the
data visualization and discovery process. It’s viable to
integrate intelligence (the ability to learn) into visual-
ization systems to achieve this goal—that is, to eventu-
ally remove the need of a human user to handle tedious
or repetitive tasks by learning from previous sessions
and  input  data.  The  problem  would  then  become
designing an appropriate user interface for each visual-
ization task. Since visualization is an effective means for
both  inputting  domain  knowledge  and  interpreting
complex information, a promising design would realize
intelligence augmentation in a way that visualization
becomes the user interface of the visualization tool.3
All tasks are then performed by operating directly on
visualizations. To explain what I mean, I will describe
intelligent  visualization  designs  for  three  different 
applications.

Volume classiﬁcation and visualization

In volume rendering, a critical task is the classiﬁca-
tion of different materials in a volume according to the
visualization’s purpose. A transfer function maps voxel
values to color and opacity, and the user usually deﬁnes
it through an interactive graphical editor. Guidelines
exist for making color transfer functions. Designing
opacity transfer functions for volume rendering is less
intuitive for the users since they must work in some
derived transfer function space. Furthermore, the con-
ventional 1D transfer function is of limited effectiveness
in performing the actual classiﬁcation. For example, in
an MRI head data set, specifying a 1D transfer function
that can correctly differentiate the brain and the region
near the skull might be difﬁcult since the intensity val-
ues of the two regions are so similar. As such, the result-
ing visualization would show both materials together; in
this case, the outer layer might obscure the brain mate-
rial of interest. The user could reduce the outer layer’s
opacity to make the brain more visible, but the brain
would simultaneously become more transparent and
difﬁcult to see. Trying to ﬁnd a transfer function that is
a compromise between the two is a nontrivial task with-
out the appropriate visual interface support. To obtain
better classiﬁcation results, we need higher dimension-
al transfer functions that take into account more data
properties—such as gradient, neighboring texture, and

6

September/October 2007

Published by the IEEE Computer Society

0272-1716/07/$25.00 © 2007 IEEE

position. A great deal of research exists devoted to the
generation of transfer functions for volume visualiza-
tion.4 Two-dimensional transfer functions have proved
effective, but the complexity of the conventional user
interface rises with the dimensionality of transfer func-
tions. Higher dimensional transfer functions are too con-
fusing for the user to edit; when a transfer function has
ﬁve, ten, or even more dimensions, it’s nearly impossi-
ble for the user to directly deﬁne the function. 

One intelligent interface that proved effective for
specifying high-dimensional classiﬁcation functions is
a painting interface.5 Users interactively paint directly
on the volume-rendered images or selected cross sec-
tions of the volume. The users are given full control of
what materials to classify by applying one paint color to
parts of the volume representing materials of interest,
and another paint color to regions they don’t desire.
Abstracted from the user is the generation of a high-
dimensional classiﬁcation function using a supervised
machine learning technique, such as artificial neural
network or support vector machines. Figure 1 shows
such an intelligent visualization system. The system uses
the painted regions (a small subset of the volume) as
training data to learn how to classify the whole volume.
The classiﬁcation step maps each voxel into a value indi-
cating the likelihood that the voxel is part of the mater-
ial of interest. The system can then map this uncertainty
to opacity for rendering. 

In practice, users might want to classify more than a
single object in a volume. For example, they might want
to show more than one material at a time or a certain
organ with a high opacity value and other regions with
low  opacity  to  provide  context.  Figure  2  shows  the
results of the classiﬁcation of multiple materials. These
images would be difﬁcult, if not impossible, to generate
with a 1D or 2D transfer function. For example, the cere-
brum and the cerebellum of the brain have similar den-
sity values. Users can separate these two regions using
texture and position information. To classify more than
one material at a time requires multiple networks; how-
ever,  only  one  neural  network  is  trained  at  a  time.
Rendering occurs using multiple passes, one for each of
the material classes. A good strategy would be to employ
the more expensive hardware neural network renderer
only when displaying the most recent material class,
with the previous materials rendered from precomput-
ed classiﬁed volumes. The ability to learn is powerful
since the system can apply what it has learned to per-

Interface

Neural
network

Classifier

Volume

data

Renderer

Classified
volume

1 Intelligent, high-dimensional volume classiﬁcation.5
A painting-based interface lets the user specify regions
of interest by brushing. The user trains the neural
network to classify wanted and unwanted materials
using the voxels (marked by the red and blue strokes).

forming similar tasks, possibly in a fully automated fash-
ion. An example of the strategy is to reuse a network
trained to classify the brain from a scan of a patient for
new scans of the same patient.5

4D ﬂow feature extraction and tracking
In the study of time-varying ﬂow data, feature extrac-
tion and tracking is mostly done by explicitly separat-
ing the feature of interest from the raw data and creating
(and following) either a volumetric or geometric repre-
sentation of the extracted feature. A data set typically
consists of hundreds to thousands of time steps. A single
transfer function could not accommodate the general-
ly varying dynamic range of data values over time.
Furthermore, some of the features can only be deﬁned
by the relationship between multiple variables, or by its
size, shape, location, and neighborhood, suggesting that
the feature extraction must occur in a high-dimension-
al space, similar to the aforementioned volume classiﬁ-
cation problem. 

Conventional feature extraction methods are mainly
based on an analytical description of the feature of inter-
est. In the case that the properties cannot be easily
deﬁned and are sometimes unknown, feature extrac-
tion and tracking become a manual-driven and trial-
and-error  process.  The  same  intelligent  system
introduced for classifying materials can be applied to
4D feature extraction and tracking in the context of ﬂow
visualization. In other words, it’s possible for a visual-
ization system to learn to extract and track features in

(a)

(b)

(c)

(d)

2 These images demonstrate classiﬁcation of multiple materials: (a) the brain rendered opaque and the skull and
skin semitransparent. (b) This image uncovers the brain while keeping the bottom half of the head. (c-d) These
images highlight the cerebrum and cerebellum. 

IEEE Computer Graphics and Applications

7

Visualization Viewpoints

3 Learning to extract and track time-varying ﬂow features.6 In modeling turbulent mixing on reacting layers,
scientists want to verify that for high mixing rates the ﬂame can become locally extinguished, as highlighted in
red. Using the intelligent interface, the scientist extracted the desired feature for a few selected time steps and
then applied the learned network to the hundreds of other time steps. 

complex 4D ﬂow ﬁelds according to their visual prop-
erties, location, shape, and size.6 Again, such an intelli-
gent system approach is powerful because it lets us
extract and track a feature of interest in a high-dimen-
sional space without explicitly specifying the relations
between those dimensions, resulting in a greatly sim-
pliﬁed and intuitive visualization interface. 

According to the properties of the feature of interest,
the extraction and tracking can occur in either the trans-
fer function space or the data space. In the transfer func-
tion space, the user provides good transfer functions for
a small number of selected time steps, and the system
generates transfer functions for other time steps, which
can be in the number of hundreds or even thousands.
In the data space, the user speciﬁes values of interest for
selected variables, so the system can construct a feature
vector for training. Figure 3 shows an example of data-
space extraction and tracking using four variables. 

Network scan characterization

Another  example  occurs  from  network  security.
Scanning a network is the ﬁrst step in a network attack
attempt. To attempt to make a scan anonymous, an

attacker can use a variety of techniques, such as coming
from different source addresses or scanning in a ran-
dom order. Certain variations in arrival time of the scan-
ning connections can help identify such an attacker.
However, because these variations are rather chaotic,
statistical methods alone are not enough. Analysts need
enhanced capabilities for understanding subtle timing
characteristics in high-volume Internet activities—for
example, hostile probes—as these activities exhibit a
fascinating degree of structural detail. They need to dif-
ferentiate productive activities such as Web crawlers
from malicious activities such as worms. 

One way to characterize network scans is to analyze
destination IP addresses and packet arrival timing infor-
mation retrieved from the network scans. A visual rep-
resentation of different metrics of the arrival time at each
destination IP address can show structures that help ana-
lysts characterize large numbers of network scans. We
created a visualization system to quickly and readily clas-
sify and compare a large number of scans.7 Figure 4
shows one view of the system that uses clustering. A
problem with the raw network scans is the prevalence of
noisy and distorted data, which often makes direct com-
parison of the data patterns difﬁcult and inaccurate.
Thus, we need a way to remove the noise and restore the
original scan activity pattern. Machine learning is excel-
lent for such a task. One method of choice is the asso-
ciative-memory  neural  network,  which  mimics  how
human memory works and is particularly effective at pat-
tern recognition tasks even when the patterns are dis-
torted or incomplete. Figure 5 shows a proposed network
scan analysis system that uses both associative memory
learning and interactive visualization to characterize
scans. A set of controlled data (that is, known scan pat-
terns) helps train the system. The trained system can
then correctly ﬁx and classify most of the incomplete or
distorted scans in the newly collected data. The system
provides the analysts with both overviews (for example,
a graph as scan clusters) and detailed views of the scans
to verify and reﬁne the classiﬁcation. The classiﬁcation
not only allows the analysts to examine scans as groups
rather than a large amount of individual scans, but also
single out scans that require attention.

4 Clustering of scans and some of the representative scan patterns.7 Color
represents various metrics based on arrival time at each destination IP
address. In this picture, blue and red indicate arriving early and late,
respectively, in the scan.

Conclusion

I anticipate seeing a growing interest in the develop-
ment of intelligent interfaces for data visualization. Such
systems will replace the current clutter of hardware- and

8

September/October 2007

algorithm-speciﬁc controls with a simple and intuitive
interface supported by an invisible layer of complex
intelligent algorithms. The user need only make high-
level, goal-oriented decisions, making cutting-edge data
visualization technology directly accessible to a wide
range of application scientists. This exciting direction
will draw new attention to several other research areas.
One subject of study will be the visualization of machine
learning operations.8 When an artiﬁcial-intelligence sys-
tem can learn a visualization task, presenting the user
with the reasoning behind the learning might help with
the user interface’s transparency. An interface that pro-
vides this learning process could also help further reﬁne
and optimize the visualization process. A second sub-
ject of research will be the development of models of
intelligent interfaces for data visualization. In this arti-
cle, I’ve shown several supervised intelligent systems.
Unsupervised learning is also promising and can lead
to new interface designs.9 Another subject of relevant
research is how to visually assess and communicate the
uncertainty of an intelligent system’s outputs. No real
data is perfect, and the learning-based classiﬁcation and
visualization process can introduce an additional layer
of ambiguity.8,10 Providing feedback about the quality
of classification and visualization results will be an
important part of designing a reliable, intelligent visu-
alization system. Finally, the effort to couple machine
learning with interactive visualization and to evaluate
the  effectiveness  of  the  resulting  interface  designs
should occur using a variety of applications. These stud-
ies will pave the way to the creation of next-generation
visualization technology. This type of technology will
be built upon further exploitation of human perception
to simplify visualization; advanced hardware features
to accelerate visualization calculations; and machine
learning to reduce the complexity, size, and high-dimen-
sionality of data.
■

Acknowledgments

The reported intelligent visualization research is sup-
ported in part by the National Science Foundation (grant
IIS-0552334), and by the Department of Energy’s SciDAC
program. I am grateful for the reviewers’ constructive
suggestions, which helped improve the ﬁnal article. 

References
1. P.-N. Tan, M. Steinbach, and V. Kumar, Introduction to Data

Mining, Addison-Wesley, 2005. 

2. A. Hertzmann, “Machine Learning for Computer Graph-
ics: A Manifesto and Tutorial,” Proc. Paciﬁc Graphics Conf.,
IEEE CS Press, 2003, pp. 22-36. 

Network

data

Scan

detection

Controlled

data

Visual characterization

Scans

Associative
memory
learning

Model

Associative
memory

reconstruction

5 Intelligent visual characterization of network scans. The intelligent sys-
tem can not only reconstruct those incomplete or distorted scans but also
classify them to facilitate subsequent analysis tasks. 

3. K.-L. Ma, “Visualizing Visualization: User Interfaces for
Managing and Exploring Scientific Visualization Data,”
IEEE Computer Graphics and Applications, vol. 20, no. 5,
2000, pp. 16-19.

4. H. Pfister et al., “The Transfer Function Bake-Off,” IEEE
Computer Graphics and Applications, vol. 21, no. 3, 2001,
pp. 16-23. 

5. F.-Y. Tzeng, E.B. Lum, and K.-L. Ma, “An Intelligent System
Approach to Higher-Dimensional Classiﬁcation of Volume
Data,” IEEE Trans. Visualization and Computer Graphics,
vol. 11, no. 3, 2005, pp. 273-284.

6. F.-Y. Tzeng and K.-L. Ma, “Intelligent Feature Extraction
and Tracking for Large-Scale 4D Flow Simulations,” Proc.
Int’l Conf. High Performance Computing, Networking, Stor-
age and Analysis, IEEE CS Press, 2005. 

7. C. Muelder, K.-L. Ma, and T. Bartoletti, “A Visualization
Methodology for Characterization of Network Scans,” Proc.
Workshop Visualization for Computer Security (VizSEC),
IEEE CS Press, 2005, pp. 29-38. 

8. F.-Y. Tzeng and K.-L. Ma, “Opening the Black Box—Data
Driven Visualization of Neural Networks,” Proc. IEEE Visu-
alization Conf., IEEE CS Press, 2005, pp. 383-390.

9. F.-Y. Tzeng and K.-L. Ma, “A Cluster-Space Visual Interface
for Arbitrary Dimensional Classiﬁcation of Volume Data,”
Proc. Joint Eurographics, IEEE TCVG Symp. Visualization,
Eurographics Assoc., 2004, pp. 17-24. 

10. J. Kniss et al., “Statistically Quantitative Volume Visual-
ization,” Proc. Visualization Conf., IEEE CS Press, 2005, pp.
287-294.

Contact author Kwan-Liu Ma at ma@cs.ucdavis.edu.
Contact  editor  Theresa-Marie  Rhyne  at  tmrhyne@

ncsu.edu.

IEEE Computer Graphics and Applications

9

",False,2007.0,{},False,False,journalArticle,False,9PFBNDT7,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/4302576/,,Machine Learning to Boost the Next Generation of Visualization Technology,9PFBNDT7,False,False
HLIDNE69,ZLGHZ7BY,"Visual to Parametric Interaction (V2PI)
Scotland C. Leman1.
1 Department of Statistics, Virginia Tech, Blacksburg, Virginia, United States of America, 2 Department of Computer Science, Virginia Tech, Blacksburg, Virginia, United
States of America

, Dipayan Maiti1, Alex Endert2, Chris North2

.
, Leanna House1*

Abstract

Typical data visualizations result from linear pipelines that start by characterizing data using a model or algorithm to reduce
the dimension and summarize structure, and end by displaying the data in a reduced dimensional form. Sensemaking may
take place at the end of the pipeline when users have an opportunity to observe, digest, and internalize any information
displayed. However, some visualizations mask meaningful data structures when model or algorithm constraints (e.g.,
parameter specifications) contradict information in the data. Yet, due to the linearity of the pipeline, users do not have a
natural means to adjust the displays. In this paper, we present a framework for creating dynamic data displays that rely on
both mechanistic data summaries and expert judgement. The key is that we develop both the theory and methods of a new
human-data interaction to which we refer as ‘‘ Visual to Parametric Interaction’’ (V2PI). With V2PI, the pipeline becomes bi-
directional in that users are embedded in the pipeline; users learn from visualizations and the visualizations adjust to expert
judgement. We demonstrate the utility of V2PI and a bi-directional pipeline with two examples.

Citation: Leman SC, House L, Maiti D, Endert A, North C (2013) Visual to Parametric Interaction (V2PI). PLoS ONE 8(3): e50474. doi:10.1371/journal.pone.0050474

Editor: Fabio Rapallo, University of East Piedmont, Italy

Received April 4, 2011; Accepted October 24, 2012; Published March 20, 2013

This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for
any lawful purpose. The work is made available under the Creative Commons CC0 public domain dedication.

Funding: This work was supported by National Science Foundation, Computer and Communications Foundations, #0937071. The funders had no role in study
design, data collection and analysis, decision to publish, or preparation of the manuscript.

Competing Interests: The authors have declared that no competing interests exist.

* E-mail: lhouse@vt.edu
. These authors contributed equally to this work.

Introduction

Organizing and understanding large datasets are complex tasks
for many scientists, engineers, and intelligence analysts. To aid
them in such sensemaking endeavors, tools have been developed to
visualize high-dimensional data. These tools rely on mathematical
models or algorithms that collapse high-dimensional data matrices
to much smaller visual spaces (i.e., spaces of only two or three
dimensions). For example, common visualizations of high-dimen-
sional text data extend upon a geography metaphor and use
algorithms to display such data in two-dimensional maps [1]. One
problem is that visualizations can mislead users, just as any data
summary might, by over-simplifying features or structures in high-
dimensional datasets. Therefore, low-dimensional versions of high-
dimensional data have the potential to be misleading. When this
happens, users currently have limited options to correct
the
problem.

Namely, displays of data in two or three dimensions result
typically from a linear visualization pipeline shown in Figure 1,
where data D are summarized by a mathematical model or
algorithm M(h) first and subsequently mapped to a visual display
V . A display is controlled solely by the algorithm that generated it
and adheres to predefined mathematical objectives, constraints, or
parameters denoted by h. When these constraints contradict
expert judgment, they warp or miss useful data features and
visualizations can lose interpretability. For examples, consider
visualization methods Principal Component Analysis (PCA) [2]
and Multidimensional Scaling (MDS) [3]. PCA is a common
analytical approach that projects datasets to two dimensions (in the
case of visualization) in the directions with the highest variance.
PCA loses its utility when meaningful features in the data do not

correspond with variance. Similarly, MDS is an analytical
approach that solves for low-dimensional (e.g., two-dimensional)
coordinates of data points by minimizing the difference between
pairwise distances of observations in high- and low-dimensional
spaces. When the the chosen distance function lacks relevance to
the application, MDS can produce visualizations that are hard to
interpret.

As defined by the current pipeline (Figure 1), users do not have
an intuitive or natural means to correct visual inaccuracies-beyond
the option of starting the pipeline over. For example, users can
transform the data or adjust the display-generating model (e.g.,
tweak model parameters h) to re-implement the pipeline and
create new visualizations. This means that users, who may not
have the appropriate mathematical training, must have a deep
enough understanding of the display-generating models to change
them or the data in a way that will result in useful visualizations.
When users cannot parameterize their expert judgements, the
pipeline is broken and sensemaking stalls.

In the field of Visual Analytics (VA), the disconnect between
static displays of data and usability has been studied extensively
and has motivated research in human-computer interaction [4–7].
It has been shown that when users interact with visualizations,
users learn more from the data than when they do not, even when
the model is arguably poor. In the Methods Section, we define
common forms of interaction that are readily available in many
VA tools: surface-level and parametric interaction. VA tools that
enable surface-level
to edit displays
independent of the underlying models or algorithms; e.g., high-
lighting or filtering observations. Whereas, when users interact
parametrically, they manually change influential parameters in the
display-generating models or algorithms. For example, iPCA [8]

interactions allow users

PLOS ONE | www.plosone.org

1

March 2013 | Volume 8 |

Issue 3 | e50474

Figure 1. Standard visualization pipeline. Data D feeds into a
mathematical model M that relies also on parameters h, and produces
a visualization V . The users U make sense of the visualization to the
best other their abilities. To correct any visual inaccuracies, users must
either change M, D, or h.
doi:10.1371/journal.pone.0050474.g001

and XGvis [9] are VA tools that allow users to adjust dials or
sliders
that either augment or out-right change influential
parameters in PCA or MDS, respectively.

Despite the extensive development of human-computer inter-
actions and VA tools, sensemaking of complex data is still limited.
The pitfall of current forms of interactions is that users are still
constrained by the linear pipeline and placed at either the
beginning or end. For example, surface-level
interactions take
place at the end of the pipeline by users who are trying to salvage
information from a potentiality misleading view of
the data.
Whereas, parametric interactions take place at the beginning of
the pipeline by users who are forced to make model adjustments.
Although parameter-controlling dials and sliders are easy to
manipulate, users must still understand the mathematical models
or algorithms to explore data efficiently. Without understanding
the mathematics, users have two options a) hope (not know) that
their parametric adjustments will convey their expert judgments
appropriately or b) evaluate data visualizations given every possible
combination of the parametric settings. For one parameter (e.g.,
one dial), option b)
is do-able, even when the parameter is
continuous. However, given two or more continuous parameters,
we think that option b) will quickly overwhelm users by the infinite
number of parameter combinations.

is communicated by the users

In this paper, we discuss a new form of human-computer
interaction to which we refer as ‘‘Visual to Parametric Interaction’’
(V2PI). We recognized that when users make certain surface-level
changes to displays, the users are communicating that the display-
generating algorithm is not working properly. V2PI s interpret
quantitatively what
to make
parametric model changes (and, subsequently, new visualizations).
In Endert et al. [10], we provide specific examples of V2PI. Here,
we not only apply V2PI, but also define V2PI explicitly, highlight
the framework to develop a V2PI, discuss advantages and
disadvantages of V2PI, and explain fundamental changes in the
process to visualize and explore data when V2PI is possible. V2PI
transforms data visualizations from being static to dynamic in that
V2PI enables
information to flow fluidly between display-
generating algorithms and users. Users learn from visualizations
and the visualizations adjust to expert judgement. Thus, with
V2PI, the visualization pipeline becomes bi-directional
in that
users are not simply at the starting nor receiving end of the
pipeline, but are embedded in the visualization scheme formally.
To be clear, VA tools that enable V2PI and rely on the bi-
directional visualization pipeline foster data exploration. V2PI
does not guarantee the discovery of all or any particular feature in
the data. As a data exploration tool, V2PI merges two learning
technologies: 1) statistical/data mining methods and 2) interactive
visualization techniques. The first technology focuses on mathe-
matical/algorithmic representations of data, whereas the second
provides cognitive representations of data. While V2PI maintains
the rigor of mathematical/algorithmic technologies, users only
operate within visual
the
methodology we develop from the merger is one for data

layouts of data. Hence, again,

V2PI

exploration. In our examples (Results Section) we reiterate the
exploratory nature of our methods, but for explanatory reasons we
offer a ‘‘ground-truth’’ to exemplify how visualizations adjust and
what we can learn using V2PI.

The remainder of the paper has four main sections: Methods,
Results, Discussion, and Conclusion. In the Methods Section, we
provide background about visual analytic interactions and
introduce V2PI. We define V2PI, develop the bi-directional
visualization pipeline, and explain required steps to construct
V2PI VA tools. In the Results Section, we apply V2PI in two case
studies. For each case study, we describe the data at hand, a
reasonable method for visualizing it, potential feature-masking
constraints of the methods, and implement V2PI to relax those
constraints. We reflect on the case studies in the Discussion Section
to acknowledge both the benefits and limitations of V2PI. In the
Conclusion Section, we summarize our current and future work.

Methods

2.1 Background: Visual Analytic Interactions

The process of using data to update domain specific knowledge
is referred to as sensemaking [11,12] and has been represented in
the form of a sensemaking process [13,14]. In this process, analysts
(i.e., experts, users, applied researchers, etc.) begin with a
knowledge base that they hope to either expand or adjust given
the data. The information discernible in data is often unclear to
analysts. Thus, learning from data may take place over time or a
series iterations during which analysts explore the data and
assimilate what they observe with their knowledge bases. Such
explorations/assimilations may take place each time analysts
interact with data.

In fact, Pike et al. [7] states, ‘‘interaction is the insight,’’ and
according to Thomas and Cook [12], Visual Analytics (VA) ‘‘is the
science of analytical reasoning facilitated by interactive visual
interfaces.’’ In VA, various types of interactions have been studied,
and Pike et al. [7] categorize them into two main groups: lower-
and higher- level
interactions. The primary difference between
these groups pertains to the goal of the users when they interact
with the data. With lower-level
interactions, users aim to
summarize ‘‘low-level structure’’ in the data including maxima,
minima, simple patterns, and linear trends. Examples of such
interactions include filtering, sorting, and other specific formal
queries. Any interactions that are not considered lower-level are
higher-level. The purpose of higher-level
to
‘‘understand’’ the data by uncovering features based on abstract
or complex (e.g., nonlinear) data characterizations.

interactions

is

In this section, we refine the interaction groups further as surface-
level and parametric to motivate the development of V2PI. We
explain each the interactions within the context of Figure 2.
Figure 2 was created by a VA tool called IN-SPIRE [15] and
displays a ‘‘Galaxy View’’ of text data that were collected for an
intelligence analysis. In this spatialization, the data points, i.e.,
documents, are represented by dots and clustered algorithmically
by IN-SPIRE. The aim for IN-SPIRE is to assist users in grouping
similar documents together and displaying them in an accessible
fashion.

2.1.1 Surface-Level Interactions. Surface-level interactions
are performed purely within the visual domain and are contained
in the lower-level class of interactions. Data rotations, reflections,
and translations, highlighting or editing observations, and
zooming into a portion of the visual space are each examples of
surface-level
interactions. These interactions, while capable of
enhancing the understanding of complex data structures, do not
necessarily relate coherently to mathematical data structures.

PLOS ONE | www.plosone.org

2

March 2013 | Volume 8 |

Issue 3 | e50474

V2PI

Figure 2. A ‘‘galaxy view’’ of text data created by the IN-SPIRE suite of data visualizations. In-SPIRE uses complex mathematical models in
order to discern structure (e.g., clusters) in high-dimensional data.
doi:10.1371/journal.pone.0050474.g002

Within the context of Figure 2, surface-level
interactions may
include opening, closing, highlighting, and filtering documents or
repositioning clusters. For example, users may wish to drag the
cluster labeled by, ‘‘rain, snow, storm,’’ to the bottom right of the
screen because they feel that the cluster is unimportant. This
adjustment
the underlying algorithm and
committed purely for organizational purposes.

independent of

is

iPCA and XGvis are VA tools

2.1.2 Parametric Interactions. Parametric interactions are
performed directly on the mathematical models that control
visualizations.
that permit
parametric interactions;
iPCA allows users to interact directly
with the principle eigenspace of the data, and XGvis enables users
to change either the analytical metric scaling method (measure for
distance between observations) or the local optimization scheme
used to solve for lower dimensional versions of high-dimensional
observations. If IN-SPIRE had the capability for a user to specify,
say, the number of clusters in Figure 2, it would be an example of a
tool that also permits parametric interactions. Table 1 provides a
non-exhaustive list of other parametric interactions.

Regardless of whether users apply parametric or surface-level
interaction, they are often trying to match the visualization to their
personal mental maps of the data. A user is more likely to make
sense of the data when the data appear in an expected form.
However, editorial changes to visualizations dismiss their math-
ematically driven interpretations, and, parametric changes may
not produce ideal visualizations for users. Mental maps of data

Table 1. A non-exhaustive list of parametric interactions.

may not comply to rigid, parametric characterizations of the data.
This means that regardless of how many times parameters are
adjusted, it is possible that suitable images of data may never be
obtained by users. What users need is an interaction that balances
surface-level and parametric adjustments to displays of data. For
this reason, we develop Visual to Parametric Interactions (V2PI)

2.2 Visual to Parametric Interactions (V2PI)

Surface-level interactions are intuitive to implement, but may
lack analytical interpretation because they are independent of the
mathematical underpinnings of visualizations. Parametric interac-
tions maintain the integrity of mathematical data characteriza-
tions, but can be difficult for analysts to understand. To combine
the ease of surface-level interactions and the mathematical rigor of
parametric interactions, we introduce Visual
to Parametric
Interaction (V2PI). In this section, we define Visual to Parametric
Interaction (V2PI), show how it fits in a bi-directional visualization
pipeline, and refine technical points about V2PI. Subsequently, in
the Results and Discussion Sections respectively, we apply and
summarize V2PI in case studies.

2.2.1 Definition. V2PI is the act of making surface-level
interactions that are interpreted by software quantitatively to make
parametric model changes (and, subsequently, new visualizations).
For example, one interpretation of the clustering structure in
Figure 2 is that observations within clusters are more correlated
than observations between clusters. Suppose a user chose to

Visualization

Data in clusters

Data network

Parametric Interactions

A user defines a cluster by specifying the required shape, minimum distance from other clusters, or minimum
number of elements.

A user adjusts the number of nodes and/or edges.

Classification tree diagram

A user adjusts the probabilities that branches split.

doi:10.1371/journal.pone.0050474.t001

PLOS ONE | www.plosone.org

3

March 2013 | Volume 8 |

Issue 3 | e50474

commit a surface-level interaction by merging two neighboring
clusters together. This interactions suggests that the algorithm (as
parameterized) underlying the IN-SPIRE visualization under
estimates the correlation between a subset (those selected) or all
observations. If IN-SPIRE had V2PI capabilities, IN-SPIRE
would quantify and parametrize the merger to adjust all or a
subset of pairwise correlation measurements. In turn, IN-SPIRE
would use the adjusted correlation measurements to create a new
display with different clusters and ready for further V2PI.

surface-level

The novelty of V2PI is that developers of VA tools with V2PI
functionality must learn users’ intent from surface-level interac-
tions and develop a strategy to automate mathematical adjust-
ments to display-generating models accordingly. Thus, developers
must know, in advance, how to interpret, process, and parametrize
interactions. Table 2 provides a non-
various
comprehensive list of
interactions with possible
parametric interpretations. Alas, not every surface-level interaction
will have a meaningful parametric interpretation and, for those
that do, the process to parameterize the surface-level interaction is
model specific. We discuss the process in the Section 2.2.3,
Parameterizing Feedback, and provide examples in the Results
Section.

surface-level

The primary advantage of V2PI is that displays of data that
were once static become dynamic. They can respond indefinitely
to surface-level interactions (with the parametric interpretations) to
account for expert judgment and potentially reveal additional
information in the data. Thus, as users
learn more, new
visualizations can update accordingly; and, as visualizations
update, users can learn more. With V2PI, is a bi-directional flow
of information in the visual domain of the data between display-
generating models and users. In the next section, we develop the
concept of a bi-directional visualization pipeline in detail and
further explain V2PI.

2.2.2 V2PI and the Bi-directional Pipeline. By construc-
tion, visualizations that result after a user’s V2PI are dynamic and
represent both the high-dimensional data according to the model
or algorithm and expert
learn from the
visualizations and the visualizations adjust to user feedback, as
defined by the parametric interpretation of some surface-level
interactions. By interpreting the interactions in a parametric form,
a) the models or algorithms work as defined originally, but now

judgement. Users

Table 2. A non-exhaustive list of V2PI.

V2PI

rely on both the data and user feedback; and b) the models create
new visualizations that are subsequently available for additional
feedback.

0

to the original

To see this, consider Figure 3, a bi-directional version of the
original visualization pipeline. This version is similar to Figure 1,
except users may now receive and distribute information in the
visualization iteratively. Specifically, Steps 1 and 2 of the bi-
directional pipeline are similar
in that a
mathematical model M that relies on data D and parameters h
constructs a visualization V that users U assess for sensemaking.
Now, with V2PI, users have the opportunity to commit either
standard (surface-level or parametric)
interactions or offer
feedback about the model via the visualization. If users choose
V2PI, they make surface-level adjustments to visualization V to
create V
(the original V with adjustments). We distinguish
standard surface-level
interactions from those associated with
V2PI by referring to the latter as cognitive feedback, Fc. That is, in
Step 3 of the bi-directional pipeline, users communicate their Fc
by creating visualization V
. In Step 4, the cognitive feedback is
parameterized to update h for M(D,h) accordingly; we refer to the
parameterized version of Fc as parametric feedback Fp. This step is
represented by a dashed line because,
in practice, users are
protected from the parameterization of Fc. VA developers of
visualization tools with V2PI capabilities must have the compu-
tational and mathematical machinery in place to parametrize
cognitive feedback. Given Fp and the updated h, the pipeline steps
may repeat.

0

Steps 1–4 may iterate until any of the following occurs: users are
satisfied with the display; the data have been explored thoroughly;
or the sensemaking process is complete. For this reason, the bi-
directional pipeline is similar in spirit to typical depictions of
sensemaking and human-computer interaction,
including those
developed by Norman, Abowd and Beale, and Keim et al.
[5,16,17]. Such depictions outline actions that need to be taken by
the user and/or
the system (e.g., data analysis, computer,
to enable sensemaking of data. The bi-
visualization, etc.)
directional pipeline, however,
is more detailed than these
depictions. For example, Keim et al. [5] present an iterative
‘‘Visual Analytics Process’’ that considers the potential for users to
obtain insight from visualizations and commit interactions ‘‘to

Visualization

Data in clusters

Surface-Level Interaction

Move two points from different clusters to the same cluster

Two-dimensional map or
spatialization of data

Change the relative locations of points

Data network across nodes/data
points

Delete a connection between nodes

Classification tree diagram

Delete a classification braU nch

V2PI requires parametric interpretations of surface-level interactions.
doi:10.1371/journal.pone.0050474.t002

A Parametric
Interpretation

Up weight the current
clustering role of the
dimensions in which
the observations are
similar

Down weight the
dimensions that
dictate the current
map

Decrease the current
correlation between
the nodes

Reduce the current
marginal probability of
belong to the
corresponding class

PLOS ONE | www.plosone.org

4

March 2013 | Volume 8 |

Issue 3 | e50474

Figure 3. The bi-directional visualization pipeline. Step 1) Create
visualization V based on a mathematical model or algorithm M that
depends on data D parameters h; Step 2) display the visualization for
users U to assess, Step 4) Users adjust the visualization to offer model
feedback; and Step 5) Update the model M (e.g., via the parameters h).
doi:10.1371/journal.pone.0050474.g003

refine parameters in the analysis process and steer visualization.’’
Whereas, the bi-directional pipeline describes specifically how
users interact with displays of data and how the system interprets
these interactions to update the analytical process, when the
parametrization machinery (Step 4) is in place. Also, communi-
cation between the system and the users in the bi-directional
pipeline takes place explicitly in the data visual domain. In fact,
visualizations V and V
are connected in Figure 3 to emphasize
this point.

0

0

thought

simply result

2.2.3 Parameterizing Feedback.

The process to parameterize cognitive feedback is model and
application specific. Unlike standard constraint-based user inter-
faces that are described in Myers et al. [18], new visualizations in
the bi-directional pipeline do not
from fixing
adjustments in V
and configuring what remains in the visualiza-
tion accordingly. Rather, we learn from V
how we might adjust
display-generating parameters that would impact the entire display
jointly. Careful
is needed to interpret and quantify
cognitive feedback in a form that both captures the users’ intent
(reasons for injecting the cognitive feedback) and is compatible
with the model. In the next section, we highlight what needs to be
considered when parameterizing feedback and,
in subsequent
sections, we provide examples within the context of case studies.
In both the original and
bi-directional visualizations pipelines, the visualizations depend
upon a model M with inputs, data D and parameters h. If we
consider the data D to be given (e.g., we do not transform nor filter
the data), visualizations can only change when we alter specifica-
tions for h. Thus,
in some sense, all visualizations rely on
potentially tunable parameters h. Within the context of V2PI, we
parameterize feedback that
to tune
specifications for h; i.e., we use Fp to adjust the model parameters
from an original setting, h~T , to a setting that accounts for
feedback, h~T F . In turn, new visualizations rely on the model M,
data D, and expert-adjusted parameter specifications h~T F . For
example, some models M rely on an optimization procedure to set
h~T and visualize data D. Based on Fp, we might adjust the
procedure which will subsequently result in calculating h~T F .

is communicated by V

0

0

0

V2PI

The choice to take a weighted average of T and T F is both flexible
and justifiable theoretically when h is assessed using Bayesian
methods [19]. If users are unclear about weight r, they may apply
parametric interaction to observe how their feedback impacts a
visualization by slowly transitioning r between 0 and 1.

i.e.,

We have mentioned several times that V2PI may occur in
the bi-directional pipeline may repeat several
sequence;
iterations before a user feels satisfied with the data exploration.
With each injection of cognitive feedback, a parametric form is
derived and a new visualization is created. To convey this
mathematically, consider the ith execution of V2PI such that

T F ,i~riF i
p

z(1{ri)T F ,i{1,

ð2Þ

where T F ,i{1 represents the specification for h that created the
visualization which was adjusted for the ith iteration and T F ,0~T
(the original specification for h). There is no notion of convergence
when considering V2PI. Users choose to stop iterating when the
data visualizations make sense. In some cases this means that users
may stop when a particular structure in the data appears or, in
other cases, when users assess the data from multiple perspectives
(based on multiple implementations of V2PI) and simply feel
comfortable with the data exploration. For the sake of being clear
about V2PI, we exemplify it and the bi-directional pipeline in the
next section using case studies that fall into the former category.

Results

We provide two case studies that rely on either PCA or MDS to
demonstrate the development and use of V2PI. PCA and MDS
are similar (in fact, under some conditions, the same) in that each
produces a spatialization of data for which the relative pairwise
distances between observations has meaning; observations that
appear close or far apart in visualizations are similar and different,
respectively, in the high-dimensional data spaces. Thus, each case
study allows users to explore data and adjust the coordinates of
two or more observations (hence change the relative distances
between points) to communicate cognitive feedback in the bi-
directional visualization pipeline. However, PCA and MDS can
differ by the way they learn the low-dimensional relative distances
between observations. In turn, the methods we use to interpret and
parameterize the cognitive feedback in the visualizations are
different.

We begin each case study with a description of data and
theoretical details of the analytical procedures, PCA or MDS.
Then, we use the steps of the bi-directional pipeline to guide our
discussions. We develop V2PI based on the parameterization of
one form of cognitive feedback per example.

3.3 Case Study 1: PCA

3.3.1 Data. The bi-directional pipeline and V2PI fosters data
exploration and has the potential to reveal structure (when it exists)
in data spatializations, such as clusters. To begin an exploration,
experts often use what they know about the data. However, what
they ‘‘know,’’ may be incomplete or reflect mere conjectures. For
example, in genetic analyses, biologists might know the pathways
to which some genes belong, but not all; or, to assess voting
tendencies, political analysts might know the party affiliations of
some voters, but not all. For such cases, it is reasonable to take
semi-supervised analytical approaches to assess data and infer the
global data structures. In this section, we use simulated data to
emulate such scenarios.

The challenge is formulating Fp from V

so that we can specify
T F . Our solution is two fold. First, we solve an inverse problem in
that we estimate a value for h that would result in either the
adjusted display V
. This
solution is Fp, a parametric interpretation of Fc. Second, we take a
weighted average of T and Fp to set T F ,

or the adjusted observations within V

0

0

T F ~rFpz(1{r)T,

ð1Þ

where r[½0,1 and r reflects the weight users want to place on
their judgements relative to the current visualization, e.g., when
r~1, h is specified entirely by expert judgement in that T F ~Fp.

PLOS ONE | www.plosone.org

5

March 2013 | Volume 8 |

Issue 3 | e50474

V2PI

We simulated a p~3 dimensional data set x that contains
n~300 observations and three clusters, as shown in Figure 4A.
Since we simulated the data, we have access
to detailed
information concerning the cluster assignments of each observa-
tion. However, we only reveal the cluster assignments for 20 of the
300 observations; ten observations were selected at random from
clusters 1 and 2 each. To visualize these data we apply PCA and
highlight
in Figure 4B. Notice in
Figure 4B that observations from clusters 1 and 2 do not group.
Rather, they are mixed in a seemingly random scatter within the
remaining data. Based solely on the display, we cannot make

the selected observations

judgements about, say, the number of clusters, size of clusters, and
assignments of observations to clusters.

If we were willing to use the true classifications

for the
remaining 290 observations, we could define the clusters as a
function of the dimensions in x by using fully supervised learning
strategies, such as a labeled version of PCA or linear discriminant
analysis [20,21]. However, we consider only what is known about
the 20 highlighted observations in Figure 4B and we take a visual
data exploration approach. In the sections that follow, we develop
both the mathematical and computational machinery to apply

Figure 4. V2PI with PCA. Figure A displays the simulated data in three dimensions. Observations in red, green, and blue denote groups 1, 2, and 3
respectively. Figure B displays the PCA projection of the simulated data with 20 observations (that were selected at random) highlighted. Again, red
and green points represent observations in groups 1 and 2 respectively. Figures C and D show updated displays after an adjustment to Figure B.
Figure C is the result of moving points marked by ‘z’ in Figure B apart and Figure D is the result of moving the points marked by ‘|’ in Figure B
together. Notice that both adjusted visualizations capture the clustering structure.
doi:10.1371/journal.pone.0050474.g004

PLOS ONE | www.plosone.org

6

March 2013 | Volume 8 |

Issue 3 | e50474

V2PI and create new PCA visualizations. We start
explaining the technicalities of PCA.

first by

3.3.2 Description of PCA. PCA is a deterministic, analytical
procedure that relies on an optimal linear projector to reduce the
dimension of a data set. Consider a center-shifted, p-dimensional
data set x that contains observations xi where i[f1,:::,ng; i.e.,
x~fx1,:::,xng and x is p|n. In our simulated example above,
p~3 and n~300. PCA relies on the solution for a q|p
transformation matrix W , where qvp,
the
variance of a low-dimensional version of x which we denote by z.
To solve for W , one option is to take the eigen-decomposition of
the sample variance (of x), S, such that S~ULV T , where U is
p|p and contains the eigenvectors of S, U~V, and L is a
diagonal matrix that includes the ordered eigenvalues of S (e.g.,
the element in the first column and row of L contains the largest
eigenvalue of S). Since the eigenvectors that correspond to the two
largest eigenvalues determine the two orthogonal directions that
explain the most amount of variance in x, W is assigned to equal
the first two columns of U,

that maximizes

2
64

W~U

3
75

,

1

0

0

1

0p{2

0p{2

where 0p{2 represents a (p{2)|1 vector of zeros. Given W ,

the calculation for z is straightforward,

z~W

0

x:

ð3Þ

When q~2, a PCA visualization simply plots z (e.g., Figure 4B)
in a two-dimensional scatterplot. The axes of the plot are hard to
interpret, but, fortunately, it is only the configuration of the points
in the plot that matters. PCA spatializes observations so that the
relative distance between them reflects their relative similarity in
the dimensions most preserved. As defined by the current form of
PCA, these dimensions are those with the largest variances. Alas,
because of PCA’s strict variance criteria and explicit assignment of
W , the spatialization can mask structures in data that do not
correspond with variance. For example,
the within-cluster
variance is larger than the between-cluster variance in the data
shown in Figure 4A. Thus, the clusters do not appear in z as
plotted in Figure 4B, and despite knowing the presence and/or
characteristics of the clusters, we cannot adjust W .

In the next section, we transform PCA from a deterministic,
dimension reduction algorithm to an expert guided projection
method via V2PI and the bi-directional pipeline. We explain
within the context of the data set described in Section 3.3.1. The
goal is to allow experts to explore data from different perspectives
using PCA methods so that the clusters may (or may not) be
revealed. The advantage is that the interpretation of each data
spatializations from the different perspectives is maintained (i.e.,
relative distances between observations reflect relative similarity),
but structures that do not depend on variance have the potential to
be discovered.

3.3.3 PCA with V2PI. We start by applying PCA for Steps 1
and 2 in Figure 3. We derive W in accordance with Equation (3)
and display z as we did in Figure 4B. For Step 3, experts
participate in the data analysis by assessing and injecting feedback
Fc about the projection. Since the configuration of points has
meaning in data spatializations, a natural surface-level interaction
Fc to parameterize is a reconfiguration of the points. Here, we

V2PI

0

i.e.,

to create V

develop V2PI so that users may re-configure the location of two
observations;
, users may either drag two
observations together or apart. The choice to drag observations
together or apart depends upon expert judgment. If an expert
believes two observations are similar in the high-dimensional
space, but they appear distant in the visualization, the expert may
drag the observations together. Whereas, if an expert believes two
observations are different in the high-dimensional space, but they
appear close together in the visualization, the expert may drag the
observations apart. For example, in Figure 4B, a user may choose
to drag two observations from cluster 1 (in red) together, two
observations from cluster 2 (in green) together, or one observation
from each cluster apart.

As a VA tool developer, we could have developed a more
complex version of V2PI; e.g., allow users
to move many
observations. However, what we propose is still a viable form of
V2PI and helps to convey the relative simplicity for how to use
V2PI. Namely, experts need only have knowledge about the
relationship between two observations to re-assess data from a
different perspective. They do not need to have reliable judgments
concerning, say, the dimensions in the data that define clusters; the
number of clusters in the data set; nor the size of data clusters.
Also, the methods can be extended to allow cognitive feedback
with more than two observations. In fact, for the next case study,
we do just that based on MDS (an analytical method that can re-
produce PCA plots under some constraints); we allow users to
move several observations to communicate cognitive feedback.

To parameterize Fc Step 4 of the PCA bi-directional pipeline,
we must a) determine a user’s intent and b) represent it in a
quantitative form that is compatible with PCA. When users drag
observations together, the users are suggesting the need for a
display that up-weights the dimensions for which the observations
are similar and down-weights the dimensions for which they are
different; whereas, when users drag observations apart, the users
are suggesting the need for a display that up-weights
the
dimensions for which the observations are different and down-
weights the dimensions for which they are same. For PCA, the
dimensions that have relatively large and small weights are those
with a high and low variances respectively-the transformation
matrix W results from deterministic procedure based on the
sample variance S. Thus, depending upon Fc, we re-weight the
elements of variance matrix S accordingly.

To do so, we derive a distance matrix as F p that is both
indicative of the observation adjustments and similar in nature to a
data variance matrix in that it is p|p and semi-definite. We
describe one procedure for deriving the distance matrix from Fc in
File S1. Given Fp, we take a weighted average as described in
Section 2.2.3 to calculate SF ,

SF ~rFpz(1{r)S:

For a new visualization, we re-apply the PCA machinery; i.e.,
we determine the transformation matrix W based on SF and re-
calculate z as defined by Equation (3).

We provide two adjusted PCA visualizations in Figure 4.
Figures 4C and 4D are based on the cognitive feedback that two
observations were dragged together and apart
in step 3,
respectively. Notice that regardless of the action taken for Fc,
the adjusted figures display structure. In fact,
from injecting
information about the relationship between two observations, we
learn from the updated view of the data that 1) the data include
three clusters and 2) the cluster-assignments of every observation.

PLOS ONE | www.plosone.org

7

March 2013 | Volume 8 |

Issue 3 | e50474

3.4 Case Study 2: MDS

3.4.1 Data.

In the previous case study, we used a simulated
example to show how V2PI works. Now, we consider a more
realistic dataset x that describes n~25 cities (Amherst, Ann Arbor,
Atlanta, Atlantic City, Blacksburg, Bloomington, Boston, Chapel
Hill, Charlotte, Chicago, Davis, Denver, Detroit, Fort Collins,
Helena, Houston, Knoxville, Los Angeles, Miami, New York City,
Reno, San Francisco, Seattle, Tucson, and Washington D.C.)
based on ten variables: Latitude, Longitude, Income (median), Age
(median), Population, Housing price (median), Population density,
Highschool (percent over 25 who have completed high, school),
Divorce rate (of those who have married), and Politics (percent
voting for Obama versus McCain in 2008, county-wide). To add
complexity to the data set, we append 20 noise variables; i.e.,
variables that were generated from Gaussian distributions with
means zero and variances comparable to that of either the latitude
or longitude variables.

To visualize these data and assess varying structures in the data,
we apply MDS as plotted in Figure 5A. To create this figure, all of
the variables in the data set were weighted equally. Thus, the
orientation we see of the data depends on both the real and noisy
variable equally. A better orientation would isolate the important
variables and down-weight those that are superfluous. For this
reason, we develop V2PI for MDS.

3.4.2 Description of MDS.

In a classical MDS scheme
[3,22], the objective is to preserve pairwise distances between
observations in low-dimensional representations of high-dimen-
sional data. Using the same notation from the PCA example, we
have a standardized data set x~½x1,:::,xnT with n observations
and xi[Rp (for i[f1,:::,ng). We aim to estimate a low-dimensional
version of x that we denote by z, where z~½z1,:::,znT , zi[Rq (for
i[f1,:::,ng), and qvp. For the sake of visualization, q~2 and, for
our above example, p~30. MDS solves for z by minimizing the

V2PI

absolute difference between pairwise distances of observations in x
and z,

X



z~ min
z1,...,zn

ivjƒn



Ezi{zjE{d(xi,xj)

,

ð4Þ

where d(xi,xj)~Exi{xjE, and Ea{bE is a predefined norm of
the distance between points a and b. The right hand side of
Equation (4) is typically referred to as a stress function, and the
resolved minimum is called the stress. The norms used in Equation
(4) will influence the MDS solution, if the distances themselves are
sensitive to the norm under which they are computed. A common
choice is the L2 norm so that

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
vuut
X

p

d(xi,xj)~

d~1

(xid {xjd )2

,

ð5Þ

where xid and xjd represent the d th element in observations xi
and xj respectively. This choice is arbitrary and can be adjusted
easily to accommodate other norms.

Similar to PCA, MDS produces a spatialization of the data x
where relative distance between observations reflects their relative
similarity. In fact, in the L2 space, MDS will reproduce PCA
visualizations. However, the explicit specification of a distance
metric provides another means to parameterize feedback. For
example, in the L2 norm (Equation 5), all of the variables have
equal
importance or weight, even though we know that 20
variables are noise. Based on expert feedback, it makes sense to
weight the variables in the distance metric so that only those that

Figure 5. MDS view of the the city-data and an example of cognitive feedback. Figure A displays an Initial MDS view of the data set that
describes 25 cities with 10 real variables and 20 noise variables. Figure B displays an example of cognitive feedback that arranges a set cities by
relative geographic locations.
doi:10.1371/journal.pone.0050474.g005

PLOS ONE | www.plosone.org

8

March 2013 | Volume 8 |

Issue 3 | e50474

are relevant influence the visualization. Next, we develop V2PI for
MDS by reparameterizing the distance metric.

3.4.3 MDS with V2PI. To include expert judgments in MDS
displays, we enable users to adjust (via cognitive feedback) a
version of MDS known as Weighted Multi-Dimensional Scaling
(WMDS) [23,24]. Just as MDS, WMDS minimizes the stress
function in Equation (4) to find a solution for z. However, WMDS
replaces d(:) (the L2 norm of two high-dimensional observations)
with a weighted norm dw(:),

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X

vuut

p

V2PI

be important. In this case, the users ‘‘learn’’ that the Latitude and
Longitude explain their arrangement the observations because
they have the largest weights. Although, we advocate the
suppression of parametric information to users, weights have
intuitive scales that are easy to interpret. That said,
in an
extremely high dimensional examples, creative, additional VA
methods would be needed to provide this parametric information
to users.

To assess how the remaining data spatialize given cognitive and
parametric feedback, we technically apply a weighted average as
described in Section 2.2.3 so that

dw(xi,xj)~

wd (xid {xjd )2

,

d~1

wF ~rFpz(1{r)w,

P

where w represents a pre-specified p{vector of dimension
weights, w~fw1,:::,wpg, and
d~1 wd ~1. Given w, variables
with large weights have more relevance than those with low
weights in WMDS displays. Also, the MDS and WMDS solutions
for z are identical when wi~1=p for each i[f1,:::,pg.

p

Now, using the data from Section 3.4.1 and the bi-directional
pipeline (Figure 3) as a guide for our discussion, we develop V2PI
for MDS based on the WMDS machinery. We start by applying
WMDS with wi~1=p for Steps 1 and 2 of the bi-directional
pipeline. For Step 3, experts may reconfigure three or more
observations to reflect a conjecture about the data and commu-
nicate cognitive feedback Fc. For example, since the data describe
25 cities, it might be helpful to visualize how the cities distribute
geographically across the United States. However, we can see from
Figure 5A that the cities are geographically misplaced; e.g., no
matter how we rotate the display, Seattle, Miami, San Francisco
and Houston will not orient geographically. Thus, as cognitive
feedback Fc, we enable MDS users to rearrange the locations of
three or more cities to create V
. Figure 5B shows a possible
orientation of five cities, Seattle (Sea), Los Angeles (LA), Houston
(Hou), Miami
(Mia), and New York City (NYC). Note that,
similar, to PCA, there are a variety of surface-level interactions
(which may eventually have a parametric interpretation) that users
could perform with MDS visualizations. For this paper, we
selected one.

0

0

The reason cities to do not map geographically is that the
information in the variables, Latitude and Longitude, is masked by
the remaining variables (both real and noise). Thus, for Step 4 of
the bi-directional pipeline, we parameterize Fc in the form of a
weight vector that will up-weight the variables which seem to best
explain V
and down-weight those that do not. Let A represent the
set of k observations that were adjusted so that matrices xA and zA
include only the high-dimensional and adjusted low-dimensional
coordinates of the selected observations. To estimate new weights
Fp, we solve the inverse problem; we solve for the weights that
minimize the stress function based only data xA and zA. Explicitly,
Fp~w, where

X



Fp~ min
w1,...,wp

ivjƒK

Ezi

A{zj

AE{dw(xi

A,xj

A)

:

ð6Þ



P

p

The solution Fp is found easily using a gradient search method
d~1 wd ~1. In our example, with
[25] with the constraint
cognitive feedback displayed in Figure 5B, the solution Fp weights
Latitude and Longitude by 0.47 and 0.52, respectively. The total
weight of the remaining variables equals 0.01. Note, based on Fp,
users may learn the variables that define the structure they find to

but we set r~1 for this application of V2PI. In the standard
MDS or WMDS procedures, the weight vector is pre-specified and
set independently of the data, thus we do the same with V2PI.
However, now, the weight vector is set according to user feedback.
Subsequently, new data visualizations are created with the WMDS
machinery and weight vector wF . Figure 6 includes a new
visualization of the data. Since Latitude and Longitude define the
geographic locations of cities and we want to demonstrate the
success of MDS with V2PI, we re-scaled and rotated the updated
low-dimensional coordinates so that we could overlay them on a
US map. On this map, we also include the true city coordinates.
The user-guided visualization approximates the true map fairly
well.

From our exploration of the data with V2PI and MDS, users a)
visualize how the cities in the data set distribute across the United
learn from Fp that Latitude and
States from Figure 6 and b)
Longitude are the primary variables that explain the visual
differences between the cities; e.g., Seattle and Miami are the
furthest cities apart in Figure 6 because they differ the most in
Latitude and Longitude. The data exploration could stop here, if
users wanted. Or, users may reiterate the bi-directional visualiza-
tion pipeline and inject more cognitive feedback to asses the data
from another perspective. To show the latter is possible, we
continue with the data exploration using V2PI in the next section.
3.4.4 Continuation of MDS Data Exploration. Looking at
Figures 5 and 6, we see that the data set includes major cities and
college-towns. Suppose a user is unable to classify all of the
observations (only a small set) and the user wants to learn which
variables differentiate major cities from college towns. Figures 5A
and 6 (two MDS visualizations of the data), do not help the user.
Thus, the users apply V2PI again.

0

the methods are in place to learn weights Fp

For cognitive feedback Fc, users move the cities about which the
classification is known into two separate groups. Specifically, three
college-towns, Blacksburg (BB), Davis (Dav), and Fort Collins (FC),
are placed away from two cities, New York City (NYC) and
Washington D.C. (WDC), to create V
(Figure 7A). As previously
shown,
(the
set wF to Fp, and create a new
parameterized form Fc),
visualization (Figure 7B). According to Fp, the selected observa-
tions differ most by Politics, Highschool, Age, and Population
Density with weights 0.62, 0.22, 0.05, and 0.03, respectively. Also,
based on proximity in Figure 7B, we see that Amherst (Amh), Ann
Arbor (AA), Bloomington (Blt), and Chapel Hill (CH), are more
similar to the selected college-towns (in the up-weighted variables)
than the cities; and conversely, Boston (Bos), Chicago (Chi),
Denver (Den), Detroit (Det), and San Francisco (SF), are more
similar to the cities than the college-towns.

PLOS ONE | www.plosone.org

9

March 2013 | Volume 8 |

Issue 3 | e50474

V2PI

Figure 6. A visualization of the city-data that was updated by a parametric version of the cognitive feedback plotted in Figure 5B.
The updated locations of the cities were stretched and rotated to overlay on a map of the United States. The symbols % and 0 mark true and
projected city coordinates by WMDS- V2PI. The estimated and true city coordinates are close.
doi:10.1371/journal.pone.0050474.g006

Discussion

We applied V2PI using two common data visualization
in the mathematical

methods. In each example, constraints

the data limited the utility of

initial data
characterization of
displays;
i.e., Figures 4A and 5A did not reveal expected nor
meaningful structure. In turn, we included users in the visualiza-
tion pipeline via V2PI to guide the mathematics and obtain

Figure 7. New cognitive feedback and updated view of city-data. Figure A plots another example of cognitive feedback that groups college
towns separately from large cities. Figure B plots an updated visualization of the data that accounted for the feedback in Figure A.
doi:10.1371/journal.pone.0050474.g007

PLOS ONE | www.plosone.org

10

March 2013 | Volume 8 |

Issue 3 | e50474

visualizations worth assessing. The case studies provided successful
examples and avoided some practical challenges that we discuss
here.

to portray user

intent-as

judgements about

V2PI does not guarantee the display of obvious data structure;
V2PI only guarantees
interpreted
parametrically within the constraints of the display-generating
model. For example, in the case studies, V2PI guaranteed new
spatializations that reflected the users’
the
observations’ pairwise relationships, as defined by PCA and MDS.
The improved depictions of
the pairwise relationships were
estimated by one, updated linear projection (by either PCA or
MDS) of the data. Given different visualization methods, the
pairwise relationships might have updated differently. For
example, V2PI could be developed for other visualization
methods, including, Isomap, Generative Topographical Models,
and Mixture PCA [26–28]. Such approaches characterize data
spatially using non-linear methods and/or multiple visualizations.
Had V2PI been in place with these methods, the updated displays
of data in Figures 4C, 4D, 6, and 7B might have configured the
observations differently.

It is important that an appropriate method is applied to assess
data visually. V2PI, in its current form, only enables parameter
adjustments within the chosen methods, not adjustments to the
methods themselves. VA tools that enable users to switch the
underlying analytical methods of visualization could be useful, as
the selection depends upon both characteristics of the data and the
analytical goal of the data exploration. In the case studies, there
was little to no difference between PCA or MDS to visualize the
datasets. We developed them to demonstrate differences in how
we can conceptualize and parameterize feedback. Had either
dataset within the case studies included outliers or non-linear
relationships between observations neither PCA nor MDS (based
on the L2 norm) would have been appropriate; non-linear
methods, such as Isomap [26] or Generative Topographical
Mapping [27] might be better. Also, there are several visualization
methods
to display
information. For example, cluster algorithms or network models
may plot dendograms or directed graphs to group and link one or
more observations together. Albeit, clusters were revealed in the
first, PCA case study, but PCA is not formally a cluster-discovering
algorithm; the cluster assignments were up to the user (which has
advantages and disadvantages). If a user wants to formally classify
observations, an appropriate analytical method should be applied
and V2PI can be developed accordingly.

that do not use geographic metaphors

Crucially, the selected analytical method determines both the
ways by which users can communicate cognitive feedback and
how it is parameterized. This was discussed in Section 2.2. In the
case studies, we presented only one form of cognitive feedback per
analytical method; users adjusted the locations of either two or
more observations. However, there are multiple forms of cognitive
feedback that are applicable to data spatializations,
including
filtering, querying, and annotating, that can be parameterized.
Future work of this research includes the implementation of user
studies to learn the various forms in which it is natural for users to
convey cognitive feedback based on a variety of visualizations.

In such user studies, we would also assess how analysts learn to
use VA tools with V2PI and quantify what they gain from V2PI.
Different analytical methods and datasets may result in visualiza-
tions
that vary in difficulty to interpret. Only once users
understand the meaning of the displays, can they effectively inject
feedback and make sense of data. Thus, V2PI is most advanta-
geous when users can learn how to interpret displays and interact
with them more efficiently than understand the display-generating
parameters. For example, in the first case study with PCA, it is

V2PI

reasonable to argue that users can assess and compare the relative
differences between observations in visualizations with less effort
than interpret the meaning of variance matrices, eigenvalues, and
eigenvectors. Also, with V2PI, users can compare observations
with PCA from varying perspectives to discover multiple structures
or relationships in data easier than without V2PI. User studies that
evaluate how well analysts understand PCA visualizations and
compare what analysts learn using PCA with and without V2PI
would likely support this argument. We envision a study that asks
several questions about dataset(s)
that analysts would be
challenged to answer using what they know and VA tools with
and without V2PI capabilities. The answers to the questions and
the time it takes to answer the questions would illuminate the ease
at which the analysts interpret the visualizations and the utility of
the V2PI.

As with any user study, the dataset(s) that analysts are requested
to assess is an important experimental-design element and may
impact study results for several reasons. For example, datasets
about which some users have prior knowledge and others do not
will confound study findings. To evaluate V2PI, the size of datasets
(in both the number of rows and columns) is also important to
consider. Large datasets may impact the interpretability of some
visualizations and computation time. In Figure 4, the dataset is
small enough so that the points (i.e., observations) are distinguish-
able; distance between many observations was clear enough to
inject feedback. Given millions of overlapping observations, this
might not be the case. Also, with V2PI, real-time visualization
updates enable users to explore data in parallel with them learning
or thinking about the data. Yet, V2PI, as any analytical method, is
limited by computational
it is
important to select datasets for studies that meet the constraints of
the analytical method. Or, conversely, select/develop analytical
and updating methods that can scale with data size. Both PCA and
MDS (for certain distance metrics) are scalable.

feasibility and efficiency. Thus,

Conclusion

In this paper, we discussed two fundamental concepts: the bi-
directional visualization pipeline and V2PI. When we combine the
two, we have a visualization scheme that enables experts to
explore data from multiple perspectives without understanding the
display-generating models. Since users do not need to understand
the mathematical underpinnings of visualizations, they are free to
build upon their knowledge base and merge their expertise with
the information in data instantly. That is, they may have an
opportunity to learn and interact with a dataset directly in its
visual domain-the domain in which experts host their expertise
and intuition.

An important feature of the bi-directional pipeline is that users
receive and distribute information, thus both expert judgement
and standard datasets are valid components of quantitative
analyses (that underlie data visualizations). The use of each
component is not particularly novel when analyses are constructed
within the Bayesian paradigm. Bayesian models combine prior
distributions that may represent subjective, expert-driven infor-
mation, with likelihoods to formulate inferences. However, the bi-
directional pipeline 1) does not require formal probabilistic
specifications to operate; 2) enables experts to communicate their
judgements via data visualizations; and 3) allows experts to inject
their judgements during multiple stages of data analyses. Experts
have multiple opportunities to recall, include, and reflect upon
their judgements in analyses by adjusting visualizations at each
iteration of the bi-directional pipeline.

PLOS ONE | www.plosone.org

11

March 2013 | Volume 8 |

Issue 3 | e50474

We exemplified the use of

the bi-directional visualization
pipeline and V2PI within two case studies. For each, a projection
method was used to spatialize data in two dimensions and we
described a unique approach to implement V2PI. The approaches
differed due to the subtle differences in the projection methods.
For visualizations that do not rely on linear projections, the bi-
directional visualization pipeline and V2PI may still apply.
However, V2PI has practical limitations that were discussed when
we reflect on the case studies,
including the selection of
visualization methods that are appropriate for both the data and
the necessary learning curve for using V2PI, and
expert,
computational
feasibility. Each limitation is addressable with
careful thought and flexible VA tools.

That said, successful interactions with data via visualizations
rely upon the development of VA tools that support V2PI. The

References

1. Andrews NO, Fox EA (2007) Recent developments in document clustering.

Technical report, Computer Science, Virginia Tech.
Jolliffe I (2005) Principal Component Analysis. New York: Springer-Verlag.

2.
3. Kruskal JB, Wish M (1978) Multidimensional scaling. Sage University Paper

series on Quantitative Application in the Social Sciences 48: 07–011.

4. Keim DA (2002) Information visualization and visual data mining. IEEE

Transactions On Visula-tions and Computer Graphics 7: 100–107.

5. Keim D, Mansmann F, Schneidewind J, Thomas J, Ziegler H (2008) Visual
analytics: Scope and challenges. In: Simoff S, Bohlen M, Mazeika A, editors,
Visual Data Mining: Theory, Techniques and Tools for Visual Analytics,
Springer-Verlag, Berlin. pp. 767–90.
Icke I, Sklar E (2009) Visual analytics: A multifaceted overview. Technical
report, City University of New York.

6.

7. Pike WA, Stasko J, Chang R, OConnell TA (2009) The science of interaction.

8.

Information Visu-alization 5: 78–99.
Jeong DH, Ziemkiewicz C, Fisher B, Ribarsky W, Chang R (2009) ipca: An
interactive system for pca-based visual analytics. Computer Graphics Forum 28:
767–774.

9. Buja A, Swayne DF, Littman ML, Dean N, Hoffman H (1998) Xgvis: Interactive
data visualization with multidimensional scaling. Journal of Computational and
Graphical Statistics 5: 78–99.

10. Endert A, Han C, Maiti D, House L, Leman S, et al. (2011) Observation-level
interaction with statistical models for visual analytics. In: Visual Analytics
Science and Technology (VAST) 2011 IEEE Conference. pp. 121–130.

11. Lederberg J (1989) Excitement and Fascination of Science, Twelve-Step Process
,

for Scientific Experiments: Epicycles of Scientific Discovery. Palo Alto,
California: Annual Reviews, Inc.

12. Thomas J, Cook K (2005) Illuminating the Path. National Visualizations and

Analytics Center.

13. Pirolli P, Card S (2005) Sensemaking processes of intelligence analysts and
possible leverage points as identified through cognitive task analysis. Proceddings
of the 2005 International Conference on Intelligence Analysis.

V2PI

best VA tools are intuitive and accessible to users with varying
levels of expertise. In this paper, we did not mention aesthetic
aspects of VA tools that need to be considered for human cognitive
purposes. Rather, we discussed the analytical mechanics needed in
tools that enable V2PI.

Supporting Information

File S1 Parameterizing feedback for PCA.
(PDF)

Author Contributions

Conceived and designed the experiments: SL LH DM AE CN. Analyzed
the data: SL LH DM AE CN. Contributed reagents/materials/analysis
tools: SL LH DM AE CN. Wrote the paper: SL LH DM AE CN.

15. Pak Chung W, Hetzler B, Posse C, Whiting M, Havre S, et al. (2004) In-spire
infovis 2004 contest entry. Proceedings of the IEEE Symposium on Information
Visualization (INFOVIS’04).

16. Norman DA (1990) The Design of Everyday Things. Doubleday Books, New

York.

17. Abowd G, Beale R (1991) Users, systems, and interfaces: Unifying framework for
interaction. In: Human Computer Interaction HCI91: People and Computers.
Cambridge University Press, Cambridge, volume 6, pp. 73–87.

18. Myers B, Hudson SE, Pausch R (2000) Past, present, and future of user interface

software tools. ACM Trans Comput-Hum Interact 7: 3–28.

19. House L, Leman SC, Han C (2012) Bayesian visual analytics: Bava. Technical

report, Department of Statistics, Virginia Tech.

20. Koren Y, Carmel L (2003) Visualization of

labeled data using linear
transformations. In: Proceedings of the Ninth Annual IEEE Conference on
Information Visualization. Washington, DC, , USA: IEEE Computer Society,
INFOVIS’03, pp. 121–128. URL http://dl.acm.org/citation.cfm?id = 1947368.
1947392.

21. Hastie T, Tibshirani R, Friedman J (2008) Elements of Statistical Learning: Data

mining, Inference and Prediction (2nd Edition). Springer-Verlag.

22. Torgerson WS (1958) Theory and Methods of Scaling. New York: John Wiley.
23. Carroll JD, Chang JJ (1970) Analysis of individual differences in multidimen-
sional scaling via an n-way generalization of eckart-young decomposition.
Psychometrika 35: 238–319.

24. Schiffman SS, Reynolds ML, Young FW (1981) Introduction to Multidimen-
sional Scaling: Theory, Methods, and Applications. New York: Academic Press.
25. Mordecai A (1976) Nonlinear Programming: Analysis and Methods. New Jersey:

Prentice-Hall.

26. Tenenbaum JB, V de Silva JCL (2000) A global geometric framework for

nonlinear dimensionality reduction. Science 290: 2319–2323.

27. Bishop CM, Svense´n M, Williams CKI (1998) GTM: the generative topographic

mapping. Neural Computation 10: 215–234.

14. Card S, Mackinlay J, Shneiderman B (1999) Information Visualization: Using

28. Tipping ME, Bishop CM (1999) Mixtures of probabilistic principal component

Vision to Think. San Francisco: Morgan Kaufmann.

analyzers. Neural Computation 11: 443–482.

PLOS ONE | www.plosone.org

12

March 2013 | Volume 8 |

Issue 3 | e50474

",False,2013.0,{},False,False,journalArticle,False,HLIDNE69,[],self.user,False,False,False,False,https://dx.plos.org/10.1371/journal.pone.0050474,,Visual to Parametric Interaction (V2PI),HLIDNE69,False,False
LB4J9ZUR,M86EJB84,"VisTrails: Visualization meets Data Management

Juliana Freire Emanuele Santos
Steven P. Callahan
Carlos E. Scheidegger Cl´audio T. Silva Huy T. Vo

SCI Institute and School of Computing – University of Utah

1.

INTRODUCTION

Scientists are now faced with an incredible volume of data to
analyze. To successfully analyze and validate various hypothesis,
it is necessary to pose several queries, correlate disparate data, and
create insightful visualizations of both the simulated processes and
observed phenomena. Data exploration and visualization requires
scientists to go through several steps. They need to select data sets
and design complex dataﬂows that apply series of operations to the
data to create appropriate visual representations, before they can ﬁ-
nally view and analyze the results. Often, insight comes from com-
paring the results of multiple visualizations. Unfortunately, today
this process contains many error-prone and time-consuming tasks.
In addition, once a data product, e.g., an image, is generated, all
the scientist is left with is the bitmap; if a detailed caption is not
created, it may not even be possible to reproduce that image at a
later time. As a result, the generation and maintenance of visual-
izations is a major bottleneck in the scientiﬁc process, hindering
both the ability to mine and use scientiﬁc data.

The VisTrails system [2, 3] represents our initial attempt to stream-

line the visualization process. Our long-term goal is to provide the
necessary infrastructure to improve the scientiﬁc discovery process
and reduce the time to insight. In VisTrails, we address the prob-
lem of visualization from a data management perspective: Vis-
Trails manages the data and metadata of visualization products. By
capturing the provenance of both the visualization processes and
data they manipulate, VisTrails enables reproducibility and simpli-
ﬁes the complex problem of creating and maintaining visualiza-
tions. It also allows scientists to efﬁciently and effectively explore
data through visualization:
they can explore their visualizations
by returning to previous versions of a dataﬂow (or visualization
pipeline); apply a dataﬂow instance to different data; explore the
parameter space of the dataﬂow; query the visualization history;
and comparatively visualize different results. Data management
techniques used in many different components of the system are
key to providing these functionalities, which have been absent in
previous visualization systems.
Outline. The rest of this paper is outlined as follows. In Section 2,
we describe an application scenario that motivated us to build Vis-
Trails. We discuss the limitations of existing visualization systems

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGMOD 2006, June 27–29, 2006, Chicago, Illinois, USA.
Copyright 2006 ACM 1-59593-256-9/06/0006 ...$5.00.

in Section 3 and describe the architecture of VisTrails in Section 4.
Finally, we provide an overview of our demonstration in Section 5.

2. MOTIVATING EXAMPLE: EOFS

Paradigms for modeling and visualization of complex ecosys-
tems are changing quickly, creating enormous opportunities for
scientists and society. For instance, powerful and integrative mod-
eling and visualization systems are at the core of Environmental
Observation and Forecasting Systems (EOFS), which seek to gen-
erate and deliver quantiﬁably reliable information about the envi-
ronment at the right time and in the right form to the right users.
As they mature, EOFS are revolutionizing the way scientists share
information about the environment and represent an unprecedented
opportunity to break traditional information barriers between sci-
entists and society at large [1]. However, the shift in modeling
paradigms is placing EOFS modelers in an extremely challeng-
ing position, and at the risk of losing control of the quality of op-
erational simulations. The problem results from the breaking of
traditional modeling cycles: tight production schedules, dictated
by real-time forecasts and multi-decade simulation databases, lead
even today to tens of complex runs being produced on a daily basis,
resulting in thousands to tens of thousands of associated visualiza-
tion products.

As an example, Professor Ant´onio Baptista, the lead investigator
of the CORIE1 project prepares ﬁgures for presentations showing
results of simulations that he has designed, but that are executed
by a research scientist in his group. The component elements of
his ﬁgures are generated over a few hours by a sequence of scripts,
activated by a different staff member in Baptista’s group who is
a visualization specialist. Once he receives the images, Baptista
draws the composite ﬁgure for a particular run in PowerPoint us-
ing cut-and-paste. This process is repeated for similar and comple-
mentary runs. Because element components are visually optimized
for each run, cross-run synthesis often have scale mismatches that
make interpretation difﬁcult.

The process followed by Baptista is both time consuming and
error prone. Each of these visualizations is produced by custom-
built scripts (or programs) manually constructed and maintained by
several members of Baptista’s staff. For instance, a CORIE visual-
ization is often produced by running a sequence of VTK [5] and
custom visualization scripts over data produced by simulations.
Since there is no infrastructure to manage these scripts (and as-
sociated data), often, ﬁnding and running them are tasks that can
only be performed by their creators. This is one of main reasons
Baptista is not able to easily produce the visualizations he needs
in the course of his explorations. Even for their creators, it is hard
to keep track of the correct versions of scripts and data. Since
1http://www.ccalmr.ogi.edu/CORIE

745Figure 1: VisTrails Architecture.

these visualization products are generated in an ad-hoc manner,
data provenance is not captured in a persistent way. Usually, the
ﬁgure caption and legends are all the metadata available for this
composite visualization in the PowerPoint slide.

3. EXISTING VISUALIZATION SYSTEMS
Visualization systems such as VTK [5] and SCIRun [6] allow
the interactive creation and efﬁcient manipulation of complex vi-
sualizations. These systems are based on the notion of dataﬂows,
where a visualization is produced by assembling visualization pipe-
lines out of modules that are connected in a network. However,
these systems lack basic data management capabilities and as a
result, they have important limitations.

An important limitation of existing visualization tools is that
they do not provide mechanisms for capturing provenance. Manu-
ally created captions and ﬁlenames are often the only provenance
information available for an image. They also lack history man-
agement—since a single instance of a dataﬂow is maintained, any
changes to a dataﬂow are destructive. In particular, because there
is no separation between the dataﬂow speciﬁcation and its param-
eters, as the parameters are modiﬁed, the previous values are for-
gotten. This places the burden on the scientist to ﬁrst construct the
visualization and then to remember what values led to a particular
image. As the dataﬂow evolves (i.e., operations are added, deleted
or modiﬁed) no information is kept about previous versions. An-
other limitation of existing tools is that they do not provide support
for comparative visualization. In particular, they lack the neces-
sary infrastructure for properly supporting exploratory multi-view
visualizations. The process required to create and compare a large
number of visualizations is way too cumbersome. For example, ex-
ecuting the same dataﬂow with different parameters (e.g., different
input data sets) requires users to manually specify all the parame-
ters using a Graphical User Interface (GUI). Clearly, this mecha-
nism is not scalable for generating more than a few visualizations.
Finally, existing systems lack an optimization infrastructure. In
particular, these systems may perform unnecessary and redundant
computations while executing dataﬂows.

VisTrails addresses these limitations providing infrastructure to
support the interaction of the scientist with the visualization pro-
cess. Our objective is to give scientists a dramatically improved
and simpliﬁed process to analyze and visualize large ensembles of
simulations and observed phenomena.

4. THE VISTRAILS SYSTEM

The high-level architecture of VisTrails is shown in Figure 1. We
only sketch the main features of the system here, for further details
see [2, 3]. Users create and edit dataﬂows using the Vistrail Builder
user interface. The vistrail speciﬁcations are saved in the Vistrail
Repository. Users may also interact with saved vistrails by invok-
ing them through the Vistrail Server (e.g., through a Web-based
interface) or by importing them into the Visualization Spreadsheet.

Figure 2: A snapshot of the VisTrails history management interface.
Each node in the history is a separate dataﬂow that differs from its
parent by changes to the parameters or modules.

Each cell in the spreadsheet represents a view that corresponds to a
dataﬂow instance; users can modify the parameters of a dataﬂow as
well as synchronize parameters across different cells. Dataﬂow ex-
ecution is controlled by the Vistrail Cache Manager, which keeps
track of operations that are invoked and their respective param-
eters. Only new combinations of operations and parameters are
requested from the Vistrail Player, which executes the operations
by invoking the appropriate functions from the Visualization and
Script APIs. The Player also interacts with the Optimizer module,
which analyzes and optimizes the dataﬂow speciﬁcations. A log
of the dataﬂow execution is kept in the Vistrail Log. The different
components of the system are described below.
Vistrail Speciﬁcation. A dataﬂow is a sequence of operations used
to generate a visualization. A vistrail captures the notion of an
evolving dataﬂow—it consists of several versions of a dataﬂow.
The information in a vistrail serves both as a log of the steps fol-
lowed to generate a series of visualizations, a record of the visual-
ization provenance, and as a recipe to automatically re-generate the
visualizations at a later time. The steps can be replayed exactly as
they were ﬁrst executed, and they can also be used as templates—
they can be parameterized.
In order to handle the variability in
the structure of operations, and to easily support the addition of
new operations, we represent vistrails using XML (for more detail,
see [3]). An important beneﬁt of using an open, self-describing,
speciﬁcation is the ability to query, share, and publish vistrails.
This allows a scientist to locate dataﬂows suitable for a particular
task or data products generated by a given sequence of operations,
as well as to publish an image along with its associated vistrail so
that others can easily reproduce the results.
History Management. As discussed above, a vistrail captures in-
formation about the evolution of a dataﬂow or collection of related
dataﬂows—it behaves as a versioning system for dataﬂows. A vis-
trail consists of a tree where each node corresponds to a dataﬂow
(see Figure 2). But instead of storing the dataﬂows themselves, we
store the operations that take one dataﬂow to another. An edge be-
tween a parent and child nodes in a vistrail tree represents a set of
change actions applied to the parent to obtain the dataﬂow for the
child node. The action-based provenance mechanism of VisTrails
is reminiscent of DARCS2. This structure it allows scientists to
easily navigate through the space of dataﬂows created for a given
exploration task. In particular, they have the ability to return to
previous versions of a dataﬂow and compare their results. At any
point in time, the scientist can choose to view the entire history
of changes, or only the dataﬂows important enough to be given a
name (i.e., annotated changes).
Caching, Analysis and Optimization. Having a high-level spec-
iﬁcation allows the system to analyze and optimize dataﬂows. Ex-

2http://abridgegame.org/darcs

746(a)

(b)

Figure 3: The Vistrail Builder (a) and Vistrail Spreadsheet (b) showing the dataﬂow and visualization products of the CORIE data.

ecuting a vistrail can take a long time, especially if large data sets
and complex visualization operations are used. It is thus important
to be able to analyze the speciﬁcation and identify optimization op-
portunities. In the current VisTrails prototype, we leverage the vis-
trail speciﬁcation to identify and avoid redundant operations. The
Vistrail Cache Manager (VCM) is responsible for scheduling the
execution of modules in vistrails by identifying previously com-
puted subnetworks and performing constant-time cache lookups.
Playing a Vistrail. The Vistrail Player (VP) receives as input an
XML ﬁle for a dataﬂow instance and executes it using the underly-
ing Visualization or Script APIs. The semantics of each particular
execution are deﬁned by the underlying API. Currently, the VP
supports VTK classes with a very simple interpreter.
Creating and Interacting with Vistrails. The Vistrail Builder
(VB) provides a graphical user interface for creating and editing
dataﬂows (see Figure 3(a)). It writes (and also reads) dataﬂows in
the same XML format as the other components of the system. It
shares the familiar nodes-and-connections paradigm with dataﬂow
systems. To allow users to compare the results of multiple dataﬂows,
we built a Visualization Spreadsheet (VS). The VS provides the
user a set of separate visualization windows arranged in a tabu-
lar view. This layout makes efﬁcient use of screen space, and the
row/column groupings can conceptually help the user explore the
visualization parameter space [4]. The cells may execute different
vistrails and they may also use different parameters for the same
vistrail speciﬁcation (see Figure 3(b)). To ensure efﬁcient execu-
tion, all cells share the same cache. Users can also synchronize
different cells using the VS interface.

5. DEMONSTRATION OVERVIEW

In this demonstration, we show the power and ﬂexibility of Vis-
Trails by presenting actual scenarios in which scientiﬁc visualiza-
tion is used and showing how our system improves usability, en-
ables reproducibility, and greatly reduces the time required to cre-
ate scientiﬁc visualizations. In particular, we show how dataﬂows
are created and modiﬁed using the Vistrail Builder and Vistrail
Spreadsheet. Our examples also demonstrate the usefulness of the
history management, caching capabilities, and comparative visual-
ization tools in VisTrails.
CORIE. We demonstrate, through speciﬁc examples, how Vis-
Trails can be used to improve the current visualization process
that Professor Baptista employs. This part of the demonstration

includes queries to published visualizations and the use of the his-
tory to modify existing visualizations.
Medical Imaging. Acquiring useful information from the results
of medical imaging devices has been a subject of much research
in the ﬁeld of scientiﬁc computing. We show how VisTrails can
be used to explore the parameter space using multi-view visual-
ization and how caching substantially improves the interactivity of
the process.
Uncertainty Visualization. A difﬁcult problem in scientiﬁc sim-
ulation is to represent the uncertainty of the modeling systems due
to measured or computed error. We demonstrate how VisTrails
can be used to effectively visualize uncertainty through the use of
evolving dataﬂows and comparative visualization.

An alpha release of VisTrails (available upon request) is cur-
rently being tested by a select group of domain scientists. More
information about the system is available at

http://www.sci.utah.edu/˜vgc/vistrails

Acknowledgments. Ant´onio Baptista (Oregon Health & Science
University) and Patricia Crossno (Sandia) have provided us valu-
able input for the system design. This work is partially supported
by the National Science Foundation, the Department of Energy,
Army Research Ofﬁce, and IBM.

6. REFERENCES
[1] A. Baptista. Encyclopedia of Physical Science and Technology,

chapter Environmental Observation and Forecasting Systems.
Academic Press, 2002.

[2] L. Bavoil, S. Callahan, P. Crossno, J. Freire, C. Scheidegger, C. Silva,

and H. Vo. Vistrails: Enabling interactive multiple-view
visualizations. In IEEE Visualization 2005, pages 135–142, 2005.

[3] S. Callahan, J. Freire, E. Santos, C. Scheidegger, C. Silva, and H. Vo.
Managing the evolution of dataﬂows with vistrails. In IEEE Workshop
on Workﬂow and Data Flow for Scientiﬁc Applications (SciFlow
2006), 2006. To appear.

[4] E. H. Chi, P. Barry, J. Riedl, and J. Konstan. A spreadsheet approach

to information visualization. In IEEE Information Visualization
Symposium, pages 17–24, 1997.

[5] Kitware. The Visualization Toolkit (VTK) and Paraview.

http://www.kitware.com.

[6] S. G. Parker and C. R. Johnson. SCIRun: a scientiﬁc programming
environment for computational steering. In Supercomputing, 1995.

747",False,2006.0,{},False,False,conferencePaper,False,LB4J9ZUR,[],self.user,False,False,False,False,http://portal.acm.org/citation.cfm?doid=1142473.1142574,,VisTrails: visualization meets data management,LB4J9ZUR,False,False
Q2Q8T98U,E2LPSTIC,"A Bidirectional Pipeline for Semantic Interaction in Visual

Analytics

Adam Q. Binford

Thesis submitted to the Faculty of the

Virginia Polytechnic Institute and State University

in partial fulﬁllment of the requirements for the degree of

Master of Science

in

Computer Science and Applications

Chris L. North, Chair

Denis Gracanin

Nicholas F. Polys

August 11, 2016

Blacksburg, Virginia

Keywords: Visualization, High-dimensional data, Interaction design

Copyright 2016, Adam Q. Binford

A Bidirectional Pipeline for Semantic Interaction in Visual Analytics

Adam Q. Binford

ABSTRACT

Semantic interaction in visual data analytics allows users to indirectly adjust model param-

eters by directly manipulating the output of the models. This is accomplished using an

underlying bidirectional pipeline that ﬁrst uses statistical models to visualize the raw data.

When a user interacts with the visualization, the interaction is interpreted into updates in

the model parameters automatically, giving the users immediate feedback on each inter-

action. These interpreted interactions eliminate the need for a deep understanding of the

underlying statistical models. However, the development of such tools is necessarily complex

due to their interactive nature. Furthermore, each tool deﬁnes its own unique pipeline to

suit its needs, which leads to diﬃculty experimenting with diﬀerent types of data, models,

interaction techniques, and visual encodings. To address this issue, we present a ﬂexible

multi-model bidirectional pipeline for prototyping visual analytics tools that rely on seman-

tic interaction. The pipeline has plug-and-play functionality, enabling quick alterations to

the type of data being visualized, how models transform the data, and interaction methods.

In so doing, the pipeline enforces a separation between the data pipeline and the visualiza-

tion, preventing the two from becoming codependent. To show the ﬂexibility of the pipeline,

we demonstrate a new visual analytics tool and several distinct variations, each of which

were quickly and easily implemented with slight changes to the pipeline or client.

Acknowledgments

I want to thank my advisor Chris North taking me on and guiding me through my work. I

cannot imagine a more enjoyable advisor to work with. I thank Nicholas Polys for inspiration

on my design, and Denis Gracanin for being a great professor for me through numerous classes

over the years. I would also like to thank Michelle Dowling and Jagathshree Suryanarayanan

Iyer for collaborating with me on some of the web interfaces.

Finally, I would like to thank General Dynamics for their support in this research.

iii

Contents

Chapter 1

Introduction

Chapter 2 Related Work

2.1 Semantic Interaction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2.2 Streaming Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

6

6

9

2.3 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

10

Chapter 3 Pipeline Framework

13

3.1 Data Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

3.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

16

3.3 Connector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

18

3.4 Communication within the Pipeline . . . . . . . . . . . . . . . . . . . . . . .

19

3.5 Asynchronous Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

iv

3.5.1 Asynchronous Models . . . . . . . . . . . . . . . . . . . . . . . . . . .

21

3.5.2 Pushing Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

23

Chapter 4 Pipeline Implementations and Visualizations

25

4.1 Text Analysis Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

4.1.1 Data Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

4.1.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29

4.1.3 Connector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

4.1.4 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

4.1.5 Alternative Visualizations

. . . . . . . . . . . . . . . . . . . . . . . .

39

4.2 Displaying Attributes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

4.3 Multi-source Text Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

4.3.1 Data Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

4.3.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

4.4 Streaming Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

4.4.1 Data Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

48

4.4.2 Connector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

49

4.4.3 Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

50

v

4.5 Raw High Dimensional Data Analysis . . . . . . . . . . . . . . . . . . . . . .

51

Chapter 5 Discussion and Future Work

54

5.1 A Modular Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

54

5.2 Research Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

58

5.3 Pipeline Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

60

Chapter 6 Conclusion

Bibliography

Appendix A Creating a Pipeline

62

63

70

A.1 Data Controller . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

71

A.2 Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

72

A.3 Connector . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

A.4 Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

74

A.5 Putting it Together . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

78

vi

List of Figures

1.1 The pipeline framework.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1 The sensemaking loop from [27] illustrates the complex process of turning

data into useful insights. Used under Fair Use, 2016.

. . . . . . . . . . . . .

7

2.2 The traditional visualization pipeline only allows for direct interaction with

the algorithms and raw data. . . . . . . . . . . . . . . . . . . . . . . . . . . .

7

2.3 The bidirectional pipeline from [8] allows users to interaction directly with

the visualization. Used under Fair Use, 2016. . . . . . . . . . . . . . . . . . .

8

3.1 An instance of our pipeline consists of a Data Controller, series of Models, and

Connector. The visualization is a separate entity from the pipeline. The solid

arrows indicate the forward ﬂow in the pipeline, while dotted arrows indicate

the inverse ﬂow. Dotted arrows within the Models indicates the ability to

short circuit. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

14

vii

3.2 Two examples of how a Model can modify the data blob passed through the

pipeline. In (a), a new element is added, while in (b), a current element is

modiﬁed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

20

3.3 A pipeline with asynchronous Models. . . . . . . . . . . . . . . . . . . . . . .

21

4.1 The visualization controller mediates communication between multiple pipelines

and visualization clients. A single visualization can be mirrored across multi-

ple clients, represented by the red arrows. . . . . . . . . . . . . . . . . . . . .

26

4.2 Our base pipeline implementation. . . . . . . . . . . . . . . . . . . . . . . . .

28

4.3 An initial 2D web visualization showing the general layout of the graph and

data ﬁelds as well as the visual encodings used in the graph. Points are plotted

on the screen using an interactive WMDS algorithm. Moved data points are

highlighted in green, while data points that are selected to view the raw text

data and metadata are highlighted in purple. . . . . . . . . . . . . . . . . . .

34

4.4 The initial search for Mr. Ramazi led us to ﬁnding Mr. Hallak has withdrawn

some money from him.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

4.5 Mr. Hallak appears to be involved in terrorist activities.

. . . . . . . . . . .

37

4.6 Someone has made phone calls to Mr. Hallak from a 718 number, so we search

for more documents relating to this area code.

. . . . . . . . . . . . . . . . .

38

4.7

Initiating an OLI interaction to learn more about the phone numbers. . . . .

39

viii

4.8 The results of the OLI interaction led us to discover another document relating

to these phone numbers.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

40

4.9 An alternative 2D web visualization. This visualization maps relevance to the

distance from the center of the radar and similarity to the angle. All other

visual encodings are pulled from the original 2D web visualization. . . . . . .

41

4.10 A screenshot of the 3D web visualization. The layout of the web page is

based on the layout in the original 2D visualization. With minor changes to

the base implementation of the pipeline, we are able to graph the data points

represented as spheres in 3D.

. . . . . . . . . . . . . . . . . . . . . . . . . .

42

4.11 A 2D web visualization with attributes mapped in the same graph as the data

points. Attributes are yellow, whereas data points are red. All other visual

encodings are pulled from the original 2D web visualization.

. . . . . . . . .

43

4.12 A pipeline that connects to an external search engine and calculates term

frequencies dynamically.

. . . . . . . . . . . . . . . . . . . . . . . . . . . . .

45

4.13 A pipeline that pushes data asynchronously from the Data Controller. . . . .

48

4.14 A visualization for streaming tweets. A new text box on the top lets users set

a ﬁlter for tweets to pull in.

. . . . . . . . . . . . . . . . . . . . . . . . . . .

50

4.15 A visualization that includes the current weight for each attribute in the data.

These weights can be manipulated directly to create a new projection of the

data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

ix

A.1 A data blob is created by the pipeline and passed through the setup up func-

tion of the Data Controller and each Model.

. . . . . . . . . . . . . . . . . .

75

A.2 A data blob is created by the visualization representing an interaction. The

Data Controller creates a new blob for the forward pipeline.

. . . . . . . . .

76

A.3 Inverse functions trigger a short circuit by returning a new data blob.

. . . .

77

A.4 The Data Controller asynchronously pushes a new data blob down the pipeline. 78

A.5 A Model asynchronously pushes a new data blob down the pipeline.

. . . . .

79

x

Chapter 1

Introduction

Visual data analytics tools are made to support the user’s sensemaking process by coupling

human reasoning with the analytical power of computerized mathematical models [13]. Thus,

to be eﬀective, visual analytics tools must transform raw data to a visualization that assists

the user in the basic sensemaking tasks of foraging for more data and synthesizing informa-

tion [16]. Semantic interaction (SI) is one approach for supporting the sensemaking process

by improving the usability of the underlying models in visual analytics tools [16, 17, 22, 25].

These tools use visual metaphors, such as a “near = similar” metaphor, to map the data to

the visualization. These metaphors enable the user to interact with the displayed data in

an intuitive manner. Each interaction is translated into feedback for the underlying mod-

els driving the visualization, prompting the visualization to automatically update based on

this new information. Interpreting natural interactions rather than forcing users to directly

manipulate mathematical parameters eliminates the necessity for users to understand how

1

2

the underlying mathematical models function. By continuously processing user feedback

and updating the visualization, tools that employ semantic interaction enable the tool to

progressively learn about the user’s interests within the data. Therefore, with each interac-

tion, the system can provide incrementally better results that more closely match the user’s

interest [16].

One type of semantic interaction is observation-level interaction (OLI) [17]. Here, the focus

is on users interacting with the visualized data points (observations) to update the param-

eters used to generate the visualization. OLI comes in two distinct varieties: Visual to

Parametric Interaction (V2PI) [23, 25] and Bayesian Visual Analytics (BaVA) [22]. BaVA

is probabilistic, relying on an assumption of “a sampling distribution for the observed data

and an uncertainty over the model parameters” [17]. In contrast, V2PI is a deterministic

version of OLI and is the primary focus in our current research. As Endert et al. explain

in [17], this method can be used in conjunction with the “near = similar” metaphor where

users can express knowledge or test hypotheses by dragging points within the visualization.

When the user interacts with the visualization, this triggers an inverted computation of the

spatialization algorithm in order to determine what parameters for the algorithm produce

that layout. Once found, the system can record this set of parameters for future use and

rerun the spatialization algorithm to redisplay the data given these new parameters. The

resulting visualization gives the user immediate feedback on the given interaction, including

what data is similar or dissimilar to the moved data points. This continues to shield users

from the needing to understand these underlying mathematical models while still enabling

3

the user to alter these models and gain new insights.

While tools using V2PI have been developed [8, 16, 32], each has been designed to analyze a

speciﬁc type of data, using distinct visual metaphors and mathematical models. For example,

in [32], a Weighted Multidimensional Scaling (WMDS) algorithm is used to visualize raw

numerical high dimensional data. Additionally, only a small, ﬁxed set of data is visualized,

with no method of foraging for new data. Conversely in [8], text data is analyzed using

a Force Directed Layout in [8] and uses a multi-model approach to maintain thousands of

data points which can be foraged through. However, it is still unable to scale to the level of

millions or billions of data points. This work was later expanded to connect with Bing and

work with larger datasets [9]. However, the tool does not directly handle millions of data

points. All interactions must be translated to text queries to outside services. While this

is likely a necessary step in real world scenarios, not controlling this external service limits

possible experimentation on how to forage through large data sets. Semantic interactions

may result in a set of complex term or attribute weights, but translating this to a simple

text query results in a signiﬁcant loss of information with which to forage.

These existing analytics tools lack a method for experimenting with vastly diﬀerent types

of data, mathematical models, and visual encodings. Each implements a speciﬁc pipeline

to answer a small subset of research questions. This limits the pace at which research into

semantic interaction-enabled visual analytics tools can proceed.

To address this issue, we present a ﬂexible pipeline framework for creating prototypes of

visual analytic tools which use semantic interaction and V2PI. Our layered multi-model

4

interactive pipeline structure can be seen in Figure 1.1. We deﬁne the method of communi-

cation between diﬀerent models within the pipeline as well as between the pipeline and the

visualization. Thus, the models and data are modular components of the pipeline, with the

visualization being a separate modular entity outside the pipeline. This allows for simple,

separate modiﬁcations to the type and source of data being visualized, the models used,

and the visual encodings. Diﬀerent mathematical models can easily be tested with a single

visualization. Similarly, diﬀerent visualizations can be tested with few, if any, changes to

the pipeline. To demonstrate the power of this pipeline, we present several examples that

demonstrate the simplicity and ﬂexibility of creating new visual analytic tools with this

framework. With these diﬀerent prototypes, we will be able to research diﬀerent methods

for using semantic interaction and V2PI to support the user’s sensemaking process.

Figure 1.1: The pipeline framework.

Our main contribution is a new framework for quickly creating visual analytics tools that

support semantic interaction and V2PI. This framework captures both transforming the

raw data to the visualization and interpreting semantic interactions. We exemplify these

points through a set of prototypes we developed. Our goal is to accelerate progress in visual

analytics research by providing a platform for semantic interaction research and development.

More broadly we seek to expand the traditional visualization pipeline to realize a modern

vision of interactive visual analytics [13].

5

Chapter 2

Related Work

2.1 Semantic Interaction

The sensemaking process was originally deﬁned by Pirolli and Card [27]. As seen in Fig-

ure 2.1, it involves several steps. However, these steps can be grouped into two main phases:

the foraging loop and the sense-making loop, also know as the synthesis loop. The foraging

loop involves discovering new documents, while the synthesis loop involves generating and

testing hypothesis from discovered documents. Visualization tools can support this process

by using mathematical models to visualize the data. However, in traditional visualizations,

interaction is treated as an afterthought, typically involving directly tweaking the parameters

of these models, as represented in Figure 2.2. This means that analysts are required to have

an expert understanding of how the models work and the meaning of the model parameters.

6

7

Figure 2.1: The sensemaking loop from [27] illustrates the complex process of turning data

into useful insights. Used under Fair Use, 2016.

Semantic interactions address this issue by allowing analysts to alter model parameters by

interacting within the visual metaphors [8, 14, 16]. By merging the foraging and synthe-

sizing processes into a uniﬁed set of interactions, analysts can test hypotheses, express new

knowledge and forage for information without an expert understanding of the underlying

models [16]. This means that users do not need to distinguish between foraging and syn-

Figure 2.2: The traditional visualization pipeline only allows for direct interaction with the

algorithms and raw data.

8

Figure 2.3: The bidirectional pipeline from [8] allows users to interaction directly with the

visualization. Used under Fair Use, 2016.

thesizing in their interactions, keeping the sensemaking loop tight. However, when building

a tool that enables semantic interaction, the complexity of foraging and synthesis together

cannot be captured by a single model. Figure 2.1 illustrates how each of the foraging and

synthesis phases may require multiple models.

A multi-model approach to semantic interaction was presented in [8]. Here, Bradel et al.

deﬁned a visualization pipeline which uses separate models for the foraging and synthesis

processes. Additionally, to enable a feedback loop between the user and the visualization,

this pipeline is bidirectional, containing models to transform the data and inverse models

to interpret user interactions. This pipeline can be seen in Figure 2.3. After an interaction,

the system runs a complete iteration of the pipeline in which each inverse model updates a

single set of parameters to describe the user’s interest. This implies a codependence between

the models. Additionally, the data itself is not factored into this pipeline.

It is treated

as an initial input that never changes, limiting the types of data that can be visualized

and how new data can be brought in. This presents problems for experimentation with

diﬀerent types of data, models, or visualizations. Any changes to a piece of the pipeline

necessitates large changes elsewhere. To remedy this, we are seeking to improve this idea of

a bidirectional pipeline. By establishing more formal model entities which contain their own

set of parameters, we can reduce codependence between models.

9

2.2 Streaming Data

Streaming data provides many challenges to data visualization [11, 24]. When adding new

data to a visualization, many problems can arise, such as a sudden shift in data scale due to

a new maximum or minimum value, or a complete rearrangement of the data on the screen.

These issues can disrupt a user’s thought process and halt the ﬂow of the sensemaking loop.

Additionally, streaming adds a new temporal attribute to the data, and a decision must be

made on if and how to convey this temporal component.

Previous research has indicated that scatterplots, which are similar to the similarity based

layouts of this research, fare well with streamed data [24]. If the new data points are within

the current bounds, very little context is lost on the old set of points. If the new points

are outside the current bounds, more signiﬁcant loss of context may occur, but otherwise

the new data does not greatly harm the user’s sensemaking process. Unfortunately, this

research focused on non-interactive visualizations. In these cases, slight movement of current

data points when new ones arrive would not cause much issue to the user. However, once

interaction is added to the mix, even slight movement can cause problems. What if a user is

trying to select a point, and it moves right as they attempt to select it? Issues like this must

10

be carefully considered when designing interactive visualizations that support streaming

data.

While usability issues are a great concern, enabling streaming data from an architectural

standpoint is also nontrivial. Most analytic tools designed to study interactive visualizations

do not support true streaming data, where new data may appear in the visualization without

a user interaction occurring [8, 9, 16, 32]. Solving the architectural problem is a prerequisite

to studying the usability concerns. In this work we provide a solution for easily creating

visualization prototypes that support data streaming. By enabling this functionality, we

hope to simplify research of the usability problems related to data streaming in interactive

visual analytics tools.

2.3 Research Questions

We have generated a list of research questions as a long-term research agenda. The goal

of this framework is to provide a platform for creating prototypes and running user studies

to answer these questions. Each of the example visualizations created using this framework

address one of these issues and can be used to study them in the future. These questions are

divided into four main categories: data, models, visualizations, and interactions. For each

of these categories, our research questions include:

• Data

– How can we combine multiple types of data into a single semantic interaction-

enabled visualization?

– How can we support streaming data with semantic interaction?

11

• Models

– Which statistical methods would enable semantic interaction with big data in real

time?

• Visualizations

– What visual encodings are most easily understood by users?

– How can we stream data to the user without causing a loss of context?

– How can we best use an extra dimension when working with 3D visualizations?

– How can we combine multiple views in a single visualization?

• Interactions

– Which interaction methods best support the user’s sensemaking process?

– How can you interact with streaming data that is constantly updating the visu-

alization?

– How can semantic interaction be extended to 3D or immersive environments?

The limitations of current systems necessitates a method of creating ﬂexible pipelines to

answer these kinds of research questions. Although creating a new singular visualization tool

12

would enable us to address some research questions, it would follow the same pitfall of being

inﬂexible to new changes, prohibiting us from asking any further research questions with that

tool. Furthermore, it does not address the issue of the visualization being tightly coupled

to the pipeline. This creates a need for a decoupled pipeline that modularizes the data,

diﬀerent models, and the visualization. Separating these concerns enables experimentation

with various visualizations or input devices for the same transformed data as well as diﬀerent

methods of transforming the data using the same visualization.

By creating a modular pipeline framework, we enable fast prototyping for visual analytics

tools that aid the sensemaking process by using V2PI and other semantic interactions. Thus,

we facilitate researching tools that use diﬀerent data types, models, visualizations, or interac-

tions. We accomplish this by developing the framework with plug-and-play functionality. By

modularizing each piece of the pipeline, we can modify the pipeline to test research questions

related to data types and models without changing the rest of the pipeline. Additionally,

by establishing the pipeline as a separate entity from the visualization, we enable eﬃcient

research into diﬀerent types of visualizations and interactions using the same pipeline. The

details of this framework are explained in the following section.

Chapter 3

Pipeline Framework

Several key goals motivated our design of the pipeline framework. We need to allow for

multiple models, as previously discussed, without limiting the number of possible models.

These sets of models should also be able to operate on multiple types and sources of data.

Additionally, pieces of a pipeline should be reusable such that diﬀerent sets of models can

easily operate on the same set of data. These models also need a way to communicate so

multiple models can work together for greater functionality, and a suitable language for writ-

ing data processing and statistical algorithms must be supported. However, we do not want

to limit what languages visualizations can be implemented in, creating a need to separate

the visualization from the pipeline. Thus, our framework must be able to communicate with

any external program.

Finally, our framework should simplify creating visual analytics tools for large datasets.

13

14

Previous works have demonstrated that interactive visualizations cannot scale past tens of

thousands of data points while remaining responsive [8, 15, 16]. This is due to the retrieval

model having to calculate a relevance for every data point upon each interaction that updates

the model, which becomes an expensive process as the size of the dataset grows. While more

eﬃcient algorithms, more powerful hardware, and intermediary dimensionality reduction

techniques can increase the number of data points that can be handled, there will always be

a limit to what can be done in real time. In order to work with datasets expanding into the

millions and billions of data points, some processing must be done in the background. The

framework should assist developers by hiding as much of the threading issues as possible.

Figure 3.1: An instance of our pipeline consists of a Data Controller, series of Models, and

Connector. The visualization is a separate entity from the pipeline. The solid arrows indicate

the forward ﬂow in the pipeline, while dotted arrows indicate the inverse ﬂow. Dotted arrows

within the Models indicates the ability to short circuit.

These design goals led us to a pipeline made up of three key pieces: a Data Controller, a

series of Models, and a Connector. Figure 3.1 illustrates this pipeline and how the pieces

ﬁt together. We chose to implement our framework in Python since it is widely used in

15

data and text processing, with a vast array of available libraries and packages for doing

so. Python is also a quick language to prototype with, matching the motivation for this

framework. The pieces of our pipeline are fairly simple, but the independence of each module

and communication deﬁned between them provides powerful functionality with little eﬀort.

This chapter describes the Data Controller, Models, and Connector, how they communicate,

and how they were modiﬁed to allow for asynchronous behavior. In the following chapter

we describe our base implementation of a pipeline for sensemaking visualizations, as well as

several variations, to illustrate these diﬀerent pieces.

3.1 Data Controller

The Data Controller serves as the main access point to the underlying data that is being

visualized. Its key purpose is to serve as a method to retrieve the raw data as well as any

possible metadata. This can enable users to view the raw data directly or allow the pipeline

to pull additional data to process and visualize. Diﬀerent Data Controllers can be developed

to work with diﬀerent types of data, enabling fast prototyping with diﬀerent types of data

by merely swapping out the Data Controller. The Data Controller can also perform any

necessary preprocessing of the data before passing it down the pipeline. For example, a

Data Controller could read in raw text documents and calculate a term frequency vector.

3.2 Models

16

The Models are the next piece along the pipeline. This section can contain multiple Models

placed in series between the Data Controller and the Connector, each containing a forward

algorithm and an inverse algorithm, and an internal set of parameters. The forward algo-

rithm deﬁnes a speciﬁc method for how the data is processed as it works its way to the

visualization. Thus, these forward algorithms rely on other Models that lie closer to the

Data Controller in the pipeline. In contrast, the inverse algorithm of each Model interprets

the user’s interactions and updates the Model’s parameters accordingly. This means that

inverse algorithms can receive parameter changes and results from Models that are closer

to the Connector. These updates to the Model are then reﬂected in the output during the

next run of the forward algorithm. Parameters in one Model are not directly accessible in

others, but a Model can choose to share its parameters using the communication mechanism

described below in Section 3.4.

These algorithms are run in response to interactions by the user. An iteration of the pipeline

starts by running the inverse algorithm of the Model closest to the Connector. All subsequent

inverse algorithms are then run in series, after which the forward algorithms are executed.

These forward algorithms use the updated parameters from the inverse algorithms to provide

updates to the visualization. After all forward algorithms have completed, the results are

sent on to the visualization.

One important feature of the Models is the ability to short circuit the rest of the pipeline.

17

Short circuiting happens when the inverse algorithm of a Model doesn’t need to send the

interaction any further up the pipeline. Thus, instead of running the entire pipeline, we

short circuit, executing the forward pipeline beginning at the current Model to update the

visualization. This is represented by the dotted upward arrow within each Model in the dia-

grams. This is a key new feature of a multi-model pipeline not found in earlier deﬁnitions [8].

There are two main reasons for short circuiting. A Model may interpret an interaction and

want to provide immediate feedback for the user without executing any more Models. Ad-

ditionally, once the pipeline starts to expand and include large datasets, it is infeasible to

iterate through the entire pipeline upon each interaction. Models for diﬀerent scales of data

can decide whether they need additional data to fulﬁll the request or already contain enough

relevant data. Speed and response time become limiting factors when working with large

datasets, and every possible way of improving this aspect of usability becomes critical.

On the other hand, there is no opposite of short circuiting within the framework, or an arrow

downward within a Model from the forward algorithm to the inverse algorithm. The logical

ﬂow always transitions from the inverse algorithms to the forward algorithms, never the other

way around. There should be no need for this ﬂow, as inverse algorithms should solely be

interpreting user interactions. Without a new interaction from the user, there should be no

need to execute an inverse algorithm again. Likewise, a forward algorithm should not need

to request new data from a forward algorithm earlier in the pipeline, and instead just work

with the data it is given. This type of circular communication would greatly complicate the

framework and creating new pipeline instances without much foreseeable beneﬁt.

3.3 Connector

18

The ﬁnal piece of the pipeline is the Connector. The purpose of the Connector is to mediate

messages between the pipeline and the visualization. To do so, the Connector must have a

way to receive and respond to three types of messages from the visualization: Update, Get,

and Reset messages. The Update message signals that the user has performed some type of

interaction that warrants an execution of the pipeline, starting with the inverse algorithm

of the Model closest to the Connector. An Update message may make its way through the

entire pipeline, all the way back to the Data Controller, or it may get short circuited by one

of the Models along the way. The second type of message is a Get message, which is sent

in response to a user’s request for more information about a speciﬁc data point. These are

sent straight to the Data Controller for retrieving raw data or metadata directly, which is

then returned straight to the visualization. Finally, there is a Reset message that signals all

the Models and the Data Controller of the pipeline to reset back to their initial state.

The Connector is a very simple piece of the pipeline, yet it allows for greater ﬂexibility

in deciding where and how to run the pipeline. With a networked messaging system, the

visualization itself could be implemented with any type of language or toolkit that supports

the messaging protocol deﬁned in the Connector. Furthermore, the pipeline could be run on

a separate computer from the visualization itself. This is another key enabling feature for

expanding to large datasets that most other semantic interaction tools don’t use [8, 16, 31,

32]. Regardless of the eﬃciency of the algorithms, there is a limit to the processing speed

19

and data storage available on personal machines. To account for this, the pipeline could

be run on high performance machinery, while the user interacts with the visualization on a

personal computer. Oﬄoading the pipeline to these high performance computers will enable

interactions with larger datasets not previously feasible.

3.4 Communication within the Pipeline

We have deﬁned the pieces that make up the pipeline, but there needs to be a form of com-

munication between them. This communication is controlled by the pipeline and enables the

modularity and ﬂexibility provided by the framework. The various components communicate

with each other through a JSON-like data blob maintained by the pipeline. Since the entire

pipeline currently exists as a single program, this object is simply passed in memory with

Model algorithms implemented as functions operating on this object. Models can choose to

interact with one another by operating on the same elements, or keys, within this data blob.

To ensure that a pipeline has been properly constructed, each Model speciﬁes requirements

for its forward and inverse algorithms. These requirements are speciﬁed as keys within the

data blob, and are then checked against the output of previous algorithms. If the input re-

quirements of all algorithms are met, then the pipeline is valid. Checking for a valid pipeline

prevents unwanted behavior caused by incorrectly constructed Models.

Figure 3.2 illustrates how this communication works. In (a), we have an example Sum Model

that takes in a list of values and under the values key, calculates their sum, and adds it to

20

the data blob under the sum key. In (b), instead of adding a new key to the data blob, we

modify an existing one. The Square Model takes a list of values and squares each of them

in place. These are both valid methods for enriching data as it passes through the pipeline,

and this same behavior is used for both the forward and inverse algorithms. This allows

each Model to contain just the logic required for transforming the data, and not the logic

necessary to communicate with other pieces of the pipeline.

Figure 3.2: Two examples of how a Model can modify the data blob passed through the

pipeline. In (a), a new element is added, while in (b), a current element is modiﬁed.

With the communication deﬁned, we can see how this framework ﬁts into a model-view-

controller (MVC) design pattern [18]. Clearly the Models within the framework directly

translate to the model in the MVC design pattern. The Data Controller would also be

considered part of the MVC model. The visualization itself acts as the view, and provides

the controls to interact with the models. The controller exists as part of the visualization

itself providing the interactions, as well as the framework for mediating communication from

the visualization to all of the Models and the Data Controller.

3.5 Asynchronous Processing

21

The presented framework works well for creating simple visualizations that respond to user

interactions and work with small, static data sets. However, research into these types of tools

is already well covered [8, 14, 15, 16, 23, 32]. What these previous works fail to adequately

address are visualizing large data sets and dynamically streamed data [11, 24]. To make

meaningful contributions to the study of visual analytics tools, this framework must support

these endeavors in some way. This is accomplished through two additions to the framework

enabling certain asynchronous behaviors. While multithreading code will undoubtedly need

to be included in any complex Models, the approaches taken here aim to reduce as much as

the multithreading burden as possible.

3.5.1 Asynchronous Models

Figure 3.3: A pipeline with asynchronous Models.

In order to provide simple background processing capability, a new asynchronous type of

Model was created. The structure of a pipeline using asynchronous models can be seen

in Figure 3.3. By grouping a series of asynchronous Models followed by a series of non-

22

asynchronous Models, we create a simple background threading system. Upon an interaction,

the non-asynchronous Models are executed as previously described. When an asynchronous

Model is reached along the inverse path, the input data containing the interaction and any

new Model parameters are added to a queue. The forward algorithms, starting with the

ﬁrst non-asynchronous Model, are then immediately run, and the visualization is notiﬁed of

any updates. In a separate thread, the asynchronous Models are run in a similar manner to

the original pipeline. Each inverse algorithm is executed in series until the Data Controller

is reached or a Model short circuits. The forward algorithms are then run until the ﬁrst

non-asynchronous Model is reached. The forward algorithm of this ﬁrst Model is run using

the data passed down through the asynchronous Models, and the background thread then

takes the next item oﬀ the queue and iterates again.

This approach enables background processing without requiring any threading code within

any of the Models. A lock controls access to the synchronous portion of the pipeline, and

the asynchronous part is still run sequentially, preventing any threading issues. While back-

ground processing could be done within a speciﬁc Model, it would require the developer to

write all the multithreading code themselves. While this is still possible, and necessary for

complex data processing, the behavior described here can enable simple pipelines to scale up

to large sizes of data without requiring any new threading code. The main drawback of this

approach is the updates from each interaction may not contain the most up to date data.

The synchronous part of the pipeline must return a response back before the asynchronous

portion completes, meaning no new data resulting from the asynchronous Models will be

returned in the update. This new data may only be available for the next interaction. But

this is a fair tradeoﬀ for requiring no threading code during pipeline development.

23

3.5.2 Pushing Data

While the previously described technique provides support for larger data sets, it is still

limited to responding to user interactions to update the visualization. What if we obtain

some new data that is extremely relevant to the user, such as a tweet that was just published

or an email that was just intercepted? We would have to wait for the user to initiate some

form of interaction with the visualization in order to provide them with this new data. The

framework so far described is pull oriented, in that the front-end initiates updates and pulls

new data from the pipeline. By enabling pushing, the pipeline can actually update the

visualization on demand.

A push can originate from either the Data Controller or from a Model, and is as simple as a

single function call. When data gets pushed, it is sent through all or a portion of the forward

pipeline, just as data would during an update triggered by a user interaction. Models need

not know whether they are processing push-generated data or interaction-generated data.

Once the forward pipeline is complete, the results are pushed to the client.

In order for this technique to be possible, the Connector must support pushing data as well.

Certain approaches to implementing a Connector may make this impossible, such as those

based on RPC that can only respond to outside requests. We will see in the next chapter that

24

we had to create a new Connector for our data streaming pipeline. Additionally, pushing

data involves running the pipeline over multiple threads. While there is a pipeline lock that

protects the synchronous portion, the asynchronous models must be sure to include their

own locking mechanisms to protect their data. Algorithms of an asychronous model could be

run multiple times concurrently due to multiple pushes, or from a push and the asychronous

background thread described previously.

Chapter 4

Pipeline Implementations and

Visualizations

Here, we illustrate some fully implemented pipelines, along with corresponding visualizations

that work with the pipelines. A key goal of the pipeline structure is allowing the visualization

and data processing to run on separate machinery. With the recent trends of transitioning

from native applications to web applications [21, 34], we saw a great opportunity to move vi-

sualization research to the web, and mature web-based graphical frameworks enable complex

and detailed visualizations in the browser [5, 12, 36].

Utilizing these frameworks requires a server to serve these web pages. This resulted in the

creation of the visualization controller, which together with the served web pages make up

a client to the pipeline framework. A server was not implemented within the framework

25

26

Figure 4.1: The visualization controller mediates communication between multiple pipelines

and visualization clients. A single visualization can be mirrored across multiple clients,

represented by the red arrows.

itself to maintain generality with the pipeline. While web technologies are the direction we

chose to head in, native applications could just as easily interact with the pipeline framework

through any networking protocol. The visualization controller has many practical beneﬁts as

well. WebSockets [36] allow visualizations to be mirrored to multiple users simultaneously,

enabling new topics of research through collaboration [28]. Additionally, the visualization

controller does not have to be collocated with the pipeline instances, allowing separate

hardware for the web server and pipeine data processing. Finally, in the future it could be

used to mediate access to diﬀerent data sources and visualizations for each user.

Figure 4.1 illustrates this setup. A single visualization controller can serve multiple visual-

izations to users, and each visualization results in the creation of a new pipeline instance.

27

The pipeline instance created is tied to the visualization used, as the pipeline outputs must

match what the visualization expects. We chose to implement the visualization controller

using Node.js [26]. This controller communicates with the pipeline in a couple diﬀerent ways,

described along with the Connectors for each pipeline below.

Adding in this visualization controller does not change how the whole system relates to the

MVC design pattern. The visualization controller mostly acts as an intermediary that passes

messages between the pipeline and the visualization. The main end points of the controller

remain the visualization itself and the pipeline framework.

The rest of this chapter details the pipelines we have created and the visualizations that go

along with each pipeline.

4.1 Text Analysis Pipeline

The ﬁrst pipeline we created aims to aid the sensemaking process with a set of text docu-

ments. It combines the relevance-based retrieval model from [8] with the similarity-based

layout model from [32]. The structure of this pipeline can be seen in Figure 4.2.

4.1.1 Data Controller

For this pipeline, we created a simple CSV-based Data Controller that loads a prepro-

cessed generic high-dimensional data. To work with text documents, we created a Term

28

Figure 4.2: Our base pipeline implementation.

Frequency-Inverse Document Frequency (TF-IDF) matrix of our set of documents for this

high-dimensional data, where each term represents an attribute or dimension in the data.

This numerical data for the text documents allows us to mimic the behavior in [32] to spa-

tialize the data. The TF-IDF metadata is passed down the pipeline to the Models for further

processing. Our Data Controller also has references back to the underlying raw text, which

the pipeline gives to the user when requested using the Get method. For example, a user can

open and view documents within the visualization, giving the user direct access to the raw

text. This ability to retrieve the raw text is the only part of the entire pipeline that makes

it speciﬁc to text. The Models described below have no requirement for working with text

data, and simply work with numerical high-dimensional data. Later we will demonstrate an

alternative use that does not work with text data.

Upon load, this Data Controller loads the data from the speciﬁed CSV ﬁle, and sends the

data down to the Models. Because it is designed to work with small to medium scales of

data, it does not interpret any user interactions or updated Model parameters to provide new

documents upon an iteration of the pipeline. Instead, it expects the Models in the pipeline

to keep track of the documents after load, and simply listens for Get requests to retrieve

the raw text of a speciﬁc document. Additionally, a diﬀerent Get request can be used to

retrieve the names of the attributes, which in the TF-IDF case is the terms representing

29

each dimension.

4.1.2 Models

This pipeline consists of two models: a Relevance Model and a Similarity Model.

Relevance Model

Our Relevance Model draws much of its functionality from [8]. It maintains a set of weights

for the attributes in the data, representing how relevant the user thinks each of those at-

tributes are. Interactions such as text queries, node deletion, and increasing the relevance

of a document are interpreted by our Relevance Model to make changes in these attribute

weights. Additionally, to prevent overwhelming the user by displaying too many data points

at once, our Relevance Model also maintains two lists of documents, which we call the Active

Set and the Working Set. The Active Set may scale up to thousands of documents that are

pulled from the Data Controller, and the Working Set contains only the most relevant docu-

ments from the Active Set. By limiting the Working Set to dozens of data points, it remains

small enough to be visualized. Maintaining these separate lists in a Model allows the Data

Controller to focus on responding to Get requests instead of maintaining any parameters

30

itself.

This Model has two key functions. First, if new documents are passed into the forward

algorithm, we insert them all into the Active Set and the most relevant ones into the Working

Set. This relevance is calculated as a dot product between the current attribute weights and

the high dimensional attributes for each document. After this, the relevance of all pre-

existing documents in the Working Set is recalculated. This Working Set, along with the

relevance for each document, is passed through the forward pipeline.

The inverse algorithm focuses on looking for interactions that will inﬂuence our relevance

weights. The interactions currently implemented include text queries, changing the relevance

of a document, and deleting a document. With text queries, all attributes containing the

query are upweighted by a constant. When the relevance of a document is altered by the user,

the weights for attributes present in that document are updated based on the magnitude

of the change and the values of the attributes in the document’s high dimensional data.

Finally, deleting documents downweights all attributes present in that document. After any

of these interactions, the inverse algorithm uses the new set of attribute weights to search

the Active Set to see if any new documents should be included in the Working Set. If no new

documents are found, the interaction and new set of weights are passed along toward the

Data Controller to pull more documents into the Active Set. Otherwise, this Model short

circuits and goes straight to the forward algorithm.

A key aspect is that the Relevance Model does not care about the source of the data. New

documents are pulled into this Model from the Data Controller, or other Models closer to the

31

Data Controller. When the Active Set does not contain any new documents to satisfy the

request, the interaction continues further inverse Model algorithms or the Data Controller,

which can use the interaction to ﬁnd new documents however it chooses. Additionally,

these Models or Data Controller can use the set of attribute weights calculated with the

Relevance Model’s inverse algorithm for its document ﬁnding. This enables interactions

such as the changing of a documents relevance to be used by Models that do not interpret

such interactions, as the Relevance Model has transformed the interaction into a more usable

form.

Similarity Model

The Similarity Model’s role is to layout documents according to their similarity, and its

logic is based on [32]. This is done using Weighted Multidimensional Scaling (WMDS), a

form of dimensionality reduction, on the high dimensional data passed down the pipeline.

The Similarity Model stores a set of attribute weights, one for each dimension in the high

dimensional data. The forward algorithm uses these weights to project the high dimensional

data down to a lower set of dimensions. This dimensionality reduction is performed by

optimizing the location of each point in the low dimensional space so that it minimizes the

stress between all pairs of points. Stress in this case is deﬁned as the diﬀerence between

the distance of two points in high dimensional space and in low dimensional space. This

optimization is captured by the equation:

n(cid:88)

n(cid:88)

i=1

j>i

r = min
r1,...rn

|distL(ri, rj) − distH(w, di, dj)|

32

where r is the low dimensional position of each point, d is the high dimensional position of

each point, n is the total number of points, w is the set of weights over the high dimensional

space, distL returns the distance between two points in low dimensional space, and distH

returns the weighted distance between two points in high dimensional space.

The inverse algorithm updates this set of weights over the high dimensional space based

on OLI interactions within the visualization. This occurs when the user moves data points

within the visualization, asserting some knowledge they have that certain data points are

either more or less similar than the visualization indicated. The new low dimensional po-

sitions of these moved points are then used by an optimization algorithm described in [33]

to create a new set of similarity weights to describe the user’s layout. This optimization is

captured in the following equation:

n(cid:88)

n(cid:88)

i=1

j>i

w = min
w1,...wn

|distL(r∗

i , r∗

j ) − distH(w, di, dj)|

Here r∗ is the new low dimensional position of a point as supplied by the user. This new set of

weights is then used on the next forward projection of the data. When this interaction occurs,

the Similarity Model short circuits the pipeline to immediately update the visualization based

on the new similarity weights.

33

4.1.3 Connector

For our initial pipeline implementations we used zerorpc to communicate between the visu-

alization controller and the pipelines. zerorpc [38] a Remote Procedure Call (RPC) imple-

mentation of ZeroMQ, an asynchronous messaging library for distributed applications [37].

This Connector creates a zerorpc server with RPC bindings for the Update, Get, and Reset

messages required of a Connector. The Node.js server then connects as a zerorpc client to

the pipelines it creates, establishing communication between the two.

4.1.4 Visualization

For our ﬁrst prototype, we decided to make a simple web-client using D3, which is a

JavaScript library that enables creating interactive visualizations within a web page [12].

As shown in Figure 4.3, this interface’s main feature is an interactive graph that allows

documents to be displayed as data points. Initially, this graph is empty, requiring the user

to search for a term. Searching sends an Update message with the query to the pipeline.

The results from this query cause the documents to be displayed as data points in the graph

using a “near = similar” metaphor. With the graph populated, the user can use OLI, ex-

pressing knowledge or testing hypotheses by moving the data points within the visualization.

After the points are moved to their desired locations, the user then clicks the Update Layout

button to send a type of Update message to the pipeline along with the coordinates for the

moved points. This information is used by the Similarity Model to determine how to replot

34

Figure 4.3: An initial 2D web visualization showing the general layout of the graph and data

ﬁelds as well as the visual encodings used in the graph. Points are plotted on the screen

using an interactive WMDS algorithm. Moved data points are highlighted in green, while

data points that are selected to view the raw text data and metadata are highlighted in

purple.

the points. Thus, the user receives immediate visual feedback on the interaction. When the

user wishes to return to the interface’s initial state, the Reset button can be pressed. This

causes a Reset message to be sent to communicate the user’s action to the pipeline, resulting

in all Models resetting their data as well.

By double clicking on a data point, the user can view the raw data and some metadata for

the document corresponding to that point. This interaction causes the data ﬁelds to the

right of the graph to be populated using a Get message. The Data Controller responds to

this message directly without having to run an iteration of the pipeline. The data ﬁelds show

35

the data point’s label, the relevance of that document, the raw text from the document, and

any notes that the user takes. Although the user cannot interact directly with the text, the

label, relevance, and note ﬁelds can be updated. Our current implementation only has the

pipeline respond to an update in the document relevance, which is handled by the Relevance

Model via an Update message. Additionally, after a point is double clicked, the user can

also choose to delete that node from the graph, indicating that this document is no longer

relevant to them. This is reﬂected in the pipeline through a type of Update message, which

is also handled by the Relevance Model.

Use Case Scenario

Here we present an example scenario demonstrating the features of this initial prototype. We

have a small dataset containing a hidden terrorist plot spread over a few dozen intelligence

collections, such as intercepted phone calls and emails. This visualization is initially empty,

so we need to have some intelligence to begin our analysis. In our example, we have heard

that a man named Mr. Ramazi might be involved in a potential terrorist plot, so we begin

by searching for his name. This search returns ﬁve documents relating to Mr. Ramazi. After

reading through them, we ﬁnd one document indicating that a Mr. Hallak had withdrawn

some money from Mr. Ramazi’s account. To remember what this document contains, we

rename it accordingly. Figure 4.4 demonstrates the state of our visualization after the initial

search about Mr. Ramazi and discovering and renaming the document relating to Mr.

Hallak.

36

Figure 4.4: The initial search for Mr. Ramazi led us to ﬁnding Mr. Hallak has withdrawn

some money from him.

After this discovery, we decide we want to learn more about Mr. Hallak. By moving the

Relevance slider to the right, we indicate that this document is relevant to us and we want

more documents like it. Several new documents appear, one of which contains information

about C4 that was found at a shop owned by Mr. Hallak. This helps us conﬁrm that this

Mr. Hallak is likely involved in terrorist activities of his own, and might be involved with

Mr. Ramazi. We rename this document to remember it’s contents as seen in Figure 4.5.

We continue reading the new documents relating to Mr. Hallak and ﬁnd someone has made

phone calls to several numbers including Mr. Hallak from a 718 area code. This may give

us some link to others involved in a plot, so we rename this node to remember it’s contents

and search for more documents relating to the 718 area code, as seen in Figure 4.6.

The search for the 718 number results in a couple new documents appearing on the screen.

37

Figure 4.5: Mr. Hallak appears to be involved in terrorist activities.

Opening one of them we ﬁnd another document with phone call information. This time, it

is someone making a phone call to this same 718 number. This gives us a series of phone

calls that may be connected, but we have no way of knowing if this is coincidental or related

to a potential terrorist plot.

We now have examined four diﬀerent documents, two regarding a list of phone calls and

two about Mr. Hallak unrelated to his phone calls. We want to know if there are any

other documents on the screen that might provide more insights about these phone calls,

so we perform an OLI interaction. By dragging the phone call-related documents together

in one corner, and those unrelated to the phone calls in the other corner, we can tell the

visualization to emphasize how these pairs of documents are similar and dissimilar when

laying out all other points. This interaction can be seen in Figure 4.7.

We execute this OLI interaction by clicking the “Update Layout” button on top. This lays

38

Figure 4.6: Someone has made phone calls to Mr. Hallak from a 718 number, so we search

for more documents relating to this area code.

out the points in a new way, and one of the documents ends up close to the phone number-

related documents. This can be seen in Figure 4.8. We ﬁnd out more about these same

phone numbers, including a message relayed to each translated to “I will be in my oﬃce on

April 30 at 9:00 AM. Try to be on time.” While not clear proof of anything, it may be some

form of code that we should continue looking into. And we now have several other phone

numbers to investigate.

This initial analysis has highlighted the key interactions available within this prototype. The

remainder of this chapter explores variations of this initial prototype we have developed.

39

Figure 4.7: Initiating an OLI interaction to learn more about the phone numbers.

4.1.5 Alternative Visualizations

Here we demonstrate diﬀerent visualizations that make use of this same pipeline.

Radar Visualization

As an alternative mapping for similarity and relevance in a visualization for data foraging,

Ruotsalo et al. propose the Intent Radar [31]. Within this interface, documents are mapped

onto the radar based on their relevance to the user’s searches and their similarity to each

other. More relevant documents will be closer to the center, while similar documents will

have a similar angle around the radar. After performing a user study, Ruotsalo et al. found

that this new interface enabled users to search through the data more quickly and eﬃciently

than interacting with a list of keywords or traditional query searching.

40

Figure 4.8: The results of the OLI interaction led us to discover another document relating

to these phone numbers.

To implement this idea with our pipeline is simple, requiring a new visualization to sit at the

end of the pipeline and a simple change to how we use similarity and relevance. Since our

pipeline already modularizes similarity, the only necessary change to the pipeline is altering

the Similarity Model so that data points are mapped to one dimension instead of two. Our

original 2D implementation was adapted to handle the speciﬁc mapping of the data of the

visualization, translating the relevance metric for each document to the distance from the

center and the similarity metric to the angle. This prototype can be seen in Figure 4.9.

3D Visualization

While the richness of 2D web visualization is growing, so also are the capabilities for interac-

tive 3D visualizations. As we explore new sensemaking interfaces and data exploration tools,

41

Figure 4.9: An alternative 2D web visualization. This visualization maps relevance to the

distance from the center of the radar and similarity to the angle. All other visual encodings

are pulled from the original 2D web visualization.

we are interested in the representations and aﬀordances 3D environments can provide. To

extend our research to 3D, we used the ideas from our 2D visualization to create a new 3D

visualization. We accomplished this by replacing the D3 code in the 2D visualization with

X3DOM. X3DOM is a framework developed by the Web3D Consortium that enables the

display 3D content natively in the browser. To do so, X3DOM uses Extensible 3D (X3D),

the ISO/IEC standard XML format for interactive 3D graphics, as part of HTML5 Docu-

ment Object Model (DOM) [5]. Since the 3D objects are part of the DOM, they can be

manipulated using mouse and keyboard events using HTML5/JavaScript.

The 3D prototype is shown in Figure 4.10, which only diﬀers from the original 2D prototype

by plotting the data points in the graph using X3DOM and contains minor changes to the

42

Figure 4.10: A screenshot of the 3D web visualization. The layout of the web page is based on

the layout in the original 2D visualization. With minor changes to the base implementation

of the pipeline, we are able to graph the data points represented as spheres in 3D.

CSS. In this new graph, the data points are represented as spheres instead of circles where

the radius of the sphere is proportional to the relevance of the document represented by

the sphere. To plot the data points in this 3D space, the Similarity Model was conﬁgured

to project to three dimensions instead of two. This parameter is set from the visualization

controller and required no changes in the pipeline itself.

In this new prototype, we also experimented with diﬀerent interaction options.

Instead

of double clicking a node to populate the data ﬁelds to the right of the graph, we chose

to use right clicking. To handle new interactions for exploring the 3D space, there are also

buttons to view the scene in multiple angles. These diﬀerent angles include the Top, Bottom,

Front, Back, Left, and Right views. The “Reset View” button resets the view to the default

viewpoint. Apart from these, all other interactions are pulled directly from the original 2D

43

Figure 4.11: A 2D web visualization with attributes mapped in the same graph as the data

points. Attributes are yellow, whereas data points are red. All other visual encodings are

pulled from the original 2D web visualization.

prototype.

4.2 Displaying Attributes

With our base implementation, we are able to quickly implement new ideas by altering the

pipeline in small ways. One example of this comes from a concept introduced by the Data

Context Map developed by Cheng et al [10]. The Data Context map provides a method for

visualizing the attributes within a given data set alongside the data points using an MDS

projection. To accomplish all this, the typical distance matrix used in MDS calculations

is augmented with additional data. In standard MDS calculations, the pairwise distances

44

between data points are calculated to create a square matrix of these distances. Additionally,

the Data Context map also calculates matrices for the pairwise distances between attributes,

the distances between data points and attributes, and the distances between attributes and

data points (i.e. the inverse of the previous matrix). These additional matrices are combined

with the original matrix, creating a new square matrix of pairwise distances between all data

points and all attributes that is used to visualize attributes in the same space as the data

points.

To demonstrate the plug-and-play nature of models in the pipeline, we modiﬁed the base

implementation to fuse data points and attributes into a single display. Using the approach

laid out in [10], we were able to create a new Model that expands the behavior of the

Similarity Model in our original pipeline to achieve this behavior. This alteration takes the

pairwise distance matrix already generated and computes a composite distance matrix, which

includes distances between points and attributes as well as distances between attributes

themselves. This composite distance matrix is then treated like any other pairwise distance

matrix in the MDS algorithm. We simply specify that point as an attribute so that the

visualization can represent it diﬀerently than the data points. By deﬁning this behavior in a

separate Model, we can easily switch between this new visualization and the original without

the attributes. Figure 4.11 shows our original 2D visualization with the addition of these

attributes.

The ﬂexible pipeline allowed us to create this fused display extremely quickly. But that is

not its only beneﬁt. In addition to simply reimplementing the idea of a composite distance

45

Figure 4.12: A pipeline that connects to an external search engine and calculates term

frequencies dynamically.

matrix, the nature of the pipeline makes it interactive. This can greatly aid the data foraging

process by helping layout the space. It may be diﬃcult for users to understand why the

dimensionality reduction algorithms laid out the data points the way they did, but plotting

most relevant attributes appear alongside the related data points may help users with this.

4.3 Multi-source Text Analysis

The examples presented thus far have demonstrated the visual analysis of static data sets.

While this can be useful for experimenting with diﬀerent visual encodings, the types of models

used and interactions implemented do not translate well to real world analysis scenarios.

Here we present a pipeline for analyzing text documents stored in an external database. The

pipeline for this example can be seen in Figure 4.12.

46

4.3.1 Data Controller

A new Data Controller was created to connect to an external search engine, rather than load

local data on startup. In order to have complete control over this search engine, we chose

to implement our own basic text search engine using Elasticsearch [19]. After connecting to

our Elasticsearch instance, the Data Controller can retrieve documents in two ways. First,

through simple text queries. If a text query interaction arrives at the Data Controller, this

query will simply be forwarded to the search engine to retrieve new documents. The other

method is through a set of attribute weights that have been calculated from one of the inverse

Models. In this case, the Relevance Model can interpret interaction such as increasing the

relevance of a document within the visualization, to update the set of relevance weights that

control which documents are currently displayed. These same set of weights can be used to

query the search engine by simply selecting the most relevant words to search for.

By working with an external service, we eliminate the static data limitation. Because we

constantly issue new queries to the search engine, we can add new documents into the store

while the pipeline is running. Interactions that occur after these documents are added may

then pull in these documents into the visualization.

4.3.2 Models

Because all the data is not known ahead of time, little preprocessing can be done. Term

frequency data must be calculated dynamically within the pipeline. This requirement led to

47

the creation of a new Model within the pipeline, the Term Frequency Model. It’s behavior

is simple: it takes in the raw text for documents and converts them to a normalized term

frequency. This is done by calculating the raw counts of each term in the text, after some

preprocessing to remove common words, and then dividing each count by the count of the

most frequent word in the document. This means that the most common word in a document

will have a frequency value of 1, and all others will be less than or equal to 1. By calculating

term frequency this way, we enable comparisons between documents of diﬀerent lengths. For

example, tweets and news articles vary greatly in their length, and would have vastly diﬀerent

scales of term frequencies. But normalizing it this way allows us to make comparisons

between these two types of text data.

Additionally, the Relevance and Similarity Models required slight changes to work with this

new type of dynamic data. These Models were initially designed to work with data containing

a static set of attributes. When you have all of your data that you will visualize ahead of time,

this makes sense. However, with this type of visualization where you have new, unknown

data coming in, the set of attributes will change and potentially grow drastically over time.

These Models were modiﬁed to account for this, and in fact are completely compatible with

the ﬁrst pipeline presented, requiring only one small change to the CSV Data Controller.

48

Figure 4.13: A pipeline that pushes data asynchronously from the Data Controller.

4.4 Streaming Data Analysis

Finally, we present an example that demonstrates asynchronous push behavior. This feature

allows us to research interactive visualizations of truly streaming data, where the visualiza-

tion can update itself with new data without an interaction occurring. In this example, we

stream live tweets to the visualization, based on ﬁlters set by the user. The Models required

no further changes, and only a new Data Controller and Connector had to be created. This

pipeline can be seen in Figure 4.13.

4.4.1 Data Controller

The Data Controller for this pipeline does not load any data at startup. Rather, it initiates a

connection to Twitter using the Tweepy package [35], and waits for an interaction to indicate

what tweets it should ﬁlter for. When an interaction arrives to set the words to track, the

Data Controller sends the request to Twitter to begin sending any tweets matching the

ﬁlter. When each tweet arrives, it is processed to eliminate common words, punctuation,

49

and URL’s. Term frequency values are then calculated for each word present in the tweet,

and this data is stored in a list within the Data Controller. After a certain number of tweets

have arrived or a certain amount of time has passed, which are both tunable parameters,

the collected tweet data is asynchronously pushed down the pipeline. This push mechanism

described in the previous chapter is the only way new data gets sent out of this Data

Controller, as only live tweets are considered.

4.4.2 Connector

A new Connector had to be created for this pipeline to enable the push functionality. The

initial RPC based Connector was only able to respond to requests from the visualization

controller, so no updates to the visualization could occur without an interaction. This new

Connector is built directly on ZeroMQ’s PAIR-PAIR socket messaging pattern [37]. A socket

is created on both the Connector and the visualization controller, with the former acting as

the server and the latter connecting as a client. Messages are then sent using JSON strings to

communicate the type of request (Update, Get, or Reset), and the corresponding arguments

to the request. With this, the Connector can send an Update message to the client without

requiring a request to respond to, enabling asynchronous pushes of data.

50

Figure 4.14: A visualization for streaming tweets. A new text box on the top lets users set

a ﬁlter for tweets to pull in.

4.4.3 Visualization

The visualization client for this pipeline can be seen in Figure 4.14. It is nearly identical to

the ﬁrst visualization presented in this chapter, as it uses the same Models to transform the

data and interpret interactions. The main addition is the second text box on the top panel

that allows for input of a tweet ﬁlter. Submitting a ﬁlter triggers the interaction interpreted

by the Data Controller to initiate a collection of tweets matching the ﬁlter. These tweets will

then be asynchronously pushed through the forward pipeline as they arrive, and the user can

use the same set of interactions previously described to start organizing and making sense

of the tweet space. Additionally, the default label for each data point is the contents of the

tweet since they are limited in length.

51

This example demonstrates how the architectural problems of working with streaming data

can be solved with the pipeline framework. Any module can listen for new incoming data

and update the visualization accordingly. However, asynchronously updating an interactive

visualization can cause serious usability issues.

If data is constantly moving around the

screen, how can you interact with it? Producing prototypes of diﬀerent streaming visual-

izations will be necessary to solve the usability problems that go along with it. By solving

the architectural problem, researching solutions to the usability problem will become much

simpler.

4.5 Raw High Dimensional Data Analysis

The previous examples were designed for visualizing and analyzing text data. They used a

term frequency based approach to transform the text into numerical high-dimensional data.

Over a large corpus of data, this leads to a number of attributes or features in the thousands

or higher. Because of this, we saw no reason to include the manual weight manipulation

techniques present in one of the existing tools we based the features oﬀ of [32]. However, the

only text-speciﬁc piece of the previous pipelines is the Data Controller, and the remainder

may be used for any type of high-dimensional data. As such, we created an alteration of the

pipeline and visualization based on the aforementioned tool to allow for the visualization and

manipulation of the attribute weights for high-dimensional data sets with a relative small

number of dimensions.

52

Figure 4.15: A visualization that includes the current weight for each attribute in the data.

These weights can be manipulated directly to create a new projection of the data.

The slider based approach presented in [32] was simple to implement. An extended version

of the Similarity Model was created to add in the functionality for interpreting interactions

where users manually change attribute weights. We simply took out the Relevance Model,

assuming we are working with a small, ﬁxed set of data as in [32], and we were able to obtain

the same core functionality. The only additional work was creating a web-based interface

for these slider displays and interactions. Again, we built oﬀ our previous designs, swapping

out the data information panel with the panel of attribute weights. This interface can be

seen in Figure 4.15.

Implementing functionality from existing tools may be no great feat, but the simplicity

in doing so demonstrates how quickly new features could be prototyped and tested. One

53

new approach that could be very easily examined is using the Relevance Model with pure

numerical data. For example, you could have thousands of survey results you want to explore,

and adding the Relevance Model to the pipeline would enable sorting through large data sets

using a combination of OLI with the data points and direct parametric interaction with the

attribute weights themselves.

Chapter 5

Discussion and Future Work

Designing interactive visualizations in a highly modular fashion provides many beneﬁts to-

ward the research of visual analytics tools. Here we explore some of these beneﬁts we have

already encountered, and reﬂect back on the research questions proposed earlier. Addition-

ally, we discuss possible improvements to the framework.

5.1 A Modular Framework

The modularity of the framework provides many beneﬁts towards creating visual analytics

tools. As seen in the previous examples, we were able to create many diﬀerent prototypes by

swapping out individual pieces of the overall pipeline. The Relevance Model and Similarity

Model provided the foundation for the data transformation. These Models, or some extension

of them, were used in each of the examples aside from the raw high dimensional data analysis

54

55

pipeline, which solely used the Similarity Model. Designing the Models in this modular

fashion forced us to make little assumptions about what type of data could be processed.

While originally these Models were designed to work with a ﬁxed set of attributes, the

creation of more complex pipelines led us to remove this restriction and work with a variable

and ever expanding set of attributes.

We have demonstrated how a new Data Controller can enable the pipeline to work with

several diﬀerent types and sources of text data. But this is only one possible type of data

to be analyzed. The Similarity and Relevance Models described in the previous section

are designed to work with numeric high dimensional data. In the examples we presented

for visualizing text data, the raw text had to be transformed into some form of numerical

data, namely term frequency data, to work with these Models. While this may seem like a

limitation, it actually provides more generic functionality that can enable data exploration

with many diﬀerent types of data. Any type of data could be transformed to numerical data

and work with this pipeline. For example, pixel data for images could be transformed into

numerical vectors in some way to enabling a similar type of exploration of images.

Moving beyond analyzing speciﬁc types of data, new pipelines could be created to analyze

multiple types or sources of data simultaneously. In the simplest case, text data from varying

sources could be analyzed together just through the creation of a new Data Controller.

It could connect to multiple search engines or text databases, and the remaining pipeline

would not have to be changed. A more complex scenario is analyzing diﬀerent types of data

together, such as text and image data. One possibility is to have separate Models to work

56

with each type of data, and visualize them separately.

However, to truly compare visualize and interact with diﬀerent data types together, there

must be a method to numerically compare them. With a statistical method to compare

the similarity between text and images, the framework could easily support this type of

analysis. For example, a new Model could be created and inserted before the Relevance

Model that takes multiple types of data and computes some singular form of numerical

data with meaning across all types. With this common numerical data, the Relevance

and Similarity Models would function just the same and provide a means of analysis. The

burden is solely discovering the mathematical techniques for comparing these data types,

not creating a system for doing so.

Modularizing the features and algorithms from other works also enables us to further ex-

periment with and improve upon them. For example, working with the inverse MDS code

previously developed in [32] was much simpler once we created our own library for it. Opti-

mal distance measurements between points become complicated in high dimensional space.

While Euclidean distances may still be valid, they may not make the most sense, because

as the dimensionality grows, points tend to be near the edge of the hypercube and distances

between all points converge [1]. We were easily able to include other distance functions, such

as cosine distance, which measures the angular distance between two points and tends to

work better for sparse text documents [20]. On top of this, we discovered a small bug in the

inverse MDS algorithm that was not comparing correctly scaled distances in low and high

dimensional space. We were able to modify it to make it completely scale independent, as

57

it was designed to be.

While most of our examples focused on creating a new Data Controller to visualize some

new type of data, these same pipelines could easily be used to test diﬀerent visual encodings

and interaction techniques on the front end. The radar example demonstrates how relevance

can be mapped in multiple ways. Clients can interpret the low dimensional projection and

relevance however they choose. Making use of the third dimension or other visual features

such as color or brightness could enable more information to be displayed to the user without

overwhelming them.

Additionally, the same Data Controllers and front end clients could be used to test diﬀerent

Models. While we chose to use simple term frequency based approaches for analyzing text

data, new Models could easily be created to perform more complex clustering and catego-

rization operations. One such method we are currently investigating for text data is using

Latent Dirichlet Allocation (LDA) to create topics over the document space [6]. With LDA,

each document gets assigned a probability vector that indicates how likely it is part of each

topic. This allows for an easy combination of diﬀerent types, sources, and scales of text

data into a single format that can be easily compared against each other. Additionally, it

provides an initial dimensionality reduction to reduce the number of attributes. This enables

algorithms operating on the high dimensional data to execute much faster, allowing for real

time interactions on larger data sets.

5.2 Research Questions

58

We previously discussed a set of research questions we hope this framework will help in

answering. Here we discuss how some of the example visualizations presented can be used

to answer these questions.

The multi-source text analysis is the ﬁrst step toward studying the combination of diﬀerent

types of data in a single visualization. The simplest case of multiple types of data is vastly

diﬀerent types of text data, such as short and sometimes incoherent tweets, to length and

grammatically correct news articles. However, this could be further expanded to more dis-

parate types of data. Structured text data, such as spreadsheets, is one extension of basic

text data that is non-trivial to analyze. Adding a completely diﬀerent data type, such as

images, adds a whole new aspect. Studying streaming data within a visual analytics tool

should be greatly simpliﬁed by this framework. We have provided a solution to the archi-

tectural challenge of enabling streaming data, demonstrating this with the Twitter example.

This will enable the future work to focus solely on how to spatialize and interact with such

data.

As previously discussed, new Models could enable us to interact with larger data sets in

real time, such as one implementing LDA [6]. The multi-source example also demonstrates

how this would be possible. Rather than working on preprocessed data, it calculates term

frequency data on the ﬂy to work with non-static data sets. This Term Frequency Model

could easily be replaced with an LDA Model that converts these raw text documents to LDA

59

topics instead of term frequency vectors.

Diﬀerent visual encodings can just as easily be studied, as the pipeline has no sense of what

encodings are used for the data it provides. The radar and 3D visualizations demonstrate

how the same pipeline can be used to visualize the same data in diﬀerent ways. This allows

researchers to focus solely on the interface and encoding the information without having to

be concerned with the data processing. The same can hold true for interactions. While some

general interactions are deﬁned within the Models, how these interactions get triggered are

left to the visualization. For example, users currently have to modify the relevance of a

document by typing in a new value. This could easily be changed to allow users to resize

nodes to indicate increased or decreased relevance, and this would not require any changes

in the Models.

These generalized types of interactions also simplify implementing non-traditional inter-

faces. Large, high resolution displays allow users more space to manage their thought pro-

cess [2, 3]. Likewise, 3D and immersive interfaces, can bring many new facets to visual

analytics [4, 7, 29, 30]. For example, node selection could be done through means of hand

tracking or through a peripheral device in such environments, but these methods just need

to be mapped to the same interaction understood by the Model. This separation between

capturing interactions and interpreting interactions enables better collaboration between

multiple bodies of research. An expert in immersive technologies could easily create their

own visual analytics tools by creating an interface for a pipeline created by a data analysis

expert. This ability to collaborate will further hasten research into visual analytics tools.

60

While the examples previously presented show independent methods for visualizing data,

one might consider combining multiple diﬀerent views or perspectives for examining a single

data set. One possibility is by creating two separate views that use the same outputs of

the pipeline. For example, a keyboard and mouse based 3D visualization could be used

simultaneously with a client supporting virtual reality and peripheral devices for interacting

in 3D. Since the inputs and outputs of these two views is the same, they could safely interact

with the same pipeline without requiring any new functionality. Alternatively, one might

consider multiple views within a single visualization, such as multiple plots that use diﬀerent

sets of weights for the dimensionality reduction, to test multiple hypotheses concurrently.

While this would require the creation of new Models to handle multiple dimensionality

reductions, there is nothing in the framework preventing such behavior.

5.3 Pipeline Improvements

One feature common in most analytics tools not present in our pipeline is undo functionality.

It is a capability present in almost any computer application a typical analytics tool user

has interacted with, and provides many beneﬁts when analyzing data. If you do not like an

interaction or to something accidentally, undoing can immediately revert that change. The

downside of the highly modular framework is the complexity in enabling such a feature. It

would require the pipeline itself keeping some kind of state over each of the modules, and the

modules themselves having some logic regarding undoing its most recent parameter changes.

61

To make matters worse, some of the algorithms used in our pipelines, such as the forward

and inverse MDS algorithms, are non-deterministic, so maintaining state of previous sets of

attribute weights would not enable proper undo functionality. Each projection of the data

would need to be stored to return them to their exact previous projections.

This modularity could also be further built upon. While we believe the current framework

can be extremely useful for studying most aspects of analytics tools, it does not scale as

well as it possibly could. The entire pipeline exists within a single Python instance, limiting

the resources that may be available to individual pieces within the pipeline. These diﬀerent

modules can create their background threads to improve throughput, but allowing each

module to exist on diﬀerent machines within their own processes could improve things even

further. Enabling this would be fairly straightforward, as the Models would be created in

the exact same fashion where they accept some JSON data blob and update it. The main

change would be within the pipeline itself coordinating communication between each piece.

Handling diﬀerent modules on diﬀerent machines would likely be a headache for researchers,

so this modiﬁcation would only be useful for studying certain algorithms that are optimized

for speciﬁc types of machines.

Chapter 6

Conclusion

We have developed a bidirectional pipeline framework for creating visual analytics tools that

use semantic interaction and V2PI to aid the sensemaking process. Three key pieces make

up this framework. A Data Controller deﬁnes what type of data is being visualized and how

it is accessed. A series of Models transform the data into a form suitable to be visualized, as

well as interpret interactions from the visualization. Finally, a Connector controls how the

visualization communicates with the pipeline. Each of these pieces can easily be replaced

to quickly prototype and experiment with diﬀerent types of data, mathematical models,

interaction techniques, and visual encodings.

We demonstrate this ability by developing several prototypes that exemplify how to research

each of these aspects. We hope that this new pipeline will replace the traditional visualization

pipeline to emphasize its bidirectional nature and the role of inverse algorithms to interpret

62

63

interactions. Additionally, the modularity well help multiple developers work together to

create visual analytics tools.

By enabling rapid prototyping of such tools, researchers will be able to quickly conduct user

studies on many of these alternative methods of semantic interaction. We intend to continue

expanding on these prototypes and conduct our own user studies. These studies will reveal

how the user perceives these diﬀerent visual encodings and interactions, which methods best

support the user’s sensemaking process, and how to develop better visual analytics tools in

the future.

Bibliography

[1] C. C. Aggarwal, A. Hinneburg, and D. A. Keim. On the surprising behavior of distance

metrics in high dimensional spaces. In Proceedings of the 8th International Conference

on Database Theory, ICDT ’01, pages 420–434, London, UK, UK, 2001. Springer-Verlag.

[2] C. Andrews, A. Endert, and C. North. Space to think: Large high-resolution displays

for sensemaking. In Proceedings of the SIGCHI Conference on Human Factors in Com-

puting Systems, CHI ’10, pages 55–64, New York, NY, USA, 2010. ACM.

[3] C. Andrews and C. North. Analyst’s workspace: An embodied sensemaking environment

for large, high-resolution displays. In Visual Analytics Science and Technology (VAST),

2012 IEEE Conference on, pages 123–131, Oct 2012.

[4] R. Ball, C. North, and D. A. Bowman. Move to improve: Promoting physical naviga-

tion to increase user performance with large displays. In Proceedings of the SIGCHI

Conference on Human Factors in Computing Systems, CHI ’07, pages 191–200, New

York, NY, USA, 2007. ACM.

64

65

[5] J. Behr, P. Eschler, Y. Jung, and M. Z¨ollner. X3dom: A dom-based html5/x3d integra-

tion model. In Proceedings of the 14th International Conference on 3D Web Technology,

Web3D ’09, pages 127–135, New York, NY, USA, 2009. ACM.

[6] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning

Research, 3:993–1022, 2003.

[7] D. A. Bowman, E. Kruijﬀ, J. J. LaViola, and I. Poupyrev. 3D User Interfaces: Theory

and Practice. Addison Wesley Longman Publishing Co., Inc., Redwood City, CA, USA,

2004.

[8] L. Bradel, C. North, L. House, and S. Leman. Multi-model semantic interaction for text

analytics. In Visual Analytics Science and Technology (VAST), 2014 IEEE Conference

on, pages 163–172, Oct 2014.

[9] L. Bradel, N. Wycoﬀ, L. House, and C. North. Big text visual analytics in sensemaking.

In IEEE International Symposium on Big Data Visual Analytics, page 8 pages, 09/2015

2015.

[10] S. Cheng and K. Mueller. The data context map: Fusing data and attributes into a

uniﬁed display. IEEE Transactions on Visualization and Computer Graphics, 22(1):121–

130, Jan 2016.

[11] G. Chin, M. Singhal, G. Nakamura, V. Gurumoorthi, and N. Freeman-Cadoret. Visual

analysis of dynamic data streams. Information Visualization, 8(3):212–229, June 2009.

66

[12] D3. https://d3js.org/, 2016. Accessed: 2015-11-05.

[13] A. Endert, L. Bradel, and C. North. Beyond control panels: Direct manipulation for

visual analytics. IEEE Comput. Graph. Appl., 33(4):6–13, July 2013.

[14] A. Endert, P. Fiaux, and C. North. Semantic interaction for sensemaking: Inferring ana-

lytical reasoning for model steering. IEEE Transactions on Visualization and Computer

Graphics, 18(12):2879–2888, Dec 2012.

[15] A. Endert, P. Fiaux, and C. North. Semantic interaction for sensemaking: Inferring ana-

lytical reasoning for model steering. IEEE Transactions on Visualization and Computer

Graphics, 18(12):2879–2888, Dec 2012.

[16] A. Endert, P. Fiaux, and C. North. Semantic interaction for visual text analytics. In

Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, CHI

’12, pages 473–482, New York, NY, USA, 2012. ACM.

[17] A. Endert, C. Han, D. Maiti, L. House, S. Leman, and C. North. Observation-level

interaction with statistical models for visual analytics. In Visual Analytics Science and

Technology (VAST), 2011 IEEE Conference on, pages 121–130, Oct 2011.

[18] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design Patterns: Elements of

Reusable Object-oriented Software. Addison-Wesley Longman Publishing Co., Inc.,

Boston, MA, USA, 1995.

67

[19] C. Gormley and Z. Tong. Elasticsearch: The Deﬁnitive Guide. O’Reilly Media, Inc.,

1st edition, 2015.

[20] M. Goto, T. Ishida, and S. Hirasawa. Statistical evaluation of measure and distance

on document classiﬁcation problems in text mining.

In Computer and Information

Technology, 2007. CIT 2007. 7th IEEE International Conference on, pages 674–673,

Oct 2007.

[21] J. Hendler. Web 3.0 emerging. Computer, 42(1):111–113, Jan 2009.

[22] L. House, S. Leman, and C. Han. Bayesian visual analytics: Bava. Statistical Analysis

and Data Mining, 8(1):1–13, 2015.

[23] X. Hu, L. Bradel, D. Maiti, L. House, C. North, and S. Leman. Semantics of directly ma-

nipulating spatializations. IEEE Transactions on Visualization and Computer Graphics,

19(12):2052–2059, Dec 2013.

[24] M. Krstaji and D. A. Keim. Visualization of streaming data: Observing change and

context in information visualization techniques. In Big Data, 2013 IEEE International

Conference on, pages 41–47, Oct 2013.

[25] S. C. Leman, L. House, D. Maiti, A. Endert, and C. North. Visual to parametric

interaction (v2pi). PLoS ONE, 8(3):1–12, 03 2013.

[26] Node.js. https://nodejs.org, 2016. Accessed: 2015-10-24.

68

[27] P. Pirolli and S. Card. The sensemaking process and leverage points for analyst tech-

nology as identiﬁed through cognitive task analysis. Proceedings of International Con-

ference on Intelligence Analysis, pages 2–4, 2005.

[28] N. F. Polys, B. Knapp, M. Bock, C. Lidwin, D. Webster, N. Waggoner, and I. Bukvic.

Fusality: An open framework for cross-platform mirror world installations. In Proceed-

ings of the 20th International Conference on 3D Web Technology, Web3D ’15, pages

171–179, New York, NY, USA, 2015. ACM.

[29] N. F. Polys, A. Mohammed, J. Iyer, P. J. Radics, F. Abidi, L. Arsenault, and S. Ra-

jamohan. Immersive analytics: Crossing the gulf with high-performance visualization.

In 2016 IEEE VR Workshop on Immersive Analytics (IA). IEEE, March 2016.

(to

appear).

[30] G. Robertson, M. Czerwinski, K. Larson, D. C. Robbins, D. Thiel, and M. van Dantzich.

Data mountain: Using spatial memory for document management. In Proceedings of

the 11th Annual ACM Symposium on User Interface Software and Technology, UIST

’98, pages 153–162, New York, NY, USA, 1998. ACM.

[31] T. Ruotsalo, J. Peltonen, M. Eugster, D. Glowacka, K. Konyushkova, K. Athukorala,

I. Kosunen, A. Reijonen, P. Myllym¨aki, G. Jacucci, and S. Kaski. Directing exploratory

search with interactive intent modeling. In Proceedings of the 22nd ACM international

conference on Conference on information and knowledge management, CIKM ’13, pages

1759–1764, New York, NY, USA, 2013. ACM.

69

[32] J. Z. Self, L. House, S. Leman, and C. North. Andromeda: Observation-level and

parametric interaction for exploratory data analysis. Technical report, Department of

Computer Science, Virginia Tech, Blacksburg, Virginia, 2015.

[33] J. Z. Self, X. Hu, L. House, S. Leman, and C. North. Designing for interactive dimension

reduction visual analytics tools to explore high-dimensional data. Technical report,

Department of Computer Science, Virginia Tech, Blacksburg, Virginia, 2015.

[34] A. Taivalsaari and T. Mikkonen. The web as an application platform: The saga con-

tinues. In 2011 37th EUROMICRO Conference on Software Engineering and Advanced

Applications, pages 170–174, Aug 2011.

[35] Tweepy. http://www.tweepy.org/, 2016. Accessed: 2016-06-15.

[36] Websockets. https://www.w3.org/TR/websockets/, 2016. Accessed: 2015-11-05.

[37] Zeromq. http://zeromq.org, 2016. Accessed: 2016-02-09.

[38] zerorpc. http://zerorpc.io, 2016. Accessed: 2016-02-09.

Appendix A

Creating a Pipeline

Creating a new pipeline is a simple task with the framework, and this chapter steps through

all the necessary pieces. Three sections make up a fully functional pipeline: a Data Con-

troller, a list of Models, and a Connector. Each of these has a base class contained in the

nebula.pipeline module. These classes contain all necessary functions, each of which can be

overridden in a subclass to deﬁne its behavior. The Pipeline class mediates communication

between each of these pieces. Here we explain exactly what the functions are in each module

that makes up a pipeline and when they are called, further explore the communication, and

ﬁnally demonstrate how to put the pieces together.

70

A.1 Data Controller

71

The purpose of the Data Controller is to be the main translator of the data to be visualized

to a form the rest of the pipeline can understand. This data may be locally stored or in

some remote database. Diﬀerent Data Controllers can easily be created for diﬀerent types

or locations of data. A Data Controller has four main functions which can be overridden:

setup, get, run, and reset.

The setup function is executed on the initial startup of the pipeline, and whenever the

pipeline is reset. It allows for data to be passed down to the setup function of the Models

within the pipeline. The argument to this function is a data blob in the form of a dictionary

object which can be modiﬁed in place to send data to the Models. This can allow the pipeline

to start oﬀ with some initial set of data or random sampling, rather than waiting for the

ﬁrst user interaction to pull in data. setup should not returning anything.

The get function is unique to the Data Controller.

Its purpose is to send raw data and

metadata directly to the visualization, rather than passing through the whole pipeline. This

function will be called by the pipeline, so the Data Controller merely needs to respond to

requests made through its dictionary argument. The return value should be a dictionary

containing the results of the query.

run is the workhorse of the Data Controller. Upon each iteration of the pipeline initiated by a

user interaction, the run function gets called after all the inverse functions of the Models are

executed. The argument to this function is a dictionary object that has passed through all

72

of the Models’ inverse functions, allowing the Models to pass data up to the Data Controller

to retrieve speciﬁc types of data. The return value of this function is important, as whatever

is returned becomes the data blob which is passed down the forward pipeline. As such, it

must be a dictionary object.

Finally, the reset function behaves exactly as expected. Whenever the pipeline is reset, the

reset function of the Data Controller and each Model is executed, allowing any parameters

to be reset to their initial values. It takes no arguments and doesn’t return anything.

A.2 Models

Models contain very similar functionality to Data Controllers. There are four key functions

that can be overridden to provide their functionality: setup, forward, inverse, and reset.

Additionally, the forward input reqs, forward output, inverse input reqs, and inverse output

functions are used to specify the inputs and outputs of each Model to ensure a valid and

properly constructed pipeline, and are further explored in Section A.4 below. The setup is

called on the initial pipeline startup, as well as after a reset. The argument passed to the

setup function is a dictionary that has been passed through all Models closer to the Data

Controller, as well as the Data Controller itself. Any changes made to this argument will

be available to the remaining Models’ setup functions as well. The reset function behaves

exactly the same as the Data Controller.

The forward function should contain the main logic for transforming the data to a form the

73

visualization can use. Its argument is the dictionary object that contains the data from the

Data Controller’s run function return value, as well as all modiﬁcations made by previous

Models’ forward functions. Any changes made to this argument will be available to the

remaining Models’ forward functions. There should be no return value.

inverse interprets user interaction to modify parameters within the Model. Like the others, a

single dictionary argument is provided with details of the interaction from the visualization

as well as any modiﬁcations made by Models’ inverse functions closer to the Connector.

Returning a value from the inverse function signals the pipeline to short circuit itself. The

return value then becomes the data blob immediately passed into the current Model’s forward

function.

A.3 Connector

The Connector deﬁnes a method for a visualization to communicate with the pipeline. This

method may be an RPC mechanism, raw sockets, or any other form of communication with

an external process. Connectors have three methods that deﬁne their behavior: set callbacks,

start, and push update. When a Connector is added to a pipeline, the pipeline will call the

set callbacks function with callbacks for an update, get, and reset function. Using what-

ever communication method is chosen, these callbacks must be called when the appropriate

message is received. The Connector should send the return value from the update and get

callbacks to the pipeline. The reset function does not return any value.

74

start simply lets the Connector know that is should start listening for incoming messages

from the visualization. For example, if using a socket based approach, the socket would start

listening for incoming messages in the start function. The push update function allows up-

dates to be asynchronously pushed to the visualization. In order to support this, a Connector

must have a way of sending a message to the visualization directly. RPC approaches may

not support this, as they can only respond when called by the visualization. This function

must only be deﬁned if the Connector wishes to support push functionality.

A.4 Communication

Communication within the pipeline is key to understand before developing new modules.

This communication is controlled by the pipeline in the form of data blobs that are passed

to the various functions within each module. These data blobs exist as Python dictionaries

within the pipeline framework and are typically transformed to JSON objects outside of the

framework. Modules communicate with each other by modifying or adding elements to these

data blobs which are passed in as an argument to the various core functions.

There are several points where a data blob is created. The ﬁrst is for the series of setup

calls, and is illustrated in Figure A.1. An empty blob is created and passed to the Data

Controller’s setup function, followed by each Models’ setup function. Second, a data blob

can be created from the visualization to initiate an iteration of the pipeline. This data blob

only can make it as far as the Data Controller, which would then create a new data blog for

75

Figure A.1: A data blob is created by the pipeline and passed through the setup up function

of the Data Controller and each Model.

the forward pipeline. This case is shown in Figure A.2. Alternatively, a Model can choose

to short circuit itself by creating a new data blob in its inverse function. This then becomes

the data blob for the forward pipeline beginning with that Model, and is demonstrated in

Figure A.3. Finally, a data blob can be created in a background thread of a Data Controller

or Model through the push functionality, shown in Figure A.4 and Figure A.5, respectively.

These data blobs are deleted at the end of their path.

When designing new Data Controllers and Models, it is key to understand what the input

requirements and outputs of each module are. For example, forward functions of Models are

designed to enrich the data in some way, but to do this they must have some data to enrich

and know where to ﬁnd it. As previously stated, this is done through acting on certain

elements in the data blob based on a common key. To help aid the development process,

76

Figure A.2: A data blob is created by the visualization representing an interaction. The

Data Controller creates a new blob for the forward pipeline.

Data Controllers and Models are required to list out their required elements in the data blob

for their forward and inverse functions, as well as the elements that they may add. The

inputs for each Model are then compared against the output of all previous Models. These

keys do not have to exist in all data blobs, but should exist some of the time for a Model to

provide any beneﬁcial behavior.

These inputs and outputs are speciﬁed through the forward input reqs, forward output, in-

verse input reqs, and inverse output functions within a Model, and the input reqs and output

functions of a Data Controller. These functions simply return a list of strings representing

keys within the data blob. At the startup of a pipeline, all of these inputs and outputs are

compared against each other to ensure that the pipeline has been properly constructed. If

any Model is missing an input, an error will be reported and the pipeline will not start.

77

Figure A.3: Inverse functions trigger a short circuit by returning a new data blob.

The format of each of these elements do not have any strict deﬁnition, however.

If two

Models or a Model and Data Controller operate on the same element within a data blob,

it is assumed that they have a common understanding of how that element is formatted.

Because of this, it is important to provide clear deﬁnitions of what each key within the

data blob represents. The element mapped to a speciﬁc key should never have diﬀerent

formats. Should a new format be required, a new element at a new key should be speciﬁed.

The elements used in the existing Models and Data Controllers use a common set of key

deﬁnitions within the pipeline module. Each of these keys has a brief description of what

the element is meant to represent. New modules are not limited to using these keys, but if

new ones are created there should be a strict deﬁnition of what element exists at that key.

78

Figure A.4: The Data Controller asynchronously pushes a new data blob down the pipeline.

A.5 Putting it Together

Each of these pieces come together in a single Pipeline object. The Pipeline class is not

meant to be overridden, and provides all the logic to mediate communication between each

diﬀerent piece. Models can be added to the Pipeline using the insert model or append model

functions, and the Data Controller and Connector can be set using the set data controller

and set connector functions, respectively. Once all the pieces are in place, a simple call to the

start function of the Pipeline object will execute the setup function of the Data Controller

and all Models and start the Connector. The Pipeline then listens for any messages from

the visualization and responds accordingly.

79

Figure A.5: A Model asynchronously pushes a new data blob down the pipeline.

",False,0.0,{},False,False,journalArticle,False,Q2Q8T98U,[],self.user,False,False,False,False,,,A Bidirectional Pipeline for Semantic Interaction in Visual Analytics,Q2Q8T98U,False,False
FA6A3U8V,GLM4JN63,"240 

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 22,  NO. 1, JANUARY 2016

The Role of Uncertainty, Awareness, and Trust in Visual Analytics

Dominik Sacha, Hansi Senaratne, Bum Chul Kwon, Member, IEEE, Geoffrey Ellis, and Daniel A. Keim, Member, IEEE

Uncertainty Propagation

Awareness

Trust Building

Uncertainty Handling and Reduction

Need for Evidence/ Need forAction / Lack of Trust

Searching for Evidence

Confrontation and Reasoning

Visualisation

Action

Data

Model

Finding

Exploration Loop

Hypothesis

Insight

Veriﬁcation Loop

Externalisation &
Internalisation

Knowledge

Knowledge Generation 

Loop

Uncertainty Inheritance and Aggregation

Increase of Trust / Internalisation of Evidence

Fig. 1: Knowledge generation model for visual analytics including uncertainty propagation and human trust building. Uncertainty
originates at the data source and propagates through the system components which introduce additional uncertainties. Uncertainty
awareness inﬂuences human trust building on different knowledge generation levels.

Abstract— Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected,
collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed
to generate a visual representation of the data. These often introduce their own uncertainties, in addition to the ones inherent in the
data, and these propagated and compounded uncertainties can result in impaired decision making. The user’s conﬁdence or trust in the
results depends on the extent of user’s awareness of the underlying uncertainties generated on the system side. This paper unpacks
the uncertainties that propagate through visual analytics systems, illustrates how human’s perceptual and cognitive biases inﬂuence
the user’s awareness of such uncertainties, and how this affects the user’s trust building. The knowledge generation model for visual
analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and
though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design
of uncertainty-aware systems are presented that can aid the user in better decision making.
Index Terms—Visual Analytics, Knowledge Generation, Uncertainty Measures and Propagation, Trust Building, Human Factors

1 INTRODUCTION
In the visual analytics process, users arrive at new knowledge after per-
forming numerous sensemaking activities. The goal of visual analytics
is to foster effective collaboration between human and machine that
improves the knowledge generation process. To succeed in this process,
end users need to be able to trust their knowledge generated by means
of visual analytics. Analysts can often be unaware of uncertainties in
their data sources, pre-processing, analysis processes or visualisations
that are hidden by a ‘black box’ approach of visual analytics systems.
In criminal investigation analysis, where analysts use a visual an-
alytics application to analyse a collection of reports and to identify
crime suspects, the system may hint at otherwise hidden connections
between pieces of evidence using a trained machine learning algorithm.
To progress, the analyst needs to trust this outcome. However, if the
analyst is not aware of the inherent uncertainties, they may waste their
time following wrong leads and may, in the worst case, incriminate
innocent people. Likewise, overestimating uncertainties can have a

• Data Analysis and Visualisation Group, University of Konstanz. E-mail:

forename.lastname@uni-konstanz.de

Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of publication
Manuscript received 31 Mar. 2015; accepted 1 Aug. 2015; date of  
xx Aug. 2015; date of current version 25 Oct. 2015.
publication 20 Aug. 2015; date of current version 25 Oct. 2015. 
For information on obtaining reprints of this article, please send
For information on obtaining reprints of this article, please send 
e-mail to: tvcg@computer.org.
e-mail to: tvcg@computer.org.
Digital Object Identifier no. 10.1109/TVCG.2015.2467591

negative impact upon decision making. It is therefore crucial for users
to be provided with an accurate estimation of uncertainties from visual
analytics systems so that they can trust acquired knowledge.

The literature describes some parts of uncertainty propagation and
trust building in visual analytics processes, however, the interplay
of trust and knowledge within the knowledge generation process in
visual analytics has not yet been established. Prior studies have inves-
tigated sources of uncertainties in subsets of the visualisation process
(e.g., [17]). Other studies have looked at human analysts behaviours
while building trust in the knowledge generation process, with respect
to perception [79], cognitive biases [31], and analytic roadblocks [48].
What is missing is a uniﬁed framework that bridges the concepts of
uncertainties on the machine side and the trust building process on the
human side. Recently, the IEEE VIS2014 Workshop on Provenance for
Sensemaking called for research in deﬁning uncertainty, trust, and data
quality. MacEachren also highlighted human’s decision making and
reasoning processes under uncertainty as future research direction [52].
Building such a framework can provide a common language of the
concepts that are largely uncharted in the visualisation domain.

Our goal is to investigate uncertainty propagation, trust building,
and the interplay between uncertainty and trust during the knowledge
generation process within visual analytics. Building on the related
work in Uncertainty Propagation and Human Trust Building under
Uncertainty, the paper describes a novel model of uncertainty and trust
using the knowledge generation model [64] as a framework and brings

1077-2626 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.  See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.SystemHumanSACHA ET AL.: THE ROLE OF UNCERTAINTY, AWARENESS, AND TRUST IN VISUAL ANALYTICS 

241

in human cognition and perception issues through the concept of Aware-
ness. We choose the most recent and complete knowledge generation
model for visual analytics because it “integrates human thinking whilst
describing visual analytics components” [64]. Furthermore, this model
was the foundation for our initial investigations and discussions about
deﬁning and relating its concepts to uncertainty and trust. To extend
the usefulness of the model, we provide guidelines on how to improve
decision making, avoiding misperceptions and pitfalls generated in the
visual analytic processes. Finally, we explore future directions and
opportunities for handling uncertainties and trust.

2 RELATED WORK
We group related work into Uncertainty Propagation and Human Trust
Building under Uncertainty. The former covers works on capturing and
deriving uncertainty measures within visual analytics pipelines. The
latter covers the knowledge generation and the trust building processes
in visual analytics.

2.1 Uncertainty Propagation
In this section we give a brief overview on Types of Uncertainty, Un-
certainty Propagation, and Data Provenance theories.

Types of Uncertainty: Many works have individually tackled un-

certainties that arise through the various components of a system.

Source: Data source uncertainty is inherent in the data. Lush et
al. [50] introduced the GEOLabel1, by which users can rapidly as-
certain the source uncertainty of particularly geospatial datasets in
terms of metadata (e.g., dataset producer information, compliance to
international standards, ratings) through specially designed icons.

Models: Model uncertainty corresponds to the structure of the model
and the parameterisation of the model. Chatﬁeld [14] describes how
uncertainty is fundamentally propagated in data models that represent
real-world phenomena. He also describes the main sources of uncer-
tainty in models, which we discuss in Section 3.1. Cullen and Frey [18]
comprehensively address the methods for variability and uncertainty
in models, while Lee and Chen [49] comparatively analyse the various
uncertainty propagation methods in terms of their performance.

Uncertainty Propagation: Uncertainty is created and passed on
from the source to the model and subsequently to the visualisation.
Haber and McNabb [33] introduced uncertainty propagation to their
visualisation reference model, where the visualisation of uncertainty
focuses on the uncertainties that are in the measurement and simula-
tion data (referred to as data source uncertainty, henceforth). They
discuss how uncertainty propagates from the ﬁltering stage, mapping
stage, to the rendering stage of a traditional pipeline model. They call
this uncertainty of visualisation. Uncertainty propagation within the
context of visual analytics is the process of quantifying the underlying
uncertainties generated throughout their components. In introducing
a framework for uncertainty-aware visual analytics, Correa et al. [17]
suggest propagating and communicating the uncertainties that arise
inherently in the data and its transformations in the information visu-
alisation pipeline. Furthermore, Zuk and Carpendale [83] extend the
data uncertainty visualisation pipeline of Pang et al. [61] to include
these propagated uncertainties. These workﬂows facilitate the analyst
in identifying the inherent and propagated uncertainties in their data.
Visualisation of Uncertainty: A large body of work has contributed
towards visualising the propagated uncertainties (e.g., [35, 55, 72]).
Based on their extensive research, MacEachren [51], and Howard and
MacEachren [37] introduced several dichotomous categories for un-
certainty visualisation, based on the principle that, the way in which
data and uncertainties are linked, should be meaningful. These act as
guidelines in designing the visualisations appropriately for the data and
task at hand. Several researchers have evaluated uncertainty visualisa-
tion techniques in various data/task settings (e.g., [68]). However, very
little effort has been put into evaluating the effects of visualisations
(e.g., visual clutter) that have major effects (e.g., cognitive load) in user
perception and problem solving ability.

1http://www.geolabel.info/

Data Provenance: Data provenance can be described as a way to
record all data derivations from its origin until the ﬁnal data product.
Consistent representations of data provenance information derived from
workﬂows and databases can be leveraged within the analysis process.
Simmhan et al. [70] survey data provenance techniques and present
a taxonomy that covers Usage, Subject, Representation, Storage and
Dissemination of provenance.
2.2 Human Trust Building under Uncertainty
We distinguish the relevant human focused theories in Knowledge
Generation, Trust Building and Analytic Provenance.

Knowledge Generation: Tory and M¨oller [76] give an introduction
to human factors and highlight that visualisations serve as cognitive
support and address human computer cooperation. They suggest that
analysts perceive visualisations and match them to their mental model
of the problem. Other human factors include a users’ knowledge, ex-
pertise and tasks but also factors on perception and cognition. Zuk and
Carpendale [83] extend the typology on uncertainties by Thomson et
al. [75] for reasoning. Both of the typologies include a category about
Subjectivity that represents the “amount of interpretation or judgment
that is included” [75] or the “amount of private knowledge or heuristics
utilised” [83]. Green et al. [31] propose the Human Cognition Model
that covers various human aspects of knowledge creation with visual
analytics. They point out that hypothesis generation is very much inﬂu-
enced by the human tendency to accept conﬁrmatory evidence more
than disconﬁrmatory and that the computer can help to mitigate this
cognitive bias. Winters et al. [80] show in their study that humans can
have different roles, expertise and knowledge that can be applied during
the analysis process. This inﬂuences how individuals approach visuali-
sations and also how they reason about their problems. Gahegan [28]
summarises different kinds of reasoning and relates them to human
activities, visualisation tools or computational tools. MacEachren et
al. [54] mention that analysts and decision makers behave differently
with and without the usage of uncertainty visualisations, whether they
are aware of the uncertainties or not. They differentiate between infor-
mation uncertainty and an “analysts’ or decision makers’ uncertainty”
and also suggest that to capture, represent and understand these uncer-
tainties are future research challenges.

Trust Building: Muir [58] discusses trust relations between hu-
mans and machines and builds on Barber’s trust dimensions, which are
Persistence, Technical Competence, and Fiduciary Responsibility [3].
Furthermore, Muir gives the following recommendations for improving
trust calibration: “(1) improving the user’s ability to perceive a decision
aid’s trustworthiness, (2) modifying the user’s criterion for trustworthi-
ness, (3) enhancing the user’s ability to allocate functions in the system,
(4) identifying and selectively recalibrating the user on the dimension(s)
of trust which is (are) poorly calibrated” [58]. Dzindolet et al. [21]
investigate how trust develops during the usage of a system. Initially,
all participants considered the decision aid as trustworthy and reliable.
Observing errors caused the participants to distrust the systems unless
an explanation was provided. Understanding the errors helped the
users to increase their trust in the decision aid, even under uncertainty.
Castelfranchi [11] relates trust to the process of knowledge manage-
ment and sharing and provides a theory that considers the process to be
a decisional act of passing and accepting knowledge. Trust is related to
these activities as a mental attitude, but also a decision (e.g., intention
to delegate trust) and a behaviour (e.g., relation between trustor and
trustee). Uggirala et al. [77] studied humans using systems that include
uncertainties by having the users rate their trust at each level through
questionnaires. Their study showed that trust relates to competence and
an inverse relation to uncertainty, meaning that an increase in uncer-
tainty decreases trust in the systems. Visser et al. [19] provide a design
taxonomy for trust cue calibration that includes all kinds of information
that may inﬂuence human judgement. Figure 2 (left) illustrates trust
calibration and the included problems. A miscalibration between the
humans’ trust and the systems’ trustworthiness leads to over- or distrust
that are directly connected to disuse and misuse of automation. Skeels
et al. [71] deliver a comprehensive perspective on uncertainties for
information visualisation and also brieﬂy discuss the role of awareness

SystemHuman242 

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 22,  NO. 1, JANUARY 2016

Fig.2:Left:Trustcalibrationadaptedfrom[19],Right:Awarenessclassiﬁcationadaptedfrom[71].(seeFigure2right).TheyidentifyUnidentiﬁedUnknownsastheworstkindofmissinginformationbecauseinthatcasehumansarebuildingmoretrustthantheyshoulddo.AnalyticProvenance:Recentresearchfocusesontrackinginterac-tioninordertoinvestigatehumananalyticprocesses.Douetal.[20]presentanapproachtocapturehumanreasoningprocessesthatdistin-guishesbetweeninternal(interactionswithinthesystem)andexternalcapturing(outsidethesystem).RaganandGoodall[63]goontomentionthatprovenancetoolssupportthehumaninmemorisingandcommunicatinganalyticprocesses.Nguyenetal.[59]surveyanalyticprovenanceandshowthattypicalprovenanceconsistsofthreestages:Capturing,VisualisingandUtilisingwithcapturingondifferentlevels(events,low-levelactions,high-levelsub-tasks,top-leveltasks).Theyalsodescribethebeneﬁtsofanalyticprovenancethatgobeyondre-callingtheanalysisprocesstosupport“evidenceinconstructingthereasoningprocess,andfacilitatingcollaboration[...]”[59].ExamplesforleveraginganalyticprovenancewillbegivenintheguidelinesforhandlinguncertaintiesinSection4.Insummary,ourreviewdiscoverstwodistinctivegroupsofliterature.Onegroupdealswithuncertaintypropagationandvisualisationonma-chineaspects;theotherinvestigateshuman-machinetrustrelations.Weobserveimportantgapsinresearchwithinvisualanalyticsframeworks.Theseareuncertaintiesinthevisualisationitself,uncertaintiesinthecouplingbetweenmodelandvisualisation,anduncertaintiesinthemodelbuilding.Onlyfewstudiesrelateuncertaintiestothesehumantrustbuildingprocesses.Furthermore,thereisnoclearterminologythatdifferentiatesbetweenuncertaintiesfrommachineandhuman.Inthefollowing,weaddresstheseissuesandprovideamodelthatintegratesuncertaintypropagationandhumantrustbuilding.3UNCERTAINTYPROPAGATIONANDTRUSTBUILDINGWITHINTHEKNOWLEDGEGENERATIONPROCESSWithintheknowledgegenerationmodelforvisualanalytics,uncertain-tiesarepropagatedinthedifferentcomponents,causingvariousissuesoftrustonthehumanside.Inthefollowing,wegiveanexpos´eonhowuncertaintyispropagatedonthemachinesideandtrustiscallibratedonthehumanside.Finally,wediscusstheconceptofawarenessthatgluesuncertaintyandtrusttogether.3.1UncertaintyPropagationontheMachineSideWeextendtheframeworkofCorreaetal.[17],toincludeuncertaintiesinthemodelbuilding,visualisation,anduncertaintiesinthecouplingbetweenthemodelandvisualisation.Abriefdescriptionoftheroleofuncertaintyineachofthesecomponentsinasystem,follows.Uncertainty,generallyknownasthestateofnotknowing,isat-tributedtothediscrepancybetweenadatameasurementanditsrepre-sentation.AccordingtoGrietheandSchumann[32],conceptsregardedasuncertaintyareerrors,imprecision,accuracy,lineage,subjectiv-ity,non-speciﬁcity,noise(theauthorsprovideafulldescriptionofthesetermsin[32]).Uncertaintiesalsovarydependingontheapplica-tiondomain,forexample,topologicalconsistencyingeospatialdata.Throughoutthispaperwewillrefertoanyoneoftheseconceptsasuncertainty.Theseuncertaintiesindatacanbeclassiﬁedmainlyintotwocategories:1)sourceuncertainty,and2)propagateduncertainty.Accountingforsuchuncertaintiesindataisimportantforthoroughdataanalysis,informationderivationandinformeddecisionmaking.3.1.1SourceUncertaintyThesourceuncertainty(Fig.3-s2)isinherentindata,andlargelyde-pendsonthewayinwhichthisdataiscollected(authoritativevs.non-authoritativedata).Inmostcases,non-authoritativedatasuchassocialmediadatacontainshighuncertaintiesduetothelackofprofes-sionalgatekeepersandqualitycontrolstandardsamongotherreasons[27].Uncertaintiesinauthoritativedataaremainlyduetoreasonssuchaserroneousmeasurements,dataentryerrors,lowresolution,andincompletedata.Onewayofrepresentingsourceuncertaintiesisintermsofqualitativemeasuressuchaspurpose,usage,andlineage(alsoknownasdataprovenanceinmostdisciplines)[5].3.1.2PropagatedUncertaintyUncertaintyindataispropagatedduringthedatamodeling(wheredataundergotransformationssuchasinterpolation,sampling,quantisationetc.,[32,60])andvisualisationstages,wherethesepropagateduncertaintieskeepaggregatingasdatatravelsthroughthesestagesinthesystemsideoftheknowledgegenerationmodel(Figures1and3).DataProcessing(Fig.3-s3):Processingtechniquestransformdataforpreparationpurposes(e.g.,datacleansing).Theycanbebroadlygroupedintonormalisation,sub-sampling(reducingtheamountofdata)orinterpolation(increasingtheamountofdata).Uncertaintymeasuresrelatedtotheseprocessingtypescanbecalculatedwithstatistics[36].ModelBuilding(Fig.3-s5):Duringthemodelbuildingphase,ifusershavepreviousknowledgeofthemodel,theyachieveabestap-proximationbytypicallyﬁttingaparameterisedformofthemodeltothedata.Issuesofuncertaintyariseduetothecomplexityoftheparameterisation(e.g.,howmanyparametersaresuitable?)ortheappropriatenessoftheparameters(aretheparametersperfect/good/bad?),oreventherandomvariationofthemodelvariables.Atthisstagemodelcalibrationintroducesalotofuncertaintiesbythepro-cessofestimatingvaluesofunknownparametersinthemodel.Otheruncertaintiesariseifthedistancefunctions(e.g.,euclideandistanceorweightingswithinthesimilarityfunction)donotﬁtdataandtasks.Chatﬁeld[14]classiﬁesthesetypesofuncertaintiesasarisingfromi)modelmisspeciﬁcation.ModelUsage(Fig.3-s8):Chatﬁeld[14]statesthatalackofpreviousknowledgeoftheunderlyingphenomenoncausesinadequaciesofthemodel,whichgivesrisetostructuraluncertainties.Heintroducesii)specifyingageneralclassofmodels,wherethetruemodelisaspecial,unknowncase,andiii)choosingbetweenseveralmodelsofdifferentstructures,asreasonsthatgiverisetouncertaintiesinmodelusage.Additionally,themodelcarriesuncertaintiesintermsofitssuitabilitytothetaskathand.Numericalerrorsandapproximationsthatoccurduringtheimplementationofamodelgivesrisetoalgorithmicuncertainty[42].AsstatedbyBrodlieetal.[7],theseuncertaintieshavenotbeenthefocusofuncertaintyvisualisationresearchthusfar.VisualMapping(Fig.3-s4):Duringthemappingprocess,thecompu-tationofthegeometricmodel(typicallydoneinthemappingprocess)maybepronetoerrorsduetoapproximations.Furthermore,themap-pingitselfcauseserrors,ifthemappingdoesnotﬁttheunderlyingdata,e.g.,whenthechosenvisualvariablesdonotcorrespondtotheunderly-ingdatatypes.Theseissuescauseuncertaintiesinthisprocess,whichmayhinderthecomprehensibilityoftheunderlyingdata.Ingeneral,datashouldbemappedtopropervisualisationtechniquesusingtherightvisualvariables(e.g.,glyphvs.colour).Visualisation(Fig.3-s7):Inadditiontothevisualisationoftheun-certaintiesofthedataandaboveprocesses,thevisualisationitselfmaycontainuncertainties.Thisismainlyduetotheresolution,clutter,andcontrasteffectsoftheoutputvisualisationwhichmayhindertheuseringaininginsightsoftheunderlyingdata.SucheffectsinvisualisationsthatcauseuncertaintyinthereasoningprocessarediscussedbyZukandCarpendale[83]andMacEachrenandGanter[53].Model-VisCoupling(Fig.3-s6):Oneotheraspectthatweidentiﬁedforuncertaintypropagationinthesystem,istheuncertaintiescausedwhilecouplingthemodelandthevisualisation.Theseuncertaintiesmainlyimpacttheusers’interactionwiththesystemandthemodelsteeringthatiscoupledtothevisualisationinteractions.EndertetSystemHumanSACHA ET AL.: THE ROLE OF UNCERTAINTY, AWARENESS, AND TRUST IN VISUAL ANALYTICS 

243

s1: System 
worthiness

Trust-

s3: Uncertainties 
caused by Data 
Transormations

s4: Compatible 
Datatype, Visual 
Encoding,   Vis-
Technique and 

Tasks

s7: Visual 

Uncertainties  and 
Artefacts caused by 

Resolution or 
Overplotting

Visualisation

h8: Technical 

Competence, System 
Usage and Familiarity, 

Expectation

Including Propagated & 

s9: System Output
Hidden Uncertainty

h5: Analysis Type and 
Strategy, Reﬁnement

h3: Knowledge Gap 

and Initial Trust

h1: Human
- Expertise
- Experience
- Competence

Action

Hypothesis

s2: Types of Data 

Source 

Uncertainties

Data

s6: Model-Vis, 
Interactions  
and Updates

s5: Appropriate Model 
Building (Metric and 

Parameter)

Model

s8: Model Selection 

and Calibration

h6:Trust 
Calibration

Knowledge

h9: Perception and 

Uncertainty 
Awareness

Finding

Insight

h10: Perceptual Competence
Pattern Validation/Veriﬁcation

Visualisation Literacy

h7: Confrontation of 
Finding and Domain 

Knowledge

h4: Internalising/

Accepting Knowledge

h2: Knowledge
- Prior Knowledge 
- Internalised 
Knowledge

- Tactic Knowledge
- Domain Knowledge

Fig. 3: Each system component may change the data and consequently introduce additional uncertainty. Human trust building within knowledge
generation processes is affected by many human factors. The relation between uncertainty and trust is included as the awareness of uncertainties.

al. [23] propose an approach where direct interactions on visualisations
are directly translated to model steering interactions (e.g., highlighting
an item will increase weighting of the models distance function). If
these mappings are not well-designed, these model interactions are
translated to model steering interactions that do not ﬁt to the users’
intent. Furthermore, the visualisation of the model can be realised in
different ways. For example, it is possible to visualise incremental
model changes during the training phase (e.g., [26]). However, many
visual analytics application just visualise the model result.

All the uncertainties are propagated to the ﬁnal system output (Fig. 3-
s9) which will be observed (Fig. 3-h9) and used by the human for
knowledge generation. As important as it is to account for the uncer-
tainties in a system mentioned above, which uncertainties to account
for is highly dependent on the application scenario of the data.

3.2 Trust Building within Human Analytic Processes
We deﬁne trust on the human side as a counter part to the machine’s
uncertainties (similar to MacEachran et al.’s [54] distinction between
human and machine uncertainties). In the following, we walk through
the human concepts of the knowledge generation model [64] and de-
scribe them with respect to trust building and highlight inﬂuences that
are caused by uncertainties. Human trust building can be described as a
process of passing and accepting knowledge [11], in our case between
human and machine. On the other hand, there are many individual
factors that indirectly inﬂuence trust building, such as the technical
competence [58] and visualisation literacy that is dependent on the
users expertise and previous experience with a system.

Trust Calibration (Fig. 3-h6): In each knowledge generation step,
users have to calibrate their trust towards their counterpart, the system
(or automation as in [19], Fig. 3-s1), and they also need to calibrate their
trust between their own previous knowledge (Fig. 3-h1), hypotheses
(Fig. 3-h2) and the information that is presented by the system (Fig. 3-
s9, h10, h7). Trust calibration is inﬂuenced by all the dimensions
mentioned by Muir [58]: The “expectation of the persistence of natural
physical laws” allows for the creation and usage of mental models
(or rule bases). Further, Muir distinguishes three types of technical
competence (expert knowledge, technical facility and everyday routine
performance) that are essential for trust building. Finally, ﬁduciary
responsibility “is invoked as a basis for trust when the trustor’s own
technical competence is exceeded by the referent’s [(in our case the
VA-system)], or is not known to him” [58]. In visual analytics this is
often the case when complex processing or data mining algorithms are
applied but hidden behind the ﬁnal visual output.

3.2.1 Knowledge Generation Loop
The knowledge generation loop steers the whole analysis process and
consists of knowledge externalisation (there is a knowledge gap) and
internalisation (when sufﬁcient trust has been developed). We start our
description with the knowledge generation loop because the analysts’

initial trust and hypotheses are based on the prior knowledge and are
the foundation for all trust building activities [64].

Knowledge: In general, knowledge can be split into many subparts
such as domain-, tactic-, data-, system- or experience-based knowledge
(Fig. 3-h2) and has consequently a very individual nature [76, 80].
However, we can distinguish prior knowledge from the knowledge that
is gained during analysis and has to be internalised, synthesised and
related to the prior knowledge. Within this process, trust develops
and pieces of evidence that match or contradict the mental model of
the problem are collected and increase or decrease human trust levels.
Therefore, trust building depends heavily on the trustworthiness of
the machine counterparts (system or data). At the beginning the prior
knowledge is assumed to be valid or veriﬁed until the analysis reveals
evidence that strengthen or weaken it. Through evidence collection
supplemental trust emerges and ﬁnally transfers gained information to
internalised knowledge (Fig. 3-h4). Within this process humans utilize
their “private knowledge” in order to judge or interprete pieces of evi-
dences [75, 83]. It is also possible to gain knowledge with analytics,
even though the knowledge is based on high uncertainty (if the uncer-
tainty is known and understood as described in [21]). At this stage,
we also consider the type of user as an important factor (Fig. 3-h1). A
domain expert will behave differently from a machine learning-expert
or a novice user. The relation and former experiences with data analysis
systems also play a crucial role in trust building. The claim by Muir
that “the trust in a machine, once betrayed, may be hard to recover”
[58] is backed up by Manzey’s study that revealed a similar relation-
ship between error and subjective trust [56]. Furthermore, knowledge
includes many sub-components that inﬂuence trust building (e.g., the
technical competence or subjective attitudes [58]).

3.2.2 Veriﬁcation Loop
This loop describes higher-level trust building and covers confrontation
(of information) and human reasoning. If the trust in the combination
of all insights related to the hypothesis exceeds a certain amount and
integrates with prior knowledge, we leave the veriﬁcation loop and
arrive at new accepted knowledge (by induction).

Hypothesis: Hypotheses are derived from prior knowledge (because
there is a gap) and are the foundation for each veriﬁcation and explo-
ration cycle (abduction). Initially the trust in a hypothesis is derived
from prior knowledge and develops during the analysis process by
revealing pieces of evidence (insights) that support or contradict them
(Fig. 3-h3). With that respect, humans callibrate their trust and reﬁne
their hypothesis in order to come up with an explanation [28]. Also
the type and the initial trust in this hypothesis more or less deﬁnes the
following analysis type (Fig. 3-h5) as the veriﬁcation loop steers the
exploration loop [64]. A very vague and open hypothesis that is weakly
trusted will originate analysis with an exploratory fashion that solidiﬁes
the analysis step by step, whereas a very deﬁned and a highly trusted
hypothesis generates a conﬁrmatory analysis. In reality there are often
multiple, conﬂicting or dependent hypotheses that can be resolved with

Fig.2:Left:Trustcalibrationadaptedfrom[19],Right:Awarenessclassiﬁcationadaptedfrom[71].(seeFigure2right).TheyidentifyUnidentiﬁedUnknownsastheworstkindofmissinginformationbecauseinthatcasehumansarebuildingmoretrustthantheyshoulddo.AnalyticProvenance:Recentresearchfocusesontrackinginterac-tioninordertoinvestigatehumananalyticprocesses.Douetal.[20]presentanapproachtocapturehumanreasoningprocessesthatdistin-guishesbetweeninternal(interactionswithinthesystem)andexternalcapturing(outsidethesystem).RaganandGoodall[63]goontomentionthatprovenancetoolssupportthehumaninmemorisingandcommunicatinganalyticprocesses.Nguyenetal.[59]surveyanalyticprovenanceandshowthattypicalprovenanceconsistsofthreestages:Capturing,VisualisingandUtilisingwithcapturingondifferentlevels(events,low-levelactions,high-levelsub-tasks,top-leveltasks).Theyalsodescribethebeneﬁtsofanalyticprovenancethatgobeyondre-callingtheanalysisprocesstosupport“evidenceinconstructingthereasoningprocess,andfacilitatingcollaboration[...]”[59].ExamplesforleveraginganalyticprovenancewillbegivenintheguidelinesforhandlinguncertaintiesinSection4.Insummary,ourreviewdiscoverstwodistinctivegroupsofliterature.Onegroupdealswithuncertaintypropagationandvisualisationonma-chineaspects;theotherinvestigateshuman-machinetrustrelations.Weobserveimportantgapsinresearchwithinvisualanalyticsframeworks.Theseareuncertaintiesinthevisualisationitself,uncertaintiesinthecouplingbetweenmodelandvisualisation,anduncertaintiesinthemodelbuilding.Onlyfewstudiesrelateuncertaintiestothesehumantrustbuildingprocesses.Furthermore,thereisnoclearterminologythatdifferentiatesbetweenuncertaintiesfrommachineandhuman.Inthefollowing,weaddresstheseissuesandprovideamodelthatintegratesuncertaintypropagationandhumantrustbuilding.3UNCERTAINTYPROPAGATIONANDTRUSTBUILDINGWITHINTHEKNOWLEDGEGENERATIONPROCESSWithintheknowledgegenerationmodelforvisualanalytics,uncertain-tiesarepropagatedinthedifferentcomponents,causingvariousissuesoftrustonthehumanside.Inthefollowing,wegiveanexpos´eonhowuncertaintyispropagatedonthemachinesideandtrustiscallibratedonthehumanside.Finally,wediscusstheconceptofawarenessthatgluesuncertaintyandtrusttogether.3.1UncertaintyPropagationontheMachineSideWeextendtheframeworkofCorreaetal.[17],toincludeuncertaintiesinthemodelbuilding,visualisation,anduncertaintiesinthecouplingbetweenthemodelandvisualisation.Abriefdescriptionoftheroleofuncertaintyineachofthesecomponentsinasystem,follows.Uncertainty,generallyknownasthestateofnotknowing,isat-tributedtothediscrepancybetweenadatameasurementanditsrepre-sentation.AccordingtoGrietheandSchumann[32],conceptsregardedasuncertaintyareerrors,imprecision,accuracy,lineage,subjectiv-ity,non-speciﬁcity,noise(theauthorsprovideafulldescriptionofthesetermsin[32]).Uncertaintiesalsovarydependingontheapplica-tiondomain,forexample,topologicalconsistencyingeospatialdata.Throughoutthispaperwewillrefertoanyoneoftheseconceptsasuncertainty.Theseuncertaintiesindatacanbeclassiﬁedmainlyintotwocategories:1)sourceuncertainty,and2)propagateduncertainty.Accountingforsuchuncertaintiesindataisimportantforthoroughdataanalysis,informationderivationandinformeddecisionmaking.3.1.1SourceUncertaintyThesourceuncertainty(Fig.3-s2)isinherentindata,andlargelyde-pendsonthewayinwhichthisdataiscollected(authoritativevs.non-authoritativedata).Inmostcases,non-authoritativedatasuchassocialmediadatacontainshighuncertaintiesduetothelackofprofes-sionalgatekeepersandqualitycontrolstandardsamongotherreasons[27].Uncertaintiesinauthoritativedataaremainlyduetoreasonssuchaserroneousmeasurements,dataentryerrors,lowresolution,andincompletedata.Onewayofrepresentingsourceuncertaintiesisintermsofqualitativemeasuressuchaspurpose,usage,andlineage(alsoknownasdataprovenanceinmostdisciplines)[5].3.1.2PropagatedUncertaintyUncertaintyindataispropagatedduringthedatamodeling(wheredataundergotransformationssuchasinterpolation,sampling,quantisationetc.,[32,60])andvisualisationstages,wherethesepropagateduncertaintieskeepaggregatingasdatatravelsthroughthesestagesinthesystemsideoftheknowledgegenerationmodel(Figures1and3).DataProcessing(Fig.3-s3):Processingtechniquestransformdataforpreparationpurposes(e.g.,datacleansing).Theycanbebroadlygroupedintonormalisation,sub-sampling(reducingtheamountofdata)orinterpolation(increasingtheamountofdata).Uncertaintymeasuresrelatedtotheseprocessingtypescanbecalculatedwithstatistics[36].ModelBuilding(Fig.3-s5):Duringthemodelbuildingphase,ifusershavepreviousknowledgeofthemodel,theyachieveabestap-proximationbytypicallyﬁttingaparameterisedformofthemodeltothedata.Issuesofuncertaintyariseduetothecomplexityoftheparameterisation(e.g.,howmanyparametersaresuitable?)ortheappropriatenessoftheparameters(aretheparametersperfect/good/bad?),oreventherandomvariationofthemodelvariables.Atthisstagemodelcalibrationintroducesalotofuncertaintiesbythepro-cessofestimatingvaluesofunknownparametersinthemodel.Otheruncertaintiesariseifthedistancefunctions(e.g.,euclideandistanceorweightingswithinthesimilarityfunction)donotﬁtdataandtasks.Chatﬁeld[14]classiﬁesthesetypesofuncertaintiesasarisingfromi)modelmisspeciﬁcation.ModelUsage(Fig.3-s8):Chatﬁeld[14]statesthatalackofpreviousknowledgeoftheunderlyingphenomenoncausesinadequaciesofthemodel,whichgivesrisetostructuraluncertainties.Heintroducesii)specifyingageneralclassofmodels,wherethetruemodelisaspecial,unknowncase,andiii)choosingbetweenseveralmodelsofdifferentstructures,asreasonsthatgiverisetouncertaintiesinmodelusage.Additionally,themodelcarriesuncertaintiesintermsofitssuitabilitytothetaskathand.Numericalerrorsandapproximationsthatoccurduringtheimplementationofamodelgivesrisetoalgorithmicuncertainty[42].AsstatedbyBrodlieetal.[7],theseuncertaintieshavenotbeenthefocusofuncertaintyvisualisationresearchthusfar.VisualMapping(Fig.3-s4):Duringthemappingprocess,thecompu-tationofthegeometricmodel(typicallydoneinthemappingprocess)maybepronetoerrorsduetoapproximations.Furthermore,themap-pingitselfcauseserrors,ifthemappingdoesnotﬁttheunderlyingdata,e.g.,whenthechosenvisualvariablesdonotcorrespondtotheunderly-ingdatatypes.Theseissuescauseuncertaintiesinthisprocess,whichmayhinderthecomprehensibilityoftheunderlyingdata.Ingeneral,datashouldbemappedtopropervisualisationtechniquesusingtherightvisualvariables(e.g.,glyphvs.colour).Visualisation(Fig.3-s7):Inadditiontothevisualisationoftheun-certaintiesofthedataandaboveprocesses,thevisualisationitselfmaycontainuncertainties.Thisismainlyduetotheresolution,clutter,andcontrasteffectsoftheoutputvisualisationwhichmayhindertheuseringaininginsightsoftheunderlyingdata.SucheffectsinvisualisationsthatcauseuncertaintyinthereasoningprocessarediscussedbyZukandCarpendale[83]andMacEachrenandGanter[53].Model-VisCoupling(Fig.3-s6):Oneotheraspectthatweidentiﬁedforuncertaintypropagationinthesystem,istheuncertaintiescausedwhilecouplingthemodelandthevisualisation.Theseuncertaintiesmainlyimpacttheusers’interactionwiththesystemandthemodelsteeringthatiscoupledtothevisualisationinteractions.EndertetSystemHuman244 

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 22,  NO. 1, JANUARY 2016

the detection of a single expected or unexpected insight.

Insight: Insights are directly related to hypotheses and can be seen
as the pieces of evidence for or against them. The trust in insights
relates to the amount of similar ﬁndings that were produced by the
system that support the insight and also on their match to the domain
knowledge or the user’s mental model of the problem (i.e. what is
expected, Fig. 3-h7). If there is a mismatch between the user’s men-
tal model and the gained insight one of them has to be adjusted. In
other words, the user has to decide whether he trusts himself or the
information that was obtained using the machine. As described by
Green and Maciejewski [30], human higher level (“System 2”) ana-
lytical reasoning is able to modify the mental model. Another aspect
is that there might be more than one possible interpretation (insight
candidates) for a ﬁnding that can be tested. The analyst develops trust
in the alternative interpretations and may (or may not) be able to verify
them. However, insights are more likely to occur if they contribute
to a plausible narrative (because conﬁrmatory evidence is more likely
accepted by humans than disconﬁrmatory [31]) and therefore calibrate
their trust towards evidences that they are comfortable with.

3.2.3 Exploration Loop
The exploration loop covers lower-level trust building and evidence
search processes through analysis (deductive). Humans stay in the
exploration loop until they develop enough trust in their ﬁndings and
gain insights by appling their domain knowledge.

Action: Actions reﬂect many aspects in trust building (Fig. 3-h8) as
they are the direct interface between human and machine. In general,
actions can help the human to develop more trust in the system itself
if the user feels in control of the system. On the other hand, hard
operable systems and unexpected behaviour (or errors) may result in
a general decrease of trust in the system [21]. This trust depends on
the humans technical competence and familiarity with the system and
its subcomponents (as described by Muir [58]). Actions further help
the human to understand and decrease uncertainties. The ability to
explore different kinds of uncertainties enables the user to develop
an understanding of these uncertainties, where they arise and how
they impact the whole system pipeline (or the sensitivity of data items
[17]). Another approach is to actively reduce uncertainties by changing
the pipeline or the data (e.g., by choosing more suitable processing
methods, mappings or models that introduce less uncertainties). Data
changes can be done manually by correction or enrichment. In this
case an expert is enabled to bring in his user truth (expert knowledge)
in order to change data. However, at this stage users adjusting the data
according to their needs are at risk of introducing new human created
uncertainties.

Finding: As immediate results of an action, users observe a system
reaction (Fig. 3-h9, h10). This reaction (either expected or unexpected)
contributes to human trust building [21]. If users develop enough trust
in their observations they make them to ﬁndings. Or in other words,
they stay in the exploration loop until they are able to trust what they
see. On a perceptual level, users have to consider being misled by their
interpretation of visual elements. For example, a user might spot a
visual artefact that is not there in reality (e.g. Muller-Lyon illusion).
Furthermore, a ﬁnding may include many known and visualised, but
also hidden, uncertainties that have been propagated through the system.
With this respect, human trust building differentiates when uncertainty
information of a ﬁnding is communicated and considered [28, 77].
Findings are directly related to insights, which are themselves directly
related to gained knowledge. Consequently uncertainty propagates
from its root, the data source, through the system and human reason-
ing until knowledge. This is illustrated by the red ﬂow lines in the
knowledge generation model in Figures 1 and 3.

3.3 Awareness
We have considered the relationship between trust and uncertainties
within the system. Thus far, we assumed that the user is aware (Fig. 3-
h9) of the uncertainties. We now consider the possible effect on trust
when the user is unaware of uncertainties and how this might manifest

Table 1: Awareness classiﬁcation

system 

no uncertainties 

uncertainties 

aware 

trust = 
chance of human error =

high
none

trust = 
chance of human error =

1
med-low
low

 

n
a
m
u
h

2
mistaken 

1
med-low
chance of human error = med-low

trust = 

trust = 
chance of human error =

(over) 

high
high

unaware 

(under) 

trust = 
chance of human error =

medium
none

trust = 

medium

chance of human error = high

1 depends on degree of understanding
2 ’mistaken awareness’ is when the user wrongly believes the opposite, e.g. no uncertainties 
when in fact there are uncertainties in the system

itself in subsequent errors (similar to [71]). In addition, we illustrate
the situation that the user mistakenly believes there are no uncertainties
when in fact there are, and vice versa. A proposed classiﬁcation of the
different states of awareness and uncertainties is shown in Table 1.

We can see that trust it is highest when the user is either aware
of no uncertainties or mistakenly believes there are no uncertainties.
The latter is a case of over-trust leading to a high chance of errors.
The lowest trust is when the user is either aware of uncertainties or
mistakenly believes there are uncertainties. These are given a value
of medium to low as it depends on the users understanding of the
uncertainties, the higher the understanding (or mistaken understanding)
the higher the trust. The situation where the user is unaware that there
are no uncertainties, is a case of under-trust. Making the user aware
would increase their conﬁdence and hence trust. In terms of the chance
of errors occurring, this is highest when the user is either unaware of
uncertainties or wrongly believes that there are no uncertainties, and
lowest when the user is aware or unaware that there are no uncertainties.
Whether or not a user becomes aware of uncertainties and indeed
takes note of information presented to them, can be inﬂuenced by cog-
nitive biases. These, so called, cognitive biases, ﬁrst introduced by
Kahneman and Tversky in the 1970s [40], are deviations in judgment
from what rational decision models would predict that occur in particu-
lar situations. Importantly, they are involuntary, affect most people to
some degree and generally have a negative impact on decision making.
For instance, most people have a poor understanding of statistics and
instead apply simplifying heuristics to cope with the uncertainty, which
leads to irrational decisions. Arnott [2] lists such statistical biases
which highlight the inability to comprehend the practical implications
of randomness, base rate, sample size, correlations, regression to the
mean and probability in many guises.

Visual analytic systems allow the user to explore datasets but this
relies on the user wanting to seek further information. Unfortunately,
conﬁrmation bias is the tendency to ignore information that does not
agree with the user preconception or hypothesis [41]. In a recent study,
Phillips et al. [62] demonstrate that users of information systems, tend
to reduce the perceived usefulness of information that does not reinforce
their current premise, which in turn reduces their likelihood to explore
the data. Over-conﬁdence and perceived expertise has a similar effect.
Completeness bias, where the user perceives that the data is logical and
correct, without uncertainties, may also reduce information seeking.

We need to be aware of other perceptual and behavioural traits when
utilising visualisation and automated systems. For instance, our visual
perceptual system is subject to errors due to effects such as contrast,
colour, clutter and pre-attentive processing. In addition, automation
bias can lead the user to overtrust and relying on wrong information
that is produced by an automation, overriding their own ability to judge
the situation (“looking-but-not-seeing effect”[56]).

As suggested at the start of this section, awareness of uncertainties
can reduce errors and increase the users trust in the data. However,
cognitive biases may impede the users awareness and additionally
may lead to poor decisions, especially when the user is in a state of
uncertainty. Principally due to the involuntary nature of cognitive

SACHA ET AL.: THE ROLE OF UNCERTAINTY, AWARENESS, AND TRUST IN VISUAL ANALYTICS 

245

biases, reducing their negative effects has proved difﬁcult, even when
the user is informed of the possible impact of particular cognitive biases.
In the next section, we will enumerate some methods to reduce the
impact of cognitive biases, perception effects and the automation bias.

4 GUIDELINES, EXAMPLES & CHALLENGES FOR HANDLING

UNCERTAINTIES

In this section we formulate guidelines for handling uncertainties and
illustrate them with examples from literature. G1 and G2 are the
foundation for uncertainty communication by tracking, quantifying and
combining uncertainties. G3, G4 and G6 aim to improve the perception
of a systems’ trustworthiness through the communication of uncertainty
information. G5, G7 and G8 take human issues into account in order
to enhance, identify and recalibrate poorly calibrated trust dimensions.
Our guidelines have been inﬂuenced by Muirs recommendations for
improving trust calibration [58] (see Section 2.2). Additionally, we put
forward some extensions and challenges that suggest future research
directions.

4.1 Uncertainties in a System

G1: Quantify Uncertainties in Each Component: We recommend
to quantify uncertainties at every stage of the visual analytics pipeline.
In the following we give examples for each component starting from
left to right side of the model in Figure 3.

Data Source: Data source uncertainty can be quantitatively measured
by, for example, standard deviation to measure the precision of the
instrument used to collect data, or counting the number of omissions
or commissions in a database to measure the completeness of the
data. Furthermore, several qualitative measures can be used to get an
overview of the source uncertainty of a dataset. These are the lineage,
purpose, and usage (described in Section 3.1). These measures are
typically documented in the metadata of a dataset by the data producer.
These qualitative measures are subjective measures as they pertain to
the views of the producer who documents this metadata according to
their use cases. In authoritative data, this metadata will be adequately
documented due to the professional gate-keeping of this data. In non-
authoritative data (such as social media data) we will see little or
none such documentation of measures, due to the lack of gate-keepers.
Measure such as credibility [12], reputation [57], or trustworthiness [6]
are used to measure the uncertainty of such non-authoritative data.

Data Processing: System inputs that go through transformations
such as interpolation, extrapolation, normalisation etc., propagate uncer-
tainty at the system output. Choosing a suitable uncertainty propagation
method depends on the conﬁdence level, the extent to which uncer-
tainty quantiﬁcation is needed, and the computational cost that one can
endure [49]. Probabilistic approaches (e.g., Monte Carlo Simulation
methods) are known to be most robust in quantifying such uncertain-
ties. Lee and Chen [49] describe in detail ﬁve types of probabilistic
approaches in their comparative study of uncertainty propagation meth-
ods. Statistics such as standard deviation, variance and range are further
used to propagate data processing uncertainties. Additionally, Cedilnk
and Mendoza [13] use distance based functions to measure the similar-
ity of values and point out that interpolated values can also be used.

Model Building: According to Chatﬁeld [14] uncertainties that arise
at the model building stage can be lessened by expert background
knowledge (e.g., to know which variables to include), and previous
experience/information from similar datasets. However, such expert
knowledge may not prevent the user in mistakenly excluding an impor-
tant variable or adding excess variables. The author points out that one
way of avoiding model building uncertainty is to use nonparametric
procedures that are based on fewer assumptions. One approach to
quantify the uncertainties is to use distance functions to measure the
distance of parameterisation from the true value.

Model Usage: Chatﬁeld [14] gives a broad discussion on how un-
certainties arise in many aspects of a model (as brieﬂy discussed in
Section 3.1). To propagate the uncertainties in the model selection
bias, he suggests using the Bayesian averaging approach, and points
out the non-triviality of biases. He recommends replicating the study
to check if the new data ﬁts the model, although makes the point that

replicating studies are not all that simple to conduct. Works of Fernan-
dez et al. [25], and Kennedy and OHagan [42] demonstrate the use of
Bayesian approaches to dealing with model uncertainty.

Visual Mapping: Uncertainties that occur at the visual mapping stage
are mainly due to the use of inappropriate visual variables that do not ad-
here to the data and task at hand. The most sensible approach to assess
these uncertainties is through analysing the chosen visual variables and
metaphors against existing systematic taxonomies. In his task by data
type taxonomy, Schneiderman [69] categorises existing information
visualisation techniques according to the type of data (e.g., temporal
data) and the task (e.g., zoom or ﬁlter). In the case of uncertainty
visualisation, we need to consider the added uncertainty dimension to
the underlying data. In addition to MacEachrens [51] work on manipu-
lating several visual metaphors to represent uncertainty, Buttenﬁeld and
Weibel [9] present a framework for categorising different cartographic
visualisation methods according to the uncertainty elements (e.g., posi-
tional accuracy or the lineage of the data) and the measurement scale
of the data (e.g. discrete or categorical data). Furthermore, in a recent
classiﬁcation, Senaratne and Gerharz [67] categorised popular uncer-
tainty visualisation methods according to the measurement scale of
the data (e.g., continuous or categorical), supported data format (e.g.,
raster or vector), and the type of uncertainty element in the data (e.g.,
positional or thematic uncertainty).

Visualisation: The works of MacEachren and Howard [37, 51] have
developed visual metaphors for representing uncertainty, that ﬁts well
with the human cognition model. Examples are the use of blurring
effects, transparency, or coarsely structured surfaces to represent un-
certainty. Their impact on decision making under uncertainty has been
explored in several studies (e.g., [68]). MacEachren and Ganter [53]
classify visualisation uncertainties as being developed through two
types of errors. Type 1: seeing what is not really there and Type 2:
over-seeing what is really there. The authors emphasise the need for
tools to aid the users in seeing through these Type 1 and Type 2 errors
in visualisations. Relating to the Type 2 errors in particular, Brodlie
et al. [7] point out uncertainties caused by the lower resolution of the
visualisation in contrast to the resolution of the data.

Model-Vis Coupling: We are not aware of existing methods that
quantify the uncertainties that arise due to the coupling between visu-
alisation and models. One possibility to quantify differences between
model and visualisations is to compare measures of the different spaces
(e.g., visual 2D compared to HD as described in [73]) in order to com-
pare model and visualisation characteristics. For example, groups and
distances between data items in model space (e.g., between cluster
centroids) can be compared to their distances in visual space (e.g.,
projected distances between cluster centroids). Another approach is
to measure how model changes (e.g., via human interaction or data
streaming) are propagated to the visualisation. Most of the visualisa-
tions take the ﬁnal model result but there are several cases, and models
that deliver incremental results that can be visualised (e.g., [26]).

G2: Propagate and Aggregate Uncertainties: Systems require
powerful and sophisticated techniques to support exploration of large
datasets. Adding different kinds of uncertainty to this data requires
an increase in the level of sophistication of the system. Works of [17]
estimate the data source uncertainty and the propagated uncertainty
through transformations, via sensitivity analysis and error modeling. To
simplify the computations, we require intelligent methods to aggregate
these propagated uncertainties. Klir and Wierman [44] describe meth-
ods to aggregate source uncertainties and propagated uncertainties in
the visualisation pipeline. Also, through a remote sensing classiﬁcation
application, Van de Wel et al. [78] describe the use of an entropy mea-
sure to build a weighted uncertainty aggregation measure. They map
the different kinds of uncertainty to one measure based on a weighted
criteria. Learning from this, an alternative would be for the user of the
system to weigh each kind of uncertainty stemming from the system,
based on its importance to the use case at hand.

G3: Visualise Uncertainty Information: Uncertainty visualisation
is known to be a most effective medium to communicate such source
and propagated uncertainties. Griethe and Schumann [32] present

246 

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 22,  NO. 1, JANUARY 2016

a pipeline to show the process of uncertainty visualisation. In their
pipeline, they differentiate between four different kinds of data ﬂows. 1)
the basic data transformation process through the visualisation pipeline:
is separated in to data components and their corresponding uncertain-
ties, such that the user sees the underlying uncertainty; 2) in/output of
the acquisition of uncertainty data: data at every stage of the visualisa-
tion will carry uncertainty, and needs to be considered; 3) dependencies
between the visualisation of the raw data and its uncertainty: while the
data is explored, its uncertainty is considered as an integral part of the
data. However, decisions in processing the uncertainty is dependent on
what raw data is focused on, which rendering techniques and geomet-
ric forms for models are chosen. 4) parameterisation of the pipeline:
uncertainty is not visible by itself at every data component as in 1). In-
stead, uncertainty will be used to parameterise visualisation of the other
data (as done by Schmidt et al. [65]). Furthermore, in visualising the
uncertainties in the different stages of the data, one needs to carefully
consider the different design principles. Works such as of Pang [60]
focus on visualising multi-dimensional uncertainties in data. These can
be used as guidelines on how to design visualisations to incorporate
different uncertainties propagated through analysis system. Griethe and
Schumann [32] further emphasise that the decisions on the amount of
user interaction on such an uncertainty visualisation process depends
on the users experience and the principles of the visualisation system.
Finally, a system should report the uncertainties as cognitive cues about
its self-conﬁdence as suggested by Cai and Lin [10]. As a result, users
are more comfortable in adjusting their trust appropriately.

G4: Enable Interactive Uncertainty Exploration: Within a vi-
sual analytics environment, enabling the user to interact and explore
different visualisations for different uncertainties stemming from the
different components of the system will enrich the users understanding
of the true nature of the data, and additionally, how different propa-
gated uncertainties inﬂuence the ﬁnal output. Also, the ability to use
a variety of visualisations may also help with illusion type cognitive
biases such as clustering and correlation. It is also important to give
the user control to decide which of these uncertainties should inﬂuence
the ﬁnal output, or with how much importance it should inﬂuence the
output. Providing the user with the possibility of giving weighted mea-
sures for each uncertainty component would be a realistic approach.
Furthermore, Correa at al. [17] present several approaches including
uncertainty projections and visualisations that enable the user to explore
the uncertainties of individual data items and the impacts of different
uncertainties (Figure 4-a).

4.2 Supporting Human Factors

G5: Make the Systems Functions Accessible: Accessible, intu-
itive and easy to use interaction techniques will increase the technical
competence of the analyst and consequently enhance human trust build-
ing [58]. In this respect, different user groups have to be considered.
Expert and power-users of an analysis tool will need different interac-
tion possibilities and guidance than novice users. For example, with
visual analytics tools we often observe users having problems with
model steering interactions such as parameter setting [64]. In this case,
switching between expert or learning mode might be a ﬁrst step in that
direction. Endert et al. give a nice example of “semantic interaction”
[22] where direct manipulation interactions are directly translated to
model steering interactions. Furthermore, Chuang et al. [16] provide
guidelines for designing trustworthy and understandable visual analyt-
ics systems. Their recommendations can verify modeling decisions and
provide model interactions during analysis.

G6: Support the Analyst in Uncertainty Aware Sensemaking:
Human sensemaking can be supported by offering note-taking or knowl-
edge management components connected to the systems, where humans
can externalise and organise their thoughts in order to bridge data and
knowledge management [64] (see Figure 4-d, e.g., the Sandbox for
Analysis [15, 81]). Our recommendation is to transfer and visualise un-
certainty information to the ﬁndings that are imported from the analysis
part in order to support humans in calibrating their trust in the ﬁndings’
trustworthiness [19]. We can imagine that a system will automatically

take care of relations between (conﬂicting) hypotheses, ﬁndings, in-
sights and take the role of an unbiased counterpart to the human by
including uncertainty information at any stage [31]. A system could, for
example, calculate aggregated uncertainty scores for pieces of evidence
that have been grouped by the user. In addition, evidence connected to
hypotheses and insights may be explicitly marked by the user as trusted
or unknown (or intermediate value). This would be a form of trust
annotation that can be matched to uncertainty measures. Furthermore,
humans can integrate external evidences from other systems or their
own knowledge that might complete the big picture of the analysis.
Utilising all the connected information enables a system to offer un-
certainty and trust cues e.g., as glyphs connected to the items (such as
trafﬁc lights, radar charts or avatars as described in [19]). Figure 4-b
illustrates an example view of automation blocks that are enriched with
specially designed glyphs that serve as trust cues.

G7: Analyse Human Behaviour in order to Derive Hints on
Problems and Biases: Tracking human behaviour can be beneﬁcial
in deriving hints on the users of a system. We therefore recommend
to leverage analytic provenance approaches suggested by Nguyen et
al. [59]. Low level interaction tracking can be used to predict users
performance [8] or infer the user frustration [34]. These methods could
be enhanced for predicting a users trust level. Closer measures related
to uncertainties and trust building can be captured by the rate of overall
decision switching. Goddard et al. [29] measured automation bias by
noting correct to incorrect prescription switching. Furthermore, Klem-
mer et al. were able to detect important notes or items based on user
tracking [43] (see Figure 4-c). These methods could be leveraged by
a system to automatically suggest alternative visualisations or items
that have not been utilised. The latter may be useful in mitigating
some selection based cognitive biases such as conﬁrmation bias [46].
Another approach to derive human trust measures is to analyse user
generated contents. A system could automatically seek for signal words
such as “unsure, uncertain, maybe, perhaps ...”. Zhou et al. describe 27
language features grouped as: quantity, informality, expressivity, affect,
uncertainty, nonimmediacy, diversity, speciﬁcity, and complexity [82].
Also, Tenbrink [74] investigated how to derive cognitive analytical
processes based on language data. Physical or other human sensors
such as eye-tracking can also be used. Kurzhals et al. give an overview
on the potential for eye tracking in visual analytics [47]. Furthermore,
user analysis may be used during system development and evaluation.
Scholtz describes ﬁve evaluation areas: Situation awareness, collab-
oration, interaction, creativity, and utility [66]. We imagine protocol
analysis [24] as a useful method to interpret “think aloud” evaluations.
User interviews using trust questionnaires could also be conducted [77]
in order to investigate the relation between uncertainty and trust for
system evaluations. In addition, Bass et al. propose a method to analyse
and predict a humans understanding of automation [4].

G8: Enable Analysts to Track and Review their Analysis: This
guideline points to post-analysis activities as a method to detect and
mitigate biases. During analysis, users often focus on searching poten-
tial evidences without considering alternatives, errors or uncertainties.
In addition, users in their “work ﬂow” should not be interrupted [38].
Therefore, we recommend that the analyst is able (or even encouraged)
to look and think about his analysis afterwards, without interruption
during the analysis. In our opinion this is a better way than warning
users during their analysis (e.g., by popup dialogs) as recent studies
show that too often warnings may lead to the opposite [1]. Support to
mitigate statistical biases (see Section 3.3) should be provided, such
as presenting the user with base rate information (e.g., typical distri-
bution), estimating realistic probabilities or indicating that a particular
‘behaviour’ is expected rather than a special case, as with regression
to the mean. Structured analytic techniques such as a devils advocate
may also be ways that help the user to detect problems and lessen
the impact of conﬁrmation bias in particular. Furthermore, analysis
process visualisation enables involving other users and story telling.
Provenance systems such as CzSaw [39], but also systems including
story telling [45], are a good starting point in that direction.

Fig.4:Examplesforuncertaintyawaretrustbuildingfromdifferentdomains:(a)Anuncertaintyprojectiontoexplorehowdataitemsareaffectedbyuncertainties[17],(b)decisionsupportincludingtrustcuedesignfrom[19],(c)derivingimportantusernotesbasedonusertracking[43],(d)integratingevidencesforcomputerassistedknowledgemanagement[15].5DISCUSSION,LIMITATIONANDCONCLUSIONInthissection,wediscussourﬁndings,providelimitationsandopenquestionsofourstudy,andconcludewithsometakeawaymessages.5.1DiscussionOurframeworkshowsthathumantrustbuildingunderuncertaintyisanextremelycomplicated,individualprocessandisinﬂuencedbymanyfactorsdirectlyaswellasindirectly.Furthermore,usersinformedwithuncertaintyinformationcanavoidfallingintotrapsconcerningmistakenuncertaintiesandunawareuncertainties.Readersalsoneedtonotethattheframeworkhastobetailoredtoconcrete,individualcaseswherethescopeofuncertainties,usersandtheirtasksareknown.Thecorevalueofourframeworkistoprovideabalancedviewontheroleofuncertaintypropagationandtrustbuildinginvisualanalyticsbyconsideringhumanandmachineaspectstogether.Theguidelinesinthispaperwillbeusefultoestimatethedynamicsofuncertaintiesindevelopingvisualanalyticsapplications.ThetermsandstructureweoutlineinSection3provideanoverviewofuncertaintypropagation,bothfromthesourcedataandfromalgorithmicmanipu-lation.Withthisstructure,practitionersandresearcherscanattempttoquantifyuncertaintiesthoughtheprocessofdatatransformations.Dependingonitsuse,thisquantiﬁcationwillhelpusersdetermineeffectivevisualisationtechniquesbythinkingofthetrade-offsbetweengaininginsightsandshowinguncertainties.TheframeworkweprovideinSection3canbeusedtoeducateusersofvisualanalyticsapplicationsaboutuncertaintiesandtheirimpact,sothattheymightreduceerrors(e.g.,cognitivebiases)andbuildtrustwhilstanalysingdata.Werecommendsystemdeveloperstoprovideasimpletutorialoftheirvisualanalyticsapplicationsusingsomeusagescenarios.Inadditiontoassistingusers,thematerialitselfprovidesagroundworkforeducationofuncertaintiesandtrustbuildingfordesigners,practitioners,andresearchers.Webelievethattheimpactofuncertaintieswilldecreaseasusersgainawareness.Therearemanyimplicationsofthismodel.Asexplained,itisnecessaryforustocapturehumansperceiveduncertaintiesandtrustlevelsatagivenmomentofanalysis.Onewayistointerveneintheanalysisprocessbyaskingtheuserstoinputthisdirectly,whichwillensureaccurateestimatesoftheircurrentstatus.However,toavoidinterruptionsitwouldbeusefultocomputethisautomatically.Thismaybepossiblethroughtracingandinterpretingusagelogstoestimatetheleveloftrust.Dataprovenancemaybeaneffectivemethodtotrackuncertaintypropagationthatenablesustoincreaseuncertaintyawareness.Ontheotherhand,ifanalyticprovenancemethodsareusedtoinferhumanmeasuresthismaygivehintsontrustbuildingprocesses.Combiningmeasures/methodsfrombothsideshasthepotentialtoidentifyrelationsbetweenuncertaintypropagationandhumantrustbuilding.Ourcontributionistocategorisetypesofuncertainties,awarenessofthem,andhumantrustbuildingprocess.However,therearemanyexternalfactorsthatcaninﬂuenceindividualprocesses.Forexample,ourmodelassumesasingleanalystperspective,whichsimpliﬁestheknowledgegenerationprocess.Intherealworld,manyinterdependentknowledgegenerationloopsruninparallelandoftenconﬂicteachother,whichcanresultinuncertainoutcomes.Furthermore,takingintoaccountcollaborationbetweenhumananalystswouldextendthemodeltoexplainthedynamicsofrealworldscenarioswithateamofanalysts.5.2LimitationsandOpenQuestionsThescopeofthisstudyprovidesaframeworkofunpackinguncertaintypropagationwithinvisualanalyticsprocessesaswellasdiscoveringthehumantrustbuildingprocess.Hereweprovidelimitationsofourapproachaswellasopenquestionsthatfutureresearcherscaninvestigate.Firstly,uncertaintiesaredifﬁculttobequantiﬁedandcategorisedintoasingleprocess.Invisualanalyticssystems,uncertaintiescanbepropagatedandimpliedthroughthepipelines,aswediscussed.Thus,combinationofuncertaintiesfrommultiplesourcescouldbelargerthanthesum.Ourmodeldoesnotprovideaquantiﬁedmodelofsuchintertwinedprocessofuncertaintypropagationjustyet.AsoutlinedinG1andG2,someeffortshavebeenmadetoquantifyandaggregatedifferentsubsetsofuncertaintypropagationwithinvisualanalyticsprocess.Researchersmayneedtointegratesucheffortsusingouroverarchingmodelandpredictsuchuncertaintypropagationinaspeciﬁccontext.Secondly,anotheropenquestioniswhetherthetransparencyofun-certaintypropagationisalwaysgoodandhowmuchofitisbeneﬁcialtousers.Ourmodelbuildsuponanassumptionthatmakingtheuncer-taintypropagationtransparentwillletusersbeawareofvariationintheiroutcomes.However,providingtoomuchinformationcouldcon-fuse,overwhelm,andmisleadusers,therebymakingunwantedhumanerrors.Furthermore,itisalsoatradeoffbetweenefﬁciencyandaccu-racy.Forinstance,applicationsforhumansafety,whereuncertaintycanresultincatastrophicresults,mayneedtoconsiderasmuchtrans-parencyaspossible.Ontheotherhand,somebusinessanalyticsmayrequirefastandreasonableanalysisresults.Thus,itwillbeinterestingtoinvestigatewhatareproperamountsandmethodstocommunicateuncertaintyinformationtodifferentgroupsofvisualanalyticsusers.Thirdly,inlinewithpreviouspoints,itisalsoanopenquestionwhethertheawarenessofuncertaintiesleadstoincreasingordecreas-ingtrust.Thisquestionmaybefromthehuman’strustbuildingprocess.Tobuildtrustinvisualanalyticsoutcomes,usersmayneedtobuildtrustinthevisualanalyticssystemﬁrst.Inthisprocess,theawarenessofuncertaintiesmayleadtoincreasingtheawarenessofvisualanalyt-icsprocessbutnottoincreasingtrustintheoutcomes.Thus,futureresearchmayinvestigatethesophisticatedprocessofhuman’strustbuildingstepsunderuncertainty.Fourthly,inthisregard,wemaythinkoftheawarenessprovenancetoverifyhuman’sunderstanding.Weintroducedtheconceptofawarenesstobridgebetweenmachine’suncertaintiesandhuman’strust.Theawarenessagainishighlysubjectivetoindividualslikethetrustlevel,soitwillbedifﬁculttoquantifytheinformation.Nonetheless,theawarenessindeedaffectstheentireprocess,sowecallforresearchintocapturingit.Thesepointsabovedonotcapturealllimitationsandopenquestionfromourstudy,butwillbeaninterestingstartforfuturework.SACHA ET AL.: THE ROLE OF UNCERTAINTY, AWARENESS, AND TRUST IN VISUAL ANALYTICS 

247

Fig.4:Examplesforuncertaintyawaretrustbuildingfromdifferentdomains:(a)Anuncertaintyprojectiontoexplorehowdataitemsareaffectedbyuncertainties[17],(b)decisionsupportincludingtrustcuedesignfrom[19],(c)derivingimportantusernotesbasedonusertracking[43],(d)integratingevidencesforcomputerassistedknowledgemanagement[15].5DISCUSSION,LIMITATIONANDCONCLUSIONInthissection,wediscussourﬁndings,providelimitationsandopenquestionsofourstudy,andconcludewithsometakeawaymessages.5.1DiscussionOurframeworkshowsthathumantrustbuildingunderuncertaintyisanextremelycomplicated,individualprocessandisinﬂuencedbymanyfactorsdirectlyaswellasindirectly.Furthermore,usersinformedwithuncertaintyinformationcanavoidfallingintotrapsconcerningmistakenuncertaintiesandunawareuncertainties.Readersalsoneedtonotethattheframeworkhastobetailoredtoconcrete,individualcaseswherethescopeofuncertainties,usersandtheirtasksareknown.Thecorevalueofourframeworkistoprovideabalancedviewontheroleofuncertaintypropagationandtrustbuildinginvisualanalyticsbyconsideringhumanandmachineaspectstogether.Theguidelinesinthispaperwillbeusefultoestimatethedynamicsofuncertaintiesindevelopingvisualanalyticsapplications.ThetermsandstructureweoutlineinSection3provideanoverviewofuncertaintypropagation,bothfromthesourcedataandfromalgorithmicmanipu-lation.Withthisstructure,practitionersandresearcherscanattempttoquantifyuncertaintiesthoughtheprocessofdatatransformations.Dependingonitsuse,thisquantiﬁcationwillhelpusersdetermineeffectivevisualisationtechniquesbythinkingofthetrade-offsbetweengaininginsightsandshowinguncertainties.TheframeworkweprovideinSection3canbeusedtoeducateusersofvisualanalyticsapplicationsaboutuncertaintiesandtheirimpact,sothattheymightreduceerrors(e.g.,cognitivebiases)andbuildtrustwhilstanalysingdata.Werecommendsystemdeveloperstoprovideasimpletutorialoftheirvisualanalyticsapplicationsusingsomeusagescenarios.Inadditiontoassistingusers,thematerialitselfprovidesagroundworkforeducationofuncertaintiesandtrustbuildingfordesigners,practitioners,andresearchers.Webelievethattheimpactofuncertaintieswilldecreaseasusersgainawareness.Therearemanyimplicationsofthismodel.Asexplained,itisnecessaryforustocapturehumansperceiveduncertaintiesandtrustlevelsatagivenmomentofanalysis.Onewayistointerveneintheanalysisprocessbyaskingtheuserstoinputthisdirectly,whichwillensureaccurateestimatesoftheircurrentstatus.However,toavoidinterruptionsitwouldbeusefultocomputethisautomatically.Thismaybepossiblethroughtracingandinterpretingusagelogstoestimatetheleveloftrust.Dataprovenancemaybeaneffectivemethodtotrackuncertaintypropagationthatenablesustoincreaseuncertaintyawareness.Ontheotherhand,ifanalyticprovenancemethodsareusedtoinferhumanmeasuresthismaygivehintsontrustbuildingprocesses.Combiningmeasures/methodsfrombothsideshasthepotentialtoidentifyrelationsbetweenuncertaintypropagationandhumantrustbuilding.Ourcontributionistocategorisetypesofuncertainties,awarenessofthem,andhumantrustbuildingprocess.However,therearemanyexternalfactorsthatcaninﬂuenceindividualprocesses.Forexample,ourmodelassumesasingleanalystperspective,whichsimpliﬁestheknowledgegenerationprocess.Intherealworld,manyinterdependentknowledgegenerationloopsruninparallelandoftenconﬂicteachother,whichcanresultinuncertainoutcomes.Furthermore,takingintoaccountcollaborationbetweenhumananalystswouldextendthemodeltoexplainthedynamicsofrealworldscenarioswithateamofanalysts.5.2LimitationsandOpenQuestionsThescopeofthisstudyprovidesaframeworkofunpackinguncertaintypropagationwithinvisualanalyticsprocessesaswellasdiscoveringthehumantrustbuildingprocess.Hereweprovidelimitationsofourapproachaswellasopenquestionsthatfutureresearcherscaninvestigate.Firstly,uncertaintiesaredifﬁculttobequantiﬁedandcategorisedintoasingleprocess.Invisualanalyticssystems,uncertaintiescanbepropagatedandimpliedthroughthepipelines,aswediscussed.Thus,combinationofuncertaintiesfrommultiplesourcescouldbelargerthanthesum.Ourmodeldoesnotprovideaquantiﬁedmodelofsuchintertwinedprocessofuncertaintypropagationjustyet.AsoutlinedinG1andG2,someeffortshavebeenmadetoquantifyandaggregatedifferentsubsetsofuncertaintypropagationwithinvisualanalyticsprocess.Researchersmayneedtointegratesucheffortsusingouroverarchingmodelandpredictsuchuncertaintypropagationinaspeciﬁccontext.Secondly,anotheropenquestioniswhetherthetransparencyofun-certaintypropagationisalwaysgoodandhowmuchofitisbeneﬁcialtousers.Ourmodelbuildsuponanassumptionthatmakingtheuncer-taintypropagationtransparentwillletusersbeawareofvariationintheiroutcomes.However,providingtoomuchinformationcouldcon-fuse,overwhelm,andmisleadusers,therebymakingunwantedhumanerrors.Furthermore,itisalsoatradeoffbetweenefﬁciencyandaccu-racy.Forinstance,applicationsforhumansafety,whereuncertaintycanresultincatastrophicresults,mayneedtoconsiderasmuchtrans-parencyaspossible.Ontheotherhand,somebusinessanalyticsmayrequirefastandreasonableanalysisresults.Thus,itwillbeinterestingtoinvestigatewhatareproperamountsandmethodstocommunicateuncertaintyinformationtodifferentgroupsofvisualanalyticsusers.Thirdly,inlinewithpreviouspoints,itisalsoanopenquestionwhethertheawarenessofuncertaintiesleadstoincreasingordecreas-ingtrust.Thisquestionmaybefromthehuman’strustbuildingprocess.Tobuildtrustinvisualanalyticsoutcomes,usersmayneedtobuildtrustinthevisualanalyticssystemﬁrst.Inthisprocess,theawarenessofuncertaintiesmayleadtoincreasingtheawarenessofvisualanalyt-icsprocessbutnottoincreasingtrustintheoutcomes.Thus,futureresearchmayinvestigatethesophisticatedprocessofhuman’strustbuildingstepsunderuncertainty.Fourthly,inthisregard,wemaythinkoftheawarenessprovenancetoverifyhuman’sunderstanding.Weintroducedtheconceptofawarenesstobridgebetweenmachine’suncertaintiesandhuman’strust.Theawarenessagainishighlysubjectivetoindividualslikethetrustlevel,soitwillbedifﬁculttoquantifytheinformation.Nonetheless,theawarenessindeedaffectstheentireprocess,sowecallforresearchintocapturingit.Thesepointsabovedonotcapturealllimitationsandopenquestionfromourstudy,butwillbeaninterestingstartforfuturework.248 

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS,  VOL. 22,  NO. 1, JANUARY 2016

5.3 Conclusion
In conclusion, we have illustrated how uncertainties arise, propagate
and impact human knowledge generation processes by relating the
concepts of trust, calibration and awareness. Further, we have given
hints on misconﬁgurations of uncertainty awareness that may cause
human errors in data analysis. We provide guidelines that describe
various ways to handle uncertainties and to include human factors in
order to enhance human trust calibration. Finally, we put forward
open research areas that will contribute to more reliable knowledge
generation in visual analytics in the future.

ACKNOWLEDGMENTS
This work was supported by the EU project Visual Analytics for Sense-
making in Criminal Intelligence Analysis (VALCRI) under grant num-
ber FP7-SEC-2013-608142 and the SPP 1335 project Visual Analysis
on Movement and Event Data in Spatiotemporal Context.

REFERENCES
[1] B. B. Anderson, C. B. Kirwan, J. L. Jenkins, D. Eargle, S. Howard, and
A. Vance. How polymorphic warnings reduce habituation in the brain—
insights from an fmri study. Proc. of CHI’15, 2013.

[2] D. Arnott. Cognitive biases and decision support systems development: a

design science approach. Inf. Syst. J., 16(1):55–78, 2006.

[3] B. Barber. The logic and limits of trust, volume 96. Rutgers University

Press New Brunswick, NJ, 1983.

[4] E. J. Bass, L. A. Baumgart, and K. K. Shepley. The effect of information
analysis automation display content on human judgment performance
in noisy environments. Journal of cognitive engineering and decision
making, 7(1):49–65, 2013.

[5] M. K. Beard, B. P. Buttenﬁeld, and S. B. Clapham. NCGIA Research
Initiative 7: Visualization of Spatial Data Quality: Scientiﬁc Report for
the Specialist Meeting 8-12 June 1991, Castine, Maine. National Center
for Geographic Information and Analysis, 1991.

[6] M. Bishr and K. Janowicz. Can we trust information?-the case of Volun-
teered Geographic Information. In Towards Digital Earth Search Discover
and Share Geospatial Data Workshop at Future Internet Symposium, vol-
ume, volume 640, 2010.

[7] K. Brodlie, R. A. Osorio, and A. Lopes. A review of uncertainty in
data visualization. In Expanding the Frontiers of Visual Analytics and
Visualization, pages 81–109. Springer, 2012.

[8] E. T. Brown, A. Ottley, H. Zhao, Q. Lin, R. Souvenir, A. Endert, and
R. Chang. Finding waldo: Learning about users from their interactions.
IEEE Trans. Vis. Comput. Graph., 20(12):1663–1672, 2014.

[9] B. Buttenﬁeld and R. Weibel. Visualizing the quality of cartographic
data. In Third International Geographic Information Systems Symposium
(GIS/LIS 88), San Antonio, Texas, 1988.

[10] H. Cai and Y. Lin. Tuning trust using cognitive cues for better human-
machine collaboration. In Proceedings of the Human Factors and Er-
gonomics Society Annual Meeting, volume 54, pages 2437–2441. SAGE
Publications, 2010.

[11] C. Castelfranchi. Trust mediation in knowledge management and sharing.
In Trust Management, Second International Conference, iTrust 2004,
Oxford, UK, March 29 - April 1, 2004, Proceedings, pages 304–318,
2004.

[12] C. Castillo, M. Mendoza, and B. Poblete. Information credibility on twitter.
In Proceedings of the 20th international conference on World wide web,
pages 675–684. ACM, 2011.

[13] A. Cedilnik and P. Rheingans. Procedural annotation of uncertain infor-

mation. In Visualization 2000. Proceedings, pages 77–84. IEEE, 2000.
[14] C. Chatﬁeld. Model uncertainty. Encyclopedia of Environmetrics, 2006.
[15] L. Chien, A. Tat, P. Proulx, A. Khamisa, and W. Wright. Grand challenge
award 2008: Support for diverse analytic techniques - nspace2 and geo-
time visual analytics. In Proceedings of the IEEE Symposium on Visual
Analytics Science and Technology, IEEE VAST 2008, Columbus, Ohio,
USA, 19-24 October 2008, pages 199–200, 2008.

[16] J. Chuang, D. Ramage, C. D. Manning, and J. Heer. Interpretation and
trust: designing model-driven visualizations for text analysis. In CHI
Conference on Human Factors in Computing Systems, CHI ’12, Austin,
TX, USA - May 05 - 10, 2012, pages 443–452, 2012.

[17] C. Correa, Y.-H. Chan, and K.-L. Ma. A framework for uncertainty-aware
visual analytics. In Visual Analytics Science and Technology, 2009. VAST
2009. IEEE Symposium on, pages 51–58. IEEE, 2009.

[18] A. C. Cullen and H. C. Frey. Probabilistic techniques in exposure assess-
ment: a handbook for dealing with variability and uncertainty in models
and inputs. Springer Science & Business Media, 1999.

[19] E. J. de Visser, M. S. Cohen, A. Freedy, and R. Parasuraman. A design
methodology for trust cue calibration in cognitive agents.
In Virtual,
Augmented and Mixed Reality. Designing and Developing Virtual and
Augmented Environments - 6th International Conference, VAMR 2014,
Held as Part of HCI International 2014, Heraklion, Crete, Greece, June
22-27, 2014, Proceedings, Part I, pages 251–262, 2014.

[20] W. Dou, W. Ribarsky, and R. Chang. Capturing reasoning process through

user interaction. Proc. IEEE EuroVAST, 2, 2010.

[21] M. T. Dzindolet, S. A. Peterson, R. A. Pomranky, L. G. Pierce, and H. P.
Beck. The role of trust in automation reliance. Int. J. Hum.-Comput. Stud.,
58(6):697–718, 2003.

[22] A. Endert, P. Fiaux, and C. North. Semantic interaction for sensemaking:
Inferring analytical reasoning for model steering. IEEE Trans. Vis. Comput.
Graph., 18(12):2879–2888, 2012.

[23] A. Endert, M. Hossain, N. Ramakrishnan, C. North, P. Fiaux, and C. An-
drews. The human is the loop: new directions for visual analytics. Journal
of Intelligent Information Systems, pages 1–25, 2014.

[24] K. A. Ericsson and H. A. Simon. Protocol analysis. MIT-press, 1984.
[25] C. Fernandez, E. Ley, and M. F. Steel. Model uncertainty in cross-country
growth regressions. Journal of applied Econometrics, 16(5):563–576,
2001.

[26] D. Fisher, S. M. Drucker, and A. C. K¨onig. Exploratory visualization in-
volving incremental, approximate database queries and uncertainty. IEEE
Computer Graphics and Applications, 32(4):55–62, 2012.

[27] A. Flanagin and M. Metzger. The credibility of volunteered geographic

information. GeoJournal, 72(3):137–148, 2008.

[28] M. Gahegan. Beyond tools: Visual support for the entire process of

giscience. Exploring geovisualization, (4):83–99, 2005.

[29] K. Goddard, A. V. Roudsari, and J. C. Wyatt. Automation bias: Empirical
results assessing inﬂuencing factors. I. J. Medical Informatics, 83(5):368–
375, 2014.

[30] T. M. Green and R. Maciejewski. A role for reasoning in visual analytics.
In 46th Hawaii International Conference on System Sciences, HICSS 2013,
Wailea, HI, USA, January 7-10, 2013, pages 1495–1504, 2013.

[31] T. M. Green, W. Ribarsky, and B. D. Fisher. Building and applying a
human cognition model for visual analytics. Information Visualization,
8(1):1–13, 2009.

[32] H. Griethe and H. Schumann. The visualization of uncertain data: Methods

and problems. In SimVis, pages 143–156, 2006.

[33] R. B. Haber and D. A. McNabb. Visualization idioms: A conceptual model
for scientiﬁc visualization systems. Visualization in scientiﬁc computing,
74:93, 1990.

[34] L. Harrison, W. Dou, A. Lu, W. Ribarsky, and X. Wang. Analysts aren’t
machines: Inferring frustration through visualization interaction. In Visual
Analytics Science and Technology (VAST), 2011 IEEE Conference on,
pages 279–280. IEEE, 2011.

[35] T. Hengl. Visualisation of uncertainty using the hsi colour model: compu-
tations with colours. In Proceedings of the 7th International Conference
on GeoComputation, pages 8–17, 2003.

[36] G. B. M. Heuvelink and P. A. Burrough. Developments in statistical ap-
proaches to spatial uncertainty and its propagation. International Journal
of Geographical Information Science, 16(2):111–113, 2002.

[37] D. Howard and A. M. MacEachren. Interface design for geographic visu-
alization: Tools for representing reliability. Cartography and Geographic
Information Systems, 23(2):59–77, 1996.

[38] S. T. Iqbal and E. Horvitz. Disruption and recovery of computing tasks:
ﬁeld study, analysis, and directions. In Proceedings of the 2007 Conference
on Human Factors in Computing Systems, CHI 2007, San Jose, California,
USA, April 28 - May 3, 2007, pages 677–686, 2007.

[39] N. Kadivar, V. Y. Chen, D. Dunsmuir, E. Lee, C. Z. Qian, J. Dill, C. D.
Shaw, and R. F. Woodbury. Capturing and supporting the analysis process.
In Proceedings of the IEEE Symposium on Visual Analytics Science and
Technology, IEEE VAST 2009, Atlantic City, New Jersey, USA, 11-16
October 2009, part of VisWeek 2009, pages 131–138, 2009.
[40] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.
[41] D. Kahneman and A. Tversky. Subjective probability: A judgment of
representativeness. In The Concept of Probability in Psychological Exper-

SACHA ET AL.: THE ROLE OF UNCERTAINTY, AWARENESS, AND TRUST IN VISUAL ANALYTICS 

249

Knowledge generation model for visual analytics. IEEE Trans. Vis. Com-
put. Graph., 20(12):1604–1613, 2014.

[65] G. S. Schmidt, S.-L. Chen, A. N. Bryden, M. A. Livingston, L. J. Rosen-
blum, and B. R. Osborn. Multidimensional visual representations for
underwater environmental uncertainty. Computer Graphics and Applica-
tions, IEEE, 24(5):56–65, 2004.

[66] J. Scholtz. Beyond usability: Evaluation aspects of visual analytic environ-
ments. In IEEE Symposium On Visual Analytics Science And Technology,
IEEE VAST 2006, October 31-November 2, 2006, Baltimore, Maryland,
USA, pages 145–150, 2006.

[67] H. Senaratne and L. Gerharz. An assessment and categorisation of quanti-
tative uncertainty visualisation methods for geospatial data. In 14th AGILE
international conference on geographic information science-advancing
geoinformation science for a changing world. AGILE, 2011.

[68] H. Senaratne, L. Gerharz, E. Pebesma, and A. Schwering. Usability
of spatio-temporal uncertainty visualisation methods. In Bridging the
Geographic Information Sciences, pages 3–23. Springer, 2012.

[69] B. Shneiderman. The eyes have it: A task by data type taxonomy for
In Visual Languages, 1996. Proceedings.,

information visualizations.
IEEE Symposium on, pages 336–343. IEEE, 1996.

[70] Y. L. Simmhan, B. Plale, and D. Gannon. A survey of data provenance
techniques. Computer Science Department, Indiana University, Blooming-
ton IN, 47405, 2005.

[71] M. M. Skeels, B. Lee, G. Smith, and G. G. Robertson. Revealing uncer-
tainty for information visualization. Information Visualization, 9(1):70–81,
2010.

[72] S. Tak and A. Toet. Color and uncertainty: It is not always black and

white. 2014.

[73] A. Tatu, G. Albuquerque, M. Eisemann, P. Bak, H. Theisel, M. A. Mag-
nor, and D. A. Keim. Automated analytical methods to support visual
exploration of high-dimensional data. IEEE Trans. Vis. Comput. Graph.,
17(5):584–597, 2011.

[74] T. Tenbrink. Cognitive discourse analysis: accessing cognitive represen-
tations and processes through language data. Language and Cognition,
7:98–137, 3 2015.

[75] J. Thomson, E. Hetzler, A. MacEachren, M. Gahegan, and M. Pavel. A
typology for visualizing uncertainty. In Electronic Imaging 2005, pages
146–157. International Society for Optics and Photonics, 2005.

[76] M. Tory and T. M¨oller. Human factors in visualization research. IEEE

Trans. Vis. Comput. Graph., 10(1):72–84, 2004.

[77] A. Uggirala, A. K. Gramopadhye, B. J. Melloy, and J. E. Toler. Mea-
surement of trust in complex and dynamic systems using a quantitative
approach. International Journal of Industrial Ergonomics, 34(3):175–186,
2004.

[78] F. J. Van der Wel, L. C. Van der Gaag, and B. G. Gorte. Visual exploration
of uncertainty in remote-sensing classiﬁcation. Computers & Geosciences,
24(4):335–343, 1998.

[79] C. Ware. Information visualization: perception for design. Elsevier, 2012.
[80] K. M. Winters, D. Lach, and J. B. Cushing. Considerations for character-
izing domain problems. In Proceedings of the Fifth Workshop on Beyond
Time and Errors: Novel Evaluation Methods for Visualization, BELIV
2014, Paris, France, November 10, 2014, pages 16–22, 2014.

[81] W. Wright, D. Schroh, P. Proulx, A. Skaburskis, and B. Cort. The sandbox
for analysis: Concepts and methods.
In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’06, pages
801–810, New York, NY, USA, 2006. ACM.

[82] L. Zhou, J. K. Burgoon, J. F. Nunamaker, and D. Twitchell. Automating
linguistics-based cues for detecting deception in text-based asynchronous
computer-mediated communications. Group decision and negotiation,
13(1):81–106, 2004.

[83] T. Zuk and M. S. T. Carpendale. Visualization of uncertainty and reasoning.
In Smart Graphics, 7th International Symposium, SG 2007, Kyoto, Japan,
June 25-27, 2007, Proceedings, pages 164–177, 2007.

iments, pages 25–48. Springer, 1974.

[42] M. C. Kennedy and A. O’Hagan. Bayesian calibration of computer models.
Journal of the Royal Statistical Society: Series B (Statistical Methodology),
63(3):425–464, 2001.

[43] S. R. Klemmer, M. Thomsen, E. Phelps-Goodman, R. Lee, and J. A.
Landay. Where do web sites come from?: capturing and interacting with
design history. In Proceedings of the CHI 2002 Conference on Human
Factors in Computing Systems: Changing our World, Changing ourselves,
Minneapolis, Minnesota, USA, April 20-25, 2002., pages 1–8, 2002.

[44] G. J. Klir and M. J. Wierman. Uncertainty-based information: elements of
generalized information theory, volume 15. Springer Science & Business
Media, 1999.

[45] R. Kosara and J. Mackinlay. Storytelling: The next step for visualization.

Computer, 46(5):44–50, 2013.

[46] D. R. Kretz, B. Simpson, and C. Graham. A game-based experimental
protocol for identifying and overcoming judgment biases in forensic de-
cision analysis. In Homeland Security (HST), 2012 IEEE Conference on
Technologies for, pages 439–444. IEEE, 2012.

[47] K. Kurzhals, B. D. Fisher, M. Burch, and D. Weiskopf. Evaluating visual
analytics with eye tracking. In Proceedings of the Fifth Workshop on
Beyond Time and Errors: Novel Evaluation Methods for Visualization,
BELIV 2014, Paris, France, November 10, 2014, pages 61–69, 2014.

[48] B. C. Kwon, B. Fisher, and J. S. Yi. Visual analytic roadblocks for novice
investigators. In 2011 IEEE Conference on Visual Analytics Science and
Technology (VAST), pages 3–11, 2011.

[49] S. H. Lee and W. Chen. A comparative study of uncertainty propagation
methods for black-box-type problems. Structural and Multidisciplinary
Optimization, 37(3):239–253, 2009.

[50] V. Lush, L. Bastin, and J. Lumsden. Developing a geo label: providing
the gis community with quality metadata visualisation tools. Proceedings
of the 21st GIS Research UK (GISRUK 3013), Liverpool, UK, pages 3–5,
2013.

[51] A. M. MacEachren. Visualizing uncertain information. Cartographic

Perspectives, 13(13):10–19, 1992.

[52] A. M. MacEachren. Visual Analytics and Uncertainty: Its Not About the
Data. In EuroVis Workshop on Visual Analytics (EuroVA). The Eurograph-
ics Association, 2015.

[53] A. M. MacEachren and J. H. Ganter. A pattern identiﬁcation approach to
cartographic visualization. Cartographica: The International Journal for
Geographic Information and Geovisualization, 27(2):64–81, 1990.

[54] A. M. MacEachren, A. Robinson, S. Hopper, S. Gardner, R. Murray,
M. Gahegan, and E. Hetzler. Visualizing geospatial information uncer-
tainty: What we know and what we need to know. Cartography and
Geographic Information Science, 32(3):139–160, 2005.

[55] A. M. MacEachren, R. E. Roth, J. O’Brien, B. Li, D. Swingley, and
M. Gahegan. Visual semiotics & uncertainty visualization: An empirical
study. Visualization and Computer Graphics, IEEE Transactions on,
18(12):2496–2505, 2012.

[56] D. Manzey, J. Reichenbach, and L. Onnasch. Human performance conse-
quences of automated decision aids: The impact of degree of automation
and system experience. Journal of Cognitive Engineering and Decision
Making, pages 57–87, 2012.

[57] P. Mau´e. Reputation as tool to ensure validity of vgi. In Workshop on

volunteered geographic information, 2007.

[58] B. M. Muir. Trust between humans and machines, and the design of
decision aids. International Journal of Man-Machine Studies, 27(5-6):527–
539, 1987.

[59] P. H. Nguyen, K. Xu, and B. Wong. A survey of analytic provenance.

Middlesex University, 2014.

[60] A. Pang. Visualizing uncertainty in geo-spatial data. In Proceedings of
the Workshop on the Intersections between Geospatial Information and
Information Technology, pages 1–14, 2001.

[61] A. T. Pang, C. M. Wittenbrink, and S. K. Lodha. Approaches to uncertainty

visualization. The Visual Computer, 13(8):370–390, 1997.

[62] B. K. Phillips, V. R. Prybutok, and D. A. Peak. Decision conﬁdence,
information usefulness, and information seeking intention in the presence
of disconﬁrming information. InformingSciJ, 17:1–24, 2014.

[63] E. D. Ragan and J. R. Goodall. Evaluation methodology for comparing
memory and communication of analytic processes in visual analytics. In
Proceedings of the Fifth Workshop on Beyond Time and Errors: Novel
Evaluation Methods for Visualization, BELIV 2014, Paris, France, Novem-
ber 10, 2014, pages 27–34, 2014.

[64] D. Sacha, A. Stoffel, F. Stoffel, B. C. Kwon, G. P. Ellis, and D. A. Keim.

",False,2016.0,{},False,False,journalArticle,False,FA6A3U8V,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/7192716/,,"The Role of Uncertainty, Awareness, and Trust in Visual Analytics",FA6A3U8V,False,False
N4UQHKLU,KNKLKXRK,"Computational Visual Media
DOI 10.1007/s41095-017-0077-5

Research Article

Vol. 3, No. 2, June 2017, 161–175

EasySVM: A visual analysis approach for open-box support
vector machines

Yuxin Ma1, Wei Chen1(",False,2017.0,{},False,False,journalArticle,False,N4UQHKLU,[],self.user,False,False,False,False,http://link.springer.com/10.1007/s41095-017-0077-5,,EasySVM: A visual analysis approach for open-box support vector machines,N4UQHKLU,False,False
QC9YTBDV,3A839ZNI,"A Nested Model for Visualization
Design and Validation

Tamara Munzner
University of British Columbia
Department of Computer Science

How do you show your system is good?
• so many possible ways!

• algorithm complexity analysis
• field study with target user population
• implementation performance (speed, memory)
• informal usability study
• laboratory user study
• qualitative discussion of result pictures
• quantitative metrics
• requirements justification from task analysis
• user anecdotes (insights found)
• user community size (adoption)
• visual encoding justification from theoretical principles

2

Contribution
• nested model unifying design and validation

• guidance on when to use what validation method

• different threats to validity at each level of model

• recommendations based on model

3

Four kinds of threats to validity

4

Four kinds of threats to validity
• wrong problem
they don’t do that

•

domain problem characterization

5

Four kinds of threats to validity
• wrong problem
they don’t do that
• wrong abstraction

•

• you’re showing them the wrong thing

domain problem characterization
     data/operation abstraction design

6

Four kinds of threats to validity
• wrong problem
they don’t do that
• wrong abstraction
• wrong encoding/interaction technique

• you’re showing them the wrong thing

the way you show it doesn’t work

•

•

domain problem characterization
     data/operation abstraction design
          encoding/interaction technique design

7

Four kinds of threats to validity
• wrong problem
they don’t do that
• wrong abstraction
• wrong encoding/interaction technique
• wrong algorithm

• you’re showing them the wrong thing

the way you show it doesn’t work

•

•

• your code is too slow

domain problem characterization
     data/operation abstraction design
          encoding/interaction technique design
               algorithm design

8

Match validation method to contributions
• each validation works for only one kind of threat to validity

threat: wrong problem

     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique

              threat: slow algorithm

9

Analysis examples

MatrixExplorer. Henry and Fekete. InfoVis 2006.
observe and interview target users
justify encoding/interaction design
measure system time/memory
qualitative result image analysis

LiveRAC. McLachlan, Munzner, Koutsoﬁos,

and North. CHI 2008.

observe and interview target users
justify encoding/interaction design
qualitative result image analysis
field study, document deployed usage

An energy model for visual graph clustering. (LinLog)

Noack. Graph Drawing 2003

qualitative/quantitative image analysis

Effectiveness of animation in trend visualization.

Robertson et al. InfoVis 2008.

lab study, measure time/errors for operation

Interactive visualization of genealogical graphs.
McGufﬁn and Balakrishnan. InfoVis 2005.

justify encoding/interaction design

qualitative result image analysis
test on target users, get utility anecdotes

Flow map layout. Phan et al. InfoVis 2005.

justify encoding/interaction design
computational complexity analysis
measure system time/memory
qualitative result image analysis

10

Nested levels in model
• output of upstream level
input to downstream level
• challenge: upstream errors inevitably cascade

• if poor abstraction choice made, even perfect technique

and algorithm design will not solve intended problem

domain problem characterization
     data/operation abstraction design
          encoding/interaction technique design
               algorithm design

11

Characterizing domain problems

problem
    data/op abstraction
         enc/interact technique
              algorithm

• tasks, data, workflow of target users

• problems: tasks described in domain terms
• requirements elicitation is notoriously hard

12

Designing data/operation abstraction

problem
    data/op abstraction
         enc/interact technique
              algorithm
• mapping from domain vocabulary/concerns to abstraction

• may require transformation!

• data types: data described in abstract terms

• numeric tables, relational/network, spatial, ...

• operations: tasks described in abstract terms

• generic

• sorting, filtering, correlating, finding trends/outliers...

• datatype-specific

• path following through network...

13

Designing encoding,interaction techniques

problem
    data/op abstraction
         enc/interact technique
              algorithm

• visual encoding

• marks, attributes, ...
• extensive foundational work exists

•

interaction
• selecting, navigating, ordering, ...
• significant guidance exists

Semiology of Graphics. Jacques Bertin, Gauthier-Villars 1967, EHESS 1998
14

Designing algorithms

problem
    data/op abstraction
         enc/interact technique
              algorithm

• well-studied computer science problem

• create efficient algorithm given clear specification
• no human-in-loop questions

15

Immediate vs. downstream validation

threat: wrong problem

     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique

              threat: slow algorithm

                      implement system

16

Domain problem validation
• immediate: ethnographic interviews/observations

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique

              threat: slow algorithm

                      implement system

17

Domain problem validation
• downstream: adoption (weak but interesting signal)

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique

              threat: slow algorithm

                      implement system

 validate: observe adoption rates

18

Abstraction validation
• downstream: can only test with target users doing real work

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique

              threat: slow algorithm

                      implement system

      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

19

Encoding/interaction technique validation
• immediate: justification useful, but not sufficient - tradeoffs

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

                      implement system

      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

20

Encoding/interaction technique validation
• downstream: discussion of result images very common

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

                      implement system

          validate: qualitative/quantitative result image analysis

      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

21

Encoding/interaction technique validation
• downstream: studies add another level of rigor (and time)

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

                      implement system

          validate: qualitative/quantitative result image analysis

          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

22

Encoding/interaction technique validation
• usability testing necessary for validity of downstream testing

• not validation method itself!

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

                      implement system

          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

23

Algorithm validation
• immediate vs. downstream here clearly understood in CS

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

validate: analyze computational complexity

                      implement system
              validate: measure system time/memory
          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

24

Avoid mismatches
• can’t validate encoding with wallclock timings

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

validate: analyze computational complexity

                      implement system
              validate: measure system time/memory
          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

25

Avoid mismatches
• can’t validate abstraction with lab study

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

validate: analyze computational complexity

                      implement system
              validate: measure system time/memory
          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

26

Single paper would include only subset
• can’t do all for same project

• not enough space in paper or time to do work

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

validate: analyze computational complexity

                      implement system
              validate: measure system time/memory
          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

27

Single paper would include only subset
• pick validation method according to contribution claims

threat: wrong problem
 validate: observe and interview target users
     threat: bad data/operation abstraction
          threat: ineffective encoding/interaction technique
          validate: justify encoding/interaction design
              threat: slow algorithm

validate: analyze computational complexity

                      implement system
              validate: measure system time/memory
          validate: qualitative/quantitative result image analysis
          [test on any users, informal usability study]
          validate: lab study, measure human time/errors for operation
      validate: test on target users, collect anecdotal evidence of utility
      validate: field study, document human usage of deployed system
 validate: observe adoption rates

28

Real design process
• iterative refinement

• levels don’t need to be done in strict order
• intellectual value of level separation

• exposition, analysis

• shortcut across inner levels + implementation

• rapid prototyping, etc.

• low-fidelity stand-ins so downstream validation can

happen sooner

29

Related work
• influenced by many previous pipelines

• but none were tied to validation
• [Card, Mackinlay, Shneiderman 99], ...

• many previous papers on how to evaluate
• but not when to use what validation methods
• exceptions

• [Carpendale 08], [Plaisant 04], [Tory and Möller 04]

• good first step, but no formal framework

[Kosara, Healey, Interrante, Laidlaw, Ware 03]

• guidance for long term case studies, but not other contexts
• only three levels, does not include algorithm

[Shneiderman and Plaisant 06]
[Ellis and Dix 06], [Andrews 08]

30

Recommendations: authors
• explicitly state level of contribution claim(s)

• explicitly state assumptions for levels upstream of

paper focus
• just one sentence + citation may suffice

• goal: literature with clearer interlock between papers

• better unify problem-driven and technique-driven work

31

Recommendation: publication venues
• we need more problem characterization

• ethnography, requirements analysis

• as part of paper, and as full paper

• now full papers relegated to CHI/CSCW
• does not allow focus on central vis concerns

• legitimize ethnographic “orange-box” papers!

observe and interview target users

32

Lab study as core now deemed legitimate

MatrixExplorer. Henry and Fekete. InfoVis 2006.
observe and interview target users
justify encoding/interaction design
measure system time/memory
qualitative result image analysis

LiveRAC. McLachlan, Munzner, Koutsoﬁos,

and North. CHI 2008.

observe and interview target users
justify encoding/interaction design
qualitative result image analysis
field study, document deployed usage

An energy model for visual graph clustering. (LinLog)

Noack. Graph Drawing 2003

qualitative/quantitative image analysis

Effectiveness of animation in trend visualization.

Robertson et al. InfoVis 2008.

lab study, measure time/errors for operation

Interactive visualization of genealogical graphs.
McGufﬁn and Balakrishnan. InfoVis 2005.

justify encoding/interaction design

qualitative result image analysis
test on target users, get utility anecdotes

Flow map layout. Phan et al. InfoVis 2005.

justify encoding/interaction design
computational complexity analysis
measure system time/memory
qualitative result image analysis

33

Limitations
• oversimplification

• not all forms of user studies addressed

• infovis-oriented worldview

• are these levels the right division?

34

Conclusion
• new model unifying design and validation

• guidance on when to use what validation method
• broad scope of validation, including algorithms

• recommendations

• be explicit about levels addressed and state
upstream assumptions so papers interlock more
• we need more problem characterization work

these slides posted at http://www.cs.ubc.ca/~tmm/talks.html#iv09

35

",False,2009.0,{},False,False,journalArticle,False,QC9YTBDV,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/5290695/,,A Nested Model for Visualization Design and Validation,QC9YTBDV,False,False
QDP98BY5,6S9SBEEU,"Building and Applying a Human Cognition Model for 

Visual Analytics 

 

 
ABSTRACT  

Tera Marie Green1, William Ribarsky1, & Brian Fisher2 

1Charlotte Visualization Center, University of North Carolina at Charlotte 

2School of Interactive Arts and Technology, Simon Fraser University 

 

 

It is well known that visual analytics addresses the difficulty of evaluating and processing large quantities 

of information. Less often discussed are the increasingly complex analytic and reasoning processes that 

must be applied in order to accomplish that goal. Success of the visual analytics approach will require us 

to develop new visualization models that predict how computational processes might facilitate human 

insight and guide the flow of human reasoning. In this paper, we seek to advance visualization methods   

by proposing a framework for human “higher cognition” that extends more familiar perceptual models. 

Based on this approach, we suggest guidelines for the development of visual interfaces that better 

integrate complementary capabilities of humans and computers. While many of these recommendations 

are novel, some can be found in existing visual analytics applications.  In the latter case, much of the 

value of our contribution lies in the deeper rationale that the model provides for those principles. We then 

assess these visual analytics guidelines through the evaluation of several visualization examples. Lastly, 

we discuss steps that can be taken towards a predictive human cognition model. 

 

KEYWORDS: visual analytics, cognition and perception theory, embodied cognition, 

visualization taxonomies and models  

  

INTRODUCTION 

In a previous paper [1], we discussed the value of considering performance characteristics of 

human reasoning, problem-solving, and decision-making, in addition to the more familiar 

perceptual processes, to develop information and knowledge visualizations. The driving force 

behind this program of research is the need to develop applications capable of attacking today’s 

complex and critical problems with the required depth of reasoning and analysis that will support 

their solution. This paper builds on the work reported in our VAST 2008 paper [41] and expands 

our argument that the creation of comprehensive models of human-computer cognitive 

processing should be a core component of the visual analytics effort, and is an essential 

prerequisite for success of visual analytics as a field. In most visualization development “higher 

cognition” processes have been considered as a “black box” that receives information from the 

visualization through a perceptual transduction filter, and that generates responsive behavior 

through unspecified internal mechanisms. (We will discuss this metaphorical view of human-

visualization interaction during our consideration of the van Wijk operational model of 

visualization in Section 4.)  However, as both interactive visualizations and cognitive tasks 

become semantically as well as perceptually complex, it becomes apparent that we must peek 

into the black box in an attempt to model and predict the behavior of the human-computer 

collaboration as a cognitive system. Psychology and related behavioral sciences have 

examined reasoning and other thought processes for decades, through classical scientific 

processes of reduction, laboratory testing, and scientific induction. One reason that much of this 

research has, as yet, been unused in the construction of interactive visualizations is the lack of 

a broad theory of human reasoning with sufficient scope and predictive validity to impact the 

design and evaluation of those applications. It is, as Newell once wrote, as if “science advances 

by playing twenty questions with nature” [2]. As Newell suggests, the reductionist approach to 

the study of higher cognition creates multiple competing theories of small, often binary, aspects 

of reasoning, and these have dominated the research. His solution to this problem was to 

propose that cognitive scientists generate models of large scale “cognitive architecture” such as 

his own SOAR [Unified theories of cognition ref], and Anderson’s ACT-R [An Integrated Theory 

of the Mind].  These models have had some success at modeling human abstract problem 

solving but have struggled to accommodate cognitive activities that depend upon perceptual or 

motor processes to a high degree, such as fluent human interaction with a visualization system 

in the course of analyzing a situation and solving a problem.  

 

But while complex, higher cognition is still predictable to some degree. The number of available 

heuristics is finite, and is therefore theoretically knowable. And while extant research may 

disagree on the finer points, there is general agreement that humans are parsimonious problem 

solvers who most frequently choose to use the simplest heuristic that is adequate to accomplish 

a given task. For our purposes it may be sufficient to find aspects of human cognition that are 

sufficiently predictive that they can constrain the range of possible designs, and propose metrics 

for testing the success of those methods. This is a simpler task than the comprehensive model 

of human information processing that cognitive science seeks, although progress towards our 

goals may well have an impact on those more comprehensive models of human cognition. 

 

This paper endeavors to lay the framework of a human cognition model, whose guidelines 

would guide the development of visual interfaces more able to attack the complex problems now 

being faced by analysts and researchers. Additionally, this paper shows how this model 

contributes new, cognition-based principles of visualization design. Some of these principles are 

already being used in the better visual analytics designs, but without the deeper rationale that 

the model provides. We will discuss and evaluate these visual analytics methods. Other 

principles from the model have not been applied or not fully applied, and we will discuss how 

their implementation and use will be of benefit.  

 

COMPLEMENTARY STRENGTHS  

Our focus is on mixed-initiative visualization, in which both the computer and the human initiate 

processes and with which both collaborate in the exploration and creation of knowledge. Both 

human and computer bring strengths to the overall cognitive system. Several of the obvious 

strengths are complementary, which further strengthens the potential of the collaboration.  

  

Human strengths  

Some of the earliest reasoning skills humans develop are those  of adaptation and 

accommodation [3]. Adaptation is the ability to incorporate newly perceived information into 

extant knowledge schema, and it relies heavily on our ability to categorize sensory stimuli at a 

rate that is nearly instantaneous. Even when what is perceived is so novel it will not fit existing 

knowledge schema, accommodation allows a human to temporarily place a marker in a closely 

similar schema or create a new one [4]. This “fast and frugal” reasoning ability [5] enables 

humans to more effectively deal with rapidly-changing situations. Biederman’s 1987 Recognition 

By Components (RBC) model provides a mechanism by which basic-level categorization of 

visual objects takes place rapidly and accurately regardless of viewpoint or changes in non-

essential characteristics of those objects. [6]. This process is effortless (i.e. it does not demand 

attentional resources) for a human, and it allows the reasoning process to advance despite 

incomplete information. It is far superior to current computer object recognition. In addition to 

our ability to categorize individual objects, human perceptual abilities are also well adapted to 

complex and rapidly changing scenes, defined here as complex sets of objects and events 

distributed in space that interact with each other in often novel ways.  

Scene perception takes place through an integrated set of mechanisms that optimize 

performance under the constraints of limitations in central resources such as focal attention. 

Scene processing begins with a fast low-level “gist” mechanism that recognizes important scene 

characteristics and relationships [7]. This preattentive visual process guides the allocation of 

multiple attentional tokens known as FINSTs, see [8, 9] that support the rapid and automatic 

calculation of a set of operations [10] known as visual routines that relay information on their 

properties and relations to each other. All of this occurs prior to, and in support of, endogenous 

focal attention (i.e. attention to the task at hand). This resulting cascade of processes frees 

cognition for consideration of higher order characteristics of the information contained in the 

display rather than the display itself -- object properties and spatial relations with other objects 

and causal relations to events taking place. Thus, cognitive operations can proceed using more 

parsimonious representations that are well- suited for the task at hand. This can be thought of 

as a two-step process by which unconscious inference, the “logic of perception”, [11] works 

hand-in-hand with cognitive processes to support reasoning. In this view, expertise is not only a 

characteristic of higher-order cognitive logic, but also of perceptual logic, which can be trained 

to better support cognitive operations through “perceptual expertise” [12].  

 

Partially due to a lifetime of experience in adaptation and accommodation, humans are much 

more flexible reasoners than AI models, and in particular much better enabled to understand 

novel situations and novel approaches to known problems. Humans rely on a compendium of 

reasoning and problem-solving heuristics, which can be used singly or concomitantly to 

accomplish the task at hand.  The simplest of these, elimination heuristics such as satisficing 

[13], eliminate available choices that do not possess an identified important attribute. Elimination 

heuristics require little cognitive effort, and so are often what a human will use first in an attempt 

to narrow down available choices.  

 Of course, if the problem becomes semantically complex, more effort is required. Our model 

assumes a mental model to inferred rules mechanization [14], wherein the human uses all 

available information to create a mental model of the concept being considered. From this 

model, the human infers generalizeable rules – sometimes in a matter of seconds – that are 

used in later instantiations of a similar concept or problem. Because these models are based 

entirely on available (including previously held) information, it is imperative that all pertinent 

information is available to avoid the creation of incomplete mental models, which are, in turn, 

likely to be the basis of invalid rules.  

 Computer strengths  

A computer is capable of two distinct processes that complement human reasoning strengths 

well: superior working memory and information processing without cognitive biases. Humans 

depend on their working memory as they reason, but are, at best, able to remember 7 ± 2 

chunks of information [15]. The computer, on the other hand, has a “working memory” limited 

only by hardware. The computer’s ability to keep all pertinent information visually available to 

the human aids in complete mental modeling, among other things. The other computer strength 

is the lack of inherent biases. This bias-free environment is, to be sure, influenced by what the 

interface is designed to see as relationally relevant. But unlike humans, computers do not 

situationally filter out pertinent information in accordance with a perceived belief or due to the 

way a problem is presented [16,17].  By presenting all relevant information, the computer can 

aid not only in mental modeling, but also in the analysis of competing hypotheses. 

 

USE OF A  HUMAN COGNITION MODEL  

This section will discuss the ways in which knowledge of higher cognition focuses a mixed-

initiative human cognition model (HCM), as well as provides several guidelines which can be 

derived from the model’s use. (See Figure 1.) On each of the HCM’s submodels, please see [1] 

for more discussion.  

  

Information Discovery & Knowledge Building  

The central process of the HCM is discovery, during which the computer presents information in 

an ontological-like structure within a relevant, possibly human-defined, context. Presenting 

information with a relevant context is one method of mitigating human cognitive overload in the 

midst of an overwhelming number of semantic data points. The human directly interacts with the 

visualized information, focusing the attention of discovery. We will explore this idea further in 

Section 5.4.  

An intuitive multi-model visualization also encourages knowledge building through new 

knowledge creation. Throughout the process of discovery, the human may uncover a 

relationship between two currently unrelated concepts or ideas. By creating a new relationship 

between the two concepts and perhaps annotating the relationship, the human collaborator can 

extend the knowledge base of the visualization, not only for what is to be accomplished in that 

particular session, but for every session by every human who uses the visualization thereafter.  

 

The computer can augment the discovery of relevant information through computer-aided 

discovery. Through observation of what interests the human collaborator, the computer can 

suggest information that is semantically related, but up to this point, has not been considered. 

This also would include relational data which has been added by other human collaborators, 

which allows one person to learn from another. The human is free to explore or to reject the 

suggestion. But by making the effort to ensure that nothing important is overlooked, the 

computer works to counteract human cognitive biases which can interfere with complete mental 

modeling.   

 

 

 

Guidelines for Discovery and Knowledge Building  

We will now briefly discuss several guidelines based upon the HCM discovery and knowledge 

submodels. These can be used to motivate visual analytics interface design. 

  

Multiple views  

When the information being explored is semantically rich, and could be visualized through a 

variety of categorization levels, it is often left to the discretion of the visualization developer as to 

which level merits the primary view. It is important to categorize information to aid the human in 

directing attention, but we would argue that a visualization that utilizes multiple organizational 

views of the same information can be a powerful aid. As the human interacts with information in 

any view, the relational changes are visualized in all views.      While the concept of multiple 

views is not a new one [18], what we would highlight is how multiple views are informed by 

human cognition. First, as humans perceive information in a variety of ways including through 

the filter of their own assumptions, patterns are more likely to be discovered if represented 

multiple ways, each tuned to particular, important aspects of the data.   

Secondly, as we have discussed previously, humans prefer to narrow down the field of choices 

by eliminating those that do not posses desired attributes. This is usually done before utilizing 

more complicated heuristics. Multiple views make the process easier; multiple layers of 

relational attributes are readily knowable without additional search.  

Thirdly, multiple views enable more intuitive manipulation. Humans themselves do not interact 

with information in one dimension; humans are capable of multi-layered processing: perceptual, 

emotional, and higher-cognitive. Indeed, of all the guidelines we will discuss, use of multiple 

views is the one most likely to lead to spontaneous insight.   

Lastly, the multiple views can be each set for different cognitive viewpoints or cognitive tasks as 

part of the overall problem-solving approach. This can be done in a general way, as is shown in 

the WireVis example below. 

   

Direct interaction 

By definition, a well-designed information visualization allows the user to directly interact with 

information. But we would take direct interaction one step further. In computer-aided reasoning 

and discovery, for example, the guideline of direct interaction would propose that whatever 

tactic the computer uses to suggest relational information to the human be done without 

interfering with a human’s train of thought or flow of reasoning.   Additionally, direct interaction 

supports the goals of other HCM guidelines by facilitating rich, fast, and effective interaction. 

The human thinks in terms of the analysis task, which is closely aligned with the interaction, and 

then looks at the visualized results. As a result, the user is more able to stay in the cognitive 

zone (as we will discuss shortly), even with multiple windows.  With this in mind, visualization 

design should avoid, as much as possible, menus or other actions that take the user outside of 

the frame of the task. Interactions should be through direct manipulation and translucent 

wherever possible, avoiding the traditional pull-down menus, which require the human to sort 

through and think about menu items.  

  

Central Role of Interaction  

Human-computer interaction is not a series of disjointed behaviors. And while the visual process 

is an important part of visualization, it does not stand alone. Interaction has a rhythm, a cycle of 

give and take.  Interaction is the process by which the computer and the human give, take, and 

create knowledge. We will see an example of this when we will consider the van Wijk 

operational model in the next section.  

 

 

 

 

Insulation of Reasoning Flow  

One goal of intuitive visualization should be the facilitation of the flow of human reasoning. Once 

the human has focused cognitive resources in an area of interest, the visualization should not 

hamper the rhythm of reasoning until the human chooses to refocus resources elsewhere. This 

insulation can be achieved partially through direct interaction within context and intuitive 

computer-aided information discovery, as is discussed elsewhere in this section. Insulation is 

also aided, where possible, by an understanding of the temporal constraints of human 

perception and patterns of cognitive activity, adapting the timing of interface events and/or 

reducing the time required to retrieve necessary information from interface interaction [19].  

 

Spivey [37] argues that the pace of cognitive operations is determined by the need to reduce the 

impact of the time required to pull information from the world on the flow of cognitive processing. 

This is due to the high cost of storing and retrieving items in working memory. Given the poor 

buffering capabilities of human information processing, it is more effective to simply reduce the 

speed of cognitive processes to match those of information uptake. For example, humans can 

think through visual material only as quickly as the eyes can move. In fact, the time that is 

required to read text is much faster if each word in the sentence is displayed in series at the 

same position in space (the Rapid Serial Visual Presentation technique). In normal reading, the 

pace of information processing slows to allow the eyes to move to the next word before the 

word just read is fully processed. Thus, the experience of reading shifts from a read-remember-

read stop-and-go rhythm to a continuous (but slower) flow of reading. This insulates cognition 

from the stop-and-go of eye movements, and the experience seems continuous. 

 

For visualization, especially interactive visualization, the temporal constraints of the perception 

and control of the image differ. To optimize the use of cognitive resources, designers need to 

understand what temporal pattern of mental activity should take place for optimal cognitive 

performance, and adapt the timing of events so as to maintain that pattern. This will require 

additional research on rhythms of human information processing that are sufficiently predictive 

to generate guidelines that can be applied in the design process, perhaps by an attentive 

system that monitors user actions (such as eye movements) to infer patterns of cognitive 

processes.  

 

In the terminology of the HCM, being “in the zone” allows the human collaborator to reason 

without encountering unnecessary attentional or cognitive impediments. In cases where task 

complexity exceeds the user’s ability to process information, or a cognitive impasse is reached 

for some other reason, the computer can provide a scaffolding of support by presenting the 

information within relevant context, suggesting what may have been overlooked, and keeping 

relevant information present.    

  

Intimate interaction  

 It is important that the interaction is so translucent to the human collaborator that the give and 

take which occurs in a successful collaboration is seamless. Entering the interaction should 

seem natural and obvious. The use of on-screen tools should not require additional cognitive 

focus – i.e. be usable without the human having to “think about it.” Intimate interaction deters 

attentional interference during the cognitive flow, and enables the reasoning process to move 

forward unabated.    When interaction is intimate, what the human should see and cognitively 

manipulate is not the tool being used or the method of interaction, but only the interaction itself. 

Intimate interaction is an important asset to flow insulation, and is supportive of the central role 

of interaction.  

  

Search by Example & Search by Pattern  

Searching for information has traditionally been done by entering a search term in a text box. 

But text boxes require humans to specify what to look for (such as in Boolean-type queries), as 

well as to stop where they are in the reasoning process to formulate a query in concrete terms. 

Finding an appropriate Boolean query will almost always break the flow of cognitive processing. 

We would argue that a better general approach would be to allow the human, where possible 

and appropriate, to indicate the search terms by pointing and clicking on an example or drawing 

a bounding box around a pattern of examples or other relational information.  This allows the 

human to indicate the search visually, without the burden of linguistic encoding. This does not 

require an interruption in thought, feels more intuitive, and allows reasoning to move forward. 

 

It is true that Boolean searches can be necessary under certain conditions. For example, 

working within the definition of an information scent model [20], a point-and-click search-by-

example is likely a better approach.  But if the user exhausts an old scent, coding a Boolean 

query becomes necessary in the hunt for a new one. Key here is to make the decision about 

which type of search to employ based on the prediction of its impact on cognition. One of the 

interfaces we consider in the Examples section, WireVis, has successfully used point-and-click 

search by example with one or more variables. This, of course, requires underpinning analytical 

tools tightly integrated with the visual interface. This might not always be feasible, but it is a 

main goal of visual analytics. 

 

 

 

 

 

Creation and Analysis of Hypotheses  

 One extension of the knowledge building process that holds great potential for multi-modal 

visualization is in the creation and evaluation of hypotheses. Hypothesis generation and 

evaluation is highly impacted by human cognitive bias, as humans are wont to seek out 

confirmatory evidence rather than to seek disconfirmation.    

 

As Heuer described it [21], hypothesis analysis starts with a list of hypotheses and a body of 

evidence that proves or disproves each one. As the human creates of list of hypotheses, the 

computer can aid in finding relevant evidence. From there, the computer, with its superior 

working memory, creates a weighted matrix or similar relational structure  that the human can 

evaluate with her superior reasoning ability. Using the edited relational structure, the human 

draws conclusions about which hypotheses are correct, and if desired, sets up a data watch in 

the visualization that will notify the user of data changes.    Hypotheses generation is initiated by 

the human, but the computer plays a significant role in shortening the process and neutralizing 

biases, contributing to more solid conclusion through use of its strengths.   

  

CONSIDERING THE VAN WIJK MODEL  

 We will now discuss what the implications of our model would look like if integrated into the van 

Wijk operational model of the visualization process [22]. We do this primarily as another way to 

envision how a human cognition model interrelates with other aspects of visualization theory, or 

as another way to broadly sketch out the basic assumptions of the HCM within the context of an 

extant model. 

Van Wijk models the “user” as P (perception and cognition), K (knowledge), and E (interactive 

exploration), as is demonstrated in Figure 2. The user perceives the image (I) and interacts 

within the visualization using a variety of available manipulation techniques (the specifications 

(S)).   

 

The model depicts Perception as feeding Knowledge, which, in turn, drives interactive 

Exploration. This is appropriate for aspects of human perception, such as what Gray calls 

“microstrategies”[36] -- simple perception/action patterns that take place largely without 

conscious direction but under executive control of cognitive processes. Some perceptions are 

not affected by the specifics of what the user knows, plans, and expects; these are aspects of 

performance we can depend on to remain more or less constant, or ""cognitively impenetrable"" 

[8,9].  

 

There is still some discussion about which perceptual tasks could be considered truly 

“preattentive” and which evidence cognitive manipulation (see for example [37]), and it is not our 

intent to explore this debate in any depth here. Our intent is to demonstrate the cooperative 

give-and-take of cognition throughout interactive visualization exploration. Thus, for the 

purposes of this control diagram, we will define Perception to include other aspects of “lower 

cognition” that do indeed inform executive cognition, such as object classification.  

 

 Additionally, it is difficult to separate “knowledge” from the reasoning process that created it. A 

person’s knowledge is not simply a compendium of declarative facts; it is also the relational or 

inferential semantic meaning a person gives the facts, patterns of facts and their relationships, 

the perceived worth of those facts, and the ways in which facts are used to reason about the 

encounter with future novel information. Indeed, facts are useless without the reasoning power 

to manipulate them, and so we believe that the ‘K’ submodel, must include the cognition 

processes that created it.  

 

Knowledge determines the methods used when new knowledge is integrated with the old. Van 

Wijk also seems to imply this in how he models his “user;” his model pictures Knowledge 

feeding Exploration. But K cannot inform E without the guiding focus of a reasoning process. 

Indeed, exploration itself is cognition in action.  

 

With these thoughts in mind, it would be more representative if Perception, Knowledge, and 

Exploration were all modeled as cognitive processes informing each other. We would see P as 

the early cognitive processes of selective attention, categorization, accommodation, including 

perceptual logic. (See Section 2.1.)  K is viewed as meaningful knowledge with the use of 

reasoning, problem-solving and other thought processes that allow the human to create 

knowledge, and E as a focused, interactive cognitive process utilizing both P and K. 

  

When viewing the model this way, it’s easy to see that two additional directional arrows need to 

be added to the model: from P to E, and from E to K. The first arrow indicates the important role 

that perception and perceptual logic plays in active exploration. The second arrow signifies how 

a rhythm of interaction feeds knowledge reasoning. As the human explores and learns, that 

learning directs and focuses the attention of further exploration.  

Additionally, van Wijk expressed Knowledge in this way:  

                                               

=)(tK

 

K

0

∫+ t

0

P(I,

 t)K,

dt 

 [22] 

While this expression does encapsulate the idea that Perception is a vital part of the process, 

our integration would express the creation of knowledge over time more like the following:  

 

 

 

       

)(
tK

=

K

+ ∫ t

0

0

E(P,

 t)K,

 

dt

 

 where Knowledge is the extension of currently held knowledge through the integrated 

perceptual and reasoning cognitive processes of Exploration.  

 

Considering the Pirolli & Card Sensemaking Model 

The HCM and its design implications share some understanding of the tasks involved in 

information processing with what is commonly called the “sensemaking model” as specified by 

Pirolli and Card [23]. This model identifies a series of tasks that are used progressively by 

analysts to find and filter evidence in the creation of hypotheses and drawing of conclusions. 

The task series is presented as iterative, using both top down and bottom up cognitive tasks to 

forage and make sense of new information. The discussion of the foraging loop places 

emphasis on the human’s ability to filter information and find patterns in the data. In the 

sensemaking loop, we see several tasks that are also the basis of the hypothesis generation 

subprocess of the HCM: listing hypotheses, listing evidence, and drawing conclusions.  

 

The sensemaking model outlines processes involved in information processing. But to some 

degree, it treats these processes as a series of small black boxes, whose tasks are described 

but whose involved cognitive processes are largely undefined. One goal of the HCM is to 

identify and explore multi-layered cognition within information spaces and, in the process, to 

start opening up those black boxes so we can describe and find ways to support the processes 

they contain. Learning of whatever variety is not linear, but involves multiple subsystems that 

feed and inform each other throughout. Thus, it is important not to see human-visualization 

interaction as linear either, but rather as an interlinking of processes working in concert. 

 

Additionally, while the sensemaking model delineates cognitive processes between top-down 

and bottom-up processes, it does not handle cognition, which either does not fit cleanly into 

either group or which defies such categorization. One obvious example is what is sometimes 

called spontaneous insight, or insights that emerge in “a-ha” moments with seemingly little 

conscious preparation. What triggers and informs this variety of problem-solving is still the 

subject of debate [24,25], but what is known suggests that spontaneous insight draws upon a 

wide scope of tacit knowledge, inferencing, and paradigm-discarding information reorganization. 

This reorganization leads to a profound understanding of the problem’s solution, without the 

plodding trial-and-error or other incremental heuristics traditionally associated with problem-

solving. Moreover, this understanding happens in what is often described as a “flash of 

time[25],” and, because much of the preparation is unconscious, the problem solver rarely can 

define where or how the solution came into focus. 

 

More generally and in less dramatic fashion, human reasoning itself often defies the 

sensemaking model’s incremental, bi-process structure. Reasoning is much more than a series 

of delineated tasks; depending on complexity, reasoning can combine any number of heuristics 

and use top-down and bottom-up thought constructions combinatorially. (For more discussion, 

please see [1].) Any generally-applicable model of human cognition must deal with this 

complexity sooner or later. 

 

Briefly, we will discuss specific HCM theory and submodels that attempt to articulate the 

cognitive processes involved in the black box task processes of the sensemaking model.  The 

foraging loop, which starts with a general, base categorization and moves downward into the 

more specific (the exploring-enriching-exploiting tradeoff) utilizes a variety of cognitive 

processes previously discussed, including the use of elimination heuristics and the use of 

 

mental models which lead to generalized inferencing rules. The foraging loop also implies (but 

without specifics) the behavior of direct interaction, which promotes rapid and responsive 

interaction, insulating the reasoning flow. On the other hand, the “leverage points” of the 

sensemaking loop described by Pirolli and Card [23] address hypothesis generation, which is 

also directly addressed by the HCM. In addition, the HCM uses mixed-initiative computer-aided 

discovery to address the biases that can confound evidentiary support needed in hypotheses 

analysis. 

 

In conclusion, this discussion is not intended to propose the HCM as a replacement for the 

sensemaking model, but rather to join it with that operational model; the HCM is  a step forward 

in the evolution of an understanding of holistic human cognition. Thus, our objective is to not 

only to outline the cognition involved in these processes, but to elucidate how that 

understanding can benefit and inform visualization design.  

 

EXAMPLES 

In this section, we will demonstrate the model guidelines by using them to evaluate and/or 

illustrate several visual analytics designs. The model, which was not used as a basis for these 

designs, provides a deeper understanding of the choices made and how the designs might be 

improved. Because we can discuss the rationale behind them more fully, we present 

predominantly designs that we helped develop. However, the arguments we make here would 

also apply to many other designs.  

 

 

 

 

 

WireVis   

Discovering financial fraud in the great stream of transactions at a large bank is a difficult, time-

consuming, and expensive process since it must employ expert analysts and uncover ever-

changing modes of operation by criminals. The WireVis system was designed to combine the 

art of analysis with the science of visual analytics [26]. WireVis is an expert system, enhancing 

insight with what, in the terms of our Knowledge expression in the last section, is presumed to 

be the human’s already sizeable K0; it provides intuitive visual representations of wire 

transactions, enhancing P; different views within the system allow the analysts to gain an 

overview of all transactions in one glance, while the ability to drill down into specific details of 

individual transactions enables finer examination and scrutiny.  A time-based visualization 

allows the analysts to detect abnormal temporal patterns. Search by example permits selection 

of a particular keyword or time distribution pattern; the system then finds patterns that are 

similar to (or quite different from) the selected pattern. Finally, a keyword network shows 

keyword links for the selected group of transactions (where linked keywords appear in the same 

transaction), uncovering relationships that significantly aid the analyst in picking out suspicious 

transactions. This process highlights the importance of E in extending K. Results of a user 

evaluation found WireVis to be an efficient, effective tool, which can discover suspicious 

patterns in a great mass of transaction data [26]; the tool is also generally applicable to other 

types of transactional analysis 

 

WireVis has a number of capabilities that conform to the above cognitive model and highlights 

some of the design choices that must be made. Four windows are tailored to specific, important 

views and tasks. Though having a single window to focus the user’s attention may be ideal in 

some conceptual sense, and there is presumably a cognitive and perceptual load during task 

switching, multiple windows seem necessary for many complex analytical problems [27, 28]. 

The key is to minimize the load in order to mitigate the interference to the human’s reasoning 

flow. This is done to a great extent through the concept of balanced interaction, as discussed 

next. 

In WireVis, the views were carefully chosen so that overviews of main aspects of financial 

analysis were maintained. Linking and brushing between all views was enacted and immediate 

update to any interaction was enforced. (There are not only perceptual but cognitive aspects to 

maintaining high interactivity.) In addition, WireVis is designed to promote balanced interaction, 

during which the multiple interlinked windows act and look like a single interface, rather than 

separate entities; changes in one window are reflected across the interface, allowing the human 

to focus on the interaction; and similar types of interaction are supported in all windows. Thus, 

various selecting, filtering, and drill-down (through the transaction hierarchy of transactions) 

interactions appear simultaneously in the multiple windows.  

Further, very lightweight cursor passover interaction is enabled in several places (for example, 

passing over specific keywords). Finally, direct manipulation is used wherever possible to 

maintain user focus. We believe that balanced interaction is an essential design principle to 

keep investigators “in the cognitive zone” when using a multi-window interface on complex 

problems. With balanced interaction, the different components of the interface merge into one 

cognitive whole where, as one of the papers co-authors remarked, “The interaction is the 

analysis” [29].    WireVis also has search by example, which has been singled out in our 

cognitive model because it is very general and it keeps the user in the context of her reasoning 

process without interrupting it to construct the appropriate search query, which can quite difficult 

to accomplish. In this case, the user selects the keyword or transaction pattern she is thinking 

about to gather similar or dissimilar patterns. Search by example has been considered generally 

useful in other types of visualization, such as image analysis [30], broadcast news event 

analysis [31], and terrorism event investigation [32]. In fact, we believe that search by example 

should be part of any visual analytics interface involving analysis or reasoning tasks for large 

amounts of information.  

 

Jigsaw  

Jigsaw is a visual analytics system used to support investigative analysis [27]. It works with 

large collections of reports or other text documents and with the entities extracted from them. Its 

two main goals are to permit investigators to handle efficiently and move quickly through large 

document collections and to support hypothesis formation and evidence gathering. Jigsaw won 

the university portion of the VAST 2007 Contest, which centered on a simulated investigation 

similar to those carried out by intelligence analysts.  

 

As with WireVis, Jigsaw makes strong use of multiple windows with carefully tailored 

representations for complex investigative problems.  The user is thought be in an “information 

cockpit” with multiple monitors, in front of and above the user [27]. Jigsaw seeks to maximize 

pixel use to take advantage of both the user’s high acuity central focus and wide peripheral field. 

This is also a valid design point for WireVis (which requires at least two desktop monitors or a 

high resolution cinema display) or any other multi-window system.  

 

However, although Jigsaw has some linking and brushing to integrate the windows, it does not 

have the balanced interaction WireVis employs. Based on the HCM guidelines, we would expect 

that users of Jigsaw would be less in the flow and require more cognitive effort than in WireVis 

during window management and connection. This is certainly a point worthy of further analysis 

and evaluation.  

As a point of contrast, Jigsaw uses a bottom-up approach, employing an incremental, query-

based method to bring in subsets of data for exploration and possible connection, as compared 

with WireVis’s top-down visualization of the whole dataset and its context. Undoubtedly both 

approaches are valid and could be available in a general tool for complex problem-solving, and 

will be the subject of future study.  Finally, Jigsaw is usable and simple. The interface permits 

direction interaction with representations of entities and reports, changing focus and detail. As 

with WireVis and other tools described here, simplicity and intuitiveness are not just goals based 

on perception principles but also based on the need for cognitive support. The HCM provides a 

point of view for investigating these goals in that light.  

 

 

 

 

 

UrbanVis  

UrbanVis is a system created to combine views of detailed geographical information with 

relational views of associated abstract information [33]. The geographical view is based on 

urban legibility theories in architecture, and the overall system permits the user to interactively 

explore an urban environment to understand the detailed characteristics of a city. 

 

 As with WireVis and Jigsaw and for the same general reasons, UrbanVis is highly dependent 

upon multiple views, with a 3D multiresolution map driving the updates of a straight category 

view and a parallel coordinates view, giving the user a rich overview of many categories and 

relations at once. These views were carefully chosen after consultation with architects, urban 

planners, and GIS experts. UrbanVis provides a general approach to reasoning with the 

combination of geographic and abstract data. We are applying UrbanVis to bridge management 

data over city and state regions and are planning to use it for situation awareness in emergency 

response. This and the other examples in Section 5 show the generality of a multi-window 

approach that is designed with principles of human cognition in mind.  Users of UrbanVis 

interact directly with the information, moving a geographic pointer and highlighting areas of 

interest or conversely choosing categories or individual coordinates in the parallel coordinates 

view to highlight specific geographic areas. This makes it easer to discover hidden geographical 

patterns for combinations of demographic or other abstract data. In the same sense as with 

WireVis, UrbanVis provides balanced interaction. This combined with direct manipulation, 

makes interaction the central focus. In addition, UrbanVis also provides a top-down, exploratory 

view with drill down controlled by simple movement of the ball up and down. The interface has 

only one menu, as it strives to keep the user cognitively focused during problem solving.  

 

 Finally, since UrbanVis utilizes the central role of interaction, the visualization makes E, as 

defined in Section 4, the seminal focus. In this interface, designed for a broad cross-section of 

users, K0 can be small or great, and an attempt is made through use of color and spatial 

organization to facilitate P.  

  

 GVis  

 Although the human is uniquely qualified for “higher order” reasoning, our human cognition 

model permits the computer to support this process in numerous ways. GVis in this section and 

SRS in the next provide some of this support. As several of the visualization approaches utilize 

similar methods, in these final sections we will highlight areas that are different from WireVis.   

Using information available in public biological databases such as NCBI, GVis pictures the 

relationships and publications known about genomic data  [34]. Due to the detail inherent in 

genomic data, the amount of information presently viewable during drill down in direct 

interaction becomes quickly overwhelming; the use of multiple views to visualize multiple levels 

of information hierarchies prevents humans from “losing their place” in the taxonomy. Also, 

similarly to WireVis, GVis is an expert visualization, requiring a rather sizeable K0 to focus 

effective Exploration. This may mitigate the cognitive overload effects of information on P and K 

in Figure 2. A popup menu allows the user to view and explore publications on the spheres in 

the main view.  This is perhaps not an optimum solution, as use of the menu is not intimate and 

can obstruct the field of view, which could threaten reasoning flow insularity and reduce the 

value of direct interaction.  

 

The visualization uses color and simple circles to highlight groups, insulating reasoning flow and 

focusing P. In addition, it employs the notion of a stretchable canvas, similar to Pad++ [30], to 

handle detail at all scales. The latest version of GVis applies the precepts of knowledge 

visualization, relying on taxonomic and ontological representations of genomic knowledge to 

determine what to visualize for the task at hand. When combined with the stretchable canvas, 

important knowledge at any scale can be made visible to support the current reasoning task. 

Thus, for example, glyphs showing how many genes are annotated (and what types) are made 

visible at the genome or even the cluster level, even though the individual genes are sub-pixel 

at these levels. This provides important support for the human reasoning process.  

  

Scalable Reasoning System (SRS)  

 In SRS, Pike et al. demonstrate nicely the capacity of visual analytics to aid in hypothesis 

generation and analysis [35]. For example, while searching by example in SRS is limited to text 

searches, queried information can become “reasoning artifacts” to be used as the basis of 

hypotheses, or as evidence for hypotheses represented in “thought chains.” The human is free 

to manipulate these artifacts directly in a sandbox-like information space, which encourages 

reasoning flow insularity and focuses P as described in Section 4. Additionally, by allowing the 

human to arrange artifacts, interaction becomes the principle objective. While SRS does not use 

multiple views to display information, by allowing rearrangement of artifacts, SRS encourages 

the human to organize them in a way that is meaningful to the individual.  Additionally, SRS is 

more than a display. New knowledge can be created by creating relationships between 

reasoning artifacts. Thus, as the human generates hypotheses and their evidence, new 

knowledge that is created during the process is not lost, either to the current analysis, or to all 

other humans given editable confidence ratings, which aids in the weighing of evidence in 

hypothesis analysis.  

 

TOWARDS A PREDICTIVE HUMAN COGNITIVE MODEL 

The current HCM is of substantial use in providing a framework for understanding the human-

computer interface, in providing some initial design principles for both visual representation and 

interaction that can then be evaluated, and in revealing what to evaluate to support human 

cognition. However, the HCM can be made many times more powerful by making it predictive. 

The GOMS model, for example, is a predictive model, based on HCI evaluation research, and is 

derived for the design of mouse-driven, window interfaces [38]. It demonstrably improved the 

speed of the design process for these systems because it permitted the designer to avoid the 

slow, laborious process of user studies for each element of the design and for the overall layout. 

Indeed, because of its predictive capability, it permitted optimization of these interfaces beyond 

what could be accomplished with painstaking user studies because the design space could be 

explored in a comprehensive, predictive manner. 

 

To develop a predictive HCM, we start with the idea of the user being in, falling out of, and 

regaining the cognitive zone (hereafter, referred to as the CZ). Initially, we want to develop a 

phenomenological model of these effects. We concentrate on two predictive mechanisms within 

the HCM, both focused on keeping the user in the CZ and in a high state of reasoning 

efficiency. The first predicts the cost and benefit of design principles and decisions for keeping 

the user within the HCM, including the cost of having to regain this state after falling out of it. 

The second predicts the enhanced (or depressed) probability of making a discovery and 

gathering an insight in a given period of time.  

 

This approach reflects two aspects of the human reasoning process. The first part is 

performance-oriented; the more effort spent on a reasoning task (measured by number of 

operations or some other measure), the more will be accomplished. The second part attempts 

to capture spontaneous insight. It is not as dependent on number of operations and may even 

be diminished by requiring too much focus and structure from the participant [39].  

We construct a cost/benefit model for the first part of our approach. Cost/benefit models have 

wide applicability and, in particular, have been used successfully on adaptive graphics rendering 

[40] and in van Wijk’s work, discussed previously [22]. A cost/benefit model would be (following 

van Wijk’s approach):  

 

 
nmW
G
CGF
 

=
=

∆
( K
=−

)

, 
(

CKWnm

∆

−

)

(

−

kC

−

)

C
i

e

−

nC

 

u

S

 

where G is the benefit due to increased knowledge from use of the interactive visualization, W is 

the value of the acquired knowledge, and F is the profit. These quantities depend on the change 

in knowledge ∆K, the number of users n, the number of sessions per user the data are 

visualized m, and the number of exploratory steps per visualization k. Costs depend on the 

initial development cost Ci, the initial cost per user Cu (e.g., for selecting, tailoring, and learning 

how to use the visualization), the initial cost per session Cs (e.g., for converting data and setting 

initial specifications), and the perception/exploration/cognition cost Ce (e.g., the user must spend 

time watching and understanding the visualization as the interactive exploration proceeds).  

 

Note that this model has a way to handle overall knowledge gain by a number of users (say, in a 

collaborative setting or for a tool that is widely used by a number of people) and takes into 

account training effort and costs per session for things such as data setup. Of course, the model 

is limited. For example, it assumes that if a user repeatedly revisits a dataset for further 

exploration, his knowledge will continue to increase. But, even recognizing these limitations, a 

predictive model would still be a powerful tool. Here, we are most interested in assessing the 

terms G and Ce. For the latter, we assess exploration costs and the penalty for falling out of the 

CZ and having to regain one’s cognitive rhythm for the task at hand. Here we will need to 

categorize and rank actions. It is undoubtedly true that some actions are more disruptive than 

others, and the HCM provides us guidance as to what those are. Then we can plan evaluation 

strategy for quantifying, at least roughly, both G and Ce. We have begun some of those 

evaluations, which will be reported elsewhere.  

 

To capture discovery and spontaneous insight, we will need a separate probabilistic model. Our 

first steps toward this model are necessarily crude; there is little in the literature on modeling 

these processes. The main aspect we wish to capture is the disruptive effect to flashes of 

insight due to falling out of the CZ [39,40], as well as the ameliorative effect of staying within the 

zone. Our basic assumption within this model is that, using the HCM, we can distinguish 

operations that keep the user in the CZ from those that are disruptive. To represent the 

cumulative effect of non-disruptive operations that lead to insight formation within the CZ (e.g., 

exploratory operations that support reasoning), we assume a probabilistic function that grows 

and then levels off over time as the user employs the interface. (See Figure 3 below.) However, 

there can be disruptive actions that take the user out of the CZ (e.g., having to form a Boolean 

search query 

or use a pull-down menu). These are represented by discontinuous downward trends, 

represented by breaks in the probability curve (as in Figures 3b and 3c), whose depth is 

determined by the degree of disruption. The idea here is that the probability of gaining a 

spontaneous insight over a given period of time can be significantly lowered by significant 

disruptions or repeated disruptions. We can determine a rough shape for this probabilistic 

function over time through controlled experiments that seed data with insights to be discovered 

(for example, several hidden, new patterns of fraud in wire transaction data within a data 

visualization) and then permit users to use a visual analytics tool to discover them. In this 

approach, the HCM provides a framework for categorizing and linking together the operations of 

the tool in terms of cognitive processes. 

 

(c
) 

 

 
Figure 3. Probabilistic model for capturing discovery and spontaneous insight. 

 

 

 

CONCLUSIONS  

 If we in the visual analytics community are to attain our aspiration of more effective, more 

human-perceptive visualizations, we must begin to understand how humans manipulate 

semantically-complex information. It is not enough to determine what is being seen and given 

attention. Nor is it appropriate to infer combinatorial, individually-variable reasoning heuristics 

from more binary cognitive behaviors. Just as an understanding of perceptual cognition based 

on evaluative research has been employed in creation of effective information visualization 

displays, a competent comprehension of reasoning, problem-solving, and decision-making must 

be employed in the development of mixed-initiative analytical interfaces.  

What we have proposed is not a full working model, but is, as yet, a framework of human 

cognition. Our goal is to sketch out a system of “thinking about thinking” as a first step to 

interface interaction which is no longer just between user and data, but between human and 

computer partners, collaborating in the discovery of information and the creation and extension 

of knowledge.  

 

What we have proposed is a model still in its rudimentary stages, whose future will undoubtedly 

be marked by additions, corrections, and multiple series of empirical evaluation. In some ways, 

this work still has the emergent expectation and general outline of an archaeological excavation; 

we cannot pretend to have all of the answers, but we know we’re digging at the right spot.   

 

Even in its infant stages, the HCM has the potential to revolutionize how cognition is considered 

and evaluated in visualization design. For too long, visual analytic research has depended on 

black box assumptions about complex cognition that tend to be anecdotal, over-simplified, and 

non-generalizeable. And because of a broad lack of standardization in protocol and incomplete 

results reporting in published visual analytics evaluations, it is difficult or impossible to replicate 

results or build on prior work. Claims are made, but the discipline as a whole stands still. For 

these reasons, among others, the study of reasoning behaviors during interactive visualization 

is often difficult and unclear. This threatens to stymie progress; our ability to visualize is 

outstripping our knowledge of the thinking system we are visualizing for. 

 

Additionally, the HCM reminds us that human cognition is not, nor shall it ever be, one-size-fits-

all. One reason that reasoning cognition is complicated is that its combinatorial nature allows for 

maximal problem-fit and solution scalability. In addition, which heuristics are used often 

depends on innate and learned differences within reasoners, as well as the challenges of the 

presented task. The potentially exponential scale of this complexity seems daunting. But 

humans tend toward the habitual; they also prefer not to think harder than they must. 

Psychological sciences have used these tendencies to their advantage, grouping reasoners 

based on learning styles and other built-in preferences, and using these groupings to develop 

trait rubrics that aid in explaining and, in some cases, predicting future behavior. There is no 

reason that we could not build on this extant work and group users based on individual 

differences, domain similarities, and interaction behavior, provided we are willing to adopt a 

similar, exacting, experimental rigor. In fact we have begun studies in this direction where we 

compare the strategies of users of our tools based on whether they are expert or novice or 

based on their demonstrated problem-solving abilities. In the former case, we are finding that 

the higher level strategies are easier to pick out and analyze because our tools are designed 

and evaluated with the HCM in mind.  

 

One of the real strengths of creating a model such as the HCM is its inherent falsifiability. It 

would have been easier, perhaps, to choose one assumption, test and retest, then chose 

another assumption, test and retest, and perhaps eventually arrive at a model construct 

inductively. But the likelihood is that the trees would have muddled our sense of direction in the 

proverbial forest of mixed-initiative interaction. And very little would have been accomplished. 

By feeling our way to the boundaries of the forest, and marking its perimeter, and through 

application of insight, we have a better sense of where we are. The HCM is itself a hypothesis 

based on the evidence of cognitive research and insights derived from the visual analytics 

designs of ourselves and others. But when it is applied, it informs the questions we ask and the 

evaluations we design. It provides a clarifying framework and a way to organize the results we 

get when we use and analyze our visual analytics tools. And it reminds us that individual 

cognitive behaviors must be evaluated within a holistic context. It is a place to start, and it is a 

thing to be tested. And as with all hypotheses, our testing will show what is valid and what 

needs to be expanded upon or changed.  

 

 We have been able to show extant examples of several of the HCM’s submodels, but there is 

still work to be done. There are, as yet, no spontaneous methods of searching by analogical 

structure. Computer-aided discovery will require both a better understanding of learning 

interfaces as well as a comprehensive understanding of human iterative thought chaining. 

There is also the pesky problem of a unified theory of reasoning. Understanding the available 

fragmented research is a good foundation, but we must approach a more complete discernment 

of how all of the pieces work together in an information space to be better able to define and 

evaluate the best ways to insulate reasoning flow, mitigate cognitive load, facilitate appropriate 

task switching, and minimize attentional interference during the reasoning process. 

Finally, there is the issue of better understanding of the temporal coordination of human 

reasoning and computation and presentation of information. The temporal dynamics of control 

actions and cognitive processing were addressed early in the history of HCI with GOMS and 

keystroke analysis. However the dynamic coupling of scene gist perception, eye movements, 

attentional allocation, cognitive processing and microstrategic perception/action patterns [36] 

remains to be explored. Recent advances in the application of nonlinear dynamical modeling[37] 

may provide sufficient predictive validity for incorporation into models of sensemaking in visual 

analytics.  

These tasks are broad items on a bold agenda. But our evaluation has also uncovered multiple 

practical problems, and directed the search for how best to tackle them:  what number of 

multiple views maximizes the cognitive return on investment, the best way for the computer to 

suggest unconsidered information without interrupting – or annoying – the human at work, and 

methods of maintaining the interactive process so as to keep cognition in the flow, whatever the 

task. Finally, it is also clear that there must be strong support for permitting and managing 

human annotations. But again, it all starts with daring to peek into reasoning’s black box.  

  

ACKNOWLEDGEMENTS  

 This work was performed with support from the National Visualization and Analytics Center 

(NVACTM), a U.S. Department of Homeland Security Program, under the auspices of the 

SouthEast Regional Visualization and Analytics Center. Special thanks to Benjamin F. Green for 

helpful insights.  

   

  

 

 

REFERENCES  

[1] Green TM, and Ribarsky W. Using a human cognition model in the creation of  collaborative 
knowledge visualizations.  SPIE Defense + Security 2008 (Orlando, FL).  
 
[2] Newell A. You can’t play 20 questions with nature and win: Projective comments on the 
papers of this symposium. In Visual Information Processing. Chase, W.G. (Ed). New York: 
Academic Press, 1973.  
 
[3] Piaget P. “Piaget’s theory.” In Cognitive development to adolescence, K. Richardson, and S. 
Sheldon, (Eds.) Erlbaum:  Hillsdale, NJ,  1988; 3 – 18.  
 
[4] Komatsu KL. Recent views of conceptual structure. Psychological Bulletin 1992; 112: 500-
526. 
 
[5] Gigerenzer G and Goldstein DG, Reasoning the fast and frugal way: Models of bounded 
rationality. Psychological Review 1996; 103(4). 
 
[6] Biederman I. Recognition by components: A theory of human image understanding.  
Psychological Review 1987; 94 (2): 115 -147.  
 
[7] Rensink RA. A dynamic representation of scenes. Visual Cognition 2000; 7.  
 
[8] Pylyshyn Z, Seeing and Visualizing: It’s not what you think, MIT Press/Bradford Books, 
Cambridge, MA. 2003.  
 
[9] Pylyshyn Z. Things and Places. IT Press/Bradford Books, Cambridge, MA. 2007.  
 
[10] Ullman S. Visual routines. Cognition 1984. 97-159.  
 
[11] Rock I. The logic of perception. MIT Press/Bradford Books, Cambridge, MA. 1985.  
 
[12] Tarr MJ. Learning to see faces and objects. Trends in Cognitive Sciences 2003. 7(1).  
 
[13] Kozielecki J. Elements of a psychological decision theory.  Studia Psychologica 1971. 
13(1): 53-60.  
 
[14] Cherubini P and Mazzocco A. From models to rules: Mechanization of reasoning as a way 
to cope with cognitive overloading in combinatorial problems. Acta Psychologica 2004. 16(3): 
223-243. (2004).  
 
[15] Miller GA. The magic number seven, plus or minus two: Some limits on our capacity for 
processing information. Psychological Review (63). 81-97.  
 
[16] Wason PC. On the failure to eliminate hypotheses in a  conceptual task.  Quarterly Journal 
of Experimental  Psychology 1960. 12: 129-140.  
 
[17] Evans J.St. BT, Varston J, and Pollard P, On the conflict between logic and belief in 
syllogistic reasoning. Memory and Cognition 1983. 11: 295-306.  
 

[18] Baldonado MQW, Woodruff A, and Kuchinsky A. Guidelines for using multiple views in 
information visualization. In  Proceedings of  Working Conference on  Advanced Visual 
Interfaces 2000 (Palermo, Italy).  
 
[19] Bederson BB, Hollan JD. Pad++: A zooming graphical interface for exploring alternate 
interface physics. In UIST  1994: 17-26.  
 
[20] Pirolli P. Computational models of information scent- following in a very large browsable 
text collection.  Proceedings of the Conference on Human Factors in  Computing Systems, CHI 
1997 (Atlanta, GA).  
 
[21] Heuer RJ, The Psychology of Intelligence Analysis. Center for the Study of Intelligence. 
CIA. 1999.  
 
[22] van Wijk JJ,  The value of visualization.  IEEE  Visualization, Proceedings of Vis 2005: 79 – 
86.  
 
[23] Pirolli P and Card S. The Sensemaking Process and Leverage Points for Analyst 
Technology   
as Identified Through Cognitive Task Analysis. Proceedings of International Conference on 
Intelligence Analysis 2005: 2-4. 
 
[24] Knoblich G, Ohlsson S, and Raney GE. An eye movement study of insight problem solving. 
Memory and Cognition 2001 29(7): 1000-1009.  
 
[25] Bowden EM, Jung-Beeman M, Fleck J, and Kounios J. New approaches to demystifying 
insight. Trends in Cognitive Sciences 2005 9(7):322–328. 
   
[26] Chang R, Ghoniem M, Kosara R, Ribarsky W, Yang J, Suma E, Ziemkiewicz C, Kern D, 
Sudjianto A. WireVis: Visualization of categorical, time-varying data from financial transactions. 
Proceedings of the 2007 IEEE Symposium on Visual Analytics Science and Technology 2007 
(Sacramento, CA): 155-162. IEEE Computer Society. 
 
[27] Stasko J, Gorg C, Liu Z and Singhal K, Jigsaw:  Supporting Investigative Analysis through 
Interactive  Visualization. Proceedings of 2007 IEEE Symposium on Visual Analytics Science 
and Technology 2007 (Sacramento, CA): 131-138.  
 
[28] Wise JA, Thomas JJ,  Pennock K, Lantrip D, Pottier M, Schur A, and Crow V. Visualizing 
the non-visual: spatial analysis and interaction with information  from text documents.  In 
INFOVIS 1995: Proceedings of  the 1995 IEEE Symposium on Information V visualization: 51. 
IEEE Computer Society.  
 
[29] Private communication, Remco Chang.  
 
[30] Yang J, Fan J, Hubball D, Gao Y, Luo H, Ribarsky W, and Ward M. Semantic Image 
Browser: Bridging Information Visualization with Automated Intelligent Image Analysis. 
Proceedings of IEEE VAST 2006: 191-198. 
 
[31] Ghoniem M, Luo D, Yang J, and Ribarsky W.  NewsLab:Exploratory Broadcast News Video 
Analysis.  IEEE VAST 2007: 123-130.  
 

[32] Wang X, Chang R, Kosara R, Ribarsky W, Smarick K,  and Miller E. Investigative Visual 
Analysis of Global Terrorism. EG/IEEE EuroVis 2008.  
 
[33] Chang R, Wessel G, Kosara R, Sauda E, and Ribarsky W. Legible Cities: Focus-
Dependent Multi-Resolution Visualization of  Urban Relationships. IEEE Transactions on 
Visualization and Computer Graphics (TVCG) InfoVis  2007 (Sacramento, CA). 
 
[34] Hong J, Jeong DH, Shaw CD, Ribarsky W, Borodovsky M, and Song C. GVis: A Scalable 
Visualization  Framework for Genomic Data,“ In Proceedings of EuroVis 2005: 191-198.  
 
[35] Pike AP, May R, Baddeley B, Riensche R, Bruce J, and Younkin K, Scalable visual 
reasoning: supporting collaboration through distributed analysis. Proceedings of IEEE 
International Symposium on Collaborative Technologies and Systems 2007. 
 
[36] Gray WD, and Boehm-Davis DA. Milliseconds Matter: An introduction to microstrategies 
and to their use in describing and predicting interactive behavior.  Journal of Experiment 
Psychology: Applied 2000 6(4): 322-335.  
 
[37] Spivey M. The Continuity of Mind. Oxford Press, NewYork, NY; 2007.  
 
[38] Card SK, Moran TP, and Newell A. The Psychology of Human-Computer  
Interaction. Lawrence Erlbaum, Hillsdale, N.J: 1983. 
 
[39] Lehrer J. The Eureka Hunt: Why do good ideas come to us when they do? New Yorker, 
July 28, 2008: 40. 

[40] Wilson TD, and Schooler JW. Thinking too much: introspection can reduce the quality of 
preferences and decisions. Journal of Personality and Social Psychology 1991 60(2): 181-192. 

[41] Tera Green, William Ribarsky, and Brian Fisher. Visual Analytics for Complex Concepts 
Using a Human Cognition Model. Proc. IEEE VAST 2008, pp. 91-98. 

 

Figure 1. The Human Cognition Model (HCM) showing process initiators. 

 

 

Figure 2. The van Wijk visualization model, with our integrations in red. 

 

",False,2009.0,{},False,False,journalArticle,False,QDP98BY5,[],self.user,False,False,False,False,http://journals.sagepub.com/doi/10.1057/ivs.2008.28,,Building and Applying a Human Cognition Model for Visual Analytics,QDP98BY5,False,False
MVFFYPN6,C5A74AMX,"www.computer.org/intelligent

Making Sense of Sensemaking 2:

A Macrocognitive Model

Gary Klein, Brian Moon, and Robert R. Hoffman

Vol. 21, No. 5

September/October 2006

This material is presented to ensure timely dissemination of scholarly and technical
work. Copyright and all rights therein are retained by authors or by other copyright
holders. All persons copying this information are expected to adhere to the terms
and constraints invoked by each author's copyright. In most cases, these works

may not be reposted without the explicit permission of the copyright holder.

© 2006 IEEE. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or

for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be

obtained from the IEEE.

For more information please see www ieee org/portal/pages/about/documentation/copyright/polilink html

H u m a n - C e n t e r e d   C o m p u t i n g

Making Sense of Sensemaking 2:  

A Macrocognitive Model

Gary Klein and Brian Moon, Klein Associates Division of ARA
Robert R. Hoffman, Florida Institute for Human & Machine Cognition

I n our first essay on sensemaking,1 we discussed various

possible meanings of the concept and debunked some
of the myths that seem current in discussions of cognitive
work. The motivation for these two essays is to question 

whether it makes sense to envision certain kinds of intelli-
gent sensemaking systems. None of the “verdicts” we an-
nounced in the first essay mean that intelligent technologies
might not assist people in sensemaking. Indeed, intelligent
technologies might help; they just won’t be the sorts of tech-
nologies that people seem to seek. 

Gary Klein and his colleagues have laid out a theory of
sensemaking that might be useful for intelligent systems
applications.2 It’s a general, empirically grounded account
of sensemaking that goes significantly beyond the myths
and puts forward some nonobvious, testable hypotheses
about the process.

When people try to make sense of events, they begin
with some perspective, viewpoint, or framework—how-
ever minimal. For now, let’s use a metaphor and call this a
frame. We can express frames in various meaningful forms,
including stories, maps, organizational diagrams, or scripts,
and can use them in subsequent and parallel processes.
Even though frames define what count as data, they them-
selves actually shape the data (for example, a house fire
will be perceived differently by the homeowner, the fire-
fighters, and the arson investigators). Furthermore, frames
change as we acquire data. In other words, this is a two-

Editors: Robert R. Hoffman, Patrick J. Hayes, and Kenneth M. Ford
Institute for Human and Machine Cognition, University of West Florida
rhoffman@ai.uwf.edu

way street: Frames shape and define the relevant data, and
data mandate that frames change in nontrivial ways. 

Figure 1 shows that the basic sensemaking act is data-
frame symbiosis. The figure captures a number of sense-
making activities. Sensemaking can involve elaborating
the frame by adding details, and questioning the frame
and doubting the explanations it provides.3 A frame func-
tions as a hypothesis about the connections among data.
One reaction to doubt is to explain away troublesome data
and preserve the frame.4,5 These two aspects, elaborating
the frame and preserving the frame, are part of the elabora-
tion cycle of sensemaking (the left side of figure 1), akin to
Jean Piaget’s notion of assimilation.

Yet another sensemaking cycle is to reframe (see the

figure’s right side). Here, questioning the frame leads us to
reconsider—to reject the initial frame and seek to replace it
with a better one. We might compare alternative frames to
determine which seems most accurate. Or we might simply be
mystified by the events. The sensemaking activity here, akin
to Piaget’s notion of accommodation, is to find some sort of
frame that plausibly links the events that are being explained.
Each of these aspects of sensemaking has its own dynam-

ics, strategies, and requirements. Recognizing a frame and
recognizing data are different from elaborating a frame that
has already been adopted, and this is different from explain-
ing away inconsistencies. Different still are the reactions to
questioning a frame—choosing between alternative frames
and constructing a frame where none exists.

The Data/Frame Theory posits a closed-loop transition

sequence between 

• mental model formation (which is backward looking

and explanatory), and 

• mental simulation (which is forward looking and antici-

patory).

Think of the simplest transition sequence as a chain of
closed loops. Each loop is triggered by a perceived sub-
event, leading to an effort to refine the existing mental
model (backward looking) and an effort to run a new men-
tal simulation (forward looking). You can construct a tran-

88

1541-1672/06/$20.00 © 2006 IEEE
Published by the IEEE Computer Society

IEEE INTELLIGENT SYSTEMS

sition sequence retrospectively to generate
an explanation of how events and subevents
unfolded, or prospectively to imagine how a
major causal factor or a situational mix of
factors might play out. For illustration, envi-
sion a transition sequence using the meta-
phor of billiards, where a player would antic-
ipate how hitting one ball would lead to
motion in a second, and a third, to the shot’s
completion.

Empirical findings

We examine five areas of empirical find-

ings: causal reasoning, commitment to
hypotheses, feedback and learning, sense-
making as a skill, and confirmation bias.
In each area the Data/Frame model, and
the research it’s based on, doesn’t align
with common beliefs. For that reason, the
Data/Frame model cannot be considered a
depiction of commonsense views.

Causal reasoning

Studies of domain practitioners’ stories
about how they understood real-life deci-
sion-making situations suggest that transi-
tion sequences—beliefs about what converts
one situation into another—are typically
based on about three to four causal factors.
For example, in explaining why one sports
team beat another, newspaper accounts typ-
ically focus on a single event such as a crit-
ical turnover (“and that cost them the game”),
or perhaps that plus one or two other events,
such as a star player doing poorly or well.
Given the game’s length, we can see these
as oversimplifications, but most people
would skim over any account that tried to
capture a game’s full complexity. That’s
why we introduced the billiards metaphor
earlier, to illustrate a preference for chains
of simple cause-effect relationships. A sin-
gle causal factor at each junction might be
the preferred form of explanation,2,6 al-
though such explanations open the decision
maker up to the reductive tendency.7

Consideration of hypotheses

Decision makers are sometimes advised
that they can reduce the likelihood of a fixa-
tion error by avoiding early consideration
of a hypothesis.8 But the Data/Frame The-
ory regards early consideration to a hypothe-
sis as advantageous and inevitable. Early
consideration—the rapid recognition of a
frame—permits more efficient information
gathering and more specific expectancies
that can be violated by anomalies, permit-

Data

Recognize/
construct
a frame

Frame

Manage attention

  and define,

connect, and filter

the data

Elaboration

cycle

Elaborate a frame

Add and fill slots
Seek and infer data
Discover new data/
   new relationships
Discard data

Question a frame

Track anomalies
Detect inconsistencies
Judge plausibility
Gauge data quality

Preserve

Reframing

cycle

Reframe

Compare
frames

Seek a new

frame

Figure 1. The Data/Frame Theory of sensemaking.

ting adjustment and reframing. Jenny Ru-
dolph9 found that decision makers must be
sufficiently committed to a frame in order to
be able to test it effectively and learn from its
inadequacies—something that’s missing
from open-minded and open-ended diag-
nostic vagabonding. Winston Sieck and his
colleagues have found that domain experts
are more likely to question data than nov-
ices, perhaps because they’re more famil-
iar with instances of faulty data.10 It might
also mean that experts are more confident
in their frames and therefore more skepti-
cal about contrary evidence, in contrast to
novices who are less confident in the frames
they identify.

These observations would suggest that
efforts to train decision makers to keep an
open mind11 can be counterproductive,
and efforts to make machines that do the
vagabonding for the human might be simi-
larly unhelpful. We hypothesize that meth-
ods designed to prevent premature consid-
eration to a frame will degrade performance
under conditions where active attention
management is needed (using frames) and
where people have difficulty finding useful
frames. Spoon-feeding interpretations to
the human (via such methods as data fusion)
can be counterproductive.

Feedback and learning

Another implication of the Data/Frame

Theory concerns using feedback to pro-

mote learning. Frames are by nature reduc-
tive. And yet, frames can help overcome the
reductive tendency. The commitment to a
frame must be coupled with a motive to test
the frame to discover when it’s inaccurate.
This process hinges on feedback of a certain
kind. Outcome feedback (“you got it wrong”)
isn’t nearly as useful as process feedback
(“you did it wrong”),12 because knowing that
performance was inadequate isn’t as valuable
as understanding what to modify in the rea-
soning process. This includes the frame itself,
because that will determine the way feedback
is understood. In other words, people need
sensemaking to understand the feedback that
might improve sensemaking—the cycle as
shown in figure 1. The implication is that
people might benefit more from intelligent
systems that guide the improvement of frames
than from systems that generate alternative
understandings and hypotheses and foist
them on the human.

Sensemaking as a skill

We haven’t seen evidence for a general
sensemaking skill. Some incidents we’ve
collected do suggest differences in motiva-
tion—an “adaptive mind-set” of actively
looking to make sense of events, as illustrated
in essay 1’s example of the patient with a
pacemaker. It might be possible to develop
intelligent systems that acknowledge the
Pleasure Principle of human-centered com-
puting13 and promote a positive motivation to

SEPTEMBER/OCTOBER 2006

www.computer.org/intelligent

89

question frames and to reframe, or at least
not to frustrate the human and thereby de-
tract from intrinsic motivation. Training
might be better aimed at increasing the range
and richness of frames, particularly causal
mental models, and skill at noticing anom-
alies. Training scenarios and decision support
might be developed for all the sensemaking
activities in figure 1 (elaborating a frame,
questioning a frame, evaluating a frame,
comparing alternative frames, reframing a
situation, and seeking anchors to generate a
useful frame). Training would aim to provide
a larger, richer repertoire of frames rather
than to improve each aspect of sensemaking
as if it were a separate skill.

Is there a confirmation bias?

The decision research literature suggests
that people are inclined to look for and no-
tice information that confirms a view rather
than information that disconfirms it.14,15
And yet more recent research looking at
experts has shown just the opposite. For ex-
ample, expert weather forecasters have
sometimes been observed to deliberately
look for information that might disconfirm
hypotheses about future severe weather.16

The Data/Frame Theory provides a richer

understanding of what’s actually going on
here. People don’t engage in simple mental
operations of confirming or disconfirming
a hypothesis. Our cognitive task analyses
of real-world decision making show that
skilled decision makers shift into an active
mode of elaborating a competing frame
once they detect the possibility (or become
worried) that the current frame is poten-
tially inaccurate. What might look like a
confirmation bias might be simply using a
frame to guide information seeking. You
need not think of it as a bias and assume
that the purpose of an intelligent decision
support system must be to help the human
overcome some inherent reasoning bias. 

Implications for AI: 
Reframing frames

Now, the other shoe must drop. Not only
might the phenomenon of sensemaking
illuminate the computational notion of
frames—conversely, that computational
notion might challenge our notion of
sensemaking.

Reframing frames

As Marvin Minsky described frames,

these organizing structures express the val-

ues of features that together define mean-
ingful entities or categories—groups of
slots into which the values of defined vari-
ables are entered.17 The primary function
of frames (in Minsky’s original discussion)
is recognition, to guide attention to fill in
missing parts of the frame, to test a frame
by searching for diagnostic information. 
To Minsky, frames are things you think
with. In the Data/Frame Theory, frames are
things that you think with but also things
you think about. The Data/Frame Theory
therefore blurs the border between phe-
nomenological description and macrocogni-
tive modeling.

We introduced the Data/Frame Theory

by suggesting that when we try to make
sense of events, we begin with some frame-
work, however minimal. In the Cartesian
view of things, sensory inputs (for exam-

We introduced the Data/Frame
Theory by suggesting that when
we try to make sense of events,
we begin with some framework,
however minimal. 

ple, a pattern of moving colored shapes)
make contact with memory, lending them
meaning in a process called perception (“it’s
a cat”). But there’s a subsequent process,
once called “apperception,” which inter-
prets the percept more broadly in terms of
knowledge (for instance, “I like cats” or
“cats can be a symbol for evil”). This is ab-
ductive inference, or something rather like
it. So the challenge is, where do these frames
come from in the first place? Here we see
one of AI’s outstanding problems, just as it
has manifested in numerous views through-
out psychology’s history:18 Any computa-
tional theory of how knowledge is formed
as self-contained bundles should come
with a full story about how these pro-
posed “frame-ish” things are supposed to
be created and what architectural assump-
tions underlie them. The AI systems Slate
and Cyc both perform abductive reasoning
to a plausible explanation using post-Minsky

systems based on expressive logics. They
both test their hypotheses by actively trying
to refute them.

The phenomenon of sensemaking ties

also to the notion that frames are chunks of
knowledge abstracted away from computa-
tional details—symbolic descriptions that
are taken off the shelf and used to perceive
things, and thereby constitute understand-
ing. However, what we see in studies of
sensemaking doesn’t fit with this view of
frames in three ways. First, understandings
shift; frames get changed. They aren’t just
“taken off a shelf.” Frames change as data
are acquired (so this isn’t just a matter of
frame reuse).

Second, even though frames define what
count as data (which could be interpreted as
a Minskyian notion), as we said earlier,
frames themselves actually shape the data
(so this isn’t just a matter of data primitives).
For example, skilled weather forecasters
don’t passively rely on the data presented
by computational aids. Many computer mod-
els exist for forecasting weather. Some are
based on climate statistics, others on com-
putational models of the atmosphere. Each
of these has known biases—for example, a
model’s tendency to overforecast the depth
of low-pressure systems as fronts pass over
the Appalachian mountains and the lows re-
form over the US East Coast. Experienced
forecasters take these biases into account
and adjust their interpretations of the com-
puter forecasts accordingly.

Finally, frames sometimes have a just-
in-time quality. Rarely do decision makers
simply identify a relevant mental model.
Instead, they construct the frame from
smaller sets of causal relationships.

Here too is a challenge for both cogni-
tive science and AI: Any computational
theory of how knowledge is formed as self-
contained bundles has to come with a full
story about how these proposed “frame-
ish” things are supposed to be manipulated.
In the first essay on sensemaking, we re-
ferred to the similarity between sensemak-
ing and mental modeling.1 Most discus-
sions of psychological research on mental
models focus on comparing student and ex-
pert mental models, the student’s use of
mental models to make (erroneous) infer-
ences, and the issue of how to train students
to move beyond their naive analogies.19 We
need richer accounts of how the structures
are constructed and manipulated.20 In con-
trast, work in intelligent systems, such as

90

www.computer.org/intelligent

IEEE INTELLIGENT SYSTEMS

the Structure Mapping Engine,21 has speci-
fied some process mechanisms.

If frames shape data, how do data man-
date any operation on the frame? What de-
fines how frames get changed? Does this
require using other frames to govern the
frame-changing process? If not, how is it
done? If so, what distinguishes the frames
being changed from the frames mandating
changes? What process can we use to ques-
tion or doubt a frame? If frames are the
vehicle that supports sensemaking, then
any doubt would seem to require us to
use a doubting-frame to represent the alter-
native hypothesis (that the frame might be
incorrect or faulty). What kind of relation-
ship between frames does this imply?

The phenomenon of sensemaking ties

also to the notion that frames organize the
large-scale structure of inference making—
they’re recipes for solving problems. What
we see in studies of sensemaking is that
frames aren’t recipes, although they do
play roles in inference making. In AI, the
lesson was that knowledge packets are
great when you can get them exactly right,
so that all you have to do is use them, but
you almost never can. For Minskyian frames
to be useful, they had to have lots of details,
but that would render them of little use
across contexts unless they had some sort
of internal machinery. Either that, or the
frames would have to be chunks that you
could pick apart by invoking some external
inference machinery. When a frame actu-
ally gets used, you must be able to take it
apart into the basic facts that constitute it
and be able to use the flexibility that this
gives you, because the chances of a frame
being a perfect fit to particular circumstances
are close to zero. Furthermore, if you want
to use a frame to tell you what to actually
do in a particular circumstance, you need
some way to connect the general “type” to
the particular exigencies. Computational
versions of frames are notoriously poor at
this, whereas decision makers are skilled at
using frames in these ways.22

A pertinent idea from sensemaking stud-
ies is that thinking about frames in terms of
an either-or (large chunks, with internal
inference machinery, versus small chunks,
with external inference machinery) might
not be the best way to proceed. Indeed, we
might think of the progression from novice
to expert as a process of learning whereby
individual cases, or small, contextually
bound understandings with specific infer-

ence possibilities attached to them, might
develop into larger, more organized under-
standings. In some cases, these understand-
ings would have inference opportunities
bound to them; in other cases, they’d be sub-
ject to inference machinery external to them.
A process akin to logical inference or proof
would assemble multiple small pieces into
useful larger pieces. A process akin to pattern
recognition might pick up big packets all at
once, whereas a process more like mutual
matching and fitting might assemble small
pieces. In other words, there might be multi-
ple assembly processes. The former might be
a mechanism to explain “recognition-primed
decision making,” when the expert goes from
an immediate understanding of a situation to
a course of action. The latter would consti-
tute the cycles described in figure 1. This is
reminiscent of Cyc, in which bundles of
related assumptions and concepts (“micro-
theories”) are logic-like in their small struc-
ture but frame-like in that they’re large and
specific to a topic or concept and the knowl-
edge inside them is specific to them.

We relied on the frame concept in

the Data/Frame Theory as a metaphor to
bootstrap a discussion of how people cre-
ate, use, and manipulate organizing struc-
tures. We do not offer any clear path to a
computational theory of how “frame-ish”
things are created or manipulated. Our
main goal in discussing the Data/Frame
Theory is to point to empirical studies of
how domain practitioners make decisions
in complex, real-world contexts and then
to mine these results for ideas that might
invigorate and inform work on these fun-
damental issues.

Acknowledgments

Robert Hoffman’s work on this essay was

supported through his participation in the Ad-
vanced Decision Architectures Collaborative
Technology Alliance, sponsored by the US Army
Research Laboratory under cooperative agree-
ment DAAD19-01-2-0009.

References

1. G. Klein, B. Moon, and R.R. Hoffman, “Mak-
ing Sense of Sensemaking 1: Alternative Per-
spectives,” Intelligent Systems, vol. 21, no. 4,
2006, pp. 70–73.

Gary Klein is
chief scientist in
the Klein Associ-
ates Division of
Applied Research
Associates. Con-
tact him at Klein
Associates, 1750
Commerce Center

Blvd. N., Fairborn, OH 45324; gary@
decisionmaking.com.

Brian Moon is a
research associate
in the Klein Asso-
ciates Division of
Applied Research
Associates. Con-
tact him at Klein
Associates, 1750
Commerce Center

Blvd. N., Fairborn, OH 45324; brian@
decisionmaking.com.

Robert R. Hoffman is a senior research
scientist at the Institute for Human and
Machine Cognition. Contact him at IHMC,
40 So. Alcaniz St., Pensacola, FL 32502-
6008; rhoffman@ihmc.us.

2. G. Klein et al., “A Data/Frame Theory of
Sensemaking,” to be published in Expertise
Out of Context: Proc. 6th Int’l Conf. Natu-
ralistic Decision Making, R.R. Hoffman, ed.,
Lawrence Erlbaum Associates, 2006. 

3. K.E. Weick, Sensemaking in Organizations,

Sage Publications, 1995.

4. C.A. Chinn and W.F. Brewer, “The Role of
Anomalous Data in Knowledge Acquisition:
A Theoretical Framework and Implications
for Science Instruction,” Rev. Educational
Research, 1993, vol. 63, pp. 1–49.

5. P.J. Feltovich, R.J. Spiro, and R.L. Coulson,
“Issues  of  Expert  Flexibility  in  Contexts
Characterized by Complexity and Change,”
Expertise in Context: Human and Machine,
P.J. Feltovich, K.M. Ford, and R.R. Hoffman,
eds., AAAI/MIT Press, 1997, pp. 125–146.

6. G.A. Klein and B.W. Crandall, “The Role of
Mental Simulation in Naturalistic Decision
Making,” Local Applications of the Ecologi-
cal Approach to Human-Machine Systems,
vol. 2, P. Hancock eds., Lawrence Erlbaum
Associates, 1995, pp. 324–358.

7. P.J. Feltovich, R.R. Hoffman, and D. Woods,
“Keeping It Too Simple: How the Reductive
Tendency Affects Cognitive Engineering,”
IEEE Intelligent Systems, May/June 2004, pp.
90–95.

8. R.  Heuer, The  Psychology  of  Intelligence
Analysis, tech. report, CIA Center for the
Study of Intelligence, 1999.

SEPTEMBER/OCTOBER 2006

www.computer.org/intelligent

91

IEEE Distributed Systems Online
brings you peer-reviewed articles, detailed

tutorials, expert-managed topic areas, and

diverse departments covering the latest news

and developments in this fast-growing field.

Log on http://dsonline.computer.org 
for free access to topic areas on

❍ Grid Computing
❍ Mobile & Pervasive
❍ Distributed Agents
❍ Security
❍ Middleware
❍ Parallel Processing
❍ Web Systems
❍ Real Time & Embedded
❍ Dependable Systems
❍ Cluster Computing
❍ Distributed Multimedia
❍ Distributed Databases
❍ Collaborative Computing
❍ Operating Systems
❍ Peer to Peer

h
t
t
p

:
/
/
d
s
o
n

l

i

n
e
.
c
o
m
p
u
t
e
r
.

o
r
g

To receive regular updates, email
dsonline@computer.org

9. J.W. Rudolph, “Into the Big Muddy and Out
Again,” doctoral dissertation, Boston College,
2003; http://escholarship.bc.edu/dissertations/
AAI3103269.

10. W.R. Sieck et al., “Basic Questioning Strate-
gies for Making Sense of a Surprise: The
Roles of Training, Experience, and Exper-
tise,” Proc. 26th Ann. Conf. Cognitive Sci-
ence Soc. (CogSci 04), Lawrence Erlbaum
Associates, 2004; www.cogsci.northwest-
ern.edu/ cogsci2004/ma/ma305.pdf.

11. M.S. Cohen et al., “Dialogue as the Medium
for Critical Thinking Training,” Proc. 6th Int’l
Conf. Naturalistic Decision Making, Law-
rence Erlbaum Associates, 2003.

12. E. Salas et al., “Myths to Avoid about Crew
Resource Management Training,” Ergonom-
ics in Design, Fall 2002, pp. 20–24.

13. R.R. Hoffman and P.J. Hayes, “The Pleasure
Principle,” IEEE Intelligent Systems, Jan./Feb.
2004, pp. 86–89.

14. C.R.  Mynatt, M.E.  Doherty, and  R.D.
Tweney, “Consequences of Confirmation and
Disconfirmation  in  a  Simulated  Research
Environment,” Quarterly  J.  Experimental
Psychology, vol. 30, 1978, pp. 395–406.

15. P.C. Wason, “On  the  Failure  to  Eliminate
Hypotheses in a Conceptual Task,” Quarterly
J. Experimental Psychology, vol. 12, 1960,
pp. 129–140.

16. R.R. Hoffman, G. Trafton, and P. Roebber,
Minding the Weather: How Expert Forecast-
ers Think, MIT Press, 2006.

17. M. Minsky, “A Framework for Representing
Knowledge,” The Psychology of Computer
Vision, P.H.  Winston, ed., McGraw-Hill,
1975, pp. 211–277.

18. H.L. Dreyfus, “A Framework for Misrepre-
senting Knowledge,” Philosophical Perspec-
tives in Artificial Intelligence, M. Ringle, ed.,
Humanities Press, 1979, pp. 110–123.

19. D. Gentner and A. Stevens, eds., Mental Mod-

els, Lawrence Erlbaum Associates, 1983. 

20. J.  Greeno, “Conceptual  Entities,” Mental
Models, D.  Gentner  and A.  Stevens, eds.,
Lawrence  Erlbaum  Associates, 1983, pp.
227–252.

21. B. Falkenhainer, K.D. Forbus, and D. Gen-
tner, “The Structure-Mapping Engine: Algo-
rithm and Examples,” Artificial Intelligence,
vol. 41, 1990, pp. 1–63.

22. G.  Klein, Sources  of  Power: How  People

Make Decisions, MIT Press, 1998.

For more information on this or any other com-
puting topic, please visit our Digital Library at
www.computer.org/publications/dlib.

92

www.computer.org/intelligent

IEEE INTELLIGENT SYSTEMS

",False,2006.0,{},False,False,journalArticle,False,MVFFYPN6,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/1705435/,,Making Sense of Sensemaking 2: A Macrocognitive Model,MVFFYPN6,False,False
QTX2P5GH,ISGCQQUV,"Readings in Information Visualization, Using vision to think
Stuart T. Kard, Jock D. Mackinlay, Ben Scheiderman

As this 15-year period draws to a close, there is a need for collecting together the results 
to date, organizing them, understanding the essence of this field, and providing materials 
for teaching. In the next period, information visualization will pass out of the realm of an 
exotic research specialty and into mainstream of user interface application design.” (xiii)

“The power of the unaided mind is highly overrated. Without external aids, memory, 
thought, and reasoning are all constrained. But human intelligence is highly flexible 
and adaptable, superb at inventing procedures and objects that overcome its own 
limits. The real powers come from devising external aids: it is things that make us 
smart” (Norman, 1993, p.43) (1)

But then direct computational devises themselves become a component of an even more 
powerful visually based system. (3)

As our brief examination illustrates, visual artifacts aid thought; in fact, they are 
completely entwined with cognition action.

Information visualization is just about that—exploiting the dynamic, interactive, 
inexpensive medium of graphical computers to devise new external aids that 
enhance cognitive abilities. (5)

Visualization: The use of computer-supported, interactive, visual representations of data to 
amplify cognition. (6)
 Cognition is the acquisition or use of knowledge. This definition has the virtue of focusing 
as much on the purpose of visualization as the means. Hamming (1973) saud, “the 
purpose of computation is insight, not numbers.” Likewise for visualization, “the purpose of 
visualization is insight not pictures.”  The main goals of this insight are 
 discovery, decision
 
  
 
making, and explanation. 
 

 (6) 
 

Visualization dates as an organized subfield from the NSF report Visualization in Scientific 
Computing (McCormick and DeFanti, 1987). There it is conceived as a tool to permit 
handling large sets of scientific data and to enhance science’s ability to see phenomena in 
the data. Although it is not a necessity of the original conception, scientific visualization 
tends to be based on physical data—the human body, the earth, molecules and other. (6)

   There is a great deal of such abstract information in the contemporary world, and its 
mass and complexity are a problem, motivating attempts to extend visualization into the 
realm of the abstract. (7)

 External cognition is concerned with the interaction of cognitive representations and 
processes across the external/internal boundary in order to support thinking. Information 
design is the explicit attempt to design external representations to amplify cognition. Data 
Graphics is the design of visual but abstract representations of data for this purpose. 
Visualization uses the computer for data graphics. Scientific Visualization is visualization 
applied to scientific data. The reasons why these two diverge are that scientific data are 
often physically based, whereas business information and other abstract data are often 
not.

However, humans with visualization displays are good at picking out new patterns as they 
occur and thus can respond to changes in the patterns quickly. Information visualization 
allows human adaptivity to be brought to bear for large sets of data under time pressure. 
(10)

Readings in Information Visualization, Using vision to think
Stuart T. Kard, Jock D. Mackinlay, Ben Scheiderman

Knowledge crystallization tasks are one form of information-intensive work can 
themselves be part of more complex forms of knowledge work, such as design.  (11)

1. Information                     Collecting articles and data
    foraging.                          On laptop computers.

2. Search for                       
    schema                             on which to compare.
    (representation)                laptops.

Identification of attributes 

3. Instantiate schema            Make table of laptops x
    with data. Residue
    is significant data
    that do not fit the 
    schema. To reduce
    Residue, go to 
    Step 2 and improve
    Schema.

attributes. Use “remarks”
column to record interesting
properties that don’t fit into

table.

4. Problem-solve to
    trade off features. 

5. Search for a new
    schema that reduces
    the problem to a 
   simple trade-off. 

6. Package the 
    patterns found in 
    Some output
    Product.    

Reorder rows and columns
Of laptop table. Create plots.
Delete or mark laptops that
Are out of the running.

Cluster into three groups by
rearranging the rows in the
table, one each for power,
Multimedia capability, and
portability. Within each cluster
delete all but the top one or
two machines.

Create concise briefing on
decision for workgroup.

Knowledge crystallization involves getting insight about data relative to some task. This 
usually requires finding some representation (schema) for the data that is efficient for the 
task.

There appears to be a general principle of Selective Omission of Information at work in all 
biological information processing systems. The sensory organs simplify and organize their 
inputs, supplying the higher processing centers with aggregated forms of information 
which, to a considerable extent, predetermine the patterned structures that the higher 
centers can detect. The higher centers in their turn reduce the quantity of information 
which will be processed at later stages by further organization of the partly processed 
information into more abstract and universal forms. (Resnikoff, 1987, p.19) (11)

In order to do knowledge crystallization, there must be data, a task, and a schema. If 
the data are not to hand, then information visualization can aid in the search for it. If there 
is a satisfactory schema, then knowledge crystallization reduces to information retrieval. If 
there is not an adequate schema, then knowledge visualization is one of the methods by 
which one can be obtained. (11)

We have associated subtasks with particular main tasks of knowledge 
crystallization; however, many of the subtasks could be associated with more than 
one task.

VISUALIZATION LEVELS OF USE

1) visualization of the inosphere: the information outside the user’s environment. 

(www., digital libraries, document collection).

2) visualization of an information workspace is the use of visualization to organize 

possibly multiple individual organizations or other information sources and tools to 
perform tasks.

3) visual knowledge tools, they arrange information to reveal patterns, or they allow 
manipulation of information for finding patterns, or they allow visual calculations. 
They are some times called wide widgets to emphasize that they are often not just 
presentations but also controls.

4) visual objects. These refer to objects, especially virtual physical objects such as the 

human body or books, that have been enhanced with visualization techniques to 
package collections of abstract information. (example both conceptual and spatial 
browsing data on a human body).

  We propose six major ways in which visualizations can amplify cognition. 

1) by increasing the memory and processing resources available to the users.
2) By reducing the search for information,
3) By using visual representations to enhance the detection of patterns,

     4)  By enabling perceptual inference operations,

5) By using perceptual attention mechanisms for monitoring,
6) By encoding information in a manipulable medium

(16)

Data Transformation map Raw Data, that is, data in some idiosyncratic format, into Data 
Tables, relational descriptions of data extended to include metadata. Visual Mappings 
transform Data Tables into Visual Structures, structures that combine spatial substrates, 
marks, and graphical properties. Finally, View Transformations create Views of the Visual 
Structures by specifying graphical parameters such as position, scaling,, and clipping. 
User interaction controls parameters of these transformations, restricting the view to 

certain data ranges, for example, or changing the nature of the transformation. The 
visualizations and their controls are used in service of some task. (17)

A tool for discovery and understanding

AUTOMATING THE DESIGN OF GRAPHICAL PRESENTATIONS OF RELATIONAL 
INFORMATION
Jack McKinlay Stanford University

3. The Graphical Presentation Problem
  The graphical presentation problem is to synthesis a graphical design that 
expresses a set of relations and their structural properties effectively.  (67)

4. Approach
  An expressiveness criterion, which is derived from precise language definition, is 
associated with each graphical language. A graphical language can be used to present 
some information when it includes a graphical sentence that expresses exactly the imput 
information, that is all the information and only the information. Expressing additional 
information is potentially dangerous because it may not be correct. (69)

5. Expressivness

All communication is based on the fact that the participants share conventions that 
determine how messages are constructed and interpreted. For graphical communication 
these conventions indicate how arrangements of graphical objects encode information. 
A set of facts is expressible in a language if it contains a sentence that
1.- encodes all the facts in the set,
2.- encodes only the facts in the set. (70)

6. Effectiveness

Given two graphical languages that express some information, the obvious question is 
which language involves a design that specifies the more effective presentation. 
…unlike expressiveness, which only depends on the syntax and semantics of the 
graphical language, effectiveness also depends on the capabilities of the perceiver.

7. Composition

Expressiveness and effectiveness criteria, which were described in the previous two 
sections, are not very useful without a method for generating alternative designs. (74)

Information Animation Applications in the Capital Markets.
William Wright, Visible Decisions Inc. Toronto Canada

In 4D information animation applications, the success of the graphics visual design (i.e. the 
shapes, layout, colors) is critical to the success of the application. Graphical elements 
need to be carefully selected and arranged to reveal data and relationships.  Poor 
graphics design will obscure the data and its meanings. The visual design simply needs to 
be perfect. Users must see the message and not the medium. 

   Edward Tufte articulates this discipline best. According to Tufte, excellence in graphics 
consists of complex ideas communicated with clarity, precision, and efficiency. Graphical 
displays should induce the viewer to think about the substance, present many numbers in 
a small space, make large data sets coherent, encourage the eye to compare different 
pieces of data, reveal the data at several levels of detail, from a broad overview to the fine 
structure.

Information workspaces are not oriented around visualizations themselves, but 
around tasks.   An information workspace might contain several visualizations related to 
one or several tasks. 
  At the third level are visual knowledge tools or “wide widgets”. These are sort of 
visualization tools described in many of the papers so far in this books. They contain a 
visual presentation of some data set and a set of controls for interacting with that 
presentation. The focus is on determining and extracting the relationships in a 
particular set of data. 
   At a fourth level are visually enhanced objects, coherent information objects enhanced 
by the addition of information visualization techniques. (463)

USING VISION TO THINK

This chapter returns to the central topic of this book: using vision to think. In particular, the 
focus is on developing theoretical and engineering principles for the design of effective 
visualizations.

Moving from information foraging to sense making, we have Pirolli and Rao’s paper 
(1996). Sense making basically requires building schema or description into which many 
pieces of information fit (Russel et al., 1993), that is, providing a compact description of 
some set of phenomena. (580)

. The point of view is primarily cognitive, including the use of the term externalization rather 
than visualization to indicate the cognitive role of interactive visual representations. (581)

The first dimension, representation, divides data into value and structure. The second 
dimension, interactivity, ranges from direct manipulation to indirect manipulation. This 
leads to the final axis of the taxonomy based on Draper’s observation that input and output 
can reference each other. These input and output relations form the basis of a more 
detailed discussion of the various user actions that must be supported by a visualization. 
(581)

CONCLUSIONS

  Information visualization is the use of computer-supported interactive visual 
representations of abstract data to amplify cognition. Its purpose is not the pictures 
themselves, but insight (or rapid information assimilation or monitoring large amounts of 
data). Information visualization is a part of the new media made possible by the 
development of the real-time visual computer. This medium has promise for five reasons:

It brings increased resources to the human in the form of perceptual 
processing and expanded working memory.
It can reduce the search for information.

1.

2.

3.
4.

It can enhance the recognition patterns.
It enables the use of perceptual inference and perceptual monitoring.

The medium itself is manipulable and interactive.

Another potential use of information visualization is in complex documents, such as 
scientific papers, technical manuals, film scripts, or computer programs. In each of these, 
readers often try to get sense of the whole or to cross-reference one part from the 
other.

Information Visualization

Robert Spence
ACM Press Essex: England 2001

Visualization is a process of forming mental model of data, thereby gaining inside into that 
data....I concentrate on the acquisition of insight through the identification of patterns and 
other features of a display. (xiii)

Visualize: (vb) to forma a mental image or vision of....
Visualize: (vb) to imagine or remember as if actually seeing.
Indeed , it results in something rather ephemeral (which we later call a mental model or 
internal model), something that cannot be printed out on paper or viewed through a 
microscope. The result is, as we say, internal to the human being. The potential value of 
visualization—that of gaining insight and understanding—follows from these definitions but 
so also, in view of the cognitive nature of visualization does the difficulty of its study. (1)

Sometimes we refer to the internal model as a cognitve map to distinguish it from a 
material map, which is real in the sense of being an object pasted to the wall of the 
underground station.

Issues
Selection; representation; presentation; scale and dimensionality; rearrangement, 
interaction and exploration; externalization; mental models; invention, experience and skill. 
(9-12)

Anyone who has seen, and specially used, a highly responsive interactive visualization 
tool will be struck by two features. First, that a mere rearrengement of how the data is 
displayed can lead to a surprising degree of additional insight into that data. Second, that 
the very property of interactivity can considerably enhance that tool’s effectiveness, 
especially if the computer’s response follows a user’s action virtually immediately, say 
within a fraction of a second. (14)

ENVISIONING INFORMATION

Edward R. Tufte

Graphic Press. Cheshire, Connecticut  1990

Introduction

To envision information—and what bright and splendid visions can result—is to work at the 
intersection of image, word, number, art. (9)

To speak of statistics as the study of variation also serves to emphasize the contrast 
between the aim of modern statisticians and those of their predecessors.  (22)

We envision information in order to reason about, communicate, document, and preserve 
that knowledge—activities nearly always carried out on two-dimensional paper and 
computer screen. (33)

2. Micro / Macro Readings

We thrive in information-thick worlds because of our marvelous and everyday capacities to 
select, edit, single out, structure, highlight, group, pair, merge, harmonize, synthesize, 
focus, organize, condense, reduce, boil down, choose, categorize, catalog, classify, list, 
abstract, scan, look into, idealize, isolate, discriminate, distinguish, screen, pigeonhole, 
pick over, sort, integrate, blend, inspect, filter, lump, skip, smooth, chunk, average, 
approximate, cluster, aggregate, outline, summarize, itemize, review, dip into, flip through, 
browse, glance into, leaf through, skim, refine, enumerate, gleam, synopsize, and separate 
the sheep from the goats. (50)

Micro/Macro designs enforce both local and global comparisons and, at the same time, 
avoid the disruption of context switching. All told, exactly what is needed for reasoning 
about information. 
High-density design also allow viewers to select, to narrate, to recast and personalize data 
for their own uses. Thus control of information is given over to viewers, not to editors 
designers, or decorators. (50)

Clutter and confusion are failures of design, not attributes of information.

The concept that “the simpler the form of a letter the simpler its reading” was an obsession 
of beginning constructivism. It became something like a dogma, and is still followed by 
“modernistic” typographers.
The notion proved to be wrong, because in reading we do not read letters but words, 
words as a whole, as a “word picture”. Ophthalmology has disclosed that the more the 
letters are differentiated from each other, the easier is the reading. (51)

3. Layering and Separation 
   
Confusion and clutter failures of design, not attributes of information. And so to point is to 
find design strategies that reveal and detail and complexity—rather than to fault the data 
for an excess of complication. Or, worse, to fault viewers for a lack of understanding. 
Among the most powerful devices for reducing noise and enriching the content of displays 
is the technique of layering and separation, visually stratifying various aspects of the data. 
Effective layering of information is often difficult; for every excellent performance, a 
hundred clunky spaces arise. An omnipresent, yet subtle, design issue is involved: the 
various elements collected together on flatland interact , creating non-information patterns 
and texture simply through their combined presence. Joseph Albers described this visual 
effect as 1 + 1 = 3. (53)

4. Small Multiples

At the heart of quantitative reasoning is a single question: Compared to what?

5. Color and Information

At work in this fine Swiss mountain map are the fundamental uses of color in information 
design: to label (color as noun), to measure (color as quantity), to represent or imitate 
reality(color as representation), and to enliven or decorate

VISUAL EXPLANATIONS
Eduard R. Tufte

Many of our examples suggest that clarity and excellence in thinking is very much like 
clarity and excellence in the display of data. When principles of design replicate principles 
of thought, the act of arranging information becomes an act of insight. (9)

..The idea is to make designs that enhance the richness, complexity, resolution, 
dimensionality, and clarity of the content. By extending the visual capacities of paper, 
video, and computer screen, we are able to extend the depth of our own knowledge and 
experience. (9)

Modern scientific graphics were now in place; the two-dimensional plane was quantified, 
available for measured data. Used with fitted models, graphics could describe and 
characterize relations between variables—thus displaying the essential evidence 
necessary for establishing cause and effect. (16)
…More generally, when scientific images become dequantified, the language of analysis 
may drift toward credulous descriptions of form, pattern, and configuration—rather than 
answers to the questions How many? How often? Where? How much? At what rate?. (23)
 
  Once again Jonson’s Principle: these problems are more than just poor design, for a lack 
of visual clarity in arranging evidence is a sign of lack of intellectual clarity in reasoning 
about evidence. (48)

…Reliable knowledge grows from evidence that is collected, analyzed, and displayed with 
some good comparisons in view. (52)

…Failure to think clearly about the analysis and presentation of evidence opens the door 
for all sorts of political and other mischief to operate in making decisions. (52)

: if displays of data are to be truthful and revealing, then the design logic of the display 
must reflect the intellectual logic of the analysis:
 Visual representations of evidence should be governed by principles of reasoning about 
quantitative evidence. For information displays, design reasoning must correspond to 
scientific reasoning. Clear and precise seeing becomes as one with clear and precise 
thinking. (53)

..Display architecture recapitulates quantitive thinking; design quality grows from 
intellectual quality. Such dual principles—both for reasoning about statistical evidence and 

for the design of statistical graphics—include (1) documenting the sources and 
characteristics of the data, (2) insistently enforcing appropriate comparisons. (3) 
demonstrating mechanisms of cause and effect, (4) expressing those mechanisms 
quantitatively, (5) recognizing the inherently multivariate nature of analytic problems, and 
(6) inspecting and evaluating alternative expressions.  

Parallelism connects visual elements. Connections are built among images by position, 
orientation, overlap, synchronization, and similarities in context. Parallelism grows from a 
common viewpoint that relates like to like. Congruity of structure across multiple images 
gives the eye a context for assessing data variation. Parallelism is not simply a matter of 
design arrangements, for the perceiving mind itself actively works to detect and indeed to 
generate links, clusters, and matches among assorted visual elements. (82)

Multiple images reveal repetition and change, pattern and surprise—the defining elements 
in the idea of information. (105)

Multiples amplify, intensify, and reinforce the meaning of images. (b105)

Since many slices of information are displayed within the eyespan, alert viewers may be 
able to detect  contrasts and correspondences at a glance—uninterrupted visual 
reasoning.  (112)

Information Visualization: Perception for Design

Colin Ware

Academic Press, San Diego, CA. 2000

  Visualization meant constructing a visual image in the mind. But now it has come to 
mean something like a graphical representation of data or concepts. From an internal 
construct of the mind to an external artifact supporting decision making. (1)

Critical Question
   How best to transform the data into something that people can understand for optimal 
decision making. (4)

The brain is clearly not an undifferentiated mass; it is more a collection of highly 
specialized parallel-processing machines with high-bandwidth interconnections. The entire 
system is designed to extract information from the world in which we live, not from some 
other environment with entirely different physical properties.

Sensory aspects of visualizations derive their expressive power from being well designed 
to stimulate the visual sensory system. In contrast, arbitrary, conventional aspects of 
visualization derive their power from how they will be learned.

The distinction between the sensory and social aspects of the symbols used in 
visualization also has practical consequences in terms of research methodology. It is not 
worth expending a huge effort carrying out intricate and highly focused experiments to 
study something that is only this year’s fashion. Howeever, if we can develop 

generalizations that apply to large clasases of visual representations, and for a long time, 
the effort is worthwhile. (13)

There is an intricate interweaving of learned conventions and hard-wire processing. The 
distinction is not as clean as we would like, but there are ways of distinguishing the 
different kinds of codes. (14)

Our visual systems are built to perceive the shapes of 3D surfaces.
A sensory code is one for which the meaning is perceived without additional 

training. 

Sensory immediacy: The processing of certain kinds of sensory information is hard-

wired into the brain... .. the way in which visual systems divides the visual world into 
regions is called segmentation. The evidence suggests that this is a function of early rapid-
processing systems. (15)

Cross-cultural validity: a sensory code will in general, be understood across cultural 
boundries. (16)

Sensory Research Metholologies
     Psychophysics: techniques that are based on applying the methods of physics to 
measurements of human sensation... extremely succesful in defining the basic set of limits 
of the visual system. (what is the smallest relative brightness change that can be 
detected?).
    If a psychophysical measurement is highly sensitive to changes in instructions, it is likely 
to be measuring something that has higher level cognitive or cultural involment.
  
modules. (short and long term memory). MRI techniques allow researchers to actually see 
which parts of the brain are active when subjects perform certain tasks. (17)

Cognitive Psychology: The brain is treated as a set of interlinked processing 

Arbitrary Symbol Research

Anthropoligists, social sciences: They advocate “thick description”. This approach 

is based on careful observation, immersion in culture, and an effort to keep ‘the analysis of 
social forms closely tied to concrete social events and ocassions” (Cliford Geertz 1973).

Complex user interfaces that they call artifact analysis (Carroll, 1989). In this 

approach, user interfaces (and presumably visualization techniques) are best viewed as 
artifacts and studied much as an anthropologist studies cultural artifacts of a religious or 
practical nature. Formal experiments are out of the question in such circumstances, and if 
they were actually carried out, they would change the cultural symbols being studied. 

Unfortunately for researchers, sensory and social aspects of symbols are closely 

intertwined in many representations. Pure instances of sensory or arbitrary coding may not 
exist but doing analysis is not invalid. We must carefully determine which aspects of visual 
coding belong in each category. (21)
  
  For the visualization designer, training in art and design is at least useful as training in 
perceptual psychology. For those who wish to do good design, the study of design by 
example is generally most appropiate. But the science of visualization can inform the 
process by providing a scientific basis for design rules, and it can suggest new design 
ideas and methods for sipalying data that heve not been thought of before. Ultimately, our 
goal should be to create a new set of conventions for information visualization designed to 
be optimal based on sound perceptual principles. (21, 22).

Gibson’s Affordance Theory

He assumed that we perceive in order to operate on the environment. 

Perception is designed for action. The perceivable possibilities for action he called 
Affordances.  He claimed that we perceive these properties of the environment in a direct 
and immediate way, this theory is clearly attractive from the perspective of visualization, 
because the goal of most visualizations is desicion making. Thinking about perception in 
terms of action is likely to be much more useful than thinking about how two adjacent 
spots of light influence each others appearence (typical approach of classical 
psicophysicist). (22)
Instead of reasoning like theorists to first understanding how a single point of light is 
perceived and then gradually understanding how two points of light interact and gradually 
build to understand the vibrant, dynamic visual world we live in. Gibson took a radically 
different approach. He claimed that we do not perceive points of light, we perceive 
possibilities for action (affordances) of the environment directly, not indirectly by piecing 
together evidence from our senses. To create a good interface, we must create it with 
appropiate affordances to make the user’s task easy. ** He rejects the view of the brain 
deducing things out about the environment based on available sensory evidence in favor 
of the idea that our visual system is tuned to perceiving the visual world and that we 
perceive it accurately except under extraordinary circumstances. He preferred to 
concentrate on the visual system as a whole and not to break perceptual processing down 
into components and operations. He used the term resonating to describe the way visual 
system responds to properties of the environment. (23)
*** ejemplo categorias en mi metodo visual 

Visualization and Direct Perception: 3 problems

1) Even if perception of the environment is direct, it is clear that visualization of 

data through computer graphics is very indirect. There may be many layers of 
processing between the data and its representation (abstract data, microscopic 
etc. )

2) There are no clear physical affordances in any graphical user interface. 

Beyond the visual stages, the visual object identification process interfaces with the verbal 
linguistic subsystems of the brain so that words can be connected to images. The 
perception-for-action subsytem interfaces with the motor systems that control muscle 
movements. (25, 26)

Bertin 1977 Data values and data structures. A more modern way of expressing this idea 
is to divide data into entities and relationships. 
Entities are the objects we wish to visualize; relations define the structures and 
patterns that relate entities to one another. We can also talk about the attributes of 
an entity or a relationship. Concepts of entity, relationship and attribute have a long 
history in database design and more recently in systems modeling. 

Entities & Relationships
   Are generally the objects of interest. Relationships can be structural and physical 
(a house) or conceptual (store and customers). They may be causal or temporal. 
(28).

Attributes of Entities or Relationships (29)

   Attribute is property of some entity and cannot be thought of independently.

Attribute Quality
   It is useful to describe data visualization methods in light of the quality of 
attributes they are capable of conveying. A useful way of considering the quality of data 
is the taxonomy of number scales defined by Stevens 1946.

Nominal.- Labeling function (ruta 100)
Ordinal.- Ordering things in a sequence (best, second best)
Interval.- The gap between data values. (schedules)
Ratio.- We have the full expressive power of a real number ( A as twice as B)

Definiciones

Information visualization is just about that—exploiting the dynamic, interactive, inexpensive 
medium of graphical computers to devise new external aids that enhance cognitive 
abilities

Visualization: The use of computer-supported, interactive, visual representations of data to 
amplify cognition. (6)
 Cognition is the acquisition or use of knowledge. This definition has the virtue of focusing 
as much on the purpose of visualization as the means. Hamming (1973) said, “the purpose 
of computation is insight, not numbers.” Likewise for visualization, “the purpose of 
visualization is insight not pictures.”  The main goals of this insight are discovery, decision 
making, and explanation. (6)

External cognition is concerned with the interaction of cognitive representations and 
processes across the external/internal boundary in order to support thinking. Information 
design is the explicit attempt to design external representations to amplify cognition. Data 
Graphics is the design of visual but abstract representations of data for this purpose. 
Visualization uses the computer for data graphics. Scientific Visualization is visualization 
applied to scientific data. The reasons why these two diverge are that scientific data are 
often physically based, whereas business information and other abstract data are often 
not.

Data Transformation map Raw Data, that is, data in some idiosyncratic format, into Data 
Tables, relational descriptions of data extended to include metadata. Visual Mappings 
transform Data Tables into Visual Structures, structures that combine spatial substrates, 
marks, and graphical properties. Finally, View Transformations create Views of the Visual 
Structures by specifying graphical parameters such as position, scaling,, and clipping. 
User interaction controls parameters of these transformations, restricting the view to 
certain data ranges, for example, or changing the nature of the transformation. The 
visualizations and their controls are used in service of some task. (17)

Information visualization is the use of computer-supported interactive visual 
representations of abstract data to amplify cognition. Its purpose is not the pictures 
themselves, but insight (or rapid information assimilation or monitoring large amounts of 
data). Information visualization is a part of the new media made possible by the 
development of the real-time visual computer. This medium has promise for five reasons:

It brings increased resources to the human in the form of perceptual 
processing and expanded working memory.
It can reduce the search for information.
It can enhance the recognition patterns.
It enables the use of perceptual inference and perceptual monitoring.

The medium itself is manipulable and interactive.

5.

6.
7.
8.

Visualize: (vb) to forma a mental image or vision of....
Visualize: (vb) to imagine or remember as if actually seeing.
Indeed , it results in something rather ephemeral (which we later call a mental model or 
internal model), something that cannot be printed out on paper or viewed through a 
microscope. The result is, as we say, internal to the human being. The potential value of 
visualization—that of gaining insight and understanding—follows from these definitions but 
so also, in view of the cognitive nature of visualization does the difficulty of its study. (1)

Visualization meant constructing a visual image in the mind. But now it has come to mean 
something like a graphical representation of data or concepts. From an internal construct 
of the mind to an external artifact supporting decision making. (1)

Critical Question
   How best to transform the data into something that people can understand for optimal 
decision making. (4)

",False,1999.0,{},False,False,conferencePaper,False,QTX2P5GH,[],self.user,False,False,False,False,,,Readings in information visualization - using vision to think,QTX2P5GH,False,False
Y9LGBJIG,TP9XGZAL,"CS-TR-3665
ISR-TR-96-66

July 1996

The Eyes Have It:

A Task by Data Type Taxonomy
for Information Visualizations

Ben  Shneiderman

Department  of  Computer  Science

Human-Computer Interaction Laboratory,

and Institute for Systems Research

University of Maryland, College Park, Maryland 20742 USA
ben@cs.umd.edu, http://www/cs.umd.edu/projects/hcil/

Abstract
 A useful starting point for designing advanced graphical user interfaces is the Visual Information-
Seeking Mantra: Overview first, zoom and filter, then details-on-demand. But this is only a starting point
in trying to understand the rich and varied set of information visualizations that have been proposed in
recent years. This paper offers a task by data type taxonomy with seven data types (1-, 2-, 3-dimensional
data, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, zoom,
filter, details-on-demand, relate, history, and extract).

A Task by Data Type Taxonomy for Information Visualizations

The Eyes Have It:

Ben Shneiderman

Department of Computer Science,

Human-Computer Interaction Laboratory, and Institute for Systems Research

University of Maryland

College Park, Maryland 20742 USA

ben@cs.umd.edu

Abstract:  A  useful  starting  point  for  designing
advanced  graphical  user  interfaces  is  the  Visual
Information-Seeking  Mantra:  Overview  first,  zoom
and  filter,  then  details-on-demand.  But  this  is  only  a
starting  point  in  trying  to  understand  the  rich  and
varied set of information visualizations that have been
proposed  in  recent  years.  This  paper  offers  a  task  by
data  type  taxonomy  with  seven  data  types  (1-,  2-,  3-
dimensional  data,  temporal  and  multi-dimensional
data,  and  tree  and  network  data)  and  seven  tasks
(overview,  zoom,  filter,  details-on-demand,  relate,
history,  and  extract).

   Everything points to the conclusion that the
phrase 'the language of art' is more than a
loose metaphor, that even to describe the
visible world in images we need a developed
system of schemata.

     E.  H.  Gombrich  Art  and  Illusion,  1959  (p.  76)

1. Introduction
    Information  exploration  should  be  a  joyous
experience,  but  many  commentators 
talk  of
information  overload  and  anxiety  (Wurman,  1989).
However,  there  is  promising  evidence  that  the  next
generation  of  digital  libraries  for  structured  databases,
textual  documents,  and  multimedia  will  enable
convenient  exploration  of  growing  information  spaces
by a wider range of users. Visual language researchers
and  user-interface  designers  are  inventing  powerful
information  visualization  methods,  while  offering
smoother integration of technology with task.
    The  terminology  swirl  in  this  domain  is  especially
colorful.  The  older  terms  of  information  retrieval
(often  applied  to  bibliographic  and  textual  document
systems)  and  database  management  (often  applied  to
more  structured  relational  database  systems  with
orderly  attributes  and  sort  keys),  are  being  pushed
aside  by  newer  notions  of  information  gathering,
seeking,  or  visualization  and 
  data  mining,
warehousing,  or  filtering.  While  distinctions  are
subtle, the common goals reach from finding a narrow
set  of  items  in  a  large  collection  that  satisfy  a  well-

  Exploring 

understood  information  need  (known-item  search)  to
developing  an  understanding  of  unexpected  patterns
within  the  collection  (browse)  (Marchionini,  1995).
 
information  collections  becomes
increasingly  difficult  as  the  volume  grows.  A  page  of
information  is  easy  to  explore,  but  when  the
information  becomes  the  size  of  a  book,  or  library,  or
even  larger,  it  may  be  difficult  to  locate  known  items
or to browse to gain an overview.
    Designers  are  just  discovering  how  to  use  the  rapid
and  high  resolution  color  displays  to  present  large
amounts  of  information  in  orderly  and  user-controlled
ways.  Perceptual  psychologists,  statisticians,  and
graphic  designers  (Bertin,  1983;  Cleveland,  1993;
Tufte,  1983,  1990)  offer  valuable  guidance  about
presenting  static  information,  but  the  opportunity  for
dynamic  displays  takes  user  interface  designers  well
beyond current wisdom.

2. Visual Information Seeking Mantra
   The  success  of  direct-manipulation  interfaces  is
indicative  of  the  power  of  using  computers  in  a  more
visual  or  graphic  manner.  A  picture  is  often  cited  to
be worth a thousand words and, for some (but not all)
tasks,  it  is  clear  that  a  visual  presentation—such  as  a
map or photograph—is dramatically easier to use than
is  a  textual  description  or  a  spoken  report.  As
computer  speed  and  display  resolution  increase,
information  visualization  and  graphical  interfaces  are
likely  to  have  an  expanding  role.  If  a  map  of  the
United  States  is  displayed,  then  it  should  be  possible
to  point  rapidly  at  one  of  1000  cities  to  get  tourist
information. Of course, a foreigner who knows a city’s
name  (for  example,  New  Orleans),  but  not  its
location,  may  do  better  with  a  scrolling  alphabetical
list.  Visual  displays  become  even  more  attractive  to
provide  orientation  or  context,  to  enable  selection  of
regions,  and  to  provide  dynamic  feedback  for
identifying  changes  (for  example,  a  weather  map).
Scientific  visualization  has  the  power  to  make
atomic,  cosmic,  and  common  three-dimensional
phenomena  (such  as  heat  conduction  in  engines,
airflow  over  wings,  or  ozone  holes)  visible  and
comprehensible.  Abstract  information  visualization
has  the  power  to  reveal  patterns,  clusters,  gaps,  or

outliers  in  statistical  data,  stock-market  trades,
computer  directories,  or  document  collections.
  Overall, the bandwidth of information presentation is
potentially  higher  in  the  visual  domain  than  for  media
reaching  any  of  the  other  senses.  Humans  have
remarkable  perceptual  abilities  that  are  greatly  under-
utilized  in  current  designs.  Users  can  scan,  recognize,
and  recall  images  rapidly,  and  can  detect  changes  in
size,  color,  shape,  movement,  or  texture.  They  can
point  to  a  single  pixel,  even  in  a  megapixel  display,
and  can  drag  one  object  to  another  to  perform  an
action.  User  interfaces  have  been  largely  text-
oriented,  so  as  visual  approaches  are  explored,
appealing new opportunities are emerging.
    There  are  many  visual  design  guidelines  but  the
basic  principle  might  be  summarized  as  the  Visual
Information Seeking Mantra:
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand
   Overview first, zoom and filter, then details-on-demand

Each  line  represents  one  project  in  which  I  found
myself rediscovering this principle and therefore wrote
it down it as a reminder. It proved to be only a starting
point 
the  multiple
information-visualization  innovations  occurring  at
university, government, and industry research labs.

to  characterize 

trying 

in 

3. Task by Data Type Taxonomy
    To  sort  out  the  prototypes  and  guide  researchers  to
new  opportunities,  I  propose  a  type  by  task  taxonomy
(TTT)  of  information  visualizations.  I  assume  that
users  are  viewing  collections  of  items,  where  items
have multiple attributes. In all seven data types (1-, 2-
,  3-dimensional  data,  temporal  and  multi-dimensional
data,  and  tree  and  network  data)  the  items  have
attributes  and  a  basic  search  task  is  to  select  all  items
that  satisfy  values  of  a  set  of  attributes.  An  example
task  would  be  finding  all  divisions  in  an  organization
structure that have a budget greater than $500,000.
    The  data  types  are  on  the  left  side  of  the  TTT
characterize  the  task-domain  information  objects  and
are  organized  by  the  problems  users  are  trying  to
solve.  For  example,  in  two-dimensional  information
such  as  maps,  users  are  trying  to  grasp  adjacency  or
navigate  paths,  whereas  in  tree-structured  information
users  are  trying  to  understand  parent/child/sibling
relationships.  The  tasks  across  the  top  of  the  TTT  are
task-domain  information  actions  that  users  wish  to
perform.
    The  seven  tasks  are  at  a  high  level  of  abstraction.
More  tasks  and  refinements  of  these  tasks  would  be

natural  next  steps  in  expanding  this  table.  The  seven
tasks  are:

O v e r v i e w :  Gain  an  overview  of  the  entire

collection.

Zoom : Zoom in on items of interest
Filter: filter out uninteresting items.
Details-on-demand:  Select  an  item  or  group  and

get  details  when  needed.

Relate: View relationships among items.
History: Keep a history of actions to support undo,

replay, and progressive refinement.

Extract:  Allow  extraction  of  sub-collections  and  of

the query parameters.

Further discussion of the tasks follows the descriptions
of the seven data types:

1-dimensional:  linear  data  types  include  textual
documents,  program  source  code,  and  alphabetical
lists  of  names  which  are  all  organized  in  a  sequential
manner.  Each  item  in  the  collection  is  a  line  of  text
containing  a  string  of  characters.  Additional  line
attributes  might  be  the  date  of  last  update  or  author
name.  Interface  design  issues  include  what  fonts,
color,  size  to  use  and  what  overview,  scrolling,  or
selection  methods  can  be  used.  User  problems  might
be  to  find  the  number  of  items,  see  items  having
certain  attributes  (show  only  lines  of  a  document  that
are  section  titles,  lines  of  a  program  that  were
changed  from  the  previous  version,  or  people  in  a  list
who  are  older  than  21  years),  or    see  an  item  with  all
its attributes.
    Examples:  An  early  approach  to  dealing  with  large
1-dimensional  data  sets  was  the  bifocal  display  which
provided  detailed  information  in  the  focus  area  and
less  information  in  the  surrounding  context  area
(Spence  and  Apperley,  1982).  In  their  example,  the
selected  issue  of  a  scientific  journal  had  details  about
each  article,  the  older  and  newer  issues  of  the  journal
were  to  the  left  and  right  on  the  bookshelf  with
decreasing  space.  Another  effort  to  visualize  1-
dimensional  data  showed  the  attribute  values  of  each
thousands  of  item  in  a  fixed-sized  space  using  a
scrollbar-like  display  called  value  bars  (Chimera,
1992).  Even  greater  compressions  were  accomplished
in  compact  displays  of  tens  of  thousands  of  lines  of
program  source  code  (SeeSoft,  Eick  et  al.,  1992)  or
textual  documents  (Document  Lens,  Robertson  and
Mackinlay,  1993;  Information  mural,  Jerding  and
Stasko,  1995).

2 - d i m e n s i o n a l :  planar  or  map  data  include
geographic  maps,  floorplans,  or  newspaper  layouts.
Each  item  in  the  collection  covers  some  part  of  the
total  area  and  may  be  rectangular  or  not.  Each  item
has  task-domain  attributes  such  as  name,  owner,
value,  etc.  and  interface-domain  features  such  as  size,
color,  opacity,  etc.  While  many  systems  adopt  a

Proc. Visual Languages 96                                  -  2  -                                                September 1996

multiple  layer  approach  to  dealing  with  map  data,
each layer is 2-dimensional. User problems are to find
adjacent  items,  containment  of  one  item  by  another,
paths  between  items,  and  the  basic  tasks  of  counting,
filtering, and details-on-demand.
    Examples:  Geographic  Information  Systems  are  a
large  research  and  commercial  domain  (Laurini  and
Thompson, 1992; Egenhofer and Richards, 1993) with
numerous  systems  available.  Information  visualization
researchers  have  used  spatial  displays  of  document
collections  (Korfhage,  1991;  Hemmje  et  al.,  1993;
Wise  et  al.,  1995)  organized  proximally  by  term  co-
occurrences.

3-dimensional:  real-world  objects  such  as  molecules,
the  human  body,  and  buildings  have  items  with
volume  and  some  potentially  complex  relationship
with  other  items.  Computer-assisted  design  systems
for  architects,  solid  modelers,  and  mechanical
engineers  are  built  to  handle  complex  3-dimensional
relationships.  Users's  tasks  deal  with  adjacency  plus
above/below  and  inside/outside  relationships,  as  well
as  the  basic  tasks.  In  3-dimensional  applications  users
must  cope  with  understanding  their  position  and
orientation  when  viewing  the  objects,  plus  the  serious
problems  of  occlusion.  Solutions  to  some  of  these
problems  are  proposed  in  many  prototypes  with
techniques 
landmarks,
perspective,  stereo  display,  transparency,  and  color
coding.
  Examples: Three-dimensional computer graphics and
computer-assisted  design  are  large  topics,  but
information  visualization  efforts  in  three  dimensions
are  still  novel.  Navigating  high  resolution  images  of
the  human  body  is  the  challenge  in  the  National
Library of Medicine's Visible Human project (North et
al.,  1996).  Some  applications  have  attempted  to
present  3-dimensional  versions  of  trees  (Robertson  et
al.,  1993),  networks  (Fairchild  et  al.,  1988),  or
elaborate desktops (Card et al., 1996).

such  as  overviews, 

Temporal:  time  lines  are  widely  used  and  vital
enough  for  medical  records,  project  management,  or
historical  presentations  to  create  a  data  type  that  is
separate  from  1-dimensional  data.  The  distinction  in
temporal  data  is  that  items  have  a  start  and  finish
time  and  that  items  may  overlap.  Frequent  tasks
include finding all events before, after, or during some
time period or moment, plus the basic tasks.
    Examples:  Many  project  management  tools  exist,
but  novel  visualizations  of 
the
perspective  wall  (Robertson  et  al.,  1993)  and
LifeLines  (Plaisant  et  al.,  1996).  LifeLines  shows  a
youth  history  keyed  to  the  needs  of  the  Maryland
Department  of  Juvenile  Justice,  but  is  intended  to
present  medical  patient  histories  as  a  compact
overview  with  selectable  items  to  get  details-on-
demand.  Temporal  data  visualizations  appear  in

include 

time 

systems  for  editing  video  data  or  composing
animations  such  as  Macromedia  Director.

Multi-dimensional:  most  relational  and  statistical
databases  are  conveniently  manipulated  as  multi-
dimensional  data  in  which  items  with  n  attributes
become  points  in  a  n-dimensional  space.  The
interface  representation  can  be  2-dimensional
scattergrams  with  each  additional  dimension
controlled  by  a  slider  (Ahlberg  and  Shneiderman,
1994).  Buttons  can  used  for  attribute  values  when  the
cardinality  is  small,  say  less  than  ten.  Tasks  include
finding  patterns,  clusters,  correlations  among  pairs  of
variables,  gaps,  and  outliers.  Multi-dimensional  data
can  be  represented  by  a  3-dimensional  scattergram
but  disorientation  (especially  if  the  users  point  of
view  is  inside  the  cluster  of  points)  and  occlusion
(especially  if  close  points  are  represented  as  being
larger)  can  be  problems.  The  technique  of  parallel
coordinates  is  a  clever  innovation  which  makes  some
tasks  easier,  but 
to
comprehend (Inselberg, 1985).
    Examples:  The  early  HomeFinder  developed
dynamic  queries  and  sliders  for  user-controlled
visualization  of    multi-dimensional  data  (Williamson
and  Shneiderman,  1992).  The  successor  FilmFinder
refined  the  techniques  (Ahlberg  and  Shneiderman,
1994)  for  starfield  displays  (zoomable,  color  coded,
user-controlled  scattergrams),  and  laid  the  basis  for
the  commercial  product  Spotfire  (Ahlberg  and
Wistrand, 1995). Extrapolations include the Aggregate
Manipulator  (Goldstein  and  Roth,  1994),  movable
filters  (Fishkin  and  Stone,  1995),  and  Selective
Dynamic  Manipulation  (Chuah  et  al.,  1995).  Related
works  include  VisDB  for  multidimensional  database
visualization  (Keim  and  Kreigal,  1994), 
the
spreadsheet-like  Table  Lens  (Rao  and  Card,  1994)
and  the  multiple  linked  histograms  in  the  Influence
Explorer (Tweedie et al., 1996).

takes  practice  for  users 

Tree:  hierarchies  or  tree  structures  are  collections  of
items  with  each  item  having  a  link  to  one  parent  item
(except  the  root).  Items  and  the  links  between  parent
and  child  can  have  multiple  attributes.  The  basic
tasks  can  be  applied  to  items  and  links,  and  tasks
related  to  structural  properties  become  interesting,  for
example,  how  many  levels  in  the  tree?  or  how  many
children  does  an  item  have?  While  it  is  possible  to
have  similar  items  at  leaves  and  internal  nodes,  it  is
also  common  to  find  different  items  at  each  level  in  a
tree.  Fixed  level  trees  with  all  leaves  equidistant  from
the  root  and  fixed  fanout  trees  with  the  same  number
of  children  for  every  parent  are  easier  to  deal  with.
High  fanout  (broad)  and  small  fanout  (deep)  trees  are
important  special  cases.  Interface  representations  of
trees  can  use  an  outline  style  of  indented  labels  used
in  tables  of  contents  (Chimera  and  Shneiderman,
1993),  a  node  and  link  diagram,  or  a  treemap,  in

Proc. Visual Languages 96                                  -  3  -                                                September 1996

which  child  items  are  rectangles  nested  inside  parent
rectangles.
    Examples:  Tree-structured  data  has  long  been
displayed with indented outlines (Egan et al., 1989) or
with  connecting  lines  as  in  many  computer-directory
file  managers.  Attempts  to  show  large  tree  structures
as  node  and  link  diagrams  in  compact  forms  include
the  3-dimensional  cone  and  cam  trees  (Robertson  et
al.,  1993;  Carriere  and  Kazman,  1995),  dynamic
pruning  in  the  TreeBrowser  (Kumar  et  al.,  1995),  and
the  appealingly  animated  hyperbolic  trees  (Lamping
et  al.,  1995).  A  novel  space-filling  mosaic  approach
shows  an  arbitrary  sized  tree  in  a  fixed  rectangular
space 
and
Shneiderman,  1991).  The  treemap  approach  was
successfully  applied  to  computer  directories,  sales
data,  business  decision-making  (Asahi  et  al.,  1995),
and  web  browsing  (Mitchell  et  al.,  1995;  Mukherjea
et  al.,  1995),  but  users  take  10-20  minutes  to
accommodate  to  complex  treemaps.

(Shneiderman,  1992; 

Johnson 

Network:  sometimes  relationships  among  items
cannot  be  conveniently  captured  with  a  tree  structure
and  it  is  useful  to  have  items  linked  to  an  arbitrary
number  of  other  items.  While  many  special  cases  of
networks  exist  (acyclic,  lattices,  rooted  vs.  un-rooted,
directed  vs.  undirected)  it  seems  convenient  to
consider  them  all  as  one  data  type.  In  addition  to  the
basic  tasks  applied  to  items  and  links,  network  users
often want to know about shortest or least costly paths
connecting two items or traversing the entire network.
Interface  representations  include  a  node  and  link
diagram,  and  a  square  matrix  of  the  items  with  the
value  of  a  link  attribute  in  the  row  and  column
representing a link.
    Examples:  Network  visualization  is  an  old  but  still
imperfect  art  because  of 
the  complexity  of
relationships  and  user  tasks.  Commercial  packages
can  handle  small  networks  or  simple  strategies  such
as  Netmap's  layout  of  nodes  on  a  circle  with  links
criss-crossing  the  central  area.  An  ambitious  3-
dimensional  approach  was  an  impressive  early
accomplishment  (Fairchild  et  al.,  1988),  and  new
interest  in  this  topic  has  been  spawned  by  attempts  to
visualize  the  World  Wide  Web  (Andrews,  1995;
Hendley et al., 1995).

These  seven  data  types  reflect  are  an  abstraction  of
the reality. There are many variations on these themes
(2  1/2  or  4-dimensional  data,  multitrees,...)  and  many
prototypes  use  combinations  of  these  data  types.  This
taxonomy is useful only if it facilitates discussion and
leads  to  useful  discoveries.  Some  idea  of  missed
opportunities  emerges  in  looking  at  the  tasks  and  data
types in depth:

Overview:  Gain  an  overview  of  the  entire  collection.
Overview  strategies  include  zoomed  out  views  of

each  data  type  to  see  the  entire  collection  plus  an
adjoining  detail  view.  The  overview  contains  a
movable  field-of-view  box  to  control  the  contents  of
the  detail  view,  allowing  zoom  factors  of  3  to  30.
Replication  of  this  strategy  with  intermediate  views
enables  users  to  reach  larger  zoom  factors.  Another
popular  approach  is  the  fisheye  strategy  (Furnas,
1986)  which  has  been  applied  most  commonly  for
network  browsing  (Sarkar  and  Brown,  1994;  Bartram
et  al.,  1995).  The  fisheye  distortion  magnifies  one  or
more  areas  of  the  display,  but  zoom  factors  in
prototypes  are  limited  to  about  5.  Although  query
language  facilities  made  it  difficult  to  gain    an
overview  of  a  collection,  information  visualization
interfaces  support  some  overview  strategy,  or  should.
Adequate  overview  strategies  are  a  useful  criteria  to
look  for.  Along  with  an  overview  plus  detail  (also
called    context  plus  focus)  view  there  is  a  need  for
navigation  tools  to  pan  or  scroll    through  the
collection.

Zoom:  Zoom  in  on  items  of  interest.  Users  typically
have  an  interest  in  some  portion  of  a  collection,  and
they  need  tools  to  enable  them  to  control  the  zoom
focus  and  the  zoom  factor.  Smooth  zooming  helps
users  preserve  their  sense  of  position  and  context.
Zooming  could  be  on  one  dimension  at  a  time  by
moving  the  zoombar  controls  or  by  adjusting  the  size
of  the  field-of  -view  box.  A  very  satisfying  way  to
zoom  in  is  by  pointing  to  a  location  and  issuing  a
zooming  command,  usually  by  clicking  on  a  mouse
button  for  as  long  as  the  user  wishes  (Bederson  and
Hollan,  1993).  Zooming  in  one  dimension  has  proven
useful  in  starfield  displays  (Jog  and  Shneiderman,
1995).

Filter:  filter  out  uninteresting  items.  Dynamic  queries
applied to the items in the collection is one of the key
ideas  in  information  visualization  (Ahlberg  et  al.,
1992;  Williamson  and  Shneiderman,  1992).  By
allowing  users  to  control  the  contents  of  the  display,
users  can  quickly  focus  on  their  interests  by
eliminating  unwanted  items.  Sliders,  buttons,  or  other
control  widgets  coupled  to  rapid  display  update  (less
than  100  milliseconds)  is  the  goal,  even  when  there
are tens of thousands of displayed items.

Details-on-demand:  Select  an  item  or  group  and  get
details  when  needed.  Once  a  collection  has  been
trimmed  to  a  few  dozen  items  it  should  be  easy  to
browse  the  details  about  the  group  or  individual
items.  The  usual  approach  is  to  simply  click  on  an
item  to  get  a  pop-up  window  with  values  of  each  of
the  attributes.  In  Spotfire,  the  details-on-demand
window  can  contain  HTML  text  with  links  to  further
information.

Proc. Visual Languages 96                                  -  4  -                                                September 1996

Relate:  View  relationships  among  items.  In  the
FilmFinder  (Ahlberg  and  Shneiderman,  1994)  users
could  select  an  attribute,  such  as  the  film's  director,
in  the  details-on-demand  window  and  cause  the
director  alphaslider  to  be  reset  to  the  director's  name,
thereby  displaying  only  films  by  that  director.
Similarly,  in  SDM  (Chuah  et  al.,  1995),  users  can
select  an  item  and  then  highlight  items  with  similar
attributes  or  in  LifeLines  (Plaisant  et  al.,  1996)  users
can  click  on  a  medication  and  see  the  related  visit
report,  prescription,  and  lab  test.  Designing  user
interface actions to specify which relationship is to be
manifested is still a challenge. The Influence Explorer
(Tweedie  et  al.,  1996)  emphasizes  exploration  of
relationships  among  attributes.  and  the  Table  Lens
emphasizes  finding  correlations  among  pairs  of
numerical  attributes  (Rao  and  Card,  1994).

History  :  Keep  a  history  of  actions  to  support  undo,
replay,  and  progressive  refinement.  It  is  rare  that  a
single  user  action  produces  the  desired  outcome.
Information  exploration  is  inherently  a  process  with
many  steps,  so  keeping  the  history  of  actions  and
allowing  users  to  retrace  their  steps  is  important.
However,  most  prototypes  fail  to  deal  with  this
requirement.  Maybe  they  are  reflecting  the  current
state  of  graphic  user  interfaces,  but  designers  would
be  better  to  follow  information  retrieval  systems
which  typically  preserve  the  sequence  of  searches  so
that they can be combined or refined.

Extract:  Allow  extraction  of  sub-collections  and  of
the  query  parameters.  Once  users  have  obtained  the
item  or  set  of  items  they  desire,  it  would  be  useful  to
be  able  to  extract  that  set  and  save  it  to  a  file  in  a
format that would facilitate other uses such as sending
by  email,  printing,  graphing,  or  insertion  into  a
statistical  or  presentation  package.  An  alternative  to
saving  the  set,  they  might  want  to  save,  send,  or  print
the  settings  for  the  control  widgets.  Very  few
prototypes  support  this  action,  although  Roth's  recent
work  on  Visage  provides  an  elegant  capability  to
extract  sets  of  items  and  simply  drag-and-drop  them
into the next application window.

    The  attraction  of  visual  displays,  when  compared  to
textual  displays,  is  that  they  make  use  of  the
remarkable  human  perceptual  ability  for  visual
information.  Within  visual  displays,  there  are
opportunities  for  showing  relationships  by  proximity,
by  containment,  by  connected  lines,  or  by  color
coding.  Highlighting  techniques  (for  example,  bold-
face  text  or  brightening,  inverse  video,  blinking,
underscoring, or boxing) can be used to draw attention
to  certain  items  in  a  field  of  thousands  of  items.
Pointing  to  a  visual  display  can  allow  rapid  selection,
and  feedback  is  apparent.  The  eye,  the  hand,  and  the

mind  seem  to  work  smoothly  and  rapidly  as  users
perform actions on visual displays.

to  many  users 

for  many 

a 

form-fill-in  query 

4. Advanced Filtering
    Users's  have  highly  varied  needs  for  filtering
features.  The  dynamic  queries  approach  of  adjusting
numeric  range  sliders,  alphasliders  for  names  or
categories,  or  buttons  for  small  sets  of  categories  is
appealing 
tasks
(Shneiderman,  1994).  Dynamic  queries  might  be
called  direct-manipulation  queries,  since  they  share
the  same  concepts  of  visual  display  of  actions  (the
sliders or buttons) and objects (the query results in the
task-domain  display);  the  use  of  rapid,  incremental,
and  reversible  actions;  and  the  immediate  display  of
feedback  (less  than  100  msec).  Additional  benefits
are  no  error  messages  and  the  encouragement  of
exploration.
    Dynamic  queries  can  reveal  global  properties  as
well  as  assist  users  in  answering  specific  questions.
As  the  database  grows,  it  is  more  difficult  to  update
the  display  fast  enough,  and  specialized  data
structures or parallel computation are required.
    The  dynamic-query  approach  to  the  chemical  table
of  elements  was  tested  in  an  empirical  comparison
with 
interface.  The
counterbalanced-ordering  within-subjects  design  with
18  chemistry  students  showed  strong  advantages  for
the  dynamic  queries  in  terms  of  faster  performance
and lower error rates (Ahlberg et al., 1991).
    Dynamic  queries  usually  permit  OR  combinations
within  an  attribute  with  AND  combination  of
attributes  across  attributes  (conjunct  of  disjuncts).
This  is  adequate  for  many  situations  since  rapid
multiple  sequential  queries  allow  users  to  satisfy  their
information  needs.  Commercial  information-retrieval
systems,  such  as  DIALOG  or  Lexis/Nexis,  permit
complex  Boolean  expressions  with  parentheses,  but
widespread  adoption  has  been  inhibited  by  the
difficulty  of  using  them.  Numerous  proposals  have
been  put  forward  to  reduce  the  burden  of  specifying
complex  Boolean  expressions  (Reisner,  1988).  Part  of
the  confusion  stems  from  informal  English  usage
where  a  query  such  as  List  all  employees  who  live  in
New  York  and  Boston  would  result  in  an  empty  list
because  the  “and”  would  be  interpreted  as  an
intersection;  only  employees  who  live  in  both  cities
would  qualify!  In  English,  “and”  usually  expands  the
options;  in  Boolean  expressions,  AND  is  used  to
narrow  a  set  to  the  intersection  of  two  others.
Similarly,  in  the  English  “I’d  like  Russian  or  Italian
salad  dressing,”  the  “or”  is  exclusive,  indicating  that
you  want  one  or  the  other  but  not  both;  in  Boolean
expressions, an OR is inclusive, and is used to expand
a set.
    The  desire  for  full  Boolean  expressions,  including
nested  parentheses  and  NOT  operators,  led  us  toward

Proc. Visual Languages 96                                  -  5  -                                                September 1996

novel  metaphors  for  query  specification.  V e n n
diagrams  (Michard,  1982),  decision  tables  (Greene  et
al.,  1990),  and  the  innovative  InfoCrystal  (Spoerri,
1993)  have  been  used,  but  these  both  become
confusing  as  query  complexity  increases.  We  sought
to  support  arbitrarily  complex  Boolean  expressions
with  a  graphical  specification.  Our  approach  was  to
apply the metaphor of water flowing from left to right
through  a  series  of  pipes  and  filters,  where  each  filter
lets  through  only  the  appropriate  documents,  and  the
pipe  layout  indicates  relationships  of  AND  or  OR.
(Young  and  Shneiderman,  1993)
    In  this  filter–flow  model,  ANDs  are  shown  as  a
linear  sequence  of  filters,  suggesting  the  successive
application  of  required  criteria.  As  the  flow  passes
through  each  filter,  it  is  reduced,  and  the  visual
feedback  shows  a  narrower  bluish  stream  of  water.
ORs are shown two ways: within an attribute, multiple
values  can  be  selected  in  a  single  filter;  and  across
multiple  attributes,  filters  are  arranged  in  parallel
paths.  When  the  parallel  paths  converge,  the  width  of
the flow reflects the size of the union of the document
sets.
  Negation was handled by a NOT operator that, when
selected,  inverts  all  currently  selected  items  in  a
filter.  For  example,  if  California  and  Georgia  were
selected and then the NOT operator was chosen, those
two  states  would  become  deselected  and  all  the  other
states  would  become  selected.  Finally,  clusters  of
filters  and  pipes  can  be  made  into  a  single  labeled
filter.  This  facility  ensures  that  the  full  query  can  be
shown  on  the  display  at  once,  and  allows  clusters  to
be saved in a library for later reuse.
    We  believe  that  this  approach  can  help  novices  and
intermittent  users  to  specify  complex  Boolean
expressions  and  to  learn  Boolean  concepts.  A
usability  study  was  conducted  with  20  subjects  with
little  experience  using  Boolean  algebra.  The  prototype
filter–flow  interface  showed  statistically  significant
improved  performance  against  a  textual  interface  for
comprehension  and  composition  tasks.  The  filter-flow
interface was preferred by all 20 subjects.

5. Summary
Novel  graphical  and  direct-manipulation  approaches
to query formulation and information visualization are
now  possible.  While  research  prototypes  have
typically  dealt  with  only  one  data  type  (1-,  2-,  3-
dimensional  data,  temporal  and  multi-dimensional
data,  and 
tree  and  network  data),  successful
commercial  products  will  have  to  accommodate
several.  These  products  will  need  to  provide  smooth
integration  with  existing  software  and  support  the  full
task  list:  Overview,  zoom,  filter,  details-on-demand,
relate,  history,  and  extract.  These  ideas  are  attractive
because they present information rapidly and allow for
rapid  user-controlled  exploration.  If  they  are  to  be
fully  effective,  some  of  these  approaches  require

novel  data  structures,  high-resolution  color  displays,
fast  data  retrieval,  specialized  data  structures,
parallel computation, and some user training.
  Although the computer contributes to the information
explosion,  it  is  potentially  the  magic  lens  for  finding,
sorting,  filtering,  and  presenting  the  relevant  items.
Search  in  complex  structured  documents,  graphics,
images,  sound,  or  video  presents  grand  opportunities
for the design of user interfaces and search engines to
find the needle in the haystack. The novel-information
exploration 
tools—such  as  dynamic  queries,
treemaps,  fisheye  views,  parallel  coordinates,
starfields,  and  perspective  walls—are  but  a  few  of  the
inventions that will have to be tamed and validated.

Acknowledgements:  This  taxonomy  was  brewing  in
my mind at the time of the Gubbio, Italy conference
on  Advanced  Visual  Interfaces  (May  1996).  Stu
Card's  opening  talk  provoked  me  to  start  it  in  time
for  my  closing  talk,  and  I  have  refined  it  into  the
structure for the infomration visualization chapter in
my  forthcoming  (1997)  third  edition  of  Designing
the User Interface, Addison-Wesly Publishers.  I am
delighted  and  appreciative  of  Margaret  Burnett  and
Wayne  Citrin  for  giving  me  the  chance  to  include
and  present  these  ideas  in  the  Visual  Languages  96
Conference.

References
Ahlberg,  Christopher  and  Shneiderman,  Ben,  Visual
information  seeking:  Tight  coupling  of  dynamic
query  filters  with  starfield  displays,  Proc.  CHI94
Conference:  Human  Factors  in  Computing  Systems,
ACM,  New  York,  NY  (1994),  313-321  +  color
plates.

Ahlberg,  Christopher  and  Shneiderman,  Ben,
AlphaSlider: A compact and rapid selector, Proc.  of
ACM  CHI94  Conference  Human  Factors 
in
Computing  Systems,  ACM,  New  York,  NY  (1994),
365-371.

Ahlberg,  Christopher,  Williamson,  Christopher,  and
Shneiderman, Ben, Dynamic queries for information
exploration:  An  implementation  and  evaluation,
Proc.  ACM  CHI’92:  Human  Factors  in  Computing
Systems, ACM, New York, NY  (1992), 619-626.

Ahlberg,  Christopher  and  Wistrand,  Erik,  IVEE:  An
information  visualization  & 
exploration
environment,  Proc.  IEEE  Information  Visualization
  IEEE  Computer  Press,  Los  Alamitos,  CA
'95,
(1995), 66-73.

Andrew,  Keith,  Visualising  cyberspace:  Information
visualisation in the Harmony internet browser, Proc.
 IEEE Computer
IEEE Information Visualization '95,
Press, Los Alamitos, CA (1995), 97-104.

Asahi,  T.,  Turo,  D.,  and  Shneiderman,  B.,  Using
treemaps  to  visualize  the  analytic  hierarchy
process,  Information  Systems  Research  6,  4
(December 1995), 357-375.

Proc. Visual Languages 96                                  -  6  -                                                September 1996

Bartram,  Lyn,  Ho,  Albert,  Dill,  John,  and  Henigman,
Frank,  The  continuous  zoom:  A  constrained  fisheye
technique  for  viewing  and  navigating 
large
information  spaces,  Proc.  User  Interface  Software
and  Technology  '95 ,  ACM,  New  York,  NY  (1995),
207-215.

Becker,  Richard  A.,  Eick  ,  Stephen  G.,  and  Wilks,
Allan  R.  Visualizing  Network  Data,  I E E E
Transactions  on  Visualization  and  Computer
Graphics 1, 1 (March 1995), 16-28.

Bederson,  Ben  B.  and  Hollan,  James  D.,  PAD++:  A
zooming  graphical  user  interface  for  exploring
alternate  interface  physics,  Proc.  User  Interfaces
Software and Technology '94 (1994), 17-27.

Bertin,  Jacques,  Semiology  of  Graphics,  University  of

Wisconsin  Press,  Madison,  WI  (1983)

Card,  Stuart  K.,  Robertson,  George  G.,  and  York,
William,  The  WebBook  and  the  WebForager:  An
information  workspace  for  the  World-Wide  Web,
Proc.  CHI96  Conference:  Human  Factors  in
Computing  Systems,  ACM,  New  York,  NY  (1996),
111-117.

Carriere,  Jeremy  and  Kazman,  Rick,  Interacting  with
huge  hierarchies:  Beyond  cone  trees,  Proc.  IEEE
 IEEE Computer Press,
Information Visualization '95,
Los Alamitos, CA (1995), 74-81.

Chimera,  Richard,  Value  bars:  An  information
visualization  and  navigation  tool  for  multiattribute
listings, Proc. CHI92 Conference: Human Factors in
Computing  Systems,  ACM,  New  York,  NY  (1992),
293-294.

Chimera,  Richard  and  Shneiderman,  Ben,  Evaluating
three user interfaces for browsing tables of contents,
ACM  Transactions  on  Information  Systems  12,  4
(October 1994).

Chuah,  Mei  C.,  Roth,  Steven  F.,  Mattis,  Joe,  and
Kolojejchcik,  John,  SDM:  Malleable  Information
Graphics, Proc.  IEEE  Information  Visualization  '95,
IEEE  Computer  Press,  Los  Alamitos,  CA  (1995),
66-73.

Cleveland,  William,  Visualizing  Data,  Hobart  Press,

Summit, NJ (1993).

Egan,  Dennis  E.,  Remde,  Joel  R.,  Gomez,  Louis  M.,
Landauer,  Thomas  K.,  Eberhardt,  Jennifer,  and
Lochbum,  Carol  C.,  Formative  design-evaluation  of
SuperBook,  ACM  Transactions  on  Information
Systems 7, 1 (January 1989), 30–57.

Egenhofer,  Max  and  Richards,  J.,  Exploratory  access
to  geographic  data  based  on  the  map-overlay
metaphor,  Journal  of  Visual  Languages  and
Computing 4, 2 (1993), 105-125.

Eick, Stephen G. , Steffen, Jospeh L., and Sumner, Jr.,
Eric E., SeeSoft- A tool for visualizing line-oriented
software  statistics,  IEEE  Transactions  on  Software
Engineering 18, 11 (1992) 957-968.

Eick,  Stephen  G.  and  Wills,  Graham  J.,  Navigating
Large  Networks  with  Hierarchies,  Proc.  IEEE
Visualization '93 Conference , (1993), 204--210.

Fairchild,  Kim  M.,  Poltrock,  Steven  E.,  and  Furnas,
George  W.,  SemNet:  Three-dimensional
representations  of  large  knowledge  bases,  In
Guindon,  Raymonde  (Editor),  Cognitive  Science
and 
for  Human-Computer
Interaction,  Lawrence  Erlbaum,  Hillsdale,  NJ
(1988), 201-233.

its  Applications 

Fishkin,  Ken  and  Stone,  Maureen  C.,  Enhanced
dynamic  queries  via  movable  filters,  Proc.  CHI95
Conference:  Human  Factors  in  Computing  Systems,
ACM, New York, NY (1995), 415-420.

Furnas,  George  W.,  Generalized  fisheye  views,  Proc.
CHI86  Conference:  Human  Factors  in  Computing
Systems, ACM, New York, NY (1986), 16-23.

Goldstein,  Jade  and  Roth,  Steven  F,  Using
aggregation and dynamic queries for exploring large
data sets, Proc.  CHI95  Conference:  Human  Factors
in  Computing  Systems,  ACM,  New  York,  NY
(1995), 23-29.

Greene,  S.  L.,  Devlin,  S.  J.,  Cannata,  P.  E.,  and
Gomez,  L.  M.,  No  IFs,  ANDs,  or  ORs:  A  study  of
database  querying,  International  Journal  of  Man–
Machine  Studies 32 (March 1990), 303–326.

Hendley,  R.  J.,  Drew,  N.  S.,  Wood,  A.  S.,  Narcissus:
Visualizing  information,  Proc.  IEEE  Information
  IEEE  Computer  Press,  Los
Visualization 
Alamitos, CA (1995), 90-96.

'95,

Humphrey,  Susanne  M.  and  Melloni,  Biagio  John,
Databases:  A  Primer  for  Retrieving  Information  by
Computer,  Prentice-Hall,  Englewood  Cliffs,  NJ
(1986).

Inselberg,  Alfred,  The  plane  with  parallel  coordinates,

The Visual Computer 1 (1985), 69-91.

Jerding, Dean F. and Stasko, John T., The information
mural:  A  technique  for  displaying  and  navigating
large  information  spaces,  Proc.  IEEE  Information
  IEEE  Computer  Press,  Los
Visualization 
Alamitos, CA (1995), 43-50.

'95,

Jog,  Ninad  and  Shneiderman,  Ben,  Information
visualization  with  smooth  zooming  on  an  starfield
display,  Proc.  IFIP  Conf.  Visual  Databases  3,
Chapman and Hall, London (1995), 1-10.

Johnson,  Brian,  and  Shneiderman,  Ben,  Tree-maps:  A
space-filling  approach  to  the  visualization  of
hierarchical  information  structures,  Proc.  IEEE
Visualization’91,  IEEE,  Piscataway,  NJ  (1991),
284–291.

Keim,  D.  A.  and  Kriegal,  H.,  VisDB:  Database
exploration  using  multidimensional  visualization,
IEEE  Computer  Graphics  and  Applications
(September 1994), 40-49.

Korfhage,  Robert,  To  see  or  not  to  see  --  Is  that  the
query?,  Communications  of    the  ACM  34  (1991),
134-141.

Lamping,  John,  Rao,  Ramana,  and  Pirolli,  Peter,  A
focus  +  context  technique  based  on  hyperbolic
geometry  for  visualizing  large  hierarchies,  Proc.  of
ACM  CHI95  Conference:  Human  Factors  in

Proc. Visual Languages 96                                  -  7  -                                                September 1996

Shneiderman,  Ben,  Tree  visualization  with  tree-maps:
A 2-d space-filling approach, ACM  Transactions  on
Graphics 11, 1 (January 1992), 92-99.

Shneiderman,  Ben,  Dynamic  queries  for  visual
information  seeking,    IEEE  Software  11,  6  (1994),
70-77.

Spence,  Robert  and  Apperley,  Mark,  Data  base
navigation:  An  office  environment  for 
the
professional, Behaviour & Information Technology 1,
1 (1982), 43-54.

Spoerri,  Anselm,  InfoCrystal:  A  visual  tool  for
information  retrieval  &  management,  Proc.  ACM
Conf    on  Information  and  Knowledge  Management
(1993).

Tufte,  Edward,  The  Visual  Display  of  Quantitative

Information, Graphics Press, Cheshire, CT (1983).

Tufte,  Edward,  Envisioning  Information,  Graphics

Press,  Cheshire,  CT  (1990).

Tweedie,  Lisa,  Spence,  Robert,  Dawkes,  Huw,  and
Su,  Hua,  Externalising  abstract  mathematical
models,  Proc.  of  ACM  CHI96  Conference:  Human
Factors  in  Computing  Systems,  ACM,  New  York,
NY (1996), 406-412.

Williamson,  Christopher,  and  Shneiderman,  Ben,  The
Dynamic  HomeFinder:  Evaluating  dynamic  queries
in  a  real-estate  information  exploration  system,
Proc. ACM SIGIR’92 Conference,  ACM, New York,
NY  (1992),  338-346.  Reprinted  in  Shneiderman,  B.
(Editor),  Sparks  of  Innovation  in  Human-Computer
Interaction, Ablex Publishers, Norwood, NJ, (1993),
295-307.

Wise,  James  A.,  Thomas,  James,  J.,  Pennock,  Kelly,
Lantrip,  David,  Pottier,  Marc,  Schur,  Anne,  and
Crow,  Vern,  Visualizing  the  non-visual:  Spatial
analysis  and  interaction  with  information  from  text
documents, Proc. IEEE Information Visualization '95,
IEEE  Computer  Press,  Los  Alamitos,  CA  (1995),
51-58.

Wurman,  Richard  Saul,  Information  Anxiety,

Doubleday, New York (1989).

Young,  Degi  and  Shneiderman,  Ben,  A  graphical
for  boolean  queries:  An
filter/flow  model 
implementation  and  experiment,  Journal  of  the
American Society for Information Science 44, 6 (July
1993), 327-339.

Computing  Systems,  ACM,  New  York,  NY  (1995),
401-408

Laurini, R. and Thompson, D., Fundamentals of Spatial
Information  Systems,  Academic  Press,  New  York,
NY (1992).

Marchionini,  Gary,  Information  Seeking  in  Electronic
Environments,  Cambridge  University  Press,  UK
(1995).

Michard,  A.,  A  new  database  query  language  for  non-
professional  users:  Design  principles  and  ergonomic
evaluation,  Behavioral  and  Information  Technology
1, 3 (July–September 1982), 279–288.

Mitchell,  Richard,  Day,  David,  and  Hirschman,
Lynette,  Fishing  for  information  on  the  internet,
  IEEE
Proc.  IEEE  Information  Visualization  '95,
Computer Press, Los Alamitos, CA (1995), 105-111.
Mukherjea,  Sougata,  Foley,  James  D.,  and  Hudson,
Scott,  Visualizing  complex  hypermedia  networks
through  multiple  hierarchical  views,  Proc.  of  ACM
CHI95  Conference:  Human  Factors  in  Computing
Systems,  ACM,  New  York,  NY  (1995),  331-337  +
color plate.

North,  Chris,  Shneiderman,  Ben,  and  Plaisant,
Catherine,  User  controlled  overviews  of  an  image
library:  A  case  study  of  the  Visible  Human,  Proc.
1st  ACM  International  Conference  on  Digital
Libraries (1996), 74-82.

Pirolli,  Peter,  Schank,  Patricia,  Hearst,  Marti,  and
Diehl,  Christine,  Scatter/gather  browsing
communicates  the  topic  structure  of  a  very  large
text  collection,  Proc.  of  ACM  CHI96  Conference ,
ACM, New York, NY (1996), 213-220.

Plaisant,  Catherine,  Rose,  Anne,  Milash,  Brett,
Widoff,  Seth,  and  Shneiderman,  Ben,  LifeLines:
Visualizing personal histories,  Proc. of ACM CHI96
Conference:  Human  Factors  in  Computing  Systems,
ACM, New York, NY (1996), 221-227, 518.

Rao,  Ramana  and  Card,  Stuart  K.,  The  Table  Lens;
Merging  graphical  and  symbolic  representations  in
an  interactive  focus  +  context  visualization  for
tabular  information,  Proc.  CHI94  Conference:
Human  Factors  in  Computing  Systems,  ACM,  New
York, NY (1994), 318-322.

Reisner,  Phyllis,  Query  languages.  In  Helander,
Martin  (Editor),  Handbook  of  Human–Computer
Interaction,  North-Holland,  Amsterdam,  The
Netherlands (1988), 257–280.

Robertson,  George  G.,  Card,  Stuart  K.,  and
Mackinlay,  Jock  D.,  Information  visualization  using
3-D  interactive  animation,  Communications  of    the
ACM 36, 4 (April 1993), 56-71.

Robertson  George  G.  and  Mackinlay,  Jock  D.,  The
document  lens,  Proc.  1993  ACM  User  Interface
Software  and  Technology,  ACM  New  York,  NY
(1993), 101-108.

Sarkar,  Manojit  and  Brown,  Marc  H.,  Graphical
fisheye  views,  Communications  of  the  ACM  37, 12
(July 1994), 73–84.

Proc. Visual Languages 96                                  -  8  -                                                September 1996

",False,2003.0,{},False,False,bookSection,False,Y9LGBJIG,[],self.user,False,False,False,False,,,The eyes have it: A task by data type taxonomy for information visualizations,Y9LGBJIG,False,False
3F7CPP79,HJYRJXFK,"Semantic Interaction for Visual Text Analytics 
Chris North 
Alex Endert 
Virginia Tech 
Virginia Tech 

Patrick Fiaux 
Virginia Tech 

Blacksburg, VA USA 

aendert@vt.edu 

Blacksburg, VA USA 

pfiaux@vt.edu 

 

Blacksburg, VA USA 

north@vt.edu 

by 

For 

through 

ABSTRACT 
Visual analytics emphasizes sensemaking of large, complex 
datasets 
interactively  exploring  visualizations 
generated 
example, 
statistical  models. 
dimensionality  reduction  methods  use  various  similarity 
metrics to visualize textual document collections in a spatial 
metaphor,  where  similarities  between  documents  are 
approximately  represented  through  their  relative  spatial 
distances  to  each  other  in  a  2D  layout.  This  metaphor  is 
designed to mimic analysts’ mental models of the document 
collection  and  support  their  analytic  processes,  such  as 
clustering similar documents together. However, in current 
methods, users must interact with such visualizations using 
controls  external  to  the  visual  metaphor,  such  as  sliders, 
menus, or text fields, to directly control underlying model 
parameters  that  they  do  not  understand  and  that  do  not 
relate  to  their  analytic  process  occurring  within  the  visual 
metaphor.  In  this  paper,  we  present  the  opportunity  for  a 
new  design  space  for  visual  analytic  interaction,  called 
semantic  interaction,  which  seeks  to  enable  analysts  to 
spatially interact with such models directly within the visual 
metaphor using interactions that derive from their analytic 
process,  such  as  searching,  highlighting,  annotating,  and 
repositioning  documents.  Further,  we  demonstrate  how 
semantic  interactions  can  be  implemented  using  machine 
learning 
tool,  called 
ForceSPIRE, for interactive analysis of textual data within 
a  spatial  visualization.    Analysts  can  express  their  expert 
domain knowledge about the documents by simply moving 
them,  which  guides  the  underlying  model  to  improve  the 
overall layout, taking the user’s feedback into account. 
Author Keywords 
Visualization; visual analytics; interaction 
ACM Classification Keywords 
H5.m.  Information  interfaces  and  presentation  (e.g.,  HCI): 
Miscellaneous.  
General Terms 
Design; Human Factors; Theory 

in  a  visual  analytic 

techniques 

 
Permission to  make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that copies 
bear this notice and the full citation on the first page. To copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior 
specific permission and/or a fee. 
CHI’12, May 5–10, 2012, Austin, Texas, USA. 
Copyright 2012 ACM 978-1-4503-1015-4/12/05...$10.00. 
 

 

INTRODUCTION 
Visual analytics bases its success on combining the abilities 
of statistical models, visualization, and human intuition for 
users to gain insight into large, complex datasets [23]. This 
success often hinges on the ability for users to interact with 
the  information,  manipulating  the  visualization  based  on 
their  domain  expertise,  interactively  exploring  possible 
connections, and investigating hypotheses. It is through this 
interactive exploration that users are able to make sense of 
complex  datasets,  a  process  referred  to  as  sensemaking 
[19].  
The  two  primary  parts  of  sensemaking  are  foraging  and 
synthesis. Foraging refers to the stages of the process where 
users filter and gather collections of interesting or relevant 
information.  Then,  using  that  information,  users  advance 
through  the  synthesis  stages  of  the  process,  where  they 
construct  and  test  hypotheses  about  how  the  foraged 
information  may  relate  to  the  larger  plot.  Tools  exist  that 
support users for either foraging or synthesis – but not both. 
In  this  paper  we  present  semantic  interaction,  combining 
the  foraging  abilities  of  statistical  models  with  the  spatial 
synthesis abilities of analysts. Semantic interaction is based 
on the following principles: 
1. Visual  “near=similar”  metaphor  supports  analysts’ 
spatial  cognition,  and  is  generated  by  statistical  models 
and similarity metrics. [22] 

2. Use  semantic  interactions  within  the  visual  metaphor, 
based  on  common  interactions  occurring  in  spatial 
analytic  processes  [4]  such  as  searching,  highlighting, 
annotating, and repositioning documents.  

3. Interpret  and  map  the  semantic  interactions  to  the 
underlying parameters of the model, by updating weights 
and adding information. 

4. Shield  the  users  from  the  complexity  of  the  underlying 

mathematical models and parameters. 

5. Models  learn  incrementally  by  taking  into  account 
interaction during the entire analytic process, supporting 
analysts’ process of incremental formalism [10]. 

6. Provide  visual  feedback  of  the  updated  model  and 

learned parameters within the visual metaphor. 

7. Reuse  learned  model  parameters  in  future  or  streaming 

data within the visual metaphor. 

To  demonstrate  the  concept  of  semantic  interaction,  we 
present  a  prototype  visual  analytics  tool,  ForceSPIRE,  for 
spatial analysis of textual information. In ForceSPIRE, the 
user  interaction  takes on  a deeper,  more  integrated role in 

the  exploratory  spatial  analytic  process.  This  is  done 
through capturing the semantic interaction, interpreting the 
analytical  reasoning  associated  with  the  interaction,  and 
updating the statistical model, and ultimately updating the 
spatialization.  Hence,  users  are  able  to  leverage  semantic 
interaction  to  explore  and  analyze  the  data  interactively, 
while  the  system  is  responsible  for  properly  updating  the 
underlying statistical model.  
RELATED WORK 
Foraging Tools 

 

Figure  1.  A  model  of  interaction  with  foraging  tools.  Users 
interact  directly  with  the  statistical  model  (red),  then  gain 
insight  through  observing  the  change  in  the  visualization 
(blue). 
We  categorize  foraging  tools  by  their  ability  to  pass  data 
through  complex  statistical  models  and  visualize  the 
computed structure of the dataset for the user to gain insight 
(Figure  1).  Thus,  users  interact  with  these  tools  primarily 
through directly manipulating the parameters of the model 
used  for  computing  the  structure.  As  such,  users  are 
required  to  translate  their  domain  expertise  and  semantics 
about  the  information  to  determine  which  (and  by  how 
much) to adjust these parameters. The following examples 
further describe this category of tools. 
Visualizations such as IN-SPIRE’s “Galaxy View” (shown 
in  Figure  3)  present  users  with  a  spatial  layout  of  textual 
information where similar documents are proximally close 
to  one  another  [25].  An  algorithm  creates  the  layout  by 
mapping the high-dimensional collection of text documents 
down  to  a  two-dimensional  view.  In  these  spatializations, 
the  spatial  metaphor  is  one  from  which  users  can  infer 
meaning  of  the  documents  based  on  their  location.  The 
notion  of  distance  between  documents  represents  how 
similar the two documents are (i.e., more similar documents 
are  placed  closer  together).  For  instance,  a  cluster  of 
documents  represents  a  group  of  similar  documents,  and 
documents  placed  between  two  clusters  implies  those 
documents are connected to both clusters. These views are 
beneficial  as  they  allow  users  to  visually  gain  a  quick 
overview  of  the  information,  such  as  what  key  themes  or 
groups  exist  within  the  dataset.  The  complex  statistical 
models  that  compute  similarity  between  documents  are 
based on the structure within the data, such as term or entity 
frequency. In order to interactively change the view, users 
are  required  to  directly  adjust  keyword  weights,  add  or 
remove documents/keywords, or provide more information 
on how to parse the documents for keywords/entities upon 
import. 

 

to  a 

to  understand 

the 

[15].  Through  adjusting 

Similarly, an interactive visualization tool called iPCA uses 
Principal  Component  Analysis  (PCA)  to  reduce  high-
dimensional  data  down 
two-dimensional  plot, 
providing  users  with  sliders  and  other  visual  controls  for 
directly  adjusting  numerous  parameters  of  the  algorithm, 
such  as  individual  eigenvalues,  eigenvectors,  and  other 
components  of  PCA 
the 
parameters,  the  user  can  observe  how  the  visualization 
changes.  This  allows  users  to  gain  insight  into  a  dataset, 
given  they  have  a  thorough  understanding  of  PCA, 
necessary 
the 
changes they are making to the model parameters. 
Alsakran  et  al.  presented  a  visualization 
system, 
STREAMIT,  capable  of  spatially  arranging  text  streams 
based  on  keyword  similarity  [3].  Again,  users  can 
interactively  explore  and  adjust  the  spatial  layout  through 
directly  changing  the  weight  of  keywords  that  they  find 
important.  In  addition,  STREAMIT  allows  for  users  to 
conduct  a  temporal  investigation  of  how  clusters  change 
over time. 
Synthesis Tools 

implications  behind 

 

Figure  2.  A  model  of  interaction  with  synthesis  tools.  Users 
manually  create  a  spatial  layout  of  the  information  to 
maintain and organize their insights about the data. 
Synthesis  tools  focus  on  allowing  users  to  organize  and 
maintain their hypotheses and insight regarding the data in 
a  spatial  medium.  In  large  part,  this  is  done  through 
presenting users with a flexible spatial workspace in which 
they  can  organize  information  through  creating  spatial 
structures,  such  as  clusters,  timelines,  stories,  etc.  (Figure 
2). In doing so, users externalize their thought processes (as 
well  as  their  insights)  into  a  spatial  layout  of  the 
information. 
For example, Analyst’s Notebook [2] provides users with a 
spatial workspace where information can be organized, and 
connections  between  specific  pieces  of  information  (e.g., 
entities, documents, events, etc.) can be created. Similarly, 
The Sandbox [26] enables users to create a series of cases 
(collections  of 
information)  which  can  be  organized 
spatially within the workspace.  
From  previous  studies,  we  found  cognitive  advantages 
associated  with  the  manual  creation  of  a  spatial  layout  of 
the  information  [4].  By  providing  users  a  workspace  in 
which  to  manually  create  spatial  representations  of  the 
information, users were able to externalize their semantics 
of the information into the workspace. That is, they created 
spatial  structures  (e.g.,  clusters,  timelines,  etc.),  and  both 
the structures as well as the locations relative to remaining 
layout  carried  meaning  to  the  users  with  regards  to  their 
sensemaking process. Marshall et al. have pointed out that 

this 

interaction  (and 

From  the  sensemaking  loop  presented  by  Pirolli  and  Card 
[19],  we  learn  that  in  intelligence  analysis,  that  analytic 
process  consists  not  only  of  the  information  that  is 
explicitly  within  the  dataset  being  analyzed,  but  also  the 
domain knowledge of the analyst performing the analysis. It 
is through this domain knowledge that analysts interact and 
explore  the  dataset  to  “make  sense”  of  the  information. 
Thus,  we  believe 
the  domain 
knowledge  associated  with  it)  is  equally  important  as  the 
raw data, and must be incorporated into the visualization by 
tightly coupling the model with the interaction. 
From this body of work, we most notably come away with 
an understanding that 1) analysts fundamentally understand 
the spatial metaphor used in many spatial visualizations, 2) 
many  of  these  systems  are  constructed  using  complex 
mathematical  algorithms  to  transform  high-dimensional 
data  to  two  dimensions,  and  3)  in  most  cases  these 
algorithms  can  be  controlled  by  analysts  largely  through 
visual  controls  (e.g.,  sliders,  knobs,  etc.)  to  directly  adjust 
parameters of the algorithms, updating the spatial layout. 
SEMANTIC INTERACTION 

 

Figure 4. A model of semantic interaction. Users are able to 
interact directly in the spatial metaphor. The system updates 
the corresponding parameters of the statistical model based on 
the analytic reasoning of the users. Finally, the model updates 

the visualization based on the changes, thus unifying the 
synthesis and foraging stages of the sensemaking loop. 

In the purest sense, semantic interaction refers to interaction 
occurring  within  a  spatial  visualization,  with  the  added 
benefit that it is tightly coupled to the model calculating the 
spatial layout (Figure 4). Given the previous work of what 
interaction  in  visual  analytic  tools  is,  semantic  interaction 
occupies a new design space for interaction. It merges the 
ability to change the statistical model while maintaining the 
flexibility  and  familiar  methods  for  interacting  within  the 
metaphor  of  spatial  visualizations.  Users  can  benefit  from 
semantic  interactions  in  that  they  can  interact  within  a 
metaphor  which 
they  are  familiar  with,  performing 
interactions  which  are  part  of  the  spatial  analytic  process 
[4], without having to focus on formal updates to the model.  
Semantic  interaction  leverages  the  cognitive  connection 
formed  between  the  user  and  the  spatial  layout.  The 
following intelligence analysis scenario is representative of 
the strategies and interactions of analysts when performing 
an  intelligence  analysis  task  of  textual  documents  in  a 
spatial visualization, as previously found by Andrews et al. 
[4],  and  further  motivates  and  explains  the  concept  of 
semantic interaction: 

 
Figure  3.  The  IN-SPIRE  Galaxy  View  showing  a 
spatializtiation  of  documents  represented  as  dots.  Each 
cluster of dots represents a group of similar documents.  
 
allowing users to create such informal relationships within 
information  is  beneficial,  as  it  does  not  require  users  to 
formalize these relationships [17].  
From this related work, we believe a trend is emerging in 
how interaction is currently handled in many visual analytic 
systems where complex statistical models are used – users 
are  required  to  go  outside  of  the  metaphor.  That  is,  while 
the  visual  representation  given  to  users  is  spatial,  the 
methods of interaction require users to step outside of that 
metaphor  and  interact  directly  with  the  parameters  of  the 
statistical model using visual controls, toolbars, etc.  
There  has  been  some  work  in  providing  more  easy  to  use 
interactions  for  updating  statistical  models.  For  example, 
relevance feedback has been used for content-based image 
retrieval, where users are able to move images towards or 
away  from  a  single  image  in  order  to  portray  pair-wise 
similarity  or  dissimilarity  [24].  From  there,  an  image 
retrieval algorithm determines the features and dimensions 
shared between the images that the user has determined as 
being  similar.  We  view  this  as  one  example  where  the 
interaction stays in the spatial metaphor of the visualization.  
Also, spatializations of document sets exist that allow users 
to place “points of interest” into the spatial layout. In VIBE, 
users are allowed to define multiple points of interest in the 
spatial  layout  that  correspond  to  a  series  of  keywords 
describing  a  subject  matter  of  interest  to  the  user  [18]. 
Similarly,  Dust  &  Magnet  [27]  allows  users  to  place  a 
series  of  “magnets”  representing  keywords  into  the  space 
and observe how documents are attracted or repelled from 
the  locations  of  these  magnets.  Through  both  of  these 
systems, users can interact in the spatial metaphor through 
these  placements  of  “nodes”  representing  keywords. 
However, the focus of semantic interaction is on interacting 
with  data  (i.e.,  documents),  an 
important  distinction 
discussed in the following section. 

 

 

 
Figure  5.  (top)  The  basic  version  of  the  “visualization 
pipeline”.  Interaction  can  be  performed  on  directly  the 
Algorithm  (blue  arrow)  or  the  data  (red  arrow).  (bottom) 
Our  modified  version  of 
for  semantic 
interaction,  where  the  user  interacts  within  the  spatial 
metaphor (purple arrow). 

the  pipeline 

During her analysis, an intelligence analyst finds a 
suspicious  and 
interesting  phrase  within  a 
document. While reading through the document, she 
highlights  the  phrase  “suspicious  individuals  were 
spotted  at  the  airport”,  in  order  to  more  easily 
recall  this  information  later.  After  she  finishes 
reading the document, she moves the document into 
the  bottom  right  corner  of  her  workspace,  in  the 
proximity of other documents related to an event at 
an airport. To remind herself of her hypothesis, she 
annotates  the  document  with  “might  be  related  to 
Revolution  Now  terrorist  group”.  Now,  with  the 
goal  of 
the 
“airport”, she searches for the term, continuing her 
investigation. 

further  examining 

the  events  at 

investigating 

that  each  of 

instead  point  out 

the  analytic  process  of 

In addition to the three forms of semantic interaction in the 
scenario,  Table  1  provides  a  list  of  various  forms  of 
semantic  interaction,  including  how  each  can  be  used 
within 
textual 
information  spatially.  We  do  not  claim  that  this  list  is 
complete,  but 
these 
interactions  can  relate  to  a  user’s  reasoning  within  the 
analytic process.  
Designing for Semantic Interaction 
In order for analysts to interact with information in a spatial 
metaphor, it must first be created. Following the model of 
the visualization pipeline [13], this creation calls for a series 
of  mathematical  transformations,  turning  raw  data  into  a 
spatial  layout  –  much  the  way  many  of  the  visualizations 
mentioned  previously  are  constructed.  However,  these 
visualizations  fit  this  model,  as  their  user  interactions  are 
primarily  focused  on  directly  modifying  the  statistical 
model  (as  well  as  other  attributes  of  the  visualization  or 
data  transformation).  Designing  for  semantic  interaction 
requires  a  fundamentally  different  model  for  how  tools 
integrate  user  interaction  –  one  that  can  capture  the 
interaction,  interpret  the  associated  analytical  reasoning, 
and update the appropriate mathematical parameters.  
Figure  5  illustrates  this  model,  where  the  spatialization  is 
treated  a  medium  through  which  the  user  can  perceive 

 

Figure 6. Overview of how nodes and edges in ForceSPIRE’s 
force-directed layout are created from documents (Doc) and 
entities (Ent), respectively.  

 

 

it 

interaction, 

information  and  gain  insight,  as  well  as  interact  and 
perform  his  analysis.  Through  expanding  the  pipeline  to 
accommodate  for  semantic 
is  a  more 
appropriate match to the user’s sensemaking process. 
Capturing the Semantic Interaction 
A  non-trivial  first  step  in  the  model  is  capturing  the  user 
interaction.  Much  research  has  been  done  in  this  area, 
primarily  for  the  purpose  of  maintaining  process  history 
(e.g., [5], [21], [12], etc.). When considering how to capture 
interaction,  one  decision  to  be  made  is  at  what  “level”  to 
capture  it.  For  example,  GlassBox  [6]  captures  interaction 
at a rudimentary level (i.e. mouse clicks and key strokes), 
while  Graphical  History  [14]  keeps  track  of  a  series  of 
previous  visualizations  as  a  user  changes  the  visualization 
during the exploration of the data.  
Semantic  interaction  is  captured  at  a  data  level,  as  the 
interactions  occur  on  the  data,  and  within  the  spatial 
metaphor.  Using 
the 
interaction being captured would be: 

the  earlier  analytic  scenario, 

•  The highlighted phrase 
•  When the highlighting occurs (timestamp) 
•  The color chosen for the highlight 
•  The document in which the highlight occurs 
•  The new document location 
•  The text of the annotation 

By  capturing  (and  storing)  the  interaction  history,  we  can 
interpret the analytical reasoning of the user. Thus, we not 
only capture the interaction, but also use it. 
Interpreting the Associated Analytical Reasoning 
In interpreting the interaction, the goal is for the system to 
determine  the  analytical  reasoning  associated  with  the 
interactions  and  update  the  model  accordingly.  From 
previous findings [4], we can associate analytical reasoning 
with  forms  of  semantic  interaction  (see  Table  1).  It  is 
essentially the model’s task to determine  why, in terms of 
the data, the interaction occurred. To answer this question, 
we do not propose that this model can accurately gauge user 
intent.  Instead,  the  goal  is  to  calculate,  based  on  the  data, 

Figure 7. Using ForceSPIRE on a 32 megapixel large, 
high-resolution display. 

 

 
what information is consistent with the captured interaction. 
For  instance,  we  associate  text  highlighting  with  adding 
importance to the text being highlighted. We do not claim 
that we can associate the interaction of highlighting to the 
intuition that spurred the analyst to highlight the text, which 
is far more challenging, and arguably impossible. 
We refer to the captured and interpreted interactions as soft 
data, in comparison to the hard data that is extracted from 
the raw textual information (e.g., term or entity frequency, 
titles,  document  length,  etc.).  We  define  soft  data  as  the 
stored result of user interaction as interpreted by the system. 
In  representing  interaction  as  soft  data,  the  algorithm  can 
calculate  and  reconfigure  the  spatial  layout  accordingly. 
Figure  5  illustrates  how  our  approach  differs  from  the 
traditional visualization pipeline. 
There has been previous work in capturing and interpreting 
reasoning from user interaction. For instance, Dou et al. [7] 
performed  a  study  where  financial  analysts  were  asked 
analyze  a  dataset  using  WireVis,  an  interactive  financial 
transaction visualization. The tool developers then analyzed 
the captured interaction, and assumptions were made about 
the  reasoning  of  the  analysts  at  specific  points  in  the 
investigation. These results were compared to the analysts’ 
self-recorded  reasoning,  and  found  to  be  accurate  up  to 
82%. While our work has similar goals (i.e., interpreting the 
analytical reasoning associated with the analysts through an 
evaluation  of  the  interaction)  our  model  does  so  through 
tightly  integrating  the  interaction  with  the  underlying 
mathematical model. In doing so, the interpretation can be 
done algorithmically. 
Updating the Underlying Model 
Through  metric  learning  of  distance  weights,  the  layout 
uses  the  soft  data  to  update  the  underlying  model. 
Depending  on  the  algorithm  used  to  compute  the  spatial 
layout,  the  precise  parameters  being  updated  will  vary.  In 
general,  this  will  refer  to  weighting  of  a  combination  of 
dimensions  that  will  help  guide  the  model  as  to  which 
dimensions the user finds important.  
FORCESPIRE: SYSTEM OVERVIEW 
ForceSPIRE  is  a  visual  analytics  prototype  designed  for 
specific 
(document 
movement,  text  highlighting,  search,  and  annotation)  for 

forms  of 

interaction 

semantic 

 

Figure  8.  Moving  the  document  shown  by  the  arrow, 
ForceSPIRE  adapts  the  layout  accordingly.  Documents 
sharing entities with the document being moved follow. 

 

interactively exploring textual data. The system has a single 
spatial  view  (shown  in  Figure  12),  where  a  collection  of 
documents is represented spatially based on similarity (i.e., 
documents closer together are more similar).  
ForceSPIRE is designed for large, high-resolution displays 
(such  as  the  one  shown  in  Figure  7).  As  semantic 
interaction emphasizes the importance of context in which 
the  interaction  takes  place  (e.g.,  highlighting  text  in  the 
context  of  the  document),  having  the  full  detail  text 
available  in  the  context  of  the  spatial  layout  is  beneficial 
over having a single document viewer. Further, the physical 

Table  1.  Forms  of  semantic  interaction.  Each  interaction 
corresponds  to  reasoning  of  users  within  the  analytic 
process. 

Form of Semantic 

Interaction 

Document Movement 

Text Highlighting 

Pinning  Document 
Location 
Annotation, “Sticky Note” 

to 

Document Coloring 

Level of Visual Detail 

Query Terms 
 

Associated Analytic Reasoning 

• Similarity/Dissimilarity 
• Create 

spatial  construct 

timeline, list, story, etc) 

• Test 

hypothesis, 

see 
document “fits” in region 

(.e.g 

how 

• Mark 

importance  of  phrase 

(collection of entities) 

• Augment  visual  appearance  of 

document for reference 

to 

in 

• Give 

semantic  meaning 

space/layout 

• Put 

semantic 

information 

workspace, within context 
• Create visual group/cluster 
• Mark group membership 
• Change 

ease 

of 

visually 
referencing  information  (e.g.  full 
detail = more important = easy to 
reference) 

• Expressive search for entity 

(and 

to  match 

is  positioning 

Semantic Interaction in ForceSPIRE 
The  semantic  interactions  in  ForceSPIRE  are:  placing 
information  at  specific  locations,  highlighting,  searching, 
and annotating in order to incrementally change the spatial 
layout 
their  mental  model.  The  primary 
parameters  of  the  force-directed  model  that  are  being 
updated  through  this  learning  model  are  the  importance 
values of the entities.  
Document  Movement.  The  predominant  interaction  in  a 
spatial  workspace 
repositioning) 
documents.  In  previous  work,  we  have  demonstrated  how 
users can perform both exploratory and expressive forms of 
this type of interaction [9]. In ForceSPIRE, we allow for the 
following  exploratory  interaction  (i.e.,  interaction  that 
allows users to explore the structure of the current model, 
but  does  not  change  it).  Users  are  able  to  interactively 
explore the information by dragging a document within the 
workspace, pinning a document to a particular location (see 
Figure  8),  as  well  as  linking  two  documents.  When 
dragging a document, the force-directed system responds by 
finding the lowest energy state of the remaining documents 
given  the  current  location  of  the  dragged  document. 
Mathematically, this adds a constraint to the stress function 
being  optimized  (in  this  case  the  force-directed  model). 
This  allows  users  to  explore  the  relationship  of  that 
document in comparison to the remaining documents.  
In addition to the exploratory dragging of a document, users 
have the ability to pin a document. By pinning a document, 
users  are  able  to  incrementally  add  semantic  meaning  to 
locations in their workspace. By specifying key documents 
to  user-defined  locations,  the  layout  of  the  remaining 
documents will adapt to these constraints. Thus, users can 
explore  how  documents  are  positioned  based  on  their 
similarity  (or  dissimilarity)  to  the  pinned  documents.  For 
instance,  if  the  layout  places  a  document  between  two 
pinned  documents, 
the  particular 
document holds a link between the two pinned documents, 
sharing entities that occur in both. 
Finally,  users  can  perform  an  expressive  form  of  this 
interaction  by  linking  two  documents,  performed  by 
dragging  one  document  onto  another  pinned  document.  In 
doing so, ForceSPIRE calculates the similarity between the 
documents,  and  increases  the  importance  value  of  the 
entities  shared  between  both  documents.  As  a  result,  the 
layout will place more emphasis on the characteristics that 
make those two documents similar. 
Highlighting.  When  highlighting  a  term,  ForceSPIRE 
creates an entity from the term (if not already one), and the 
importance  value  of  that  term  is  increased.  Similarly, 
highlighting  a  phrase  results  in  the  phrase  being  first 
parsed for entities, then increasing the importance value of 
each  of  those  entities.  For  example,  Figure  11  shows  the 
effect of highlighting the terms “Colorado” and “missiles” 
in the document pointed to with the arrow. As a result, the 

it  may 

imply 

that 

 
Figure  9.  The  Effect  of  adding  an  annotation  (“these 
individuals  may  be  related  to  Revolution  Now”)  to  the 
document shown with an arrow. As  a result,  the document 
becomes 
linked  with  other  documents  mentioning  the 
terrorist organization “Revolution Now”.  

presence of these displays creates an environment in which 
the  virtual  information  (in  this  case  the  documents)  can 
occupy  persistent  physical  space.  As  a  result,  users  are 
further  immersed  into  the  spatial  metaphor,  as  they  can 
point and quickly refer to information based on the physical 
locations.  
Constructing the Spatial Metaphor 
The spatial layout of the text documents is determined by a 
modified  version  a  force-directed  graph  model  [11].  This 
model  functions  on  the  principle  of  nodes  with  a  mass 
connected  by  springs  with  varying  strengths.  Thus,  each 
node has attributes of attraction and repulsion: nodes repel 
other  nodes,  and  two  nodes  attract  each  other  only  when 
connected  by  a  spring  (edge).  The  optimal  layout  is  then 
computed  by  iteratively  calculating  these  forces  until  the 
lowest energy state of all the nodes is reached. A complete 
description of this algorithm can be found in [11].  
We  apply  this  model  to  textual  information  by  treating 
documents  as  nodes  (an  overview  is  shown  in  Figure  6). 
The entire textual content of each document is parsed into a 
collection  of  entities  (i.e.,  keywords).  The  number  of 
entities corresponds to the mass of each document (heavier 
nodes  do  not  move  as  fast  as  lighter  nodes).  A spring  (or 
edge) represents one or more matching entities between two 
nodes.  Therefore,  the  initial  distance  metric  is  a  based  on 
co-occurrence  of  terms  between  documents.  For  example, 
two  documents  containing  the  term  “airport”  will  be 
connected  by  a  spring.  The  strength  of  a  spring  (i.e.  how 
close together it tries to place two nodes) is based on two 
factors:  the  number  of  entities  two  documents  have  in 
common,  and  the  importance  value  associated  with  each 
shared entity (initially, importance values are created using 
a  standard  tfidf  method  [16]).  The  sum  of  all  importance 
values add up to 1. 
The resulting spatial layout is one where similarity between 
documents  is  represented  by  distance  relative  to  other 
documents.  Similarity  in  this  system  is  defined  by  the 
strength of the spring between two documents. A stronger 
spring  (and  therefore  a  larger  amount  of  shared  entities) 
will pull two documents closer together, and thus represent 
two similar documents. 

 

 
Figure  10.  Searching  for  the  term  ”Atlanta”,  documents 
containing the term highlight green within the context of the 
spatial  layout.  Additionally,  the  importance  value  of  entity 
“Atlanta” is increased. 

other  documents  containing  that  term  are  clustered  more 
tightly. 
Searching.  When  coming  across  a  term  of  particular 
interest, analysts usually search on that term in order to find 
other  occurrences.  In  a  spatial  workspace,  this  is  of 
particular  importance,  because  the  answer  to  “where  the 
term  is  also  found”  is  not  only  given  in  terms  of  what 
documents,  but  also  where  in  the  layout  those  documents 
occur. The positions of documents containing the term are 
shown in context of the entire dataset, from which users can 
infer the importance of that term (as shown in Figure 10).  
ForceSPIRE  first  creates  an  entity  from  the  search  term 
(unless  it  is  already  one),  then  increases  the  importance 
value  of  the  search  term.  Figure  10  gives  an  example  of 
how a search result appears in ForceSPIRE. Searching for 
the  term  “Atlanta”,  documents  that  contain  the  term  are 
highlighted  green,  and  links  are  drawn  to  show  where  the 
resulting documents are in relation to the current document.  
Annotation.  Annotations  (i.e.,  “sticky  notes”)  are  also 
viewed as a form of semantic interaction, occurring within 
the analytic process, from which analytic reasoning can be 
inferred. When a user creates a note regarding a document, 
that semantic information should be added to the document. 
For example, if Document A refers to “Revolution Now” (a 
suspicious  terrorist  group),  and  Document  B  refers  to  “a 
group of suspicious individuals”, and the user has reason to 
believe  these  individuals  are  related  to  Revolution  Now, 
adding a note to Document B stating “these individuals may 
be  related  to  Revolution  Now”  is  one  way  for  the  user  to 
add semantic meaning to the document.  
ForceSPIRE  handles  the  addition  of  the  note  (shown  in 
Figure 9) by 1) parsing the note for any currently existing 
entities,  then  2)  increasing  the  importance  value  of  each, 
and 3) creating any new springs between other documents 
sharing these entities. In the example in Figure 9, edges are 
created between Document B and Document A (as well as 
any  other  documents  that  mention  “Revolution  Now”). 
Additionally,  if  the  note  contains  any  new  entities  not 
currently in the model, they are created, with the intent that 

 

 
Figure 11. The effect of highlighting a phrase containing the 
entites  “Colorado”  and  “missiles”.  Documents  containing 
these  entities  move  closer,  as  the  increase  in  importance 
value increases the edge strength.  

the 

importance  values  of 

any future entities that may match to that note can be linked 
at that time. ForceSPIRE also handles cases where notes are 
edited,  with  text  added  or  removed  from  the  note,  by 
updating  the  entities  associated  with  the  document,  and 
adjusting 
these  entities 
accordingly. 
Model Updates 
Each  of  the  semantic  interactions  in  ForceSPIRE  impacts 
the  model  by  updating  the  importance  values  of  entities, 
and  the  mass  of  each  document.  The  calculation  for 
updating the importance value of an entity is the same for 
each interaction. If an entity was “hit” (i.e., it was included 
in  a  highlight,  it  was  searched,  it  was  in  a  note,  etc.), 
ForceSPIRE increases its importance value by 10%. As the 
sum  of  all  importance  values  of  entities  adds  up  to  1, 
ForceSPIRE  subtracts  an  equal  amount  from  all  other 
entities’ importance values. As a result, importance values 
decay over time, and entities that are rarely used during the 
analysis  have  less  impact  on  the  layout.  The  mass  of  a 
document  uses  a  similar  calculation,  in  that  each  time  a 
document  is  “hit”  (i.e.,  text  was  highlighted,  it  was  the 
result of a search hit, etc.), it increases by 10%.  
When  undoing  an 
standard 
the 
“Control+Z”  keyboard  shortcut,  a  linear  history  of  the 
interactions will be reversed, and the importance values of 
affected  entities  will  be  returned  to  their  prior  values  (as 
well  as  document  masses).  As  for  the  locations  of  the 
documents,  the  reverted  importance  values  and  document 
masses  will  be  responsible  for  updating 
layout. 
However, this does not guarantee that the layout will return 
to  the  exact  previous  view,  and  the  user  may  find  it 
necessary to perform small adjustments. 
The model updates used in ForceSPIRE serve as an initial 
approach at how to couple semantic interactions with model 
updates. Other, more complex methods may exist, and we 
encourage  further  research  in  this  area.  Sensemaking  is  a 
complex exploratory process. As such, semantic interaction 

interaction  using 

the 

through 

more  central  documents.  While  reading 
the 
documents, he highlighted phrases of interest. For example, 
he highlighted the phrase “Nizar A. is now known to have 
spent six months in Afghanistan”. In doing so, ForceSPIRE 
increased  the  importance  value  of  the  entities  within  the 
phrase,  particularly  “Afghanistan”  and  “Nizar  A”.  As  a 
result, the layout forms more tightly around those entities. 
Each change incrementally changes the layout. 
Continuing  with  his  investigation,  he  began  searching  for 
words  of  interest  (e.g.,  “weapons”,  “Colorado”,  “Atlanta”, 
etc.). ForceSPIRE provided him with quick visual feedback 
on where in the dataset each terms showed up (the search 
result  for  “Atlanta”  is  shown  in Figure  10).  In  addition  to 
gaining an overview of the distribution of the term within 
the  dataset  (by  highlighting  each  document  containing  the 
term  green),  ForceSPIRE  treats  performing  a  search  as 
either  creating  a  new  entity  from  the  search  term,  or 
increasing the importance value if an entity corresponding 
to the search term already exists. As a result of the multiple 
search terms and highlights corresponding to locations (e.g., 
“Atlanta”,  “Los  Angeles”,  “Missouri”,  etc.),  ForceSPIRE 
adapts  the  spatialization  by  creating  a  more  geographic-
oriented layout (shown in the “Mid Stage” layout in Figure 
12).  
During  further  investigation,  he  began  opening  more 
documents and adding annotations to documents where he 
found  information  missing  that  he  knew.  For  example, 
Figure  9  shows  how  he  opened  one  document  where 
“suspicious individuals” were mentioned. Earlier, he read a 
document  containing 
terrorist 
organization  named  “Revolution  Now”.  While  reading 
about  the  suspicious  individuals,  the  other  information  in 
the document triggered him to make a connection between 
these  individuals  and  Revolution  Now.  He  made  added  a 
note  to  the  document  about  the  suspicious  individuals 
stating  “these  individuals  may  be  related  to  Revolution 
Now”. As a result, ForceSPIRE parsed the note for entities, 
added  them  to  the  document,  and  pulled  the  document 
closer to other documents containing the entity “Revolution 
Now”.  
After  continuing  his  investigation  in  this  manner,  he 
ultimately  made  the  connections  within  the  dataset  to 
uncover  the  terrorist  plot.  The  progression  of  the  spatial 
layout,  shown  in Figure 12, shows the final layout, where 
he  was  able  to  pinpoint  regions  of  the  layout  as  being 
important  in  his  finding.  Some  of  the  spatial  locations  of 
clusters  are  a  result  of  him  pinning  documents  to  that 
region (e.g., “Atlanta”, “Los Angeles”, etc.). These pinned 
documents are shown in red. Perhaps more interestingly is 
not the regions that were created as a result of him pinning 
documents  to  that  location,  but  rather  how  the  remaining 
documents respond in the layout. For example, in the final 
state  shown  in  Figure  12,  a  group  of  documents  began  to 
emerge  in  the  middle  of  all  the  pinned  locations.  Upon 
examining  these  documents,  he  discovered  that  these 

information  about  a 

the 

layout 

 

interaction, 

instances  during 

 
Figure 12. The incremental change of the spatial layout (main 
view  of  ForceSPIRE)  from  the  initial  to  the  final  state. 
Through  semantic 
incrementally 
changed  based  on the  semantic  input of the user. We labeled 
the regions based on what the user told us the regions meant to 
him at each stage. 
can  enable  analysts  to  explore  their  hypothesis  in-situ, 
while  the  provenance  of  their  insights  is  captured  and 
stored. An open area of research is what analyzing the soft 
data might reveal about the analytic process. For instance, if 
the  importance  values  of  entities  converge  on  a  small 
number  of  entities,  specific  biases  might  be  revealed. 
Similarly, 
the  analysis  when  new 
hypotheses  are  being  explored  may  be  indicated  by 
diverging importance values. 
Use Case 
We  demonstrate  the  functionality  of  ForceSPIRE  through 
the  following  use  case.  In  this  scenario,  we  simulate  an 
intelligence  analysis  scenario  where  the  task  is  to  find  a 
hidden terrorist plot in a pre-constructed, ficticious textual 
dataset.  The  dataset  consists  of  50 
text  documents, 
containing  a  complex  terrorist  plot  (explosives  are  being 
transported to various cities in the U.S. using trucks). The 
combination of the task of finding the hidden terrorist plot 
and  the  textual  dataset  is  representative  of  daily  work 
performed  by  professional  intelligence  analysts  [8].  The 
analysis  described  below  lasted  70  minutes,  and  was 
performed  by  an  individual  computer  science  graduate 
student.  
The user began the investigation by loading the collection 
of  documents  into  ForceSPIRE.  The  documents  were 
automatically  parsed  for  entities  using 
the  LingPipe 
keyword  extraction  library  [1].  From  these  entities,  an 
initial layout was generated, shown in Figure 12(top). From 
this  layout,  he  began  investigation  by  reading  through  the 

 

interpreting 

leverage 

interactions 

DISCUSSION 
Unifying the Sensemaking Loop 
With the fundamentally different role occupied by semantic 
interaction, we explore a new design space for interaction in 
visual analytic tools. With the addition of soft data, and a 
model  capable  of 
the  user’s  analytical 
reasoning,  we 
that  are  already 
occurring in the spatial analytic process to further aid users 
in their sensemaking process.  
With  semantic  interaction,  the  amount  of  formalization 
between foraging and sensemaking (Figure 13) on the part 
of the user is reduced. For instance, in moving a document, 
users  can  formulate  a  hypothesis  based  on  that  document, 
expecting  similar  documents 
to  follow.  ForceSPIRE 
attempts to update the layout based on the interaction, and 
gives the user feedback. Thus, the foraging stage occurs as 
a  result  of  the  hypothesis  being  formed  through  semantic 
interaction.  By  not  forcing  users  to  over-formalize  their 
analytic  reasoning  too  early  in  order  to  forage  for  the 
relevant  information,  semantic  interaction  creates  a  more 
seamless 
transition  between 
foraging  and  synthesis, 
unifying the sensemaking loop.  
Future Work 
Semantic 
interaction,  as  a  concept,  opens  up  many 
possibilities for further research, such as: what interactions 
to  capture  and  store,  which  parameters  of  the  model  to 
update,  how  to  store  the  soft  data,  and  which  models 
present a metaphor that can be extended upon.  
In  order  to  make  more  concrete  claims  regarding  the 
usability  and  effectiveness  of  ForceSPIRE  (and  thus,  of 
semantic  interaction),  a  formal  user  study  is  needed.  Our 
plan is to introduce ForceSPIRE to professional intelligence 
analysts  and  have  them  solve  scenarios  that  model  their 
daily  task,  such  as  one  of  the  VAST  datasets  [2020].  The 
observations  and  feedback  from  these  users  will  provide 
ecological validity for semantic interaction. 
CONCLUSION 
In  this  paper  we  have  discussed  how  the  concept  of 
semantic  interaction  leads  to  a  new  design  space  for 
interaction 
information. 
Semantic  interactions  occur  directly  within  the  spatial 
metaphor,  support  spatial  cognition,  and  exploit  spatial 
analytic  interactions.  We  describe  semantic  interaction, 
discussing  the  three  components  required  –  capturing  the 
interaction, 
the  analytical  reasoning,  and 
updating  the  mathematical  model.  Further,  we  present 
ForceSPIRE, designed for semantic interaction with textual 
information, discussing its functionality and demonstrating 
how it can be used through a use case. Lastly, we discuss 
how  semantic  interaction  has  the  opportunity  to  unify  the 
sensemaking  loop,  creating  a  more  seamless  analytic 
process.  In  allowing  users  to  interact  within  the  spatial 
metaphor, they can remain more focused on their analysis 
of  the  data,  without  having  to  become  experts  in  the 
underlying mathematical models of the system.  

in  spatializations  of 

interpreting 

textual 

 

Figure  13.  The  sensemaking  loop,  illustrating  the  complex 
sequence  of  steps  used  by  intelligence  analysts  in  order  to 
gain insight into data.  
 
documents  are  about  the  terrorist  organization  using  “U-
Haul”  or  “Ryder”  trucks  for  transportation  between  these 
locations. ForceSPIRE placing these documents in between 
these  cities  in  the  layout  was  helpful,  as  these  documents 
contain  information  “connecting”  the  events  in  these 
locations.  Immediately  after  noticing  this  event,  he  also 
made use of the expressive form of interaction, performed 
by dragging two of these documents together to determine 
what  made  them  similar.  After  seeing  that  it  was  indeed 
terms  such  as  “Ryder”  and  “U-Haul”,  the  layout  formed 
more tightly around these terms. 
ForceSPIRE interpreted the analytical reasoning of the user 
through the creation of new entities that were not found by 
the  initial  keyword  extraction,  as  well  as  the  increase  of 
importance values of existing entities. This is evidenced by 
the  creation  of  39  new  entities  during  the  course  of  the 
analysis.  LingPipe  extracted  89  initial  entities  from  this 
dataset,  and  at  the  time  of  completing  our  investigation 
ForceSPIRE  included  128.  Examples  of  newly  created 
entities  are  “big  event”,  “grenades”,  “Fisher  Island”, 
“weapons”,  and  others.  The  ability  for  new  entities  to  be 
created  via  semantic  interaction  did  not  interfere  with  the 
fluid sensemaking process of the user. Instead, it aided the 
process  by  creating  new  entities,  which  in  turn  created 
semantically relevant connections within the dataset. 
In  addition  to  creating  new  entities,  existing  entities 
dynamically  changed  their  importance  value  based  on  the 
semantic 
interpreted 
reasoning 
interactions.  Examples  of  entities 
their 
importance  values  are  “Atlanta”,  “Revolution  Now”, 
“Colorado”,  “L.A.”,  and  others.  As  a 
the 
ForceSPIRE incrementally adapted the layout based on the 
user  input.  This  shows  that  adjusting  importance  values, 
creating entities, and changing locations of key documents 
helped  the  user  discover  the  structure  of  the  dataset,  and 
ultimately make out the hidden terrorist plot.  

of 
that  changed 

analytical 

result, 

the 

 

ACKNOWLEDGEMENTS 
This research was funded by the NSF grant CCF-0937071 
and the DHS center of excellence. 
REFERENCES 
1.  Alias-i. 2008. LingPipe 4.0.1. City, 2008. 
2.  i2 Analyst's Notebook. City. 
3.  Alsakran, J., Chen, Y., Zhao, Y., Yang, J. and Luo, D. 

STREAMIT: Dynamic visualization and interactive 
exploration of text streams. In Proceedings of the IEEE 
Pacific Visualization Symposium, 2011.  

4.  Andrews, C., Endert, A. and North, C. Space to Think: 
Large, High-Resolution Displays for Sensemaking. In 
Proceedings of the CHI '10, 2010.  

5.  Callahan, S. P., Freire, J., Santos, E., Scheidegger, C. E., 

C, Silva, u. T. and Vo, H. T. VisTrails: visualization 
meets data management. In Proceedings of the 
SIGMOD international conference on Management of 
data (Chicago, IL, USA, 2006). ACM.  

6.  Cowley, P., Haack, J., Littlefield, R. and Hampson, E. 

Glass box: capturing, archiving, and retrieving 
workstation activities. In Proceedings of the workshop 
on Continuous archival and retrival of personal 
experences (Santa Barbara, California, USA, 2006). 
ACM.  

7.  Dou, W., Jeong, D. H., Stukes, F., Ribarsky, W., 

Lipford, H. R. and Chang, R. Recovering Reasoning 
Processes from User Interactions. IEEE Computer 
Graphics and Applications, 2009. 

8.  Endert, A., Andrews, C., Fink, G. A. and North, C. 

Professional Analysts using a Large, High-Resolution 
Display. In Proceedings of the IEEE VAST Extended 
Abstract (2009).  

9.  Endert, A., Han, C., Maiti, D., House, L., Leman, S. C. 

and North, C. Observation-level Interaction with 
Statistical Models for Visual Analytics. IEEE VAST, 
2011. 

10. Frank M. Shipman, I. and Marshall, C. C. Formality 

Considered Harmful: Experiences, Emerging Themes, 
and Directions on the Use of Formal Representations 
inInteractive Systems. ACM CSCW, 8, 4, 1999, 333-352. 

11. Fruchterman, T. M. J. and Reingold, E. M. Graph 

drawing by force-directed placement. Software: Practice 
and Experience, 21, 11 1991, 1129-1164. 

12. Gotz, D. Interactive Visual Synthesis of Analytic 

Knowledge. IEEE VAST, 2006. 
13. Heer, J. prefuse manual, 2006. 
14. Heer, J., Mackinlay, J., Stolte, C. and Agrawala, M. 

Graphical Histories for Visualization: Supporting 
Analysis, Communication, and Evaluation. IEEE 
Transactions on Visualization and Computer Graphics, 
14, 6 , 2008, 1189-1196. 

 

15. Jeong, D. H., Ziemkiewicz, C., Fisher, B., Ribarsky, W. 

and Chang, R. iPCA: An Interactive System for PCA-
based Visual Analytics. Computer Graphics Forum, 28, 
2009, 767-774. 

16. Karen A Statistical Interpretation of Term Specificity 

and its Application in Retrieval. Journal of 
Documentation, 28, 1972, 11-21. 

17. Marshall, C. C., Frank M. Shipman, I. and Coombs, J. 

H. VIKI: spatial hypertext supporting emergent 
structure. In Proceedings of the European conference on 
Hypermedia technology (Edinburgh, Scotland, 1994). 
ACM.  

18. Olsen, K. A., Korfhage, R. R., Sochats, K. M., Spring, 
M. B. and Williams, J. G. Visualization of a document 
collection: the vibe system. Information Process 
Management, 29, 1 1993, 69-81. 

19. Pirolli, P. and Card, S. Sensemaking Processes of 

Intelligence Analysts and Possible Leverage Points as 
Identified Though Cognitive Task Analysis Proceedings 
of the International Conference on Intelligence 
Analysis,2005, 6. 

20. Plaisant, C., Grinstein, G., Scholtz, J., Whiting, M., 

O'Connell, T., Laskowski, S., Chien, L., Tat, A., Wright, 
W., Gorg, C., Zhicheng, L., Parekh, N., Singhal, K. and 
Stasko, J. Evaluating Visual Analytics at the 2007 
VAST Symposium Contest. Computer Graphics and 
Applications, IEEE, 28, 2 2008, 12-21. 

21. Shrinivasan, Y. B. and Wijk, J. J. v. Supporting the 

analytical reasoning process in information 
visualization. In Proceedings of the CHI '08 (Florence, 
Italy, 2008). ACM.  

22. Skupin, A. A Cartographic Approach to Visualizing 
Conference Abstracts. IEEE Computer Graphics and 
Applications, pp. 50-58, January/February, 2002. 

23. Thomas, J. J., Cook, K. A., National, V. and Analytics, 
C. Illuminating the path. IEEE Computer Society, 2005. 
24. Torres, R. S., Silva, C. G., Medeiros, C. B. and Rocha, 

H. V. Visual structures for image browsing. In 
Proceedings of the conference on Information and 
knowledge management (New Orleans, LA, USA, 
2003). ACM.  

25. Wise, J. A., Thomas, J. J., Pennock, K., Lantrip, D., 

Pottier, M., Schur, A. and Crow, V. Visualizing the non-
visual: spatial analysis and interaction with information 
for text documents. Morgan Kaufmann Publishers, 1999. 

26. Wright, W., Schroh, D., Proulx, P., Skaburskis, A. and 

Cort, B. The Sandbox for analysis: concepts and 
methods. In Proceedings of the CHI '06 (New York, 
NY, 2006). ACM.  

27. Yi, J. S., Melton, R., Stasko, J. and Jacko, J. A. Dust & 
magnet: multivariate information visualization using a 
magnet metaphor. Information Visualization, 4, 4, 2005, 
239-256. 

",False,2012.0,{},False,False,conferencePaper,False,3F7CPP79,[],self.user,False,False,False,False,http://dl.acm.org/citation.cfm?doid=2207676.2207741,,Semantic interaction for visual text analytics,3F7CPP79,False,False
HVRY632L,VLBUMPME,"Beyond Control Panels
Direct Manipulation for Visual Analytics

Alex Endert
Pacific Northwest National Laboratory

Lauren Bradel and Chris North
Virginia Tech

To  tackle  the  onset  of  big  data,  visual  ana-

lytics  (VA)  seeks  to  marry  the  human  in-
tuition  of  visualization  with  mathematical 
models’ analytical horsepower. A critical question 
is, how will humans interact with and steer these 
complex mathematical models? Initially, users ap-
plied  direct  manipulation  to  such  models  in  the 
same way they applied it to simpler visualizations 
in  the  premodel  era—by  using  control  panels  to 
directly  manipulate  model  parameters.  However, 
opportunities  are  arising  for  direct  manipulation 
of  the  model  outputs,  where  the  users’  thought 
processes take place, rather than the inputs. Here 
we  present  this  new  agenda  for  direct  manipula-
tion for VA.

Direct Manipulation for  
Information Visualization
Direct  manipulation  specifies  three  principles  for 
interaction design for information visualization:1

 ■ continuous  representation  of  the  object  of  in-

terest,

 ■ physical  actions  or  labeled  button  presses  in-

stead of complex syntax, and

 ■ rapid  incremental  reversible  operations  whose 
impact on the object of interest is immediately 
visible.

Typically, these principles are applied through a con-
trol panel, containing visual widgets such as sliders, 
buttons, or query fields, coupled to the parameters 
of a visual representation in the main view. For ex-
ample,  in  Spotfire,  analysts  can  choose  attributes 

to  map  to  available  visual  encodings  (node  color, 
size,  shape,  and  so  on);  select  variables  for  the  x-, 
y-, and z-axes; and adjust sliders to filter by ranges 
on specific data dimensions (see Figure 1). We con-
tend that for VA, with the introduction of complex 
mathematical  models  behind  the  visualizations, 
direct-manipulation  interaction  has  the  opportu-
nity to evolve beyond the use of control panels.

Spatializations for Sensemaking
Spatializations create a visual representation of in-
formation in which data items’ relative proximity 
approximately depicts their similarity. (That is, the 
“near ≈ similar” metaphor holds true.) For exam-
ple,  in  Figure  2,  clusters  of  documents  represent 
themes  or  topics  of  interest.  Such  spatializations 
can be generated manually or computationally.

Manual Generation 
Analysts can leverage manually generated spatial lay-
outs to aid their analyses. For example, by organizing 
spatial  layouts,  they  can  externalize  their  insights 
about  a  dataset  on  the  basis  of  the  information’s 
positions.2  They  frequently  organize  such  layouts 
according  to  complex  schemas  using  mixed  meta-
phors,  often  organized  topically  according  to  the 
semantics relevant to their current analysis needs.

Analysts  use  tools  that  support  manually  con-
structing spatializations to visually synthesize hy-
potheses.3 That is, they directly manipulate spatial 
structures (often mixing clusters, timelines, con-
nections,  geography,  order  of  discovery,  process 
waypoints, and so on) that help reveal their sense-
making  process.  Such  informal  relationships  in 

6 

July/August 2013 

Published by the IEEE Computer Society 

0272-1716/13/$31.00 © 2013 IEEE

Visualization ViewpointsEditor:  Theresa-Marie RhyneFigure 1. Typical use of direct manipulation. The Spotfire scatterplot view can represent several dimensions 
of the data through spatial position and visual encodings; users manipulate it through buttons and sliders on 
control panels.

Figure 2. The In-Spire Galaxy View represents documents as dots. Each cluster of dots represents a group of 
similar documents.

the spatial layout are beneficial because they don’t 
require  analysts  to  overformalize  relationships 
too early in the process. This process of gradually 
increasing  relationships’  formality  is  called  incre-
mental formalism.4

Computational Generation 
Computationally  generated 
spatializations  are 
driven by the recent emphasis on big data and in-
volve complex mathematical models. These models, 
combined  with  user  intuition  and  visualizations, 

 

IEEE Computer Graphics and Applications 

7

directly adjust numerous model parameters, such 
as individual eigenvalues, eigenvectors, and other 
PCA  components.  In  this  way,  they  can  observe 
how the visualization changes. This lets them gain 
insight into a dataset, assuming they know enough 
about  the  underlying  PCA  model  to  understand 
the implications of changing model parameters.

The  straightforward  application  of  direct  ma-
nipulation suggests creating graphical controls for 
each parameter. This use of control panels might 
have  been  appropriate  for  early  information  vi-
sualizations  in  which  the  controls  mapped  natu-
rally to dimension filters and plot axes. However, 
it might be problematic for more complex models 
used in VA applications.

This  approach  has  three  fundamental  usabil-
ity  problems.  First,  many  analysts  aren’t  experts 
in  complex  mathematical  models  and  thus  don’t 
understand the meaning of the parameters for the 
interactive controls. Second, analysts think about 
and  understand  the  documents  at  the  semantic 
level,  yet  the  interactive  controls  for  the  models 
operate  at  the  lower  syntactic  level  of  the  model 
parameters. This creates a mismatch. Third, when 
analysts haven’t yet gained a good understanding 
of the documents and their insights are still infor-
mal, they don’t yet have a basis for expressing their 
inputs  into  the  formal  model  parameters.  These 
problems arise because the focus of direct manipu-
lation in the computationally generated spatializa-
tions  (the  model  parameters)  differs  significantly 
from  that  in  manually  generated  spatializations 
(the documents).

Suppose  an  analyst  recognizes  a  small  set  of 
documents  in  a  spatialization  that  she  believes 
are  related  to  a  semantic  topic  X  of  her  interest, 
but the current layout doesn’t reflect her hypoth-
esized similarity. She directly increases the weight 
of term X in the control panel (for example, by di-
rectly manipulating the layout parameters). How-
ever,  this  has  no  effect  because  X  doesn’t  appear 
in the documents.

Alternatively,  she  could  move  the  documents 
together herself (for example, by directly manipu-
lating  the  layout  output).  She  could  then  receive 
feedback from the models concerning other inter-
esting keywords that do relate to those documents. 
Also, the layout could be automatically updated to 
include other relevant documents. This would en-
able her to gain insight that helps to better formal-
ize her understanding of X.

This approach presents an opportunity to evolve 
the design of user interaction beyond control pan-
els  to  achieve  direct-manipulation  VA.  The  need 
exists  to  cooperatively  integrate  computationally 

Figure 3. iPCA (Interactive Principle Component 
Analysis) provides a dimension reduction algorithm 
that users manipulate through buttons and sliders in 
a control panel.8 (Source: Remco Chang; used with 
permission.)

form  the  basis  for  VA,  in  which  analysts  operate 
dynamic  tools  that  facilitate  analysis  and  sense-
making  of  large,  complex  datasets.5  Models  lever-
aged in VA tools include, but aren’t limited to, those 
for entity extraction, topic modeling, link analysis, 
dimensionality reduction, clustering, and labeling.
These models employ various distance metrics to 
measure the similarity between data objects. Ana-
lysts can use these metrics to spatialize data. For 
example, unstructured text can be represented as 
a “bag of words”—high-dimensional data in which 
each dimension is a unique keyword or phrase in 
the  text.  For  example,  in  In-Spire’s  Galaxy  View 
layout, nearby points represent similar documents 
(see Figure 2).6 This helps analysts recognize rela-
tionships  between  documents  and  between  clus-
ters of documents.

Designing User Interaction for Spatializations
For  computationally  generated  spatializations, 
the  question  arises  of  how  to  design  user  interac-
tion.  The  complex  statistical  models  that  com-
pute similarity using a combination of algorithms 
have  numerous  parameters  to  tune  on  the  basis 
of  the  analysis’s  context.  For  example,  for  visual 
text  analysis,  users  must  directly  adjust  keyword 
weights (measures of importance for each keyword 
and  how  much  it  influences  the  overall  layout), 
add  or  remove  documents  and  keywords,  or  pro-
vide more information on how to parse the docu-
ments for keyword entities upon import.

One such spatialization for streaming text data 
is  Streamit,  in  which  users  explore  a  dataset  by 
directly manipulating keyword weights.7 Similarly, 
iPCA (see Figure 3) is an interactive visualization 
tool that uses principal component analysis (PCA) 
to  reduce  high-dimensional  data  to  a  2D  plot.8 
Users  employ  sliders  and  other  visual  controls  to 

8 

July/August 2013

Visualization Viewpointsgenerated  spatializations  with  those  manually 
generated.  This  would  shift  the  focus  of  interac-
tion from control panels for model input param-
eters to direct manipulation of the model outputs 
as represented by the spatialization itself.

Direct Manipulation of Spatializations
A  trend  is  emerging  in  how  VA  systems  that  use 
complex statistical models handle interaction. This 
trend stems from letting users directly manipulate 
the  data  in  a  spatialization  to  guide  and  improve 
the  layout  according  to  their  interests  or  inter-
pretations. For example, to indicate that two data 
points in a spatialization differ more than is com-
putationally indicated, users can move them apart 
directly in the view. So, the model learns about the 
dissimilarity  and  updates  the  spatialization  to  re-
flect the desired structure of the data. Thus, users 
can  employ  familiar  direct-manipulation  interac-
tions within familiar spatialization metaphors, en-
abling  them  to  interact  with  complex,  unfamiliar 
mathematical models.

Within the spatial metaphor, we see three levels 
of  interactivity  that  motivate  this  emerging  con-
cept  of  direct-manipulation  VA.  These  levels  are 
based  on  the  extent  to  which  machine  learning 
steers the model.

The  first  level  is  direct  manipulation  of  spatial 
constraints. These interactions let users place (and 
move)  spatial  constraints  directly  in  the  spatial-
ization. For example, the Dust & Magnet tool lets 
users place a series of “magnets” representing spe-
cific  data  dimensions  or  keywords  in  the  spatial-
ization.9 Data objects rich in those dimensions are 
more  attracted  to  the  magnets.  Such  direct  ma-
nipulation enables users to guide the spatialization 
layout  by  placing  additional  query-like  attractors 
in the space.

The  second  level  is  direct  manipulation  of  pa-
rameter weighting. Such data-centric interactions 
leverage  metric-learning  techniques  to  adjust  the 
weighting  schema  of  the  dimensions  or  features 
used in distance metric calculations.10 Specifically, 
updates  to  the  weighting  scheme  reflect  the  fea-
tures  emphasized  by  the  user’s  interaction  (the 
weight  of  relevant  features  of  interest  increases, 
and  the  weight  of  other  features  decreases).  The 
weights are adjusted incrementally on the basis of 
heuristics associated with each type of interaction. 
For example, ForceSpire tightly couples several in-
teractions related to text analytics, such as reposi-
tioning documents, highlighting text, annotating, 
and searching, to the underlying dimension reduc-
tion  model.10  For  instance,  highlighting  a  phrase 
in  a  document  that  contains  a  set  of  keywords 

Figure 4. A ForceSpire spatialization’s progression. As 
the user gains insight, ForceSpire’s model learns to 
emphasize relevant features.

increases  those  keywords’  weight  in  the  distance 
metric.

interactions 

The third level is direct manipulation for model 
steering.  These 
leverage  machine 
learning to calculate the amount of change to each 
feature in the weighting schema. Basically, the VA 
application receives an updated spatial layout from 
the user and, given that layout, inverts the model 
to determine the updated model parameters. This 
might  require  an  optimization  search  process  to 
find the best overall fit. Then, the application can 
apply  the  updated  parameters  in  the  forward  ap-
plication of the model to show how the updated fit 
changes the layout. For example, observation-level 
interaction11  and  Dis-Function12  let  users  move 
groups of data points in a multidimensional-scaling 
layout closer together or farther apart to guide ma-
chine  learning  and  explore  alternative  structures 
in the data.

In summary, all these interactions let users in-
teract  directly  with  the  information  in  context. 
Over continuous use, the spatialization updates to 
reflect the incremental insights the user generated 
(see Figure 4). This creates a symbiotic relationship 
between  the  user’s  sensemaking  process  and  the 
system’s machine learning.

Opportunities and Challenges
The following areas provide opportunities and pose 
challenges for research on direct-manipulation VA.

 

IEEE Computer Graphics and Applications 

9

Data

Algorithm

Visualization

User

(a)

Hard data

Soft data

(b)

Algorithm
(project)

Algorithm
(interpret)

Spatialization

User

(perceive)

User

(interact)

Figure 5. Changing the visualization pipeline to support direct-
manipulation visual analytics. (a) In the traditional pipeline, users 
interact directly with the algorithm (the blue arrow) or data (the red 
arrow). (b) In the new, bidirectional pipeline, users interact directly 
with the spatial metaphor; interaction must be interpreted through the 
model (the purple arrows).

Model Steering
The steering of mathematical models has become 
a popular way to adapt those models’ visual output 
to  the  user’s  domain,  task,  and  workflow.  Users 
can augment the statistical determination of im-
portant  features  and  characteristics  in  a  dataset. 
Because  the  resulting  visualizations  include  the 
user’s domain expertise, they become more appli-
cable to the domain.

Figure 5 highlights the changes to the visualiza-
tion pipeline necessary to support such direct ma-
nipulation.  In  the  traditional  pipeline  (see  Figure 
5a), control panels directly adjust model input pa-
rameters. In the new pipeline (see Figure 5b), direct 
manipulation of the spatialization requires invert-
ing  the  model  to  interpret  the  action’s  intent,  as 
we mentioned before. The pipeline maps the inter-
action  backward  by  interpreting  the  actions  and 
adjusting the parameter data—for example, learn-
ing  dimension  weights.  There  are  many  possible 
approaches  to  this  interpretation  step.  Addition-
ally, using multiple models would further compli-
cate  the  pipeline,  necessitating  a  many-to-many 
mapping of interactions to models.

This  area  involves  two  main  challenges.  First, 
how  do  you  invert  models  and  map  interactions 
to the parameter-learning process? Second, how do 
you  incorporate  multiple  models  into  the  visual-
ization pipeline?

Feature Selection
A common stage of spatialization is feature selec-
tion. Features can be selected algorithmically from 
most  forms  of  data,  such  as  extracting  keywords 
from  text,  extracting  visual  and  audio  signatures 
from  images  and  sound,  and  so  on.  The  purpose 
is  to  represent  otherwise  unstructured  data  as 
high-dimensional.  For  example,  a  VA  application 
could use a number of natural-language-processing 

10 

July/August 2013

models to select keywords or key phrases from un-
structured text. These models determine keywords 
that are statistically more expressive than others, 
for that dataset. A frequent additional step selects 
features to optimize the signal-to-noise ratio.

This area involves two challenges. First, how do 
you  incorporate  users’  domain  expertise,  which 
includes features that might not be in the dataset? 
Second, how do you interactively combine features 
from different data types (for example, text, audio, 
and video)?

Feature Extraction
Another common stage of spatialization is feature 
extraction. A high-dimensional representation must 
be reduced to a low-dimensional spatialization. This 
process typically applies a weighting schema to the 
set  of  selected  dimensions  to  emphasize  each  di-
mension  differently  when  projecting  it  onto  the 
2D layout. Because the low-dimensional represen-
tations  are  inherently  ambiguous  representations 
of  high-dimensional  data,  interactions  in  these 
low-dimensional  spaces  can  also  be  ambiguous. 
Multiple  inferences  might  be  possible,  requiring 
assumptions or more user input.

The  challenge  here  is,  how  do  you  accurately 
interpret  the  interaction  in  the  spatialization 
and apply the high-dimensional representation or 
weighting scheme to it?

Mixed Metaphors
As  we  mentioned  before,  users  employ  different 
contexts  and  metaphors  to  refer  to  information 
in different regions of spatializations.2,13 Common 
metaphors include topical clusters, timelines, geo-
spatial  layouts,  social  networks,  and  process  his-
tory. Users frequently mix these metaphors in the 
same workspace as either separate areas or nested 
schemas.  These  metaphors  might  be  well  defined 
or ambiguous and might evolve.4

This  mixed-metaphor  use  of  spatializations 
poses  challenges  to  layout  and  clustering  models 
that  are  generally  designed  to  compute  one  type 
of  layout  across  the  entire  visualization.  So,  you 
might  need  to  combine  multiple  types  of  models 
in complex ways. For example, you could combine 
iCluster,  which  enables  direct  manipulation  of  a 
cluster  membership  model,14  with  ForceSpire  to 
enable  dynamic  layouts  of  clusters  in  space,  in 
much the same way analysts currently do manu-
ally.  The  space’s  continuity  and  flexibility  could 
represent probabilistic membership.

This area involves two challenges. First, how do 
you detect, interpret, compute, and visualize mixed 
models  that  represent  mixed  metaphors?  Second, 

Visualization ViewpointsTable 1. Using multiscale models to address big-data challenges for direct-manipulation visual analytics (VA).

Usage description

Data scale of manipulation 
(no. of data items)
Algorithms

Display

Level of scale

Database

Cloud

The system lays out the data 
according to the user’s spatial-
organization feedback.
<1 million

The system aggregates clusters 
of data in the layout according to 
the user’s grouping feedback.
<1 billion

The system uses the layout 
to query very large data and 
retrieve additional relevant data.
<1 trillion

Dimensionality reduction

Clustering

Information retrieval

Classification

Topic modeling
Clusters

Hierarchy

Containment

Sampling

Streaming
Salience

Depth

Visual salience = similarity

Visual aggregate = similarity
Grouping items

Piling
Cluster counts and contents

Deleting items

Searching
Object relevance

Visualization

Spatial layout

Visual proximity = similarity

Interaction

Moving items

Interactive feedback for 
machine learning

Similarities

Dimension weights

Centroid landmarks

Keyword dimensions and weights

Object weights

Labels

how  do  you  learn  which  model  best  captures  the 
user’s interaction, on the basis of the layout?

Multiscale Models
To support big data, VA can leverage multiple mod-
els  that  deal  with  information  at  multiple  scales 
(see Table 1). For small amounts of data, you could 
display all the data points on the screen by using 
dimensionality reduction (DR) models to organize 
space.  At  larger  scales,  cluster  models  can  aggre-
gate  data  into  fewer  groups  that  could  then  be 
fed to DR models. At even larger scales, informa-
tion retrieval (IR) algorithms become essential for 
streaming or sampling data to dynamically display 
only relevant data.

You can apply a consistent direct-manipulation 
approach  across  all  levels  of  scale  by  implement-
ing  a  system  of  mutual  learning  across  models. 
For example, the IR model can query for data rele-
vance based on the dimension weights that the DR 
model  learned.  Likewise,  the  IR  model  can  learn 
from  user  actions  such  as  placing  uninteresting 
data in the trash.

This  area  involves  two  challenges.  First,  how  do 
you coordinate direct manipulation to steer models 
across multiple levels of scales for big data? Second, 
how  do  you  enhance  algorithm  performance  to 
support real-time direct manipulation of big data?

Implicit and Explicit User Interaction
With  direct-manipulation  VA,  the  system  must 
infer user intentions from user interactions. How-

ever,  one  action  could  have  multiple  possible  in-
tentions.  For  example,  dragging  a  document  out 
of  a  cluster  might  indicate  that  it  didn’t  belong 
in that cluster, that the user is establishing a new 
cluster  with  new  nearby  documents,  or  nothing 
at  all.  More  implicit  or  explicit  user  input  might 
be needed to accurately represent the user’s actual 
reasoning process. The amount of approximate or 
specific input needed might vary.

These options imply the possibility of many pa-
rameters  for  the  interaction.  Too  much  explicit 
input might pull the analyst out of his or her cog-
nitive  zone.  Analysts  should  be  able  to  focus  on 
the task, not the tool, using interaction to support 
their reasoning process.

This area involves two challenges. First, how can 
the user interface balance explicit and implicit user 
interaction  for  model  feedback?  Second,  how  can 
users  easily  undo  or  revise  direct-manipulation 
interactions?

Multiparameter Interaction
Novel input modalities might offer more powerful 
ways for users to express their complex intentions. 
For  example,  multitouch  interfaces  can  provide 
richer  interaction  for  individuals  and  groups  by 
providing  more  simultaneous  input  points  with 
which to express parameters. For instance, in the 
machine-learning  step,  a  user  could  move  a  data 
point with one hand while specifying target data 
points with the other hand to indicate which simi-
larity  relationships  he  or  she  intends.  The  added 

 

IEEE Computer Graphics and Applications 

11

Table 2. The principles of direct manipulation for information visualization are recast for VA.

Direct manipulation for information visualization1 Direct-manipulation VA
Continuous visual representations of objects 
and actions

Physical actions or button presses instead of 
complex syntax

Rapid, incremental, and reversible actions with 
immediately visible effects

Spatializations provide a common ground between models and cognition.
Users are shielded from the complexity of underlying models and parameters.
Interactions occur in the visual representation.
Interactions are tightly coupled between the spatialization and the underlying models.
Models incrementally learn from interactions throughout the analytic process.
Visual feedback of the updated model is displayed in the visual metaphor.

bandwidth  of  these  multitouch  interactions  can 
more  accurately  define  such  manipulations’  ana-
lytical reasoning.

Large, high-resolution displays can provide more 
area with which to construct spatial relationships. 
They give the analyst real, meaningful space as a 
communication medium and as common ground 
between the human and model. For example, dis-
tances between documents can imply a similarity 
measure,  whereas  the  absolute  location  of  infor-
mation  can  serve  as  a  landmark  for  themes  and 
concepts.  Direct-manipulation  VA  might  be  the 
killer app for these novel hardware technologies.

This  area  involves  two  challenges.  First,  how 
much user input is needed to convey intention to 
the  models?  Second,  how  can  the  system  provide 
real-time visual feedback regarding the interpreted 
actions?

Bias
Model  steering  potentially  introduces  user  biases 
into  visualizations.  Researchers  have  attempted 
to  address  this  challenge.  For  example,  captur-
ing  interaction  data  over  time10  can  reveal  new 
keywords added to the model. The distribution of 
weight  between  these  user-derived  keywords  and 
those extracted from the data might indicate how 
much  the  user’s  domain  expertise  influences  the 
spatialization.

Furthermore,  the  temporal  history  of  keyword 
weighting  can  indicate  trends  in  the  analysis. 
Converging  trends  in  the  weighting  of  entities 
might indicate confirmation bias, whereas diverg-
ing weights might represent an analysis involving 
multiple hypotheses. In particular, it might be pos-
sible to quantify specific biases such as confirma-
tion  bias15  and  alert  users  to  them  in  real  time. 
Biases  are  also  opportunities  to  steer  algorithms 
toward  a  user’s  expression  of  interest,  but  down-
sides  such  as  overfitting  and  missing  other  inter-
esting  insights  could  occur.  Such  data  could  also 
be used to compare multiple analysts’ processes or 
support collaborative methods.

The  challenge  here  is,  how  do  you  illuminate 
the potential bias associated with introducing the 
user’s domain expertise into the model?

Direct  manipulation  is  familiar  to  informa-

tion  visualization  designers,  given  graphical 
controls over direct visual mappings (for example, 
x- and y- axes on scatterplots, dynamic queries of 
value  thresholds,  and  so  on).  However,  as  visual-
izations employ increasingly complex mathemati-
cal models, interaction designers face the challenge 
of maintaining the intrinsic principles that make 
direct  manipulation  successful,  while  adapting  it 
to  control  complex  model  parameters  that  might 
not  clearly  map  to  the  visual  representation.  As 
we  showed,  for  VA,  the  goal  of  providing  direct 
manipulation  isn’t  fully  realized  through  control 
panels for model parameters.

Direct  manipulation  of  the  visual  representa-
tion  itself  (see  Table  2)  will  enable  users  to  test 
hypotheses, discover relationships, and input their 
domain  expertise  into  the  calculations  used  to 
produce  the  view.  Tools  should  strive  to  strike  a 
balance  between  fully  automated  and  fully  man-
ual solutions. In other words, a balance must ex-
ist between cognition and computation in VA. By 
leveraging the information-rich medium of a spa-
tial layout as the primary communication method 
between  the  user  and  system,  researchers  will  be 
able  to  realize  direct-manipulation  VA.  We  hope 
that the research opportunities and challenges we 
presented will help establish a firm science of in-
teraction in VA. 

References
  1.  B. Shneiderman and C. Plaisant, Designing the User 
Interface:  Strategies  for  Effective  Human-Computer 
Interaction, 4th ed., Pearson, 2005.

  2.  C. Andrews, A. Endert, and C. North, “Space to Think: 
Large,  High-Resolution  Displays  for  Sensemaking,” 
Proc.  2010  ACM  Conf.  Human  Factors  in  Computing 
Systems (CHI 10), ACM, 2010, pp. 55–64.

  3.  W.  Wright  et  al.,  “The  Sandbox  for  Analysis: 
Concepts  and  Methods,”  Proc.  2006  ACM  Conf. 
Human Factors in Computing Systems (CHI 06), ACM, 
2006, pp. 801–810.

  4.  F. Shipman and C. Marshall, “Formality Considered 
Harmful:  Experiences,  Emerging  Themes,  and 
Directions on the Use of Formal Representations in 

12 

July/August 2013

Visualization ViewpointsInteractive Systems,” Computer Supported Cooperative 
Work, vol. 8, no. 4, 1999, pp. 333–352.

  5.  F.  Tyndiuk  et  al.,  “Cognitive  Comparison  of  3D 
Interaction  in  Front  of  Large  vs.  Small  Displays,” 
Proc.  2005  ACM  Symp.  Virtual  Reality  Software  and 
Technology (VAST 05), ACM, 2005, pp. 117–123.

  6.  J.A. Wise et al., “Visualizing the Non-visual: Spatial 
Analysis and Interaction with Information for Text 
Documents,”  Proc.  1995  IEEE  Symp.  Information 
Visualization (InfoVis 95), IEEE CS, 1999, p. 51.

  7.  J. Alsakran et al., “Streamit: Dynamic Visualization 
and  Interactive  Exploration  of  Text  Streams,”  Proc. 
2011 IEEE Pacific Visualization Symp. (PacificVis 11), 
IEEE, 2011, pp. 131–138.

  8.  D.H.  Jeong  et  al.,  “iPCA:  An  Interactive  System  for 
PCA-Based  Visual  Analytics,”  Computer  Graphics 
Forum, vol. 28, no. 3, 2009, pp. 767–774.

  9.  J.S. Yi et al., “Dust & Magnet: Multivariate Information 
Visualization Using a Magnet Metaphor,” Information 
Visualization, vol. 4, no. 4, 2005, pp. 239–256.

 10.  A.  Endert,  P.  Fiaux,  and  C.  North,  “Semantic 
Interaction  for  Sensemaking:  Inferring  Analytical 
IEEE  Trans. 
Reasoning 
Visualization and Computer Graphics, vol. 18, no. 12, 
2012, pp. 2879–2888.

for  Model  Steering,” 

 11.  A. Endert et al., “Observation-Level Interaction with 
Statistical  Models  for  Visual  Analytics,”  Proc.  2011 
IEEE  Conf.  Visual  Analytics  Science  and  Technology 

(VAST 11), IEEE, 2011, pp. 121–130.

 12.  E.T. Brown et al., “Dis-Function: Learning Distance 
Functions Interactively,” Proc. 2012 IEEE Conf. Visual 
Analytics  Science  and  Technology  (VAST  11),  IEEE, 
2012, pp. 83–92.

 13.  A.C.  Robinson,  “Design  for  Synthesis  in  Geo-
visualization,”  PhD  thesis,  Dept.  of  Geography, 
Pennsylvania State Univ., 2008.

 14.  S.  Drucker,  D.  Fisher,  and  S.  Basu,  “Helping  Users 
Sort  Faster  with  Adaptive  Machine  Learning 
Recommendations,”  Human-Computer  Interaction—
Interact  2011,  LNCS  6948,  Springer,  2011,  pp. 
187–203.

 15.  R.  Heuer,  Psychology  of  Intelligence  Analysis,  Center 

for the Study of Intelligence, 1999.

Alex  Endert  is  a  visualization  scientist  at  the  Pacific 
Northwest  National  Laboratory.  Contact  him  at  alex.
endert@pnnl.gov.

Lauren  Bradel  is  a  PhD  student  in  computer  science  at 
Virginia Tech. Contact her at lbradel1@vt.edu.

Chris North is an associate professor in Virginia Tech’s De-
partment of Computer Science. Contact him at north@vt.edu.

Contact  department  editor  Theresa-Marie  Rhyne  at 
theresamarierhyne@gmail.com.

Showcase Your 
Multimedia Content 
on Computing Now!

IEEE Computer Graphics and Applications 
seeks computer graphics-related 
multimedia content (videos, animations, 
simulations, podcasts, and so on) to 
feature on its Computing Now page, 
www.computer.org/portal/web/
computingnow/cga.

If you’re interested, contact us at 
cga@computer.org. All content will be 
reviewed for relevance and quality.

 

IEEE Computer Graphics and Applications 

13

",False,2013.0,{},False,False,journalArticle,False,HVRY632L,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/6562729/,,Beyond Control Panels: Direct Manipulation for Visual Analytics,HVRY632L,False,False
VWK8HC38,6XA248CP,"1

604

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,

2 DE EM

C

BER 2014

Knowledge Generation Model for Visual Analytics

Dominik Sacha, Andreas Stoffel, Florian Stoffel, Bum Chul Kwon, Member, IEEE,

Geoffrey Ellis and Daniel A. Keim, Member, IEEE

Computer

Data

Visualization

Action

Human

Hypothesis

Knowledge

Model

Finding

Exploration 

Loop

Insight

Veriﬁcation 

Loop

Knowledge 
Generation 

Loop

Fig. 1: Knowledge generation model for visual analytics: The model consists of computer and human parts. The left hand side
illustrates a visual analytics system, whereas the right hand side illustrates the knowledge generation process of the human. The
latter is a reasoning process composed of exploration, veriﬁcation, and knowledge generation loops. Visual analytics pursues a tight
integration of human and machine by enabling the user to interact with the system. These interactions are illustrated in Figure 2.

Abstract—Visual analytics enables us to analyze huge information spaces in order to support complex decision making and data
exploration. Humans play a central role in generating knowledge from the snippets of evidence emerging from visual data analysis.
Although prior research provides frameworks that generalize this process, their scope is often narrowly focused so they do not
encompass different perspectives at different levels. This paper proposes a knowledge generation model for visual analytics that ties
together these diverse frameworks, yet retains previously developed models (e.g., KDD process) to describe individual segments of the
overall visual analytic processes. To test its utility, a real world visual analytics system is compared against the model, demonstrating
that the knowledge generation process model provides a useful guideline when developing and evaluating such systems. The model is
used to effectively compare different data analysis systems. Furthermore, the model provides a common language and description of
visual analytic processes, which can be used for communication between researchers. At the end, our model reﬂects areas of research
that future researchers can embark on.
Index Terms—Visual Analytics, Knowledge Generation, Reasoning, Visualization Taxonomies and Models, Interaction

1 INTRODUCTION
Visual analytics research made great strides in the past decade with
numerous studies demonstrating successes in helping domain experts
explore large and complex data sets. The power of visual analytics
comes from effective delegation of perceptive skills, cognitive reason-
ing and domain knowledge on the human side and computing and data
storage capability on the machine side, and their effective coupling
via visual representations. Thus far, many application papers have
tested and veriﬁed different ways to instigate this human and machine
collaboration to reveal hidden nuggets of information.

Recent work emphasizes that visual analytics theories must go be-
yond “human in the loop” to “human is the loop” thinking in order to
recognize and integrate human work processes with analytics (Endert et
al. [7]). To achieve this goal, system, human, cognition and reasoning
based theories have to be considered. Many theoretical works have also
examined the role of visual analytics tools in data analysis, decision
making and problem solving. Visual analytics processes span from
humans high-level analytic works using their domain knowledge to
low-level activities such as interacting with tools. Many prior works

• Data Analysis and Visualization Group, University of Konstanz. E-mail:

forename.lastname@uni-konstanz.de

2014; date of current version

Manuscript received 31 Mar. 2014; accepted 1 Aug. 2014. Date of publication
11 Aug.
For information on obtaining reprints of this article, please send
e-mail to: tvcg@computer.org
Digital Object Identiﬁer 10.1109/TVCG.2014.23464 18

9 Nov.

2014.

investigated different levels of activities with regards to humans cog-
nition models (e.g., Green et al. [14], Kwon et al. [17]); interaction
models/taxonomies (e.g., Brehmer and Munzner [4], Norman [25]);
process models (e.g., Card et al. [5], Fayyad et al. [8]); sensemaking
models (e.g., Pirolli and Card [29]). We initially sought interaction
taxonomies that describe the aforementioned models. However, we
lack an overarching pipeline that connects all the dots. Previously
developed models (e.g., Keim et al. [15, 16]) are system-driven and are
not describing in detail the human reasoning part in the visual analytics
process. In particular, we have little idea how analytical components
support knowledge generation processes and how analysts’ intents drive
the insight collection action forward. It would be valuable for future
research to have an integrated framework of all processes and models
relevant for knowledge generation with visual analytics.

The paper aims to take the ﬁrst step to establish the knowledge gen-
eration model for visual analytics. First, we build our initial process
model by signiﬁcantly extending the previous models [15, 16] (Section
2). Comparing with previously developing models and theories, we
show how our model ﬁts various kinds of models (Section 3). Using
our model, we ﬁnd areas that existing visual analytics tools can im-
prove (Section 4). We discuss our model showing open issues, model
implications and future work (Section 5).

Our contributions can be described as follows. The model presented
in this paper provides a high-level description of the human and com-
puter processes within visual analytics systems which facilitates an
understanding of the functionality and interaction between the compo-
nents. This can be used in the design of new visualisation applications

1077-2626 ß 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.

See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

SACHA

ET AL.: KNOWLEDGE GENERATION MODEL FOR VISUAL ANALYTICS

1

605

or in the evaluation of existing ones in terms of how their subcom-
ponents support humans reasoning, decision making, and knowledge
generation processes.

2 KNOWLEDGE GENERATION MODEL
Visual analytics uses data to draw conclusions about the world, a pro-
cess, or an application ﬁeld. It is a structured reasoning process that
allows analysts to ﬁnd evidence in data and gain insights into the prob-
lem domain. Our model of the knowledge generation process is based
on the visual analytics process of Keim et al. [15, 16] and describes how
knowledge is generated with this process. Pohl et al. [30] also discussed
relevant theories that should be considered, besides a computer/system
based view of the analytical process. They show that visual analyt-
ics system components, analytical procedures and human perceptual
and cognitive activities and especially the interplay between these ele-
ments, lead to a complex process. The theories, namely sensemaking,
Gestalt theories (describing problem solving), distributed cognition
(interplay between humans and artifacts), graph comprehension the-
ories (derive meaning from graphs/process visual information) and
skill-rule-knowledge models (three-fold hierarchy of processing levels)
are highlighted as important aspects of the visual analytics process.
In the following sections, we deﬁne and relate common elements of
the aforementioned models/ﬁelds/concepts in order to arrive at a better
understanding of visual analytics in terms of computing and human
processes.

Our visual analytics model (see Figure 1) is split into two parts. The
computer system with data, visualization and analytic models, and the
human component modeling the cognitive processes associated with
an analytical session. The cloud in the model indicates that there is no
clear separation between the computer and human part, as both parts
are required for data analysis. Computers miss the creativity of human
analysis that allows them to create surprising, often subtle or hidden
connections between data and the problem domain. Humans are not
able to deal efﬁcient and effectively with large amount of data. In visual
analytics the connection between the human and computer uses the
humans interaction abilities and perception.

Knowledge generation in visual analytics comprises of abductive,
deductive, and inductive reasoning processes. For a detailed deﬁni-
tion and discussion of these reasoning processes see Peirce [27] and
Magnani [21]. Abductive reasoning processes formulates hypotheses
from observations that are unexpected or cannot be explained with
existing knowledge. Assuming that these hypotheses are true, expecta-
tions of effects, patterns, or relations in the analyzed data are deduced.
Through interactions with visual analytics systems, analysts try to ﬁnd
evidence and detect patterns in data to verify or falsify the hypotheses,
which is an inductive reasoning process. We decided to model human
cognitive processes with loops, because analysis does not follow de-
terministic rules but is rather chaotic or spontaneous nature. Analysts
are often working on different hypotheses, tasks, or ﬁndings and can
consequently be working on several loops in parallel.

2.1 Computer
2.1.1 Data
The starting point of all visual analytic systems is data. Data describes
facts [8] in structured, semi-, or unstructured manner. It must be repre-
sentative and related to the analytical problem, otherwise, the analytical
process is unlikely to reveal meaningful relationships in the problem
domain. In the visual analytics process, data creation, gathering and
selection processes often determines the quality of the data. During an
analysis, additional data can be created by automatic methods (e.g., clus-
tering or classiﬁcation) or by manual annotations. Hence, provenience
information about data containing details about creation, gathering, se-
lection, and preprocessing is important to estimate the trustworthiness
of analysis results (see also [31]). The term metadata describes second
order data or “data about data”. The usage of this term is ambiguous
depending on the domain and interests of users. In addition to prove-
nience data, metadata describing the structure of data is usually handled
by visual analytics system to access and display data appropriately.
This sort of metadata is usually not subject to analysis as it describes

mainly data formats, which is necessity of any kind of data. The term
metadata is also often used for data describing or summarizing other
data, e.g., keywords or topics for documents or images. In the scope of
data analysis this descriptive metadata is treated similar to data and we
see no beneﬁt or reason to treat this data differently, therefore the term
data also includes this sort of metadata.

2.1.2 Model
Models can be as simple as descriptive statistics describing a property of
a subset of the data or as complex as a data mining algorithm. The KDD
process leads to models from data (see Figure 3). These models serve
different purposes in the visual analytics process. Simple analysis tasks
might be solved by calculating a single number that conﬁrms or rejects a
preconceived notion/expectation. For instance, a statistical test can lead
to a conclusion whether or not to trust a hypothesis. Complex patterns or
abstractions found by data mining methods can be used in visualizations
by showing, for instance, clustering or classiﬁcation results visually. In
addition, the automatically created model can be analyzed or visualized
to derive insights. For example, logistic regression models learn weights
of features, which can be used to identify most important features or
feature combination.

2.1.3 Visualization
A different path from data to knowledge is the information visualiza-
tion pipeline using data visualizations (see Figure 3). Visualizations
use data or models generated from the data and enable analysts to
detect relationships in the data. In visual analytics, visualizations are
often based on automatic models, for instance, clustering models are
used to group data visually. Also, a model itself can be visualized,
for instance, a box plot shows the data distribution of a dimension.
Visualization methods for a model may vary depending on the state of
the visualization. For example, in semantic zooming a visualization
might use different properties of a model depending on the zoom level.
Visualization is often used as the primary interface between analysts
and visual analytics systems whereas understanding the model often
requires more cognitive efforts.

2.2 Exploration Loop
The exploration loop describes how analysts interact with a visual an-
alytics system in order to generate new visualizations or models and
analyze the data. Analysts explore data supported with the visual ana-
lytic system by interactions and observing the feedback. Actions taken
in the exploration loops are dependent on ﬁndings or a concrete analyt-
ical goal. In case a concrete analysis goal is missing, the exploration
loop becomes a search for ﬁndings, which may lead to new analytical
goals. Even when the exploration loop is controlled by an analysis goal,
the resulting ﬁndings are not necessarily related to it but can lead to
insights solving different tasks or opening new analytical directions.

2.2.1 Action
Different meanings of actions are present in the InfoVis community
as they may be deﬁned with different granularities or with differing
relations. Pike et al. [28] illustrate that actions may concern user goals
and tasks on the one hand and interactive visualizations on the other
hand. According to recent interaction taxonomies (e.g., Brehmer and
Munzner [4]), simple interactions, How, are related to higher level
concepts, Why.

In our deﬁnition, actions refer to individual tasks that generate tangi-
ble, unique responses from the visual analytics system. For instance, a
task can be to visualize a particular data property or calculate a model of
a relationship in the data. Actions derived from hypotheses are usually
complex actions, for instance, use a speciﬁc visualization method that
has the potential to show interesting data. Actions derived from ﬁndings
are normally simple actions, such as changing the visual mapping of
a visualization or selecting a different parameter for model building.
In visual analytics, analysts freely choose between visualization and
modeling or a combination of both for their actions. Actions naturally
provoke interactions of analysts with visual analytics systems.

1

60

6

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,

2 DE EM

C

BER 014

2

 
l
a
u
s
i
v

i

g
n
p
p
a
m

preparation

DataData

g

uildin
b
el 
d
o
m

Human

Action

m
m

o
o

d
d

e
e

l
l
-
-

v
v

i
i

s
s

 
 

m
m

a
a

p
p

p
p

i
i

n
n

g
g
g

m
o
d
e

l

 

u
s
a
g
e

manipulation

Visualization
Visuualization

o
b
s
e
r
v
a

t
i

o
n

Model

observation
inspection

Finding

Fig. 2: Detailed part of the process model including action and cognition paths. Actions can either lead directly to visual analytic components
(blue arrows) or to their mappings (blue, dashed arrows). Humans can observe reactions of the system (red arrows) in order to generate ﬁndings.

We name actions dealing with data gathering or data selection as
preparation action because these actions are used to prepare data for
the visual analytics process (Figure 2). Actions taken to create models
are summarized as model building actions which in turn are related
to the KDD process and its conﬁguration. Application of a model is
termed as model usage, which refers to actions such as calculating a
statistic or cluster data. Visual mapping actions are used to create data
visualizations, and model-vis mapping actions are those which map
models into visualizations. Manipulation of a visualization changes
the viewport or highlights interesting data in the visualization without
changing the mapping of data to the visualization. All these actions
are observable as interactions between analysts and a visual analytics
system.

2.2.2 Finding
A ﬁnding is an interesting observation made by an analyst using the
visual analytics system. The ﬁnding leads to further interaction with the
system or to new insights. For example, ﬁndings from data inspection
can be missing values or other data properties affecting the further anal-
ysis and require special data processing. In the case of visualizations
or models, a ﬁnding can be a pattern, a conspicuous model result, or
an unusual behavior of the system. Bertini et al. state that “a pattern is
made of recurring events or objects that repeat in a predictable manner.
The most basic patterns are based on repetition and periodicity.” [2, p.
13] Patterns in data can be detected with automatic analytical methods
or humans may detect patterns using their visual perception and cogni-
tion skills. Findings are in principle not limited to data, visualizations,
or models but comprise of anything interesting to an analyst, e.g., an
unexpected high number or a word or statement in some text.

In our deﬁnition, a ﬁnding is independent from the problem domain,
however to make an analytical use of a ﬁnding, the analyst has to
interpret them in the context of the problem domain. Although ﬁndings
are usually associated with detecting a visual pattern, the lack of a
pattern, when expected by an analyst, is also considered a ﬁnding. As
shown in Figure 1, ﬁndings do not necessarily lead to new insights (see
Section 2.3.2) but may trigger basic actions, such as manipulating the
viewport of a visualization to show a region in more detail.

Analysts come across ﬁndings by observing the feedback from the
system or examining visualizations and models, which in turn can
lead to further actions. The exploration loop can be characterized
as a searching activity by using the system to reveal useful ﬁndings
to solve an analysis task. Analysts might frequently change their
exploring strategies and switch between models and visualizations
to collect different ﬁndings. The strategies and analysis directions in

the exploration loop are guided by an exploration goal deﬁned in the
veriﬁcation loop. The actions and ﬁndings in the exploration loop are
closely related to the visual analytics systems. Analysts gain a new
insight when they are able to understand the ﬁndings and are able to
interpret them in the context of the problem domain.

2.3 Veriﬁcation Loop
The veriﬁcation loop guides the exploration loop to conﬁrm hypotheses
or form new ones. To verify a concrete hypothesis, a conﬁrmatory
analysis is conducted and the exploration loop is steered to reveal
ﬁndings that verify or falsify the hypothesis. Analysts gain insights
when they can interpret ﬁndings from the exploration loop in the context
of the problem domain. Insights may lead to new hypotheses that
require further investigation. Analysts gain additional knowledge when
they assess one or more trustworthy insights.

Findings during exploration my contribute to the veriﬁcation of a
concrete hypothesis but insights emerging from exploration may not be
related to the examined hypothesis. It is often the case that new insights
solve different analysis questions or open up new ones.

2.3.1 Hypothesis
Hypotheses play a central role in the visual analytics process. An
hypothesis formulates an assumption about the problem domain that
is subject to analysis. Analysts try to ﬁnd evidence that supports or
contradicts hypotheses in order to gain knowledge from data. In this
respect the visual analytics process is guided by hypotheses. Concrete
hypotheses can be tested with statistical tests or data visualizations
when the data contains the necessary information. Unfortunately, hy-
potheses are often vague, such as the assumption that there are unknown
factors that have an inﬂuence on the problem domain. In such cases
an exploratory analysis strategy allows analysts to come up with more
concrete hypotheses that are used for further analysis.

2.3.2 Insight
In the InfoVis community, insight has a variety of deﬁnitions. Saraiya
et al. deﬁne insight as “an individual observation about the data by the
participant, a unit of discovery” [33, p. 2]. North [26] takes another
view and lists some important characteristics of an insight such as being
complex, deep, quantitative unexpected and relevant, and going beyond
an individual observation of the data. Yi et al. [43] go one step further
and also consider the processes that involve insights. They focus on
how people gain insight in information visualization and identify four
types of insight-gaining processes, however, they argue that there is no
common deﬁnition of insight. Chang et al. [6] suggest two different

SACHA

ET AL.: KNOWLEDGE GENERATION MODEL FOR VISUAL ANALYTICS

1

607

Computer

View

Transformations

Visual

Structures

Views

Interaction
Taxonomies

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:1)(cid:1)

(cid:1)(cid:1)(cid:1)

Visualization

D
L
R
O
W
E
H
T

 

Model

InfoVis Pipeline

Visual

Mappings

Data

Transformations

Raw 
Data

Data 
Tables

Data

Data

Selection

Target Data

Preprocessing

Preprocessed

Data

KDD Process

Transformation

Tranformed

Data

Patterns

Data Mining

Human

Sensemaking 

Loop

Action

Hypothesis

Execution
Stages of Interaction
Evaluation

Goals

Finding

Exploration 

Loop

Insight

Veriﬁcation 

Loop

Knowledge

Knowledge 
Generation 

Loop

Fig. 3: Relating the process model for knowledge generation in visual analytics to other models and theories. Similarity is illustrated by color and
position. A detailed illustration of interactions between the human and computer is shown in Figure 2.

deﬁnitions of insight: the cognitive science insight as a moment of
enlightenment, an “Ah Ha” moment, which can occur spontaneously,
and an advance in knowledge or a piece of information. Our deﬁnition
is closer to the latter, where the analyst interprets a ﬁnding, often with
previous domain knowledge, to generate a unit of information. Hence,
an insight can be quite small, such as realizing that there is a relation
between several properties of the data, to something more important
and potentially signiﬁcant. So, insights are different from ﬁndings in
the sense that insights have an interpretation in the problem domain,
what we not required for ﬁndings. For instance, a ﬁnding might support
a hypothesis, which may convince the analyst and lead to the insight
that the hypothesis is reliable. An analyst gains insights by collecting
enough evidence to create a new hypothesis about the application or,
in the case of very strong evidence, even new trusted knowledge. We
consider an insight not directly as knowledge, because weak evidence
might lead to an insight that needs further veriﬁcation and becomes a
hypothesis. For instance, ﬁnding a cluster in a visualization during an
exploratory analysis might lead to the insight that there is a cluster with
different properties, but this insight should at ﬁrst be considered as a
new assumption or hypothesis that has to be validated.

2.4 Knowledge Generation Loop
Analysts form hypotheses from their knowledge about the problem do-
main and gain new knowledge by formulating and verifying hypotheses
during the visual analytics processes. When analysts trust the collected
insights they gain new knowledge in the problem domain that may also
inﬂuence the formulation of new hypotheses in the following analysis
process.

2.4.1 Knowledge
Data analysis usually starts with data and one or more analysis ques-
tions. In addition, analysts bring in their knowledge about the data, the
problem domain, or visual analytic tools and methodology. This prior
knowledge determines the analysis strategy and procedure. During
the visual analytics process analysts try to ﬁnd evidence for existing
assumptions or learn new knowledge about the problem domain. In gen-
eral, knowledge learned in visual analytics can be deﬁned as “justiﬁed
belief” [2]. The reasoning processes in visual analytics enables analysts
to gain knowledge about the problem domain from evidence found in
data. The evidence has different qualities, which directly affects the
trustworthiness of the concluded knowledge. The evidence collection
route also impacts the trustworthiness. For example, an outcome of a
statistical test of an hypothesis may be perceived more trustworthy than

a pattern found in a visualization. Depending on the collected evidence,
an analyst has to decide whether enough evidence was collected to trust
an insight and accept it as new knowledge or whether it is in need of
further examination, e.g., analysis with different data or discussions
with domain experts. Assessing the trustworthiness of new knowledge
requires a critical review of the overall analysis process starting from
data gathering. As well as new knowledge about the problem domain,
analysts gain knowledge about data, e.g., patterns in the data or its
quality. They also gain experience with the visual analytics systems
and methodology.

3 RELATION TO OTHER MODELS
This section covers the most important related models that have inﬂu-
enced the model presented in the paper and also aims to offer a big
picture of the whole knowledge generation process in visual analytics.
Figure 3 illustrates related models and our knowledge generation model
for visual analytics. We have used the same color codes of the original
model [15, 16] to highlight related areas in our model. Feedback loops
of other models (e.g. InfoVis pipeline) are replaced by our extensions
on the human side. Each action results in a feedback loop via ﬁnding
that may lead to new exploration loops or crosses over to higher level
loops. Relevant theories are grouped in three areas according to their
focus on interaction, human aspects or systems.

3.1 Systems
Card et al. propose the reference model for information visualization
[5] that describes visualizations as data connected to visual mappings
perceived by humans. The InfoVis-Pipeline contains the main com-
ponents of Raw Data, Data Tables, Visual Structures and Views and
transformations/mappings between these components which can be
manipulated through Human Interactions.

Fayyad et al. [8] describe the Knowledge Discovery Process in
Databases (KDD) as follows. KDD is the process of making sense
out of data using data mining techniques at the core. The process
includes the mapping of low-level data into more compact, abstract
or useful forms using data mining models with the goal to discover or
extract patterns that can be turned into knowledge. The KDD process
consists of nine steps and represents an interactive and iterative process.
Examples for interactions are selecting and ﬁltering for the relevant data,
choice of appropriate data mining methods or parameter reﬁnement.
The goal of these actions is to bring in the domain expertise of the
human in order to ﬁnd meaningful patterns. At each step the user might
make and add decisions that cannot be handled automatically. The

1

60

8

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,

2 DE EM

C

BER 014

2

al. [2]), rather than all possible interactions in visual analytics. Recent
publications attempt to structure interaction using the dimensions of
Why (the purpose of the task), How (methods used to achieve this) and
What (necessary inputs/outputs) (Brehmer and Munzner [4], Schulz et
al. [34]). There are also recent taxonomies that try to integrate existing
taxonomies out of different ﬁelds into a sound interaction taxonomy
for visual analytics (e.g. von Landesberger et al. [41]). Taking the
taxonomy of Brehmer and Munzner [4] as an example there are higher
level goals that an analyst might pursue (why) and lower level opera-
tions that can be performed (how) on a speciﬁc target (what). High and
low level interactions are both covered by the action concept. However
they can be distinguished when considering the associated loops. High
level actions are inspired by the veriﬁcation loop and are based on
insights and hypotheses whereas low level actions are inﬂuenced by the
exploration loop that covers a sequence of simple actions and ﬁndings.
Our process model implies that each action results in a feedback loop
and is perceived and processed by the human. Our model description
of action-types also shows the relevant interactions for visual analytics
that are both visualization and model centric.

Also mentionable are two mantras characterizing the analysis pro-
cess for information visualization and visual analytics because they
shed light on human analysis strategies. Shneiderman proposes the
Visual Information Seeking Mantra that summarizes the basic princi-
ples of many visual design guidelines [36]: Overview ﬁrst, zoom and
ﬁlter, then details-on-demand. Also Keim proposes a slightly different
Visual Analytics Mantra [16]: Analyse First - Show the Important -
Zoom, Filter and Analyse Further - Details On Demand. Both start
with an overview/aggregation approach and end in a reﬁnement of their
hypothesis and analysis.

3.3 Human Cognition, Sensemaking and Reasoning
Humans can observe visualization or model changes that can be used
for the knowledge generation process. The data may also be inspected
directly.

Pirolli and Card present the Sensemaking Process [29] as a descrip-
tion of intelligence analysis. Central terms are the shoebox, schemas
(that are a set of patterns around the important elements in the tasks),
hypotheses and a representation. Sensemaking tasks are described as
processes consisting of information gathering, the information represen-
tation as a schema, the generation of insight and ﬁnally the generation
of some knowledge product. The ﬁrst loop is called the foraging loop
followed by the sensemaking loop. Bottom-up or top-down-procesess
are possible. That indicates that the model does not have a ﬁxed entry
point which depends on the type of task. The model shows the data and
process ﬂow with many feedback loops. Our model splits the Sensemak-
ing Loop in three sub loops. In visual analytics it is also possible that
the system may learn from the analysts actions allowing the system to
support the user with visualization or action propositions. That is why
the loop is leading through the system (see Figure 3). Hypotheses are
generated as an entry point for an analysis process leading to repeated
exploration cycles.

The ﬁeld of reasoning and decision making both depend on the con-
struction of mental models or scenarios of relevant situations. Accord-
ing to Legrenzi et al. [19] the key components of decision making are
Information Seeking, Making Hypotheses, Making Inferences, Weigh-
ing Advantages and Disadvantages and Applying Criteria to make
Decisions.

The Human Cognition Model (HCM) proposed by Green et al. [14]
can also be applied to visual analytics. A major problem in visual ana-
lytics is that human cognition is often assumed to be an over simpliﬁed
black box. Information discovery and knowledge building are at the
core of the HCM. Information is presented by the computer that humans
can perceive and directly interact with in order to focus their attention.
The process of discovery of patterns or relations is a primary stage of
knowledge that can be created within the knowledge building process.
The computer works to counter the humans limited working memory as
well as some cognitive biases, such as conﬁrmation bias. Central parts
of the HCM are shown in Figure 5. The HCM also includes guidelines
for discovery and knowledge building. The cognition of relevant pat-

Fig. 4: Van Wijks model including Green et al.’s changes [13, 14] and
labels.

KDD process steps are included in Figure 3.

As a basis for our knowledge generation model for visual analytics
we take and extend the process model by Keim et al. [15, 16]. The main
components of the model are Data, Models, Visualization and Knowl-
edge each representing a stage of the process. Also a Feedback loop is
present going from knowledge back to data. At and in between of each
stage there are transitions. The visual analytics process consists of the
two parts, Visual Data Exploration going from data via visualization to
knowledge and Automated Data Analysis going from data via models
to knowledge, as well as of their connection. Actions on the data may
be performed in order to select, ﬁlter or preprocess the data. The data
is then mapped to a visualization (visual mapping) and to models (data
mining). Model parameters and ﬁndings are visualization and the user
may interact with the visualization or reﬁne parameters of the model
in order to steer the analysis process. Knowledge can be derived from
visualizations, automatic analysis and preceding interactions between
visualizations, models and the human.

Figure 3 relates visual analytics components to the InfoVis and
KDD system pipelines. The InfoVis pipeline corresponds to data,
visualization, and the visual mappings, whereas the KDD process
model is represented by data, model, and their mapping. Furthermore
the visual analytics process model includes human computer interaction.
Actions can be performed at each stage, namely data, visualization,
model and their mappings.

The economic model of visualization by van Wijk describes contexts
in which visualizations operate [40] (Figure 4). In brief, data is trans-
ferred into a visualization that can be perceived by a human through an
image. Based on perception, the human generates knowledge over time
which drives interactive exploration through changes to the visualiza-
tion speciﬁcation. Green et al. [14] add to van Wijk model, arguing that
perception has an important role in interactive exploration and that the
act of exploration and associated reasoning often leads to knowledge
acquisition. These relate to the exploration and knowledge generation
loops of our model.

3.2 Interaction
Norman describes a model for actions containing Seven Stages of
Action [25]. At the beginning of each action each human needs a goal
to be achieved. Afterwards, the human has to perform an action to
manipulate something. The last step is to check if the goal was achieved.
These two subprocesses are called Execution and Evaluation. Based
on previous actions the action cycle is traversed several times. The
model also explains two major problems that occur when interacting
with computer systems. The Gulf of Execution indicates if humans
do not know how to perform an action while the Gulf of Evaluation
indicates that humans are not able to evaluate the result of their actions.
The Goal concept of the stages of interaction model matches to our
Hypothesis as a starting point. The Execution path is leading from Goal
via an action to the World. As a result analysts evaluate observations of
the World in several steps (see Figure 3).

Several interaction taxonomies exist which focus on different as-
pects, ﬁelds and domains and attempt to structure different kinds of
interaction at different levels of abstraction. Most of the well-known
taxonomies focus on one or two ﬁelds, e.g. visualizations (Shneider-
man [36]), reasoning (Gotz et al. [12]) or data processing (Bertini et

SACHA

ET AL.: KNOWLEDGE GENERATION MODEL FOR VISUAL ANALYTICS

1

609

In general, though, this mapping is done as the developers designed it,
and does not allow the user to change much.

Visualization manipulation capabilities are mostly used to highlight
document instances. There are also possibilities of manipulating and
changing the visual appearance of a view. For example, in the Circular
Graph View, single terms can be selected, and the connections of this
term to all others are displayed.

In this examination, we show that Jigsaw supports all actions we
proposed in our model. It allows the visualization of different models
based on the same data set, the visual mapping can be adjusted as
designed by the authors, and the model-vis mapping can be modiﬁed to
ﬁt different analytical questions. In order to achieve a stronger coupling
between the visualization and the underlying model, the interactions
could be extended to modify the underlying model or algorithm param-
eters, for example when the user moves a document to another cluster
in the Document Cluster View.

4.2 Knowledge Generation in Jigsaw
Having examined what actions are supported by Jigsaw, we now focus
on an evaluation of the three loops, which are part of the reasoning
process as deﬁned in Section 2.

Undoubtedly, describing the human reasoning processes using a
visual analytic system is complicated. This involves a variety of aspects
from perception, cognition and reasoning. The detailed description
and evaluation of these processes exceed the scope of this paper and
requires a study with expert users. We therefore limit this examination
to a description of how Jigsaw supports human reasoning.

The exploration loop is broadly supported by Jigsaw, by providing a
number of specialized visualizations for different analytical questions.
For example, users can explore given data sets, to get an impression
of the contained topics, by opening the Document Cluster view which
gives a labeled view of document clusters. It is also possible to modify
the number of generated clusters while exploring a once-generated
cluster view. The history of consecutive runs of the clustering algorithm
with different parameters is stored. This allows for assessment of
parameter changes with respect to topic changes. All views include a
bookmark feature, which can be used to switch between different saved
states of the visualization. As a result, users can capture and annotate
ﬁndings that might be of further interest or of high importance for
further analysis. Bookmarks can be used to store the result of different
runs of the exploration loop, which in turn is done by utilizing the
actions as described in the previous Section 4.1.

The veriﬁcation loop, tightly integrated with the exploration loop,
guides users to develop ﬁndings into insights. These ﬁndings from the
exploration loop can be used to verify or falsify a concrete hypothesis,
which is the beginning of knowledge generation. The second investiga-
tive scenario given by G¨ork et. al. in [9] contains some questions which
can only be answered by using the veriﬁcation loop. In one of the
examples, Jigsaw is used to examine the sentiment of car reviews. This
is done to verify the car rating, which is given as a numeric score. The
sentiment, displayed in the Document Grid, is expected to agree with
the score in a way, that highly rated cars have more positive reviews
and those with bad ratings have more negative reviews. In this example,
the positive correlation of two measures is used to verify a hypothesis,
which had been inferred based on product reviews contained in the
analyzed data set. Natively, Jigsaw does not support the concepts of
ﬁndings nor hypotheses. Although, the Tablet View displays book-
marked states of visualizations that can be organized, annotated and
connected manually. This can be used to structure ﬁndings, derive
insights, and connect insights to hypotheses which can be added to the
view and reﬁned by the user during the analysis process. This must be
done manually, because Jigsaw does not provide any automated support
of this process. Figure 6c shows an example for hypotheses validation
using the Tablet View.

Support for the knowledge generation loop is challenging, because
knowledge generation is a process done entirely by the user, and in-
volves concepts like trust or reasoning. In addition to these human
factors, user’s domain knowledge plays an important role, which is
hard to incorporate, because it is difﬁcult to externalize. Depending

Fig. 5: Human Cognition Model by Green et al. [13, 14].

terns and schema that can be derived from insights (veriﬁcation loop)
and ﬁnally to some knowledge product (knowledge generation loop)
plays a central role in the human part of our model. All elements of the
HCM can also be found in the knowledge generation model. The key
element Discovery describes the overall process, whereas the other sub-
concepts all can be placed into our model and related to our concepts.
For example, the generation and analysis of hypotheses is a central
part of our model, because evidence (proof/disproof) is collected as the
basis for drawing conclusions.

4 MODEL APPLICATION
In this section we illustrate that our model can be applied to systems
on several levels. Firstly, the interaction possibilities can be examined
according to our deﬁnition of actions in Section 2.2.1 and shown in
Figure 2. Secondly, we can check if and how a system supports each of
the individual loops. Thirdly, we can use our model for a comparative
assessment. In the following we will demonstrate a detailed model
application with Jigsaw [9, 39] and a comparative high level assessment
of systems from different application domains.

4.1 Actions in Jigsaw
In the following section, we investigate Jigsaw in terms of supported
actions. It is important to note that in our model each of these actions
leads to its own feedback loop. Therefore, we pay special attention
to system components that are capable of accepting human inputs
beyond fully automated implementations. We also highlight some areas
of the system that might accept more user input for amplifying user
interactions.

Data preparation is not fully supported. While loading the data, the
user has no possibility of adjusting any of the data preprocessing or
transformation steps. It is also not possible to change those once the
data has been parsed and loaded. Users may ﬁnd missing data, but such
manipulation must be done outside Jigsaw.

Model building is done automatically, but some views provide possi-
bilities of adjusting the underlying models. For example, the Document
Cluster View allows users to select documents as a cluster seed, which
can be used when the document clusters are computed. After that, the
user-generated cluster seeds become part of the model used by the
application.

In terms of the visual mapping, Jigsaw offers some basic functionali-
ties. In some views, users can select the background of the visualization
or choose which attribute is mapped to the color of a visualization entity.
Taking the color mapping as an example, there are a number of different
possibilities to deﬁne such mappings, like a non-linear color scale, or
the selection of the mapped colors. In this application, the authors left
out any of those manipulation possibilities, and force the user to accept
their pre-deﬁned color mapping schemes.

Model usage is supported implicitly, because most of the visual-
izations require special models. In some views, like the Document
Grid, there are different document orderings and similarity measures to
choose from. This is an example of a per-visualization model usage,
which can be adjusted by the user.

The model-vis mapping is available in almost all views, but only at a
basic level. This includes various ordering, sorting and ﬁltering options.

1

6 0

1

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,

2 DE EM

C

BER 014

2

(a)

(b)

(c)

Fig. 6: Illustration of validation steps using Jigsaw. In 6a left, the person Daniel Keim (left list) is connected to the concept of text, displayed on
the right. To ﬁnd evidence for this fact, the Document Cluster View of the publications is opened (6b). After inspection of the cluster labels, a
document from the visualizing,interaction,text cluster is examined in detail. This document is evidence that the fact presented in the list view in
6a is true. An example for the Tablet View can be seen in 6c.

on the nature of the problem, the required kind and necessary level
of support varies. An example would be the automatic search and
suitable presentation of further evidence for a given hypothesis, which
has been already veriﬁed, but is not yet trusted enough to qualify as
knowledge. If the Tablet View has been used during the veriﬁcation
loop to externalize the analysis process, the view can be helpful in the
knowledge generation loop too.

We have showed that Jigsaw supports the three feedback loops which
are part of the inductive reasoning process, and the amount of possible
automated support varies.

Starting with the exploration loop, automated support is based mainly
on predeﬁned models, and is therefore limited by the analytic possibili-
ties of the system. Going a step further to the veriﬁcation loop, it gets
harder to provide adequate automated support. The variety of different
hypothesis types is an important reason for this. It is even more difﬁcult
to extend a system to support the knowledge generation loop, due to the
increasing inﬂuence of human and other external factors, which cannot
easily be learned and represented by computers. This makes it difﬁcult
to incorporate them in an automated process supporting knowledge
generation. Further implications of loop automation are discussed in
Section 5.3.

Jigsaw

Knime

D

D

D

V

V

M

M

V

M

EL

VL

KL

EL

VL

KL

D

D

V

M

V

M

EL

VL

KL

EL

VL

KL

Tableau

HARVEST

Legend:

weak

basic

strong

Fig. 7: Comparative model application to different kinds of systems.
Jigsaw represents visual analytics, Knime data mining, Tableau infor-
mation visualization, and HARVEST applications from the provenance
domain. Strength of support for functionalities/components of our
model is indicated by the weight of the lines.

4.3 Comparative System Assessment
In this section, we provide a high level assessment of different sys-
tems from data mining, information visualization, visual analytics and
provenance domain. The different natures of these systems illustrate
the general applicability of our model on applications that deal with
data, provide visualizations, and are designed to generate knowledge.
The comparison is shown in Figure 7.

At ﬁrst, we assess Knime [1](Version 2.9.1), which supports the
interactive creation and execution of data mining pipelines. Data prepa-
ration and inspection, model building, model observation, and the
model-vis mapping support is excellent, which is a good foundation
for the exploration loops. The available data visualizations are on a
basic level and allow brushing interactions only. Besides that, Knime
provides no explicit support for the veriﬁcation and knowledge gener-
ation loops, because there exist no tools to organize ﬁndings, derive
insights, or connect hypothesis with insights. As a substitute, separate
paths for the organization of ﬁndings and hypotheses can be added
to an already existing pipeline. This is a possible way of recording
the actions leading to a pipeline outcome, which can contribute to the
knowledge generation process.

Next, we assess Tableau Desktop (Version 8.2 PE), which offers
a number of visualizations that can be interactively adjusted by the
user. Tableau is a representative of applications from the information
visualization domain. Therefore, it has strong support for model-vis
mapping, which is also the case for data preparation and data inspection.
When it comes to model building, Tableau provides basic functionality.
For speciﬁc data sets, it is possible to add a trend line or compute a
forecast, where the model can be adjusted in designated bounds. The
various manipulation and visualization options provide strong support
for the exploration loops. Veriﬁcation loops and knowledge generation
loops support is also available with the story module, which allows
free creation of reports and references to visualizations. In addition,
all three loops are supported by the annotation tool, which allows the

addition of persistent annotations to data points or speciﬁc locations
and areas in the visualization.

Jigsaw is an example for a visual analytics application, which we
already examined in detail in Sections 4.1 and 4.2. It supports all
the actions we included in our model. In addition, all three loops are
supported, but there is some potential for improvements. For example, a
good support of the veriﬁcation loops can be easily achieved by a tighter
integration of the Tablet View in the system. Also, providing histories
of actions leading to a speciﬁc state of a visualization (bookmark) is a
step towards better support of the veriﬁcation and knowledge generation
loops.

At last, we examine HARVEST [11, 37], a system which supports
provenance. While using HARVEST, all interaction is recorded and can
be used in order to understand the way ﬁndings have been detected.
Using this data, the system is able to support analysts during the explo-
ration and veriﬁcation loops, for example by an automatic ranking of
manually created notes based on the users behavior. This comes close
to our deﬁnition of the knowledge generation process. The system also
supports the analysis process by providing visualization recommenda-
tions or ordering of notes connected insights, or ﬁndings. Similar to
Jigsaw’s Tablet View, the note-taking-interface is capable of organizing,
grouping, and ordering items, which supports the knowledge generation
loops.

The assessment of the tools from different application domains
shows that the model can be applied on applications, which work with
data and visualization in general. The result also clearly separates the
different application domains.

5 SUMMARY AND DISCUSSION
This study identiﬁes new perspectives on visual analytic processes be-
yond weaving existing frameworks into one. Our model highlights that
human and machine are a loop in the knowledge generation process
using visual analytics. While existing models focus on one of the these,

SACHA

ET AL.: KNOWLEDGE GENERATION MODEL FOR VISUAL ANALYTICS

1

611

our model integrates human thinking whilst describing visual analytics
components. Most of the visual analytics systems do not fully cover
or properly treat all aspects that our model requires as it includes the
holistic process of knowledge generation which involves visual analyt-
ics systems and human users in the loop. Our model also speciﬁes how
human loops are intertwined with the subprocesses of visual analytics
tools. We look at systems in a skeleton view using Keim’s previous
model and show how the interplay between each subcomponent can be
inﬂuenced by human decision making and reasoning processes. This is
important for researchers as it enables discussions of speciﬁc functions
and their impacts on reasoning processes more explicitly because our
model can describe the whole path from data to knowledge and vice
versa. Besides connecting to system components, the model also de-
ﬁnes human concepts and introduces three self-contained loops/stages
of reasoning/thinking. We can also use our model to assess visual ana-
lytics system in terms of its functions toward human analytic outputs.
For instance, we can detect areas of visual analytics systems that tend
to cause biases within the knowledge generation process by aligning
experts’ analytic processes and outcomes against our model. Then,
designers could improve corresponding visual analytics components to
enhance early detection of such analytic failures: wrong hypotheses,
conﬂicts between ﬁndings and insights, and dead ends of exploration
cycles. We also ﬁnd results that resonate with sensemaking, cognition
and reasoning models (e.g. Pirolli and Card [29]). Our model even
speciﬁes where our current visual analytics systems fall short of, which
is to support higher level loops, namely the exploration, veriﬁcation
and knowledge generation loops. Supporting higher level loops is more
complex, however this would be a useful addition to many of the current
systems [31].

5.1 Collaboration and Communication
Our model describes the knowledge generation processes for visual
analytics. There are speciﬁc types of visual analytics, which our model
does not explicitly mention. Visual analytics processes could be col-
laborative, so multiple stakeholders asynchronously or synchronously
analyze data together and gain insights through verbal communication
between them. Our model currently assumes an individual’s analytic
process, so it is missing collaborative components. We can simplify
this collaboration process with different but possibly shared knowledge
generation loops of all humans (i.e., white nodes in Figure 1). We can
explain that a number of users perform actions and interpret ﬁndings
together to improve the quality of their knowledge. Also distributed
cognition theories have to be considered when examining representa-
tions and interactions among humans and artifacts (Liu et al. [20]).
Nobarny et al. [24] already developed a system and performed stud-
ies focusing on distributed cognition in order to support collaborative
visual analytics.

Especially the externalization and communication of information
plays a crucial role that needs more detailed investigations with regards
to the presented concepts of ﬁndings, insights or knowledge. The im-
portance of externalization in visual analytic processes can be observed
by heavy reliance on note-taking throughout analysis processes [22].
In real world, groups of users often take a variety of approaches to
synthesize information with their own organizational language [32].
Communicating knowledge in visual analytics is actually the communi-
cation of evidence found in data supporting a belief. This evidence is
shared with ﬁndings or insights, e.g., a commented visualization reveal-
ing an interesting relationship in data. The communication counterpart
can follow the evidence and, depending on the level of trust, gain own
insights or knowledge out of it. Collaborative analysis scenarios allow
communication partners to verify this evidence with the system. We
should also note that collaboration between multiple analysts open
up chapters about maintaining exploration awareness [38]. In case of
presentations or static documents the missing visual analytics system
is replaced by the presenter or reporter. During presentations ques-
tions and answers are able to replace the knowledge generation loops.
Static documents have to convey all information from hypotheses, ﬁnd-
ings, and insights in a way that readers can follow the conclusions.
Interactive reports or documents (e.g., infographics) go beyond the

aforementioned scenario as they can be seen as a report combined with
a limited visualization that is tailored to show the relevant ﬁndings of
insights.

5.2 Visual Analytics of Streaming Data
In addition to collaborative visual analytics, we can apply our model
to visual analytics of data streams (e.g., monitoring twitter data). In
this situation, our data node is dynamically changing, so our model
reformulates its visual representations and analytic models according
to substantial changes made by the data streams. Further investigation
is necessary since a number of requirements on data management,
knowledge discovery and visualization need to be researched due to
the dynamic nature of streams. Mansmann et al. [23] illustrate this and
highlight that the analyst role changes since exploration is extended
to include real-time monitoring tasks where situational awareness and
complex decision making come into play.

5.3 Automatic Support of Knowledge Generation
We will now consider how novice users can be given guidance using
the system and supporting analysis can aid knowledge generation. Sys-
tems can make automatic suggestions based on their current state, e.g.,
choosing a suitable color mapping or selecting parameters based on
data.

5.3.1 Exploration Loop
The exploration loop is the basis of all knowledge generation in visual
analytics. An important trigger to observe ﬁndings is how the system
handles interactions. Analysts can learn from the causality between in-
teractions and reaction, hence, supporting the exploration loop requires
a system to respond with an immediate observable reaction to any inter-
action. Many algorithms in data analysis require complex computations
and are not able to calculate a complete result immediately. In these
cases, analysts should at least receive feedback that the algorithm is
still running. Systems should provide the ability to switch between the
states before and after calculation so analysts can learn from interac-
tions. If the ﬁnal result can be estimated, for instance, by intermediate
results of an incremental algorithm, the estimation could be shown to
the analysts with the additional ability to abort the calculation.

Findings are related to unexpected results or patterns in models or
visualizations. Automatically detecting unexpected results is compli-
cated, because it requires a deﬁnition of what analysts are expecting,
which is usually not known to systems. On the other side, pattern
mining algorithms are extracting patterns directly from data and in vi-
sualization, patterns could be detected with automatic methods as well.
Even automatic methods to judge the usefulness of visualization exists,
e.g., Bertini et al. [3] gives an overview of quality metrics approaches.
Actions are dependent on ﬁndings and the goal of the analysis. Vi-
sual analytic systems could provide suggestions for further actions in
the analysis process, as in behavior-driven visualization recommen-
dation [10]. Based on ﬁndings, these suggestions could ofﬂoad some
burden from analysts of having to choose the right action from all dif-
ferent possibilities. Novice users would beneﬁt from such suggestions,
because the system would present proper actions allowing users to learn
the abilities of the system. For expert users the interaction with the
system would be more efﬁcient compared to navigating a large set of
options. It is important to ﬁnd an adequate level of suggestions based
on user experience. A solution to this problem could be a learning
technique, such as active learning (e.g., Settles [35]), that adapts the
suggestions to users and minimizes the interaction costs.

5.3.2 Veriﬁcation Loop
The veriﬁcation loop is the central part in the knowledge generation
loops. Analysts combine ﬁndings from data with their domain knowl-
edge and gain new insights into the problem domain. The knowledge
of analysts play an important role in the veriﬁcation loop, therefore
automatic support for the veriﬁcation loop is limited to helping analysts
record their analysis results.

Systems are not directly generating insights but analysts gain new
insights from data when they are able to interpret ﬁndings. In this

1

6

12

IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 20, NO. 1 ,

2 DE EM

C

BER 014

2

respect systems can support this process by providing useful summaries
of ﬁndings by allowing analysts to organize ﬁndings, hypotheses, or
insights. Often insights are not dependent on a single ﬁnding but are
hidden in complex relationships in the data and often difﬁcult to ﬁnd
without prior knowledge. Systems addressing this problems are, for
instance, Shrinivasan et al. [37] and Wright et al. [42].

To formulate hypotheses about the problem domain, ﬁndings are
not enough as analysis requires insights into the domain. However,
systems can automatically formulate hypotheses about the analyzed
data from ﬁndings. Theses hypotheses can help analysts get a faster
overview of unknown data sets but their use for complex analysis tasks
is limited, because only hypotheses about relations in data can be
generated automatically.

5.3.3 Knowledge Generation Loop
Automating the steps from insights to knowledge or from knowledge
to new hypotheses is according to our deﬁnition not possible. Analysts
gain new knowledge when the evidence collected with a visual analytics
system is convincing. The best way to support knowledge generation
with visual analytics are systems with the ability to look at data from
different perspectives. This gives analysts the possibility to collect
versatile evidence and increases the level of trust in ﬁndings or insights.

5.4 Future Investigations on Visual Analytics Systems
Our model speciﬁes some areas that our current research can further
investigate. We ﬁnd some interaction types missing in many systems
and interaction models, especially in the model construction or the
coupling of models and visualizations. Many visual analytics tools tend
to maintain preloaded models or to provide a very limited capability
to manipulate models by adjusting some parameters. We observe that
data become more and more dynamic and unstructured and human
interaction on the model part is therefore crucial to analyze such data.
Secondly, we see that many improvements could be made to further sup-
port human actions. Systems can actively learn from user behavior and
adapt its models and visualizations, too. Visual analytics systems could
proactively seek next candidate actions based upon user-generated logs.
Our model points out that we need more explicit support to transfer
ﬁndings, insights, and knowledge. As Endert et al. [7] states, visual
analytics should recognize and integrate human working processes
into the system. Different ways of how systems could ofﬂoad some
burdens from human users exist. The key is the interaction with visual
analytics, although we should be aware of interaction costs (Lam [18],
van Wijk [40]). Frequent interactions triggered by the system could
demand too much effort, which may discourage user’s exploration.

After analysis, the results have to be documented or communicated
to others. An interesting ability of visual analytics systems could
be a semi-automatic approach for generating documentations. Algo-
rithms could detect interesting new ﬁndings and put them together in a
convincing form, removing dead ends and duplicate ﬁndings. Such in-
telligent journals would make creation of reports or presentations much
easier. Alternatively, written reports could be enriched with ﬁndings
supporting statements in the document.

The analysis process often follows one direction and many ﬁndings
are not explored in depth. Visual analytics systems could provide func-
tionalities to backtrack former analysis results and suggest interesting
but not investigated ﬁndings for further analysis. This would require
algorithms to judge the interestingness of ﬁndings in context of the
conducted analysis results. This functionality together with a short sum-
mary of previous results could be helpful for continuing an interrupted
analysis session.

5.5 Real World Scenario
Our model reﬂects an ideal scenario where an analyst uses one single
visual analytics system that is capable of handling all requirements but
real world scenarios are different. On the one hand, users are often
not aware of the systems capabilities and lack the required expertise to
understand complex analysis methods (Gulf of Execution). On the other
hand, the system’s capabilities are not sufﬁcient to solve an analysis
task. As an outcome, analysts may stop their analysis in order to

consult domain experts or continue their analysis with another system.
In addition our model implies that each action ends in an observable
reaction of the system. Real world systems often lack of this capability,
which is a known problem in interaction science (Gulf of Evaluation).
The analysis of real world problems requires both expertise about
the analysis and the domain. Domain experts often lack experience in
understanding computer systems, visualization techniques and analysis
methods, whereas visual analytics experts lack of sufﬁcient domain
knowledge. Thus, analysis requires collaboration between them. With-
out domain knowledge a visual analytics analysts is able to generate
ﬁndings and insights concerning the data, not the domain. The domain
expert is responsible for the formulation of problem hypotheses, the
detection and interpretation of patterns. Domain experts need to be
familiar with visual analytics methods and systems. On the other hand
analysts have to learn about the problem domain.

One thing to keep in mind is that our model simpliﬁes many different
processes, and it contains inherent fuzziness, especially on the human
side. That is the reason why our model consists of various loops that
represent several levels of thinking. Obviously, no visual analytics tools
can differentiate exactly between reasoning processes. Furthermore,
the processes can disseminate many inﬂuences into other processes,
which may not clearly appear in our model. Often many conﬂicting
hypotheses are investigated in parallel and derived ﬁndings, insights or
knowledge may affect each other.

5.6 Teaching Visual Analytics
Our model can be useful to provide a general overview for novice
students, designers, and researchers in visual analytics. Teaching visual
analytics often require teachers to provide fundamental concepts com-
monly appearing throughout applications, which inevitably involves
domain knowledge. Our model can be used to explain the knowledge
generation process without the need for such expert language. In addi-
tion, this model touches upon various components of visual analytics,
such as interactions and automatic algorithms, in the perspectives of
visual analytics applications. This guideline could point researchers to
relevant literature in case they want to ﬁnd out about speciﬁc methods.
Our model also highlights the importance of the interplay and collab-
oration between human and machines. This rather obvious but easily
forgotten notion could be highlighted, as illustrated in Figure 1.

6 CONCLUSION
In this paper we present a process model for knowledge generation in
visual analytics that integrates system and human aspects. The model
deﬁnes and relates relevant concepts and provides a knowledge gen-
eration process from knowledge to data and vice versa. The model
embeds concepts into a three loop framework and illustrates possible
human machine pairings that are fundamental in visual analytics. We
illustrated that our model integrates with existing models and theories
that focus on speciﬁc parts within the overall context. We demonstrated
the models application using Jigsaw as an example system as well as
undertaking a comparative assessment of three other well-known appli-
cations. Finally, we discussed model implications, named open issues,
and pointed to future directions. The right hand side (human part) of
our model is not restricted to visual analytics and can also be relevant
for other disciplines as it combines computer and human based theories.
The model aims to give a basis for more detailed compositions of theo-
ries. Whilst it is not our intention to cover every single details of such
processes, we do provide a overview that commonly appears in visual
analytics processes. We also acknowledge that human’s knowledge
processes cannot be linear or clearly subdivided into components of
our model (e.g., insight). However, we provide inherently limited but
meaningful distinction between human’s knowledge gaining processes.

REFERENCES
[1] M. R. Berthold, N. Cebron, F. Dill, T. R. Gabriel, T. K¨otter, T. Meinl,
P. Ohl, K. Thiel, and B. Wiswedel. Knime - the konstanz information
miner: version 2.0 and beyond. SIGKDD Explorations, 11(1):26–31,
2009.

SACHA

ET AL.: KNOWLEDGE GENERATION MODEL FOR VISUAL ANALYTICS

1

613

[27] C. S. Peirce. Collected papers: Elements of Logic, volume 2, pages 56–59.

Belknap Press, 1965.

[28] W. A. Pike, J. Stasko, R. Chang, and T. A. O’Connell. The science of

interaction. Information Visualization, 8(4):263–274, 2009.

[29] P. Pirolli and S. Card. The sensemaking process and leverage points for
analyst technology as identiﬁed through cognitive task analysis. In Pro-
ceedings of International Conference on Intelligence Analysis, volume 5,
pages 2–4, 2005.

[30] M. Pohl, M. Smuc, and E. Mayr. The user puzzle: Explaining the interac-
tion with visual analytics systems. Visualization and Computer Graphics,
IEEE Transactions on, 18(12):2908–2916, Dec 2012.

[31] J. C. Roberts, D. A. Keim, T. Hanratty, R. R. Rowlingson, R. Walker,
M. Hall, Z. Jacobson, V. Lavigne, C. Rooney, and M. Varga. From Ill-
Deﬁned Problems to Informed Decisions. EuroVis Workshop on Visual
Analytics (2014), 2014.

[32] A. Robinson. Collaborative synthesis of visual analytic results. In IEEE
Symposium on Visual Analytics Science and Technology, 2008. VAST ’08,
pages 67–74, 2008.

[33] P. Saraiya, C. North, and K. Duca. An insight-based methodology for
evaluating bioinformatics visualizations. Visualization and Computer
Graphics, IEEE Transactions on, 11(4):443–456, 2005.

[34] H.-J. Schulz, T. Nocke, M. Heitzler, and H. Schumann. A design space of
visualization tasks. Visualization and Computer Graphics, IEEE Transac-
tions on, 19(12):2366–2375, 2013.

[35] B. Settles. Active learning literature survey. University of Wisconsin,

Madison, 52:55–66, 2010.

[36] B. Shneiderman. The eyes have it: A task by data type taxonomy for
In Visual Languages, 1996. Proceedings.,

information visualizations.
IEEE Symposium on, pages 336–343. IEEE, 1996.

[37] Y. Shrinivasan, D. Gotz, and J. Lu. Connecting the dots in visual analysis.
In Visual Analytics Science and Technology, 2009. VAST 2009. IEEE
Symposium on, pages 123–130, Oct 2009.

[38] Y. Shrinivasan and J. van Wijk. Supporting exploration awareness in
information visualization. IEEE Computer Graphics and Applications,
29(5):34–43, Sept. 2009.

[39] J. T. Stasko, C. G¨org, Z. Liu, and K. Singhal. Jigsaw: Supporting Inves-
tigative Analysis through Interactive Visualization. In IEEE VAST, pages
131–138. IEEE, 2007.

[40] J. van Wijk. The value of visualization. In Visualization, 2005. VIS 05.

IEEE, pages 79–86, Oct 2005.

[41] T. von Landesberger, S. Fiebig, S. Bremm, A. Kuijper, and D. W. Fellner.
Interaction taxonomy for tracking of user actions in visual analytics ap-
plications. In Handbook of Human Centric Visualization, pages 653–670.
Springer, 2014.

[42] W. Wright, D. Schroh, P. Proulx, A. Skaburskis, and B. Cort. The sandbox
for analysis: Concepts and methods.
In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, CHI ’06, pages
801–810, New York, NY, USA, 2006. ACM.

[43] J. S. Yi, Y.-a. Kang, J. T. Stasko, and J. A. Jacko. Understanding and
characterizing insights: how do people gain insights using information
visualization? In Proceedings of the 2008 Workshop on BEyond time and
errors: novel evaLuation methods for Information Visualization, page 4.
ACM, 2008.

[2] E. Bertini and D. Lalanne. Surveying the complementary role of automatic
data analysis and visualization in knowledge discovery. In Proceedings of
the ACM SIGKDD Workshop on Visual Analytics and Knowledge Discov-
ery: Integrating Automated Analysis with Interactive Exploration, pages
12–20. ACM, 2009.

[3] E. Bertini, A. Tatu, and D. A. Keim. Quality metrics in high-dimensional
data visualization: An overview and systematization. IEEE Trans. Vis.
Comput. Graph., 17(12):2203–2212, 2011.

[4] M. Brehmer and T. Munzner. A multi-level typology of abstract visualiza-
tion tasks. Visualization and Computer Graphics, IEEE Transactions on,
19(12):2376–2385, 2013.

[5] S. K. Card, J. D. Mackinlay, and B. Shneiderman. Readings in information

visualization: using vision to think. Morgan Kaufmann, 1999.

[6] R. Chang, C. Ziemkiewicz, T. M. Green, and W. Ribarsky. Deﬁning
insight for visual analytics. Computer Graphics and Applications, IEEE,
29(2):14–17, 2009.

[7] A. Endert, M. Hossain, N. Ramakrishnan, C. North, P. Fiaux, and C. An-
drews. The human is the loop: new directions for visual analytics. Journal
of Intelligent Information Systems, pages 1–25, 2014.

[8] U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From data mining to

knowledge discovery in databases. AI magazine, 17(3):37, 1996.

[9] C. G¨org, Z. Liu, J. Kihm, J. Choo, H. Park, and J. T. Stasko. Combining
Computational Analyses and Interactive Visualization for Document Ex-
ploration and Sensemaking in Jigsaw. IEEE Transactions on Visualization
and Computer Graphics, 19(10):1646–1663, 2013.

[10] D. Gotz and Z. Wen. Behavior-driven visualization recommendation.
In Proceedings of the 14th International Conference on Intelligent User
Interfaces, page 315324, New York, NY, USA, 2009. ACM.

[11] D. Gotz, Z. When, J. Lu, P. Kissa, N. Cao, W. H. Qian, S. X. Liu, and
M. X. Zhou. Harvest: An intelligent visual analytic tool for the masses.
In Proceedings of the First International Workshop on Intelligent Visual
Interfaces for Text Analysis, IVITA ’10, pages 1–4, New York, NY, USA,
2010. ACM.

[12] D. Gotz and M. X. Zhou. Characterizing users’ visual analytic activity for

insight provenance. Information Visualization, 8(1):42–55, 2009.

[13] T. Green, W. Ribarsky, and B. Fisher. Visual analytics for complex con-
cepts using a human cognition model. In Visual Analytics Science and
Technology, 2008. VAST ’08. IEEE Symposium on, pages 91–98, Oct 2008.
[14] T. M. Green, W. Ribarsky, and B. Fisher. Building and applying a human
cognition model for visual analytics. Information Visualization, 8(1):1–13,
Jan. 2009.

[15] D. A. Keim, J. Kohlhammer, G. Ellis, and F. Mansmann. Mastering the
Information Age - Solving Problems with Visual Analytics. Eurographics
Association, 2010.

[16] D. A. Keim, F. Mansmann, J. Schneidewind, J. Thomas, and H. Ziegler.

Visual analytics: Scope and challenges. Springer, 2008.

[17] B. C. Kwon, B. Fisher, and J. S. Yi. Visual analytic roadblocks for novice
investigators. In 2011 IEEE Conference on Visual Analytics Science and
Technology (VAST), pages 3–11, 2011.

[18] H. Lam. A framework of interaction costs in information visualization.
Visualization and Computer Graphics, IEEE Transactions on, 14(6):1149–
1156, Nov 2008.

[19] P. Legrenzi, V. Girotto, and P. N. Johnson-Laird. Focussing in reasoning

and decision making. Cognition, 49(1):37–66, 1993.

[20] Z. Liu, N. J. Nersessian, and J. T. Stasko. Distributed cognition as a
theoretical framework for information visualization. Visualization and
Computer Graphics, IEEE Transactions on, 14(6):1173–1180, 2008.

[21] L. Magnani. Abduction, Reason and Science: Processes of Discovery and

Explanation. Springer, 2001.

[22] N. Mahyar, A. Sarvghad, and M. Tory. A closer look at note taking
in the co-located collaborative visual analytics process. In 2010 IEEE
Symposium on Visual Analytics Science and Technology (VAST), pages
171–178, Oct. 2010.

[23] F. Mansmann, F. Fischer, and D. A. Keim. Dynamic visual analytics facing
the real-time challenge. In Expanding the Frontiers of Visual Analytics
and Visualization, pages 69–80. Springer, 2012.

[24] S. Nobarany, M. Haraty, and B. Fisher. Facilitating the reuse process in
distributed collaboration: a distributed cognition approach. In Proceedings
of the ACM 2012 conference on Computer Supported Cooperative Work,
pages 1223–1232. ACM, 2012.

[25] D. Norman. The Design of Everyday Things. Basic Books, 2002.
[26] C. North. Toward measuring visualization insight. Computer Graphics

and Applications, IEEE, 26(3):6–9, 2006.

",False,2014.0,{},False,False,journalArticle,False,VWK8HC38,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/6875967/,,Knowledge Generation Model for Visual Analytics,VWK8HC38,False,False
V5DWUADP,E42DR755,"The Value of Visualization

Jarke J. van Wijk(cid:3)

Dept. Mathematics and Computer Science

Technische Universiteit Eindhoven

ABSTRACT

The (cid:2)eld of Visualization is getting mature. Many problems have
been solved, and new directions are sought for. In order to make
good choices, an understanding of the purpose and meaning of vi-
sualization is needed. Especially, it would be nice if we could as-
sess what a good visualization is. In this paper an attempt is made
to determine the value of visualization. A technological viewpoint
is adopted, where the value of visualization is measured based on
effectiveness and ef(cid:2)ciency. An economic model of visualization
is presented, and bene(cid:2)ts and costs are established. Next, conse-
quences for and limitations of visualization are discussed (including
the use of alternative methods, high initial costs, subjectiveness, and
the role of interaction), as well as examples of the use of the model
for the judgement of existing classes of methods and understanding
why they are or are not used in practice. Furthermore, two alter-
native views on visualization are presented and discussed: viewing
visualization as an art or as a scienti(cid:2)c discipline. Implications and
future directions are identi(cid:2)ed.
CR Categories:
H.5.2 [Information Interfaces and Presenta-
tion]: User Interfaces; I.3.6 [Computer Graphics]: Methodology
and Techniques I.3.8 [Computer Graphics]: Applications
Keywords: Visualization, evaluation

1

INTRODUCTION

Modern society is confronted with a data explosion. Acquisition
devices like MRI-scanners, large scale simulations on supercom-
puters, but also stock trading at stock exchanges produce very large
amounts of data. Visualization of data makes it possible for re-
searchers, analysts, engineers, and the lay audience to obtain insight
in these data in an ef(cid:2)cient and effective way, thanks to the unique
capabilities of the human visual system, which enables us to detect
interesting features and patterns in short time.

Many of us will have written paragraphs like the preceding one,
where I attempted to give the standard rationale of our (cid:2)eld.
In
1987, when the in(cid:3)uential ViSC report [16] of the NSF appeared,
the expectations were high. Visualization was considered as vital
and highly promising for the scienti(cid:2)c process. Nowadays, much
progress has been made. The advances in graphics hardware are
astonishing, most laptop computers are graphics superworkstations
according to the standards of just a decade ago. Many new methods,
techniques, and systems have been developed. Some of them, such
as slices, height-surfaces, and iso-surfaces are now routinely used
in practice.

On the other hand, many of these new methods are not used in
real-world situations, many research results are nowadays consid-
ered as incremental by reviewers, and our prospective users rarely
go to our conferences. So, are we, as researchers in visualization,
on the right track?

(cid:3)e-mail: vanwijk@win.tue.nl

In this paper I want to give a contribution to the discussion on the
status and possible directions of our (cid:2)eld. Rather than to pinpoint
speci(cid:2)c topics and activities, my aim is to detect overall patterns,
and to (cid:2)nd a way to understand and qualify visualization in general.
This is an ambitious and vague plan, although the basic ground for
this is highly practical.

I have to make decisions on visualization in many roles. As a
researcher, decisions have to be made ranging from which area to
spend time on to which particular solution to implement; as a su-
pervisor, guidance to students must be provided; as a reviewer, new
results and proposals for new research must be judged, and opinions
are expected if they are worth publishing or funding; as advisor in
a start-up company, novel and pro(cid:2)table directions must be spot-
ted. All these cases imply judgement of the value of visualization
in varying senses.

How to assess the value of visualization? Visualization itself is
an ambiguous term. It can refer to the research discipline, to a tech-
nology, to a speci(cid:2)c technique, or to the visual result. If visualiza-
tion is considered as a technology, i.e., as a collection of methods,
techniques, and tools developed and applied to satisfy a need, then
standard measures apply: Visualization has to be effective and ef(cid:2)-
cient. In other words, visualization should do what it is supposed
to do, and has to do this using a minimal amount of resources. One
immediate and obvious implication is that we cannot judge visu-
alization on its own, but have to take into account the context in
which it is used .

In section 2 a short overview is given of the background of the
topic discussed here. In section 3 an economic model of visualiza-
tion is proposed. The basic elements are identi(cid:2)ed (cid:2)rst, the asso-
ciated costs and gains are added next. Various implications of the
model are discussed in section 4. In section 5 this model is applied
to several cases. In section 6 the model is discussed and alternative
views are considered, followed by conclusions in section 7.

Finally, this topic is on one hand very general, high-level, and
abstract; on the other hand, it is also very personal, in the sense
that it is about values (which are subjective), and valuation of ones
own work. To re(cid:3)ect this, I use the (cid:2)rst person in this paper, to
emphasize that the opinions given are personal. Most examples I
use come from my own work, often done together with coworkers.
The main reason for this is simply that I am most familiar with it,
not only with the techniques and results, but also with the context
in which it took place.

2 BACKGROUND

If we use 1987 as the year where visualization started, our discipline
celebrates this year its 18th anniversary. In the Netherlands, at this
age a person is considered mature. Many things have changed since
1987. Graphics hardware developments are amazing, as well as the
large amount of techniques that have been developed to visualize
data in a variety of ways.

There are signals that there is a need to reconsider visualization.
First of all, there seems to be a growing gap between the research
community and its prospective users. Few, if no attendants at the
IEEE Visualization conference are prospective users looking for
new ways to visualize their data and solve their problems. Sec-
ondly, the community itself is getting both more specialized and

critical, judging from my experience as paper co-chair for IEEE
Visualization 2003 and 2004.
In the early nineties, the (cid:2)eld lay
fallow, and it was relatively easy to come up with new ideas. The
proceedings in the early nineties show a great diversity. Nowadays
the (cid:2)eld is getting more specialized, submitted work consists often
of incremental results. This could signal that our (cid:2)eld is getting ma-
ture. On the other hand, it is not always clear that these incremental
contributions have merit, and reviewers are getting more and more
critical. Thirdly, some big problems have been solved more or less
[14]. For volume rendering of medical data sophisticated industrial
packages that satisfy the needs of many users are available.

These trends urge a need to reconsider the (cid:2)eld, and to think
about new directions. Several researchers have presented [7, 9, 17]
overviews of current challenges. Another great overview of the cur-
rent status of visualization and suggestions for new directions is
provided by the position papers [3] contributed by the attendants of
the joint NSF-NIH Fall 2004 Workshop on Visualization Research
Challenges, organized by Terry Yoo. Many issues are mentioned
several times, including handling of complex and large data sets,
uncertainty, validation, integration with the processes of the user,
and a better understanding of the visualization process itself. One
particularly impressive and disturbing contribution is [14], for its
title, the name and fame of the author, and the vivid description that
indeed the (cid:2)eld has changed and new directions are needed.

In this paper no attempt is made to summarize or overview these
challenges, but the aim is to (cid:2)nd a model or procedure to judge in
general if a method is worthwhile or not. In the following sections,
a (cid:2)rst step towards such a model is presented. Much of it is evident
and obvious. As a defense, some open doors cannot be kicked open
often enough, and also, if obvious results would not come out, the
model and the underlying reasoning would be doubtful. Some state-
ments made are more surprising and sometimes contrary to main
stream thinking. To stimulate the debate, I have taken the liberty to
present these more extreme positions also, hoping that some readers
will not be offended too much.

3 MODEL
In this section a generic model on visualization is proposed. First,
the major ingredients are identi(cid:2)ed; secondly, costs and gains are
associated. The model is abstract and coarse, but it can be used to
identify some aspects, patterns and trends.

3.1 Visualization and its context
Figure 1 shows the basic model. Boxes denote containers, circles
denote processes that transform inputs into outputs. The aim here
is not to position different visualization methods, for which a tax-
onomy would be a more suitable approach, but rather to describe
the context in which visualization operates. No distinction is made,
for instance, between scienti(cid:2)c visualization and information vi-
sualization, at this level there is much more they share than what
separates them.

In the following we describe the various steps. We use a mathe-
matical notation for this, merely as a concise shorthand and to give
a sense of quanti(cid:2)cation than as an exact and precise description.
Processes are de(cid:2)ned as functions, but the domains and ranges of
these are ill-de(cid:2)ned.

The central process in the model is visualization V:

I(t) = V (D;S;t):

Data D is transformed according to a speci(cid:2)cation S into a time
varying image I(t). All these should be considered in the broadest
sense. The type of data D to be visualized can vary from a single
bit to a time-varying 3D tensor (cid:2)eld; the speci(cid:2)cation S includes
a speci(cid:2)cation of the hardware used, the algorithms to be applied

D

dK/dt

K

V

S

I

dS/dt

P

E

data

visualization

user

Figure 1: A simple model of visualization

(in the form of a selection of a prede(cid:2)ned method or in the form of
code), and the speci(cid:2)c parameters to be used; the image I will of-
ten be an image in the usual sense, but it can also be an animation,
or auditory or haptic feedback. In other words, this broad de(cid:2)ni-
tion encompasses both a humble LED on an electronic device that
visualizes whether the device is on or off, as well as a large virtual
reality set-up to visualize the physical and chemical processes in the
atmosphere. The image I is perceived by a user, with an increase in
knowledge K as a result:

dK
dt = P(I;K):

The amount of knowledge gained depends on the image, the current
knowledge of the user, and the particular properties of the percep-
tion and cognition P of the user. Concerning the in(cid:3)uence of K, a
physician will be able to extract more information from a medical
image than a lay-person. But also, when already much knowledge is
available, the additional knowledge shown in an image can be low.
A map showing the provinces of the Netherlands provides more
new information to a person from the US than to a Dutch person.
Also, the additional value of an image of time-step 321 is probably
small when time-step 320 has been studied just before. Concerning
the in(cid:3)uence of P, a simple but important example is that a color-
blind person will be less effective in extracting knowledge from a
colorful image than a person with full vision. But also, some people
are much better than others in spotting special patterns, structures,
and con(cid:2)gurations.

The current knowledge K(t) follows from integration over time

t

P(I;K;t)dt

K(t) = K0 +Z
0
where K0 is the initial knowledge.
An important aspect is interactive exploration, here represented
by E(K). The user may decide to adapt the speci(cid:2)cation of the
visualization, based on his current knowledge, in order to explore
the data further

dS
dt = E(K);

hence the current speci(cid:2)cation S(t) follows from integration over
time

S(t) = S0 +Z
0
where S0 is the initial speci(cid:2)cation.

t

E(K)dt

3.2 Economic Model
To assess if a visualization method is worthwhile, we must assess
its value. We propose to use pro(cid:2)tability in an economic sense as

a measure for this. We simplify this by assuming that there is a
homogeneous user community, consisting of n users which use a
certain visualization V to visualize a data set m times each, where
each session takes k explorative steps and time T. This is a crude
simpli(cid:2)cation of course. In the real world, the user community will
often be highly varied, with different K0’s and also with different
aims. The costs associated with using V come at four different
levels:

(cid:15) Ci(S0): Initial development costs. The visualization method
has to be developed and implemented, possibly new hardware
has to be acquired.

(cid:15) Cu(S0): Initial costs per user. The user has to spend time on
selection and acquisition of V, understanding how to use it,
and tailoring it to his particular needs.

(cid:15) Cs(S0): Initial costs per session. Data have to be converted,
and an initial speci(cid:2)cation of the visualization has to be made.
(cid:15) Ce: Perception and exploration costs. The user has to spend
time to watch the visualization and understand it, as well as in
modi(cid:2)cation and tuning of the speci(cid:2)cation, thereby explor-
ing the data set.

The total costs are now given by

C = Ci +nCu +nmCs +nmkCe:

The return on these investments consists of the value W (DK) of the
acquired knowledge DK = K(T ) (cid:0) K(0) per session, multiplied by
the total number of sessions:

G = nmW (DK)

and hence for the total pro(cid:2)t F = G (cid:0)C we (cid:2)nd

F = nm(W (DK) (cid:0)Cs (cid:0)kCe) (cid:0)Ci (cid:0)nCu:

This gives us a recipe to decide on the value of a visualization
method. Positive are high values for n, m, W (DK), and low val-
ues for Cs;Ce;Ci;Cu, and k. Or, in other words, a great visualiza-
tion method is used by many people, who use it routinely to ob-
tain highly valuable knowledge, without having to spend time and
money on hardware, software, and effort. Indeed, quite obvious.

IMPLICATIONS

4
Quanti(cid:2)cation of the elements of the model is hard. In this section
we discuss this in more detail, as well as a number of other issues
implied by this model.

4.1 Valuable knowledge
Insight is the traditional aim of visualization. The term itself is
great, and suggests a high-level contribution to the advance of sci-
ence. Users are enabled to see things they were not aware of, and
this insight helps them to de(cid:2)ne new questions, hypotheses, and
models of their data. However, from an operational point of view,
the term insight does not help us much further to assess the value
of visualization. One problem is that we cannot directly observe
or measure how much insight is acquired, and also, it is dif(cid:2)cult to
assess what the value of that insight is. In the model we use the
term knowledge, but this suffers from the same limitations. Also,
there is a strange paradox in the basic paradigm of visualization.
We don’t know what information is contained in the data, hence we
make pictures to get insight. But if we do not know which speci(cid:2)c
aspects or features should be visible, we cannot assess if we are
successful or not.

Nevertheless, we should try to measure or estimate W (DK), if we
want to assess the value of visualization, especially because it is the
only term in the model for F with a positive sign. An operational
approach is to consider the use of visualization as an element in
problem solving. The user has a problem, he must decide which
action to take, and to make that decision he needs information. The
visualization should enable him to extract the relevant information
from the data.

Decisions are typically about actions to be taken or not. For
instance, should a stock be bought or sold, should a patient be op-
erated or not, which people in an organization are candidates for
promotion, etc. Hence, I recommend my students to search for
and enumerate possible actions of users after using their prospec-
tive tools. If such actions cannot be found or de(cid:2)ned, the value of
visualization is doubtful. Just claiming that a visualization gives
insight is not enough, if we want to offer additional value.

If we know to which actions the visualization should lead to, the
next steps are assessment whether the knowledge derived from the
visualization does indeed support the decision, and also, to assess
the economic value of this decision. This is not easy, but one can try
for instance to estimate how much time is saved, or try to quantify
the consequences of a wrong decision.

4.2 Alternative methods
Ef(cid:2)ciency is relative, an aspect that is not captured explicitly in the
model. One could predict a high value for F for a new method,
however, if other methods are available to obtain the same knowl-
edge against lower costs, then very likely the value for n is overesti-
mated. Or, stated simply, if a better solution already exists, nobody
will use the newer one. The model is too simple here. The effective
value of n itself is not a parameter, but a function of, among others,
the perceived bene(cid:2)t by potential users.

Developers of new visualization methods should be aware of al-
ternative solutions, and carefully study their advantages and limita-
tions. New methods are not better by de(cid:2)nition. Especially when
existing methods are heavily used in practice, they have proven to
have value. It is often hard to beat straightforward solutions; for
instance, in many cases just using a line graph is the best way to
show a time-varying signal.

A defense often heard for a lesser performance of new methods
compared to existing ones is that the users have not had enough
time to get accustomed to them. In some cases this might hold, but
an equally viable hypothesis is that an existing method is simply
better. For instance, just showing a set of objects in a list enables
linear scanning, whereas scanning a fancy 2D or 3D display where
the objects are distributed over space is much harder [18].

ma
di
wo
do
vr
za
zo

ma
di
wo
do
vr
za
zo

ma
di
wo
do
vr
za
zo

ma
di
wo
do
vr
za
zo

 1
 2
 3
 4
 5

 1
 2
 3
 4
 5
 6

 1
 2
 3
 4
 5
 6

 1
 2
 3
 4
 5

januari
20
 6
21
 7
22
 8
23
 9
24
10
25
11
12
26

13
14
15
16
17
18
19

april
14
15
16
17
18
19
20

21
22
23
24
25
26
27

 7
 8
 9
10
11
12
13

juli
14
15
16
17
18
19
20

21
22
23
24
25
26
27

 7
 8
 9
10
11
12
13

oktober
20
 6
21
 7
22
 8
23
 9
24
10
25
11
12
26

13
14
15
16
17
18
19

1997

februari
17
 3
18
 4
19
 5
20
 6
21
 7
22
 8
 9
23

10
11
12
13
14
15
16

mei
12
13
14
15
16
17
18

 5
 6
 7
 8
 9
10
11

19
20
21
22
23
24
25

24
25
26
27
28

26
27
28
29
30
31

augustus
18
 4
19
 5
20
 6
 7
21
22
 8
23
 9
10
24

11
12
13
14
15
16
17

25
26
27
28
29
30
31

november
17
 3
18
 4
19
 5
20
 6
21
 7
22
 8
 9
23

10
11
12
13
14
15
16

24
25
26
27
28
29
30

27
28
29
30
31

28
29
30

28
29
30
31

27
28
29
30
31

 1
 2

 1
 2
 3
 4

 1
 2
 3

 1
 2

31

30

maart
17
 3
18
 4
19
 5
20
 6
21
 7
22
 8
 9
23

10
11
12
13
14
15
16

juni
 9
10
11
12
13
14
15

16
17
18
19
20
21
22

 2
 3
 4
 5
 6
 7
 8

24
25
26
27
28
29
30

23
24
25
26
27
28
29

 1
 2

 1

29
30

september
22
 8
23
 9
24
10
11
25
26
12
27
13
14
28

15
16
17
18
19
20
21

 1
 2
 3
 4
 5
 6
 7

29
30
31

december
22
 8
23
 9
24
10
25
11
26
12
27
13
14
28

15
16
17
18
19
20
21

 1
 2
 3
 4
 5
 6
 7

Cluster viewer
(c) ECN 1998
Graphs

5/12/1997
31/12/1997
Cluster 710
Cluster 718
Cluster 719
Cluster 721
Cluster 722

employees

600

500

400

300

200

100

0
 6:00

 9:00

12:00

15:00

18:00

hours

Figure 2: Visualization of daily patterns [28], an example of the
combined use of conventional statistical and graphical methods.

Alternative methods are not limited to visualization methods.
For instance, when an automatic method exists to extract the rel-
evant information, visualization is useless. Visualization is not
’good’ by de(cid:2)nition, developers of new methods have to make clear
why the information sought cannot be extracted automatically. One
reason could be that such automated methods are not fullproof.
In this case, integration of automated methods, for instance from
statistics or data-mining, and visualization is a great idea, see for
instance the work underway and led by Jim Thomas in the Visual
Analytics arena [19].

Figure 2 shows an example where we used standard methods in a
new combination [28]. For the analysis of a time-series of one year,
daily patterns were clustered, i.e., (cid:2)nding similar daily patterns was
automated. The results are shown using two conventional repre-
sentations: average daily patterns of clusters are shown as graphs,
and the days per cluster are shown on a calendar. The approach is
straightforward and conventional, and very effective.

4.3 High initial costs
One important reason that new visualization techniques are not used
in practice is the high initial cost per user Cu(S0) involved. Let us
consider a potential customer for visualization, for instance a re-
searcher doing complex simulations. First, he has to realize that
maybe visualization can help him to understand his data. This is
not obvious, he already uses some methods to extract information
from his results in a condensed form. For instance in molecular dy-
namic simulations, one typical aim is to derive large scale quantities
(temperatures, porosity, etc.) via simulation from the properties on
a small scale (size of ions, (cid:2)elds, etc.). Such large scale quantities
can be calculated fairly easily from the raw data. Mathematicians
working in Computational Fluid Dynamics are often not interested
in particular (cid:3)ow patterns, but rather in convergence of numerical
methods and conservation of quantities, which again can be calcu-
lated easily and summarized in a few numbers.

The easiest way to visualize data is to use post-processing ca-
pabilities that are integrated with the software used. Commercial
packages for, for instance, computational (cid:3)uid dynamics or (cid:2)nite
element simulation offer these. From a visualization point of view,
the techniques offered are far from state of the art: Usually just
options like iso-surfaces, color mapping, slicing, streamlines and
arrow plots are provided. But if these meet the demands of our
user, then this is a highly cost-effective way.

Suppose that this option is not available or falls short. The next
step is to (cid:2)nd alternatives. Our researcher has to get acquainted with
possible solutions. Unfortunately, there are no books that present
and compare novel visualization techniques (like volume rendering
or topology based (cid:3)ow visualization) at an introductory level. So
he has to study research papers, or search and get in contact with an
expert in the (cid:2)eld.

Next steps are also costly. Maybe he can get a research prototype
to work with, or else he has to (or let somebody) implement the
novel techniques. Often additional software has to be developed to
convert his data to a suitable format.

This all takes much time and effort, while it is unclear whether
the new method will indeed solve his problem. Hence, a rational
decision is to abstain from this.

There are of course ways to share the initial costs with others. A
group of researchers can take advantage of an initial investment by
one of them. Also, providers of simulation software can be asked to
integrate new methods. Visualization does not seem to have a high
priority here however. For an impression of what providers think to
be important for their customers, we can have a look at web-sites
of companies like MSC or Fluent, and observe that features like
advanced simulation capabilities and tight integration are promoted
much more than visualization, which is just mentioned in passing
by under the header of post-processing.

4.4 Visualization is subjective
In the ideal case, one would hope that extraction of knowledge from
data is an objective process, in the sense that the outcome does not
depend on who performs it, and that the analysis can be repeated
afterwards by others, with the same outcome. Statistics aims at this,
a typical pattern is the use of statistical tests to validate hypotheses
on the data. Such tests make assumptions on the data (such as a
normal distribution) and have free parameters (like the con(cid:2)dence
level), but furthermore, they do meet the criteria for objectiveness.
Unfortunately, visualization often does not meet this aim. Con-

sider

dK
dt = P(V (D;S;t);K):

This simply means that the increase in knowledge using visualiza-
tion not only depends on the data itself, but also on the speci(cid:2)cation
(for instance, which hardware has been used, which algorithm has
been used and which parameters), the perceptual skills of the ob-
server, and the a priori knowledge of the observer. Hence, the state-
ment that visualization shows that a certain phenomenon occurs is
doubtful and subjective.

An even harder case is the statement that a certain phenomenon
does not occur. I have often spent hours visualizing data, searching
for patterns and structure. Sometimes some result could be pro-
duced using a particular setting of the parameters, in other cases I
failed to do so. When a visualization does not show clear patterns,
it is hard to decide if this is a limitation of the visualization method,
or that the setting of the parameters was wrong, or that the data
simply does not contain signi(cid:2)cant patterns.

This does not mean that visualization is useless.

If there are
no better alternatives to inspect complex data, visualization has to
be used. Another line of defense is that visualization should not be
used to verify the (cid:2)nal truth, but rather to inspire to new hypotheses,
to be checked afterwards. Part of the subjectiveness can be elimi-
nated by simply showing the visualization to the audience, so that
they can view and judge it themselves. However, this does not take
away the subjectiveness inherent in S, as a second hand viewer we
do not know how sensitive the ultimate visualization is to changes
in scales and/or selections of the data.

4.5 Negative knowledge
In the previous subsection we considered subjective aspects of vi-
sualization. There is another problem: Visualizations can be wrong
and misleading. Or, in the terminology introduced here, negative
knowledge (jDKj < 0) can be produced. Tufte has introduced the
lie-factor [23], which he de(cid:2)ned as the ratio of the size of an effect
shown in the graphic to the size of the effect in the data.

Here, I just want to give an example of my own experience with
this. A long time ago I visualized the waves produced by ships for a
maritime research institute. The data were the result of simulations.
Figure 3 (a) shows the result of bilinear interpolation of the data. I
found these results unclear, hence I decided to use an interpolat-
ing spline, thereby smoothing the surface while remaining faithful
to the data. Figure 3 (b) shows clearly that two sets of waves are
generated: the standard waves as well as a set of waves orthogonal
to this. I proudly presented this discovery to the researcher, who
immediately replied that this was physically totally impossible. A
much better visualization is shown in (cid:2)gure 3 (c), where an approx-
imating spline is used. The artifacts in the middle image are the
result of aliasing. The data orthogonal to the ship are sampled close
to the Nyquist frequency, interpolation gives rise to aliases, which
corresponding waves have in this 2D case a different direction than
the original wave. A smoothing interpolating spline smoothes away
the high frequencies, but the (cid:2)rst aliases survive and give rise to
wrong interpretations. I learned from this that interpolation is not
by de(cid:2)nition better than approximation, and also that the judgement

5 EXAMPLES

In this section a number of (classes of) techniques are considered
and the cost model is used to explain their adoption in practice.

5.1 Texture based (cid:3)ow visualization
The use of texture to visualize (cid:3)uid (cid:3)ow has been introduced in
the early nineties. The idea is that dense textures enable viewers
to judge the direction of (cid:3)ow at all locations of the plane, whereas
the standard arrows and streamlines only give discrete and hard to
interpret samples. The topic has been studied heavily in the visu-
alization community, a recent non-exhaustive overview [13] has 90
references. The progress made in this decade is great. The early
Spot Noise technique [24] was an interesting (cid:2)rst attempt, in 1993
Cabral and Leedom introduced Line Integral Convolution (LIC),
which gave high quality renderings of 2D (cid:3)uid (cid:3)ow [5]. Many
other variations and additions have been presented since then, for
instance to handle (cid:3)ow on surfaces and in volumes, and also to
boost the performance, using software or hardware acceleration
[13]. Nowadays, high quality 2D texture images of (cid:3)ow (cid:2)elds can
easily be generated on standard hardware at 50 or more frames per
second [25]. This seems a success story, but on the other hand,
these methods are not integrated in commercial software, users of
Computational Fluid Dynamics (CFD) are typically completely un-
aware of their existence, let alone that they routinely use them to
solve their problems. Here I use texture based (cid:3)ow visualization
because I am most familiar with it, but for other classes of meth-
ods, such as topology based (cid:3)ow visualization and feature based
(cid:3)ow visualization, similar patterns seem to apply.

How can we explain this? We consider the parameters of the
cost model. The number of users n is not too great. CFD is vital
for some areas, but there are few cases where CFD is routinely used
for screening, compared to for instance medical applications. The
frequency of use m is also not very high. Often, CFD-users spend
much time on de(cid:2)ning the model, simulations can also take a long
time. By then, they are very familiar with their models (high K0).
For the analysis of the results many alternative options are avail-
able, including composite quantities (such as lift of an airfoil) and
straightforward cross-sections and arrow plots, with low costs. The
use of texture based visualization incurs at least a high value for Cu
(see section 4.3). The additional DK that texture based visualiza-
tion offers is unclear. Laidlaw et al. [12] have compared different
vector visualization methods. LIC turned out to yield better results
for critical point detection, but worse results for other aspects, such
as estimation of the angle of the (cid:3)ow. Also, standard LIC does not
give the sign of the direction of the (cid:3)ow. Hence, we can doubt about
the value of DK. And (cid:2)nally, it is not clear what the real value is
of this DK, in the sense that better visualization leads to better deci-
sions. At least, so far there does not seem to be such a strong need
for better visualization methods in the CFD community that they
have attempted to integrate these methods into their packages.

5.2 Cushion treemaps
Also in the early nineties, Johnson and Shneiderman introduced the
concept of a treemap [8] to visualize large hierarchical data sets.
The base algorithm is straightforward: A rectangle is recursively
subdivided according to the hierarchical data, in such a way that
the size of each rectangle corresponds to the size of each leaf ele-
ment. In the late nineties we proposed to use hierarchical cushions
to show the underlying hierarchical structure more clearly [26]. We
packaged this technique in 2000 in SequoiaView [1], a tool for the
visualization of the contents of a hard disk ((cid:2)gure 4), and made this
publicly available as freeware. Since then, SequoiaView has been
downloaded about 400,000 times from our site. Also, it has been

Figure 3: Wave surface, from top to bottom (a) bilinear interpo-
lation, (b) cubic interpolation, (c) cubic approximation.
Incorrect
interpolation leads to artifacts.

of an expert, with a high K0, is vital for proper interpretation and
validation. I never published this, and also, articles on limitations
and pitfalls of visualization are scarce. For an advancement of the
(cid:2)eld, more such reports would be highly bene(cid:2)cial.

4.6

Interaction

Interaction is generally considered as ’good’. One could advocate
the opposite: Interaction should be avoided, and well for two rea-
sons. First of all, as mentioned before, allowing the user to modify
S freely will lead to subjectiveness. It is tempting to tune the map-
ping so that the desired result comes out strongly, but this can be
misleading. Also, high customization can make it hard to compare
different visualizations. Secondly, interaction is costly, and leads
to a high Ce. Rerendering the image after a change of the mapping
or the point of view taken requires often a few seconds, viewing
it again also. If many options are available to modify the visual-
ization, trying them all out can take hours. A developer of a new
method therefore should think carefully about good defaults, or au-
tomatic ways to set the visualization parameters, so that as much
knowledge is transferred as possible.

Obviously, in many cases interaction strongly enhances the un-
derstanding of the data. The most important case is simply when
the amount of data to be shown does not (cid:2)t on the screen, or is too
large to be understood from a single image. In this case, navigation
and selection of the data has to be supported. Ideally, the user has to
be provided with cues that will lead him quickly to images where
something interesting can be seen. Another case is during devel-
opment of new methods.
I stimulate my students to make every
aspect of their new methods customizable via user interface wid-
gets, so that the total solution space can be explored. However, for
the (cid:2)nal versions of their prototypes I recommend them to offer
suitable presets under a few buttons, so that a good visualization
can be obtained with little effort.

In a broader sense, we can view visualization everywhere. Com-
mercial television uses visualization to show the chemical miracles
of new cosmetics, the ingenuity of vacuum-cleaners, and why a
new (cid:2)tness device does not harm your back. Obviously, such visu-
alizations are probably not the result of visualizing data, but rather
the result of fantasy of advertisement agencies. Selling stuff is not
only the realm of business, but also of science itself. Once I heard
someone state: The purpose of visualization is funding, not insight.
We can explain the value of visualization for presentation with the
cost model. If we consider the viewers of such visualizations as
the users, we see that n is high; K0 is low (the viewers know little
about the topic, so much can be gained); the action to be taken is
clear (buy a product, fund research) and has direct economic conse-
quences; the costs for the viewers are low (they just have to watch
the visualization), although they can be high for the presenter. And
furthermore, for these purposes there are almost no alternative or
competing techniques. Pure facts (product X saves Y percent of
time) can be convincing, but to make plausible why, and also to
show that this is all Scienti(cid:2)cally Sound, visualization is the way to
go.

6 DISCUSSION

In the preceding sections a number of questions were raised and var-
ious disturbing statements were made. There are many objections
that can be made, and in this section some of them are given. One
important distinction is to consider visualization either as technol-
ogy, art, or as science. Associated with these are a number of routes
for future work.

6.1 Technology
In the cost model, visualization is considered as a technology, to
be measured for utility. In this context, research in visualization
should lead to new solutions that are useful in practice. Not all the
work done is successful in this respect, but we can (cid:2)nd a number of
reasons to explain this.

First of all, innovation is a merciless process, where only few
new solutions survive. A rule of thumb in product development is
that thousand ideas lead to hundred prototypes, which lead to ten
products, out of which just one is successful. The visualization
research community operates in the start of this pipeline, hence it
should come as no surprise that not everything (cid:2)nds its way. We
can see it as a mission to develop inspiring new ideas, which are a
primary fuel in the innovation process.

Creativity however consists of two parts: creation of new ideas as
well as selection of the best ones. The (cid:2)rst task is ful(cid:2)lled properly
by the visualization community, the second is not. The number of
careful validations of visualization methods is still low, although
this seems to be improving in the last years.

Secondly, innovation is a long chain. Developing new methods
is quite different from turning these into products and marketing
them. There is a gap between our prospective users and the research
community. Both do not have the proper stimuli to bridge this gap:
individual researchers are too busy increasing the number of pub-
lications they are judged on, and for the end-users implementing
new methods is far too costly. The gap can be (cid:2)lled in different
ways. One way is via commercial companies (spin-off companies,
or companies that integrate visualization in their simulation pack-
ages), an alternative is via open source and academic development
and maintenance, funded by government agencies. VMD [2] is an
example of the latter category. As a corollary, if we think that visu-
alization is useful and that this gap causes the lack of adoption, we
should aim at increasing funding for more practical activities. Or
we should start up companies.

Figure 4: Visualization hard disk using SequoiaView [1, 26, 27], an
example of an application that has found an audience.

distributed three times via CD with the German computer magazine
C’t. This is an example how visualization has reached an audience.
The economic model helps to explain this result. First, the num-
ber of (potential) users is very large, in principle equal to the num-
ber of PC users. Typically, such a tool is used several times per year,
which is not very high, but not neglectable. Alternative solutions
for this problem are scarce (SpaceMonger, using also treemaps is
an example), and getting an overview of a hard disk is hard using
Windows Explorer.

Information can be derived fairly easy from the visualization. It
is easy to spot large (cid:2)les, large directories, and large collections of
(cid:2)les. Furthermore, this information is directly valuable for the user:
The tool can help (and many users have con(cid:2)rmed this) to delay
buying a new hard disk. The action is clear here: removal of (cid:2)les.
We offer an option to start up Windows Explorer from SequoiaView
to remove (cid:2)les manually. The initial costs per user are low: The tool
itself is freeware, it only has to be downloaded and installed. The
costs per use case are minimal as well. By default, the tool starts
to collect data from the last folder speci(cid:2)ed, and an image is shown
automatically. Exploration is easy: Extra information per (cid:2)le can
be obtained by hovering the pointer over the rectangles.

In summary, F is high in this case. We would like to think that
this is a result of our visualization method, however, the main rea-
sons are probably that our tool meets a real need of real users, and
that the costs, in all respects, are minimal.

5.3 Presentation vs. exploration
Next we consider a more general case. The main use cases for vi-
sualization are exploration (where users do not know what is in the
data), and presentation (where some result has to be communicated
to others).
It is hard to quantify this, but my impression is that
many researchers in visualization consider exploration as the major
raison d’(cid:136)etre for visualization, whereas presentation is considered
as something additional and not too serious. However, from my
own experience, presentation is at least just as important as explo-
ration. Many users (cid:2)nd videos and images attractive for presenting
their work at conferences; the popularity of visualization tools and
demos often rises sharply just before open days. For years I had
a pleasant and fruitful cooperation with Flomerics Ltd. in the UK.
This company develops CFD-based tools for, amongst others, ther-
mal assessment for the electronics industry. My major contact there
was the marketing manager, who could use visualization to show
the bene(cid:2)ts of the CFD tools to managers.

Thirdly, one could state that all this is a matter of time. It takes
time before new ideas penetrate, before new users become aware of
new methods, before initiatives are taken to integrate new methods
into existing systems. This might be true in some cases, however,
it is also too easy to use this as an excuse. It could be used for any
method, hence it does not help us to distinguish between good and
bad ones.

Fourthly, the focus in the model is on large numbers of users and
use cases. One can also consider cases where the number of users is
small, but where the value of the result is very large. In the books of
Tufte some great cases are presented, such as Snow’s discovery of
the cause of a cholera epidemic in 1854 [21]. Are there recent cases
for new visualization methods? Cases that enabled the researcher
to obtain a major scienti(cid:2)c insight, to save many lives, or to solve
a crucial technological problem? One would like to read more case
studies in this spirit, which show that visualization is worthwhile
and can make a difference.

Finally, one defense is that maybe we are not doing too bad,
compared to other disciplines. Many disciplines (for instance, in
mathematics) do not care about practical usability at all, for some
computer science (cid:2)elds that do claim to have practical relevance it
is also hard to see the adoption in practice. Why should we bother?
This notion is explored further in the next subsection.

6.2 Art
One could claim that visualization has value in its own right, and
for its own purposes. One part of this is in the results: Some of the
images we produce have a clear aesthetic value. But the art of vi-
sualization can also be found in the ideas, methods, and techniques
developed. We can consider ourselves as a group of puzzle solvers,
and the challenge is to develop new, simple, and elegant solutions,
which provide us all with intellectual and aesthetic satisfaction.

This is not a line of defense that can help us to convince our
prospective users and sponsors. Nevertheless, I do want to mention
it, because it can give a powerful thrust (and obviously also because
results of this possibly will (cid:2)nd applications in the real world). In
the early nineties, I worked hard on using texture for visualization (cid:150)
not to satisfy users, but simply because the puzzle was tough, chal-
lenging, and hard to crack. The work of our student Ernst Kleiberg
on botanically inspired tree visualization ((cid:2)gure 5, [10]) was not
driven by user requests, but just an experiment to (cid:2)nd out if it could
be done at all. At the Information Visualization Symposium in 2004
we got two messages back. Alfred Kobsa found the usability lim-
ited, compared to other methods [11]; on the other hand, Stuart
Card showed this image in his keynote speech as an example of a
nice visualization. Is this a good visualization or not?

Finally, in my own work, I found aesthetic criteria on new meth-
ods to be guiding and effective. Sometimes, each link of the chain
from idea, mathematical model, algorithm, implementation to vi-
sual result is clean, simple, elegant, symmetric, etc. It is amazing
how much effort is required to reach this. Developing great ideas is
simple, rejection of bad ideas takes all the time.

6.3 Science
Apart from considering visualization as a technology, or as an art
for its own sake, we could consider visualization research as a sci-
enti(cid:2)c discipline. If there is something like a Science of Visual-
ization, what should it bother about? Loosely de(cid:2)ned, a scienti(cid:2)c
discipline should aim at a coherent set of theories, laws, and mod-
els that describe a range of phenomena, have predictive power, are
grounded in observations, and that can be falsi(cid:2)ed.

If we look at the (cid:2)eld now, many algorithms and techniques have
been developed, but there are few generic concepts and theories.
One reason for the lack of fundamental theories is that visualization
is intrinsically complex, has many aspects, and can be approached

Figure 5: Botanic visualization contents of a hard disk [10, 27].
Useful or just a nice picture?

from different perspectives. In terms of the model proposed, vi-
sualization can be observed from the point of view of the data D
to be visualized, the various solutions proposed (S and V), from
the DK aimed at, i.e., the purpose or discipline for which it is ap-
plied, the images I themselves, or from aspects such as perception
P or exploration E. Also, developing good visualization solutions
is intrinsically a design problem, and closed form solutions for the
optimalization problem (cid:148)Given D (cid:2)nd V such that DK is optimal(cid:148)
cannot be expected.

Nevertheless, we could and should aim at more generic insights,
at several levels. First of all, a descriptive approach can be pur-
sued further. Methods are analyzed and categorized, leading to tax-
onomies that show how they relate to and differ from each other.
Such taxonomies span up the current solution space, and can lead
to insight where new opportunities are. Some examples of good
overview papers are [30, 6, 13], a great example of a taxonomy is
given in [4], where a variety of different marching cube style algo-
rithms are brought under one umbrella using computational group
theory. Even if it were only because the (cid:2)eld is still developing
and overviews are quickly outdated, more work in this area should
be encouraged. Taxonomies need not be con(cid:2)ned to methods, also
taxonomies on different kinds of data and especially on different
types of knowledge that are relevant for end users are useful.

Secondly, evaluation and validation are important. Assessment
of the effectiveness and ef(cid:2)ciency of different methods and tech-
niques is vital from a technological point of view (which method to
use), but also as a base for more generic statements on visualiza-
tion. A science of visualization should be empirical, in the sense
that concrete measurements of the phenomena studied are done,
which in our case concern people making and watching images that
depict data. Tory and M¤oller [20] give a good overview of the cur-
rent status of the use of human factors research in visualization, and
identify areas for future research.

Thirdly, in line with the previous, we should ultimately aim at
generic results (models, laws) that enable us to understand what
goes on and to predict why certain approaches do or don’t work.
In the end, explanations should be based on properties of the en-
vironment of visualization, especially the end user. The value of
visualization is ultimately determined by his perceptual abilities,

his knowledge on the data shown, the value he assigns to various
insights, and the costs he is willing to spend.

Ware’s book on Information Visualization [29] is a rich source
of insights on perception and how these can be used to improve
visualization, Tufte gives many useful guidelines and recommen-
dations in his books [23, 21, 22]. However, many of these are not
quantitative, and also, do not explain how to handle con(cid:3)icting re-
quirements. One operational and practical criterium on guidelines
is that they should allow for automated implementation, such that
the user gets a good, if not optimal view on the data without costs.
The early work of Mackinlay [15] on automated generation of vi-
sualizations is great, and it is surprising that the state of the art in
this area does not seem to have advanced much further since then.
Finally, methodological issues have to be studied further. This
concerns questions like how to design visualizations and how to
measure and evaluate the effectiveness of various solutions. And
also, how to assess the value of visualization in general.

7 CONCLUSION

In the preceding sections, I have tried to answer the question how
the value of visualization can be assessed. As a conclusion, I think
there is not a single answer, but that it depends on the point of view
one adopts. One view is to consider visualization purely from a
technological point of view, aiming for effectiveness and ef(cid:2)ciency.
This requires that costs and bene(cid:2)ts are assessed. The simple model
proposed enables us to get insight in various aspects of visualiza-
tion, and also to understand why certain classes of methods have
success and others not. Another view is to consider visualization
as an art, i.e., something that is interesting enough for its own sake,
and (cid:2)nally a view on visualization as an empiric science was dis-
cussed.

Obviously, these three different views, schematically depicted
in (cid:2)g. 6, are strongly related, and results from one view can stim-
ulate work according to the other views. Finally, each view that
is adopted does imply playing a different game, and if we want to
win, we should play those games according their own rules: aim for
provable effectiveness and ef(cid:2)ciency, aim for elegance and beauty,
and aim at generic laws with predictive power.

technology

art

Visualization

Real world

science

Figure 6: Views on visualization

REFERENCES

[1] www.win.tue.nl/sequoiaview.
[2] www.ks.uiuc.edu/Research/vmd.
[3] Position papers NIH/NSF fall 2004 workshop on visualization re-

search challenges, 2004. visual.nlm.nih.gov/evc/meetings/vrc2004.

[4] D.C. Banks and S. Linton. Counting cases in marching cubes: Toward
a generic algorithm for producing substitopes. In Proceedings IEEE
Visualization 2003, pages 51(cid:150)58, 2003. (Best Paper award).

[5] B. Cabral and L. C. Leedom. Imaging vector (cid:2)elds using line integral
convolution. Computer Graphics, 27:263(cid:150)270, 1993. Proceedings
SIGGRAPH’93.

[6] I. Herman, G. Melanon, and M.S. Marshall. Graph visualisation in in-
formation visualisation: a survey. IEEE Transactions on Visualization
and Computer Graphics, 6(1):24(cid:150)44, 2000.

[7] B. Hibbard. Top ten visualization problems. SIGGRAPH Computer

Graphics Newsletter, 33(2), 1999.

[8] B. Johnson and B. Shneiderman. Treemaps: a space-(cid:2)lling approach
to the visualization of hierarchical information structures. In Proceed-
ings of IEEE Visualization ’91, pages 284(cid:150)291. IEEE Computer Soci-
ety Press, 1991.

[9] C. Johnson. Top scienti(cid:2)c visualization research problems.

IEEE

Computer Graphics and Applications, 24(4):13(cid:150)17, 2004.

[10] E. Kleiberg, H. van de Wetering, and J.J. van Wijk. Botanical visual-
ization of huge hierarchies. In Proceedings IEEE Symposium on In-
formation Visualization (InfoVis’2001), pages 87(cid:150)94. IEEE Computer
Society Press, 2001.

[11] A. Kobsa. User experiments with tree visualization systems. In Pro-
ceedings 2004 IEEE Symposium on Information Visualization (InfoVis
2004), pages 9(cid:150)16, 2004.

[12] D.H. Laidlaw, R.M. Kirby, J.S. Davidson, T.S. Miller, M. da Silva,
W.H. Warren, and M.J. Tarr. Quantitative comparative evaluation of 2d
vector (cid:2)eld visualization methods. In Proceedings IEEE Visualization
2001, pages 143(cid:150)150. IEEE Computer Society Press, 2001.

[13] R.S. Laramee, H. Hauser, H. Doleisch, B. Vrolijk, F.H. Post, and
Daniel Weiskopf. The state of the art in (cid:3)ow visualization: Dense
and texture-based techniques. Computer Graphics Forum, 23(2):203(cid:150)
221, 2004.

[14] B. Lorensen. On the death of visualization, 2004. In [3].
[15] J.D. Mackinlay. Automating the design of graphical presentations of
relational information. ACM Transactions on Graphics, 5(2):110(cid:150)141,
1986.

[16] Bruce H. McCormick, Thomas A. DeFanti, and Maxine D. Brown
(eds.). Visualization in Scienti(cid:2)c Computing. ACM SIGGRAPH, New
York, 1987.

[17] T.-M. Rhyne, B. Hibbard, C. Johnson, C. Chen, and S. Eick. Can we
determine the top unresolved problems of visualization? In Proceed-
ings IEEE Visualization 2004, pages 563(cid:150)565, 2004.

[18] B. Shneiderman, 2004. Comment during session at InfoVis 2004.
[19] J.J. Thomas and K.A. Cook (eds.). Illuminating the Path: Research

and Development Agenda for Visual Analytics. IEEE, 2005.

[20] M. Tory and T. M¤oller. Human factors in visualization research. IEEE
Transactions on Visualization and Computer Graphics, 10(1):72(cid:150)84,
2004.

[21] E.R. Tufte. Envisioning Information. Graphics Press, 1990.
[22] E.R. Tufte. Visual Explanations: Images and Quantities, Evidence

and Narrative. Graphics Press, 1997.

[23] E.R. Tufte. The Visual Display of Quantitative Information (2nd edi-

tion). Graphics Press, 2001.

[24] J.J. van Wijk. Spot noise: Texture synthesis for data visualization.
Computer Graphics, 25:309(cid:150)318, 1991. Proceedings SIGGRAPH’91.
[25] J.J. van Wijk. Image based (cid:3)ow visualization. ACM Transactions on
Graphics, 21(3):745(cid:150)754, 2002. Proceedings of ACM SIGGRAPH
2002.

[26] J.J. van Wijk and H. van de Wetering. Cushion treemaps. In Proceed-
ings 1999 IEEE Symposium on Information Visualization (InfoVis’99),
pages 73(cid:150)78. IEEE Computer Society Press, 1999.

[27] J.J. van Wijk, F. van Ham, and H.M.M. van de Wetering. Rendering

hierarchical data. Comm. ACM, 46(9ve):257(cid:150)263, September 2003.

[28] J.J. van Wijk and E. van Selow. Cluster and calendar-based visual-
In Proceedings 1999 IEEE Symposium
ization of time series data.
on Information Visualization (InfoVis’99), pages 4(cid:150)9. IEEE Computer
Society Press, 1999.

[29] C. Ware. Information Visualization: Perception for Design. (2nd Edi-

tion). Morgan Kaufman, 2004.

[30] P.C. Wong and R.D. Bergeron. 30 years of multidimensional multi-
variate visualization. In G.M. Nielson, H.H. Hagen, and H. Mueller,
editors, Scienti(cid:2)c Visualization (cid:150) Overviews, Methodologies, and
Techniques. IEEE Computer Society Press, 1997.

",False,0.0,{},False,False,journalArticle,False,V5DWUADP,[],self.user,False,False,False,False,,,The Value of Visualization,V5DWUADP,False,False
EGS763MR,5QZQTRCR,"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/228802868

From ‘What?’ to ‘Why?’: The Social Uses of Personal Photos

READS
428

Yuri Takhteyev
University of Toronto

13 PUBLICATIONS   702 CITATIONS   

SEE PROFILE

Article · August 2011

CITATIONS
32

6 authors, including:

Nancy Van House
University of California, Berkeley

53 PUBLICATIONS   1,492 CITATIONS   

SEE PROFILE

Nathaniel Good
University of California, Berkeley

48 PUBLICATIONS   3,019 CITATIONS   

SEE PROFILE

Some of the authors of this publication are also working on these related projects:

Activity centered computing View project

All content following this page was uploaded by Nathaniel Good on 29 October 2014.

The user has requested enhancement of the downloaded file.

From “What?” to “Why?”:  

The Social Uses of Personal Photos  

 

Nancy Van House, Marc Davis, Yuri Takhteyev, Nathan Good, Anita Wilhelm, and Megan Finn 

School of Information Management and Systems 

University of California at Berkeley 

102 South Hall 

Berkeley, CA 94720-4600 

+1 (510) 642-1464 

{vanhouse, marc, yuri, ngood, awilhelm, megfinn}@sims.berkeley.edu 

 
ABSTRACT 
To  predict  the  uses  of  new  technology,  we  present  an  approach 
grounded in science and technology studies (STS) that examines 
the social uses of current technology. As part of ongoing research 
on next-generation mobile imaging applications, we conducted an 
empirical  study  of  the  social  uses  of  personal  photography.  We 
identify  three:  memory,  creating  and  maintaining  relationships, 
and self-expression. The roles of orality and materiality in these 
uses help us explain the observed resistances to intangible digital 
images and to assigning metadata and annotations. We conclude 
that this approach is useful for understanding the potential uses of 
technology and for design.  

Categories and Subject Descriptors 
H.5.1  [Information  interfaces  and  presentation  (e.g.,  HCI)]: 
Multimedia;  H.5.2  [Information  Interfaces  and  Presentation 
(e.g., HCI)] User Interfaces - User Centered Design; H.4.3 [In-
formation systems applications]: Communications Applications; 
H.3.  [Information  storage  and  retrieval]:  Information  Search 
and Retrieval 

General Terms 
Design, Human Factors. 

Keywords 
Mobile camera phones, social uses, photography, social construc-
tion  of  technology,  science  and  technology  studies,  multimedia, 
orality, storytelling, digital imaging, metadata 

1.  INTRODUCTION 
Current  trends  in  design  focus  on  users'  needs,  activities,  and 
contexts.  However,  user-centered  design  is  most  feasible  when 
there are current uses and users for whom to design. An important 
problem  for  technology  design  is  predicting  users  and  uses  for 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise, or republish, to post on servers or to redistribute to lists, re-
quires prior specific permission and/or a fee. 
CSCW’04, November 6–10, 2004, Chicago, Illinois, USA. 
Copyright 2004 ACM 1-58113-810-5/04/0011…$5.00. 
 

emerging technologies—doing user-centered design for users and 
uses  that  don't  yet  exist.  In  this  paper,  we  present  an  analytical 
perspective  that  is  useful  for  theoretically-informed  research  on 
the emergent uses of new technology.  
This paper presents findings from an ongoing study of the social 
uses of personal photos and how these relate to current and future 
uses  of  imaging  technology.  We  demonstrate  how  the  approach 
described here has shaped the interpretation of our findings. The 
primary contribution of this work is in: (1) presenting a method 
for anticipating future uses of new technology by looking at the 
social  uses  of  present  technology,  in  this  case,  personal  photos; 
and (2) identifying a robust set of social uses of personal photos. 
Among the surprising findings are that the materiality of printed 
personal photos is important to many of their social uses and that 
the  social  functions  of  face-to-face  oral  interaction  help  explain 
consumer resistance to photo annotation.  
The work reported here is part of our research and development of 
next-generation mobile imaging applications. We wish to under-
stand how better to design applications for future programmable, 
networked,  mobile  imaging  devices  (especially  cameraphones). 
Cameraphones outsold digital cameras worldwide in the first half 
of  2003  and  are  predicted  to  offer  five  megapixel  resolution  by 
2008  [26].  From  their  technical  features  (accessible  operating 
system, application APIs, and wireless networking) and economic 
advantages in the US market (subsidy by wireless service provid-
ers),  cameraphones  may  likely  emerge  as  the  primary  imaging 
device for consumers in the next decade. However, we assert that 
without  understanding  and  designing  for  the  social  uses  of  per-
sonal imaging technology—not just what people do with current 
imaging  technology,  but  why—the  future  promise  of  mobile 
imaging may not be realized. 
The current study seeks to uncover underlying social uses of im-
aging  technology  that  will  enable  us  to  understand  what  factors 
will condition the migration of existing behaviors from cameras to 
future  cameraphones,  the  adoption  of  emerging  uses  of  camera-
phones  by  current  camera  users,  the  emergence  of  new  uses  of 
cameraphones, and the resistance to these migrations, adoptions, 
and  future  uses.  We  have  built  and  tested  a  cameraphone  photo 
annotation  prototype  that  leverages  spatio-temporal  context,  so-
cial  community,  and  user  interaction  at  the  point  of  capture  to 
describe media content [11, 35, 40]. The current study grows out 
of  that  research  and  our  desire  to  develop  a  methodology  and 
growing body of knowledge that can interpret current social uses 
of imaging technology to better inform the design of next genera-

tion mobile imaging devices and applications. 
A  common  theme  in  the  HCI  and  CSCW  literatures  has  been  a 
call  for  more  socially-informed  research  (e.g.,  Dourish  [12]). 
While CSCW in particular has tried to incorporate an understand-
ing of the social in design (e.g., [2, 12, 13]), many practitioners 
still find themselves without guidance in understanding users. Our 
approach is an effort to remedy this. Our work is inspired by sev-
eral current approaches to knowledge and work in social theory, 
but  not  completely  identified  with  any  one  approach.  Our  argu-
ment is that our approach is useful, which we illustrate with the 
example of our work on the social uses of personal photos. The 
analytical perspectives that inspire our approach stress the inter-
pretive flexibility of technology, the variety of motives for human 
action,  and  the  importance  of  the  material  and  cultural  contexts 
for  action.  Our  premise  is  that  to  understand  whether  and  how 
people will use—or resist—new imaging technology, we need to 
understand how they might interpret the new technology and use 
it to accomplish their activities. We investigate not just what they 
do with current technology, but why.  
In the rest of the paper we discuss related work in personal imag-
ing technology studies and systems (Section 2), methodologies for 
understanding  social  uses  of  technology  (Section  3),  our  study 
and its findings (Section 4), the underlying social factors we have 
discerned  that  condition  the  use of imaging technology (Section 
5),  and  their  implications  for  the design of future imaging tech-
nology (Section 6). 
2.  PRIOR RESEARCH  
In  the  HCI  literature,  much  of  the  work  related  to  photography 
has focused on designing systems to manage personal photo col-
lections through assigning keywords or innovations in clustering 
and visualization [5, 16, 23, 32] or facilitating sharing [4]. Unfor-
tunately, much of this design work was not connected to in-depth 
research  into  how  people  use  photos  and  was  only  validated  by 
assessing users’ performance on narrow tasks. 
Attempts to understand photo use have been made in other fields. 
Greenhill [17] investigated the role of photography in supporting 
family narratives. She discussed the functions of phototaking and 
sharing,  in  particular  photos’  non-communicative  functions  as 
part of childrearing and the enjoyment of holidays. Unfortunately, 
Greenhill's  findings  were  based  on  in-depth interviews with just 
one family. Chalfen [10] studied what he called “Kodak culture,” 
examining  200  collections  of  personal  photos.  By  asking  inter-
viewees  why  they  think  people  take  photos  he  identified  three 
functions  of  photography:  documentation,  memory  support,  and 
definition of cultural membership. These early studies, however, 
lacked interest in the design of imaging technology and with the 
advent of digital photography and mobile imaging in the last five 
years new research is warranted. 
More  recently,  Frohlich  et  al.  [14] studied users' needs with the 
aim  of  informing  technology  design.  They  studied  eleven  fami-
lies,  using  a  combination  of  ethnographic  field observations, in-
terviews, and diaries to ask what people do with conventional and 
digital  photos.  People  tried  to  arrange  their  best  photos  into  al-
bums, but they were unable to keep up with the backlog of pho-
tos.  People  preferred  sharing  prints  in  person  to  looking  at  the 
computer screen with other people. Frohlich et al. classified what 
people  did  with  their  photos  along  two  dimensions,  here  versus 
there,  and  now  versus  later,  creating  four  categories:  “remote 

sharing,” “sending,” “archiving,” and “co-present sharing.”  
Rodden and Wood [34] gave thirteen subjects digital cameras and 
software  for  organizing  digital  photos  and  analyzed  their  use  of 
both  prints  and  digital  images  over  a  six  month  period.  Again, 
participants  attempted  to  organize  prints  into  albums,  but  often 
fell behind. Some wrote captions on the back of the photos, but 
most  only  labeled  the  envelopes.  People  tended  to  keep  digital 
photos  organized  by  a  “roll”  of  photos  taken  around  the  same 
time.  Some  organized  digital  photos  into  albums,  and  assigned 
captions  to  individual  photos,  but  many  just  labeled  the  “rolls.” 
Rodden  and  Wood  observed  that  photos  tended  to  be  of  special 
events, such as holidays or weddings, and were taken to remem-
ber the events and were often discussed with friends and family. 
While the latter two papers contribute greatly to our understand-
ing of how people use photos, they focus predominately on low-
level actions (what people do) rather than on high-level activities 
(why they do it). 
A more activity-centered analysis is presented by Okabe and Ito 
[30] who have been studying the uses of mobile devices including 
cameraphones among young people in Japan. They conclude that 
the ubiquity of cameraphones is creating a “new kind of personal 
awareness”  and  changing  the  nature  of  the  images  that  get  cap-
tured—they are more likely to be casual, immediate moments of 
beauty  or  interest.  We  borrow  some  of  their  methodology,  but 
apply it to the social uses of personal photos in general with the 
aim of informing the design of digital imaging technology. 
Photos are not the only kinds of information artifacts that people 
share  for  social  reasons.  Marshall  and  Bly  [24]  looked  at  how 
people  share  “clippings,”  physical  or  electronic—e.g.,  posting 
articles on bulletin boards, emailing news items to people, cutting 
out published pieces for later use. They concluded that much of 
the  sharing  served  social  functions  beyond  simply  informing, 
including:  establishing  mutual  awareness;  educating  or  raising 
consciousness;  using  common  interests  to  develop  rapport;  or 
demonstrating knowledge of the recipient’s unique interests.  
These studies have described actual use of existing technologies.  
To  try  to  understand  future  uses  of  imaging  technology,  recent 
studies  have  used  projective  and  performance-based  methods. 
Iacucci et al. [21] had participants carry a “magic thing” (a non-
interactive  low-fidelity  prototype)  through  their day in a variety 
of contexts. Participants were told the magic thing had the func-
tionality of future devices and were asked to note down uses that 
occurred  to  them  in  real  world  contexts.  While  this  approach  is 
helpful in eliciting potential user actions, it is not focused on un-
covering the larger social uses in which these actions are situated.  
Others  have  looked  at  social  goals  as  a  way  of  understanding 
emergent uses of new technologies. Mynatt et al. [28], comparing 
physical and virtual communities, note that actions in one didn't 
“translate transparently” to other, and so one should “focus on the 
social  goals  of  the  activity  in  relation  to  the  affordances  of  the 
online environment” (p. 136).  
In sum, much of the work in HCI on imaging technology is con-
cerned  with  technology  for  managing  photos.  Imaging  behavior 
has also been studied in the social sciences. To project the future 
uses  of  new  technology,  however,  describing  people’s  current 
actions is insufficient. The approach represented by Mynatt et al. 
and Marshall and Bly stresses looking at the social uses of a cur-
rent  technology  to  anticipate  the  existing  social  uses  that  a  new 
technology may fit. 

3.  SOCIAL THEORY APPROACHES  
We draw on a number of socially-informed approaches to under-
standing human activity We will first describe briefly three such 
approaches, and then discuss common elements in these and re-
lated analytical perspectives that inform our approach. Finally, we 
describe the approach taken in this study. 
Activity Theory has been used in HCI to help understand context, 
situation,  and  practice  [1,  19,  33].  Nardi  [29]  describes  activity 
theory  as  having  three  main  concerns:  consciousness,  the  asym-
metrical  relationship  between people and things, and the role of 
artifacts in everyday life. The stress on consciousness means that 
behavior  cannot  be  understood  without  reference  to  the  user’s 
intentions which are related to current material and social condi-
tions.  Artifacts  are  mediators  of  human  thought  and  behavior. 
They carry a history of social practices, of how people do things 
as well as how they understand, and therefore have a large role in 
shaping users’ behavior and understandings. The specific material 
form of artifacts is significant for, among other things, how they 
carry culture and history, and interact with embodied action. 
Activity theory has a precise framework and terminology for de-
scribing  the  relationships  among  “object”  (that  which  is  being 
transformed, which may be material or immaterial, e.g., a plan), 
subject,  activity,  action,  operations,  and  tools.  Since  our  goal  is 
not  to  use  activity  theory  as  our  governing  framework  but  as  a 
generative  approach,  we  can  simply  say  that  specific  actions  or 
tasks  can  only  be  understood  in  terms  of  higher-order  motives, 
intentions, or activities. A variety of actions are possible for any 
higher-order activity, and a variety of activities may motivate any 
action. A person taking a picture (an action) may be engaged in 
any number of activities. It’s impossible to understand the user’s 
activities or goals by observing her actions, and we cannot under-
stand actions without understanding the user’s intentions or activi-
ties.  Because  the emphasis is on users’ own understandings, the 
methodology of activity theory is largely ethnographic. 
Distributed cognition (DCog) [19, 20] has also been used in HCI. 
While  activity  theory  is  largely  concerned  with  the  individual, 
distributed cognition is largely concerned with the distribution of 
cognitive  activity  across  individuals.  Both  approaches  are  con-
cerned  with  the  distribution  of  activity  between  the  human  and 
non-human, people and their tools. DCog addresses how artifacts 
shape as well as are shaped by how people think, see, and under-
stand; and, like activity theory, how the study of (cognitive) activ-
ity cannot be separated from the history of material artifacts and 
social practices. Halverson [19] describes both DCog and activity 
theory as seeing “the world of artifacts, personal history, culture, 
social,  and  organizational  structure  through  a  filter  that  labels 
them  as  the  residue  of  collaborative  cognition,  analyzed  along 
numerous time scales” (p. 246).  
The  field  of  Science  and  Technology  Studies  (STS)  [38]  shares 
with HCI a concern for the relationship between the social and the 
technical. With a few exceptions [2, 37, 39], however, there has 
been  little  crossover  between  STS  and  CSCW.  One  analytical 
perspective  within  STS  is  Social  Construction  of  Technology 
(SCOT) [6, 7]. SCOT has been used to explain after the fact how 
a  given  technology  eventually  gets stabilized. A key element of 
SCOT is interpretive flexibility: a given artifact may have differ-
ent meanings (including uses) for different groups. This meaning 
is constrained but not determined by the design and is created by 
users  as  they  match  the  possibilities  of  the  technology  to  their 

problems or desires. A successful design is used by multiple rele-
vant social groups for varied uses. In a classic SCOT study, Bijker 
[6]  showed  how  the  design  of  the  bicycle  varied  over  50  years 
before  it  stabilized  into  what  we  would  recognize  today.  The 
“young  men  of  nerve  and  means”  who  wanted  racing  machines 
and  the  people  who  wanted  bicycles  for  transportation  both  ac-
cepted  rubber  tires,  for  example,  which  proved  to  be  both  com-
fortable and fast.  
Resistance occurs when the design—or the policies and practices 
of the designers or operators of the technology—does not fit the 
intentions and activities of its users. Kline [22] reports that when 
phones  were  introduced  in  rural  America,  the  telephone  compa-
nies tried to define eavesdropping and joining into others’ conver-
sations as rude, because n-way conversations drained the compa-
nies’  batteries  faster.  However,  these  practices  fit  the  commu-
nity’s  prior  practices  of  casual  group  socializing  and  helped  re-
lieve the isolation of farm residents. They key explanatory move 
in SCOT is to show how a technology gets adopted and its design 
stabilized (however briefly) when multiple groups find it a work-
able solution to one or more of their (often differing) problems. 
SCOT  and  related  approaches  have  provided  effective  post  hoc 
explanations for why some technologies have succeeded and oth-
ers failed. Our approach is a kind of reverse SCOT. We argue that 
to  conjecture  about  whether  and  how  people  will  use  emerging 
technology  and  to  optimize  the  design  accordingly,  we  need  to 
understand  people’s  prior  social  activities,  goals,  and  problems, 
and then hypothesize about how the technology in question may 
fit these conditions and be adopted, or fail to fit and be resisted.  
4.  THIS STUDY 
4.1  Conceptual Framework 
For our approach, we draw on several elements common to social 
constructivist approaches to human action. First, these approaches 
posit a “seamless web” of technology and the social, politics, and 
economics. Second, they stress ethnographically-informed meth-
ods that seek to understand participants’ own interpretations [8]. 
Social constructivist and ethnomethodological approaches assume 
that social institutions are actively constructed by ordinary mem-
bers of society in their moment-to-moment, improvisational solu-
tions  to  practical  problems.  These  situated  approaches  give  an 
important place to practice, people’s actual, daily, embodied ac-
tions, including their interactions with others and with resources, 
including  tools,  which  carry  a  history  of  prior  social  uses  and 
understandings.  Artifacts  both  shape  and  are  shaped  by  users’ 
understandings.  They  are  not  just  extensions  of  human  action; 
they  are  intimately  involved  in  the  construction  of  action  and 
meaning and its persistence across time and place. 
Our  contention  is  that  to  understand  how  people  will  use  new 
technology, we need to look, not just at what they do with current 
technology, but why. Then we can ask how new technology may 
fit  those  motives,  goals,  and  practices,  the  entire  interdependent 
matrix  of  action,  artifacts,  meanings,  practices,  and  social  rela-
tions,  and  how  it  might  be  designed  to  better  exist  within  and 
support them. Other research has asked what people do in captur-
ing, storing, retrieving, and using images. Our concern is why. It 
is  possible,  even  likely,  that  with  changes  in  technology  people 
will use personal photos for purposes other than the current ones, 
but  to  begin  with  it  is  useful  to  look  at  the  current  purposes  or 
intentions of use.  

Asking people “why” is sometimes useful but not sufficient. Their 
answers are likely to be at the action level rather than the activity 
level. And, as ethnographic research posits, people are often un-
able to articulate exactly what they do and why. At the same time, 
we  cannot  ascribe  our  reasons  to  their  actions.  Our  approach, 
therefore,  is  ethnographically-informed,  consisting  of  interviews 
with people, observations of their photos and photo use, and pro-
jective questions about possible use scenarios freed from current 
technological constraints in order to uncover social uses. 
4.2  Goals of This Study 
Our primary concern is the social uses of personal photography: 
the  reasons  people  take  photos,  the  kinds  of  photos  they  take, 
what  they  do  with  them  afterward  (including  which  photos  and 
how many photos they keep, whether and how they assign meta-
data,  including  captions  and  annotations),  and  where  and  how 
they  store  photos.  We  were  also  especially  interested  in  their 
photo sharing practices: with whom, how, when, and what kinds 
of photos they share with others. From this we derived a set of the 
social uses of personal photos. The purpose of this study was both 
to  identify  these  uses  and  to  test  the  approach of seeking social 
uses to explain observed and reported actions. 
4.3  Methods 
The  data  reported  in  this  paper  come  primarily  from  a  series of 
interviews with casual photographers about their personal photog-
raphy,  including  analog  camera  users,  digital  camera  users,  and 
cameraphone users. In addition, we have collected data from sev-
eral other sources, which we draw on in this paper. We conducted 
two focus groups of seven and eight graduate students in informa-
tion  management  and  systems  to  discuss  their  image  capture, 
storage, sharing, and retrieval habits. We examined a total of 20 
publicly  accessible  photo  collections,  ranging  from  10  to  5000 
photos. The collections included personal photo albums focusing 
on friends, family, and events, a genealogical album with photos 
of  ancestors,  portfolios  of  serious  photographers,  and  individual 
and collective photoblogs with and without themes. 
Through  informal  channels,  we  identified  willing  study  partici-
pants who had been taking pictures for at least a year; had used 
their present camera for at least six months; and took a minimum 
of  about  50  pictures  a  year.  We  did  not  require  that  they  used 
digital imaging technology; all but three did, though many were 
far from avid digital users. We interviewed a total of 13 people 
about their practices of taking, sharing, annotating and retrieving, 
and  using  photos.  Since  much  personal  photography  revolves 
around family and especially children, we sought a mix of people 
with  and  without  children,  but  we  found  that  some  of  our  “sin-
gles” still took many pictures of the children of friends and fam-
ily. We interviewed: five individuals without children; two indi-
viduals (one single, one married interviewed alone) and one cou-
ple with children living at home; one couple without children; and 
one pair of a grandmother and great-grandmother. Four of these 
interviewees  (one  couple,  one  pair  of  roommates)  were  camera-
phone  users.  [Note  to  reviewers:  we  are  continuing  these  inter-
views and will update the final paper, if accepted, to incorporate 
later interviews.] 
Interviews were conducted in the participants’ homes, and lasted 
about  two  hours.  We  asked  them  to  show  us  their  cameras  and 
their photos. We videotaped the interviews, and took both video 
and  still  photos  of  their  cameras,  photos  and  photo  storage,  and 

the photos displayed around their home. 
A subset of these interviews was specific to cameraphone users. 
Our goal in the cameraphone interviews was to interview dyads, 
at  least  one  of  which  was  a  camera  phone  user. We were inter-
ested in what sorts of photos people take with cameraphones and 
how they share them. At this point, we have interviewed two such 
pairs.  Our  focus  groups  with  graduate  students  were also all re-
cent cameraphones users. Our findings support those of [30] that 
people tend to take different kinds of pictures with cameraphones: 
random things to make friends laugh, things they find interesting 
or beautiful, and photos of friends 
We asked questions all participants about the following:  
(1) Their camera equipment and photography habits: what kind of 
camera they own and how they decided to purchase it, what they 
do and don’t like about their camera, and, finally, to get at possi-
ble future uses we asked, “If we had magic technology that could 
do anything you wanted, what do you wish your camera could do 
that it doesn’t now?” [21]. 
(2)  Their  phototaking  patterns:  when  and  under  what conditions 
they take photos, of what, how often. We asked whether the pho-
tographer gets to be in the photos, and what makes some photos 
special.  
(3) Their photo storage and retrieval, including which photos they 
keep and why, how long, where, how organized and labeled, and 
how they find older photos. We asked them what would make this 
process easier. 
(4) Their photo sharing, including under what circumstances and 
how they show or send photos to others, what kind of photos, with 
whom,  why,  how,  and  whether  and  how  they  annotated  or  cap-
tioned photos. We asked the same about the photos others share 
with them. And again, we asked what they would like to be able 
to do differently, what would make photo sharing easier. 
4.4  Findings  
In this section we report what people did: what photos they took, 
and what they did with them. In the following section we discuss 
the social uses of personal photos. 
4.4.1  Cameras  
Most of our participants owned or had access to a digital camera. 
While some were avid digital users, others were still getting ac-
quainted  with  digital  photography.  Most  had  multiple  cameras, 
often both analog and digital. Those who actively used multiple 
cameras  tended  to  have  particular  uses  or  reasons  for  each.  For 
example, one person found the shutter lag on her digital camera 
too slow for candid shots of children, so she used both a digital 
camera  and  an  analog  point-and-shoot  camera.  Several  people 
used analog cameras because they had interchangeable (especially 
zoom)  lenses.  Most  of  the  digital  cameras  were  smaller  and 
lighter than analog cameras and so people tended to carry digital 
cameras around more, reserving analog cameras for photo expedi-
tions.  Most  participants  believed  that  analog  cameras  created 
better quality images (though few had tested this for themselves). 
Participants who carried both analog and digital cameras reported 
capturing  more  “important”  images  on  the  analog  cameras  (i.e., 
pictures they think they will cherish for a long time).  
4.4.2  Phototaking Patterns 
Our findings about the kinds of pictures that people take are con-

 

 

sistent  with  those  of  earlier  research.  Among  our  participants, 
pictures tended to be of family and friends, vacations, and special 
events. Pets were also popular. We identified two other distinctive 
types  of  photos:  “art”  (taken  for  aesthetic  reasons)  and  “fun” 
(funny in and of themselves, or in the context in which they were 
to  be  used).  For  example,  one  participant  takes  daily  pictures 
containing a gnome that a friend posts on the web (See Figure 1). 

 

Figure 1. “Gnome at Grand Canyon” from gnomar.com 

Participants who had taken other kinds of photos earlier, but then 
had children come into their lives, reported a sharp drop in non-
family photos [10] 
Both  print  and  digital  photos  were  subject  to  what  we  called  a 
“funnel effect” in which many photos taken, while only a few get 
added  to  albums.  Ratios  varied  from  10-to-1  to  one  participant 
displaying all but a few “indescribable” photos. Most commonly, 
between  10%  and  25%  of  photos  taken  were  put  on  display  in 
photo albums, frames, bulletin boards, refrigerators, and the like.  
Our  preliminary  review  of  photos  online,  including  photoblogs, 
showed  that  people  use  online  sites  for  many  of  the  same  pur-
poses (friends and family, vacations, events), but with a prepon-
derance of fun or art pictures which are more likely to be mean-
ingful to strangers on public photo sites.  
4.4.3   Storage and Retrieval 
 
Consistent with other studies [16], we found that time is a major 
organizing principle for most photo users, both digital and analog. 
Photos  taken  over  time  are  automatically  ordered  by  both  tech-
nologies: all prints from a roll of film come back from developing 
sequenced in an envelope; the photos downloaded from a camera 
to a computer are given sequential identifiers based on when the 
photos  were  taken  or  downloaded.  Many  users  reported  being 
“too lazy” to annotate and impose their own organization on pho-
tos. And, for the most part, time is a useful organizing principle. 
Photos taken at or near the same time are often of the same con-
tent.  A  favored  few  images  get  added  organization  by  person, 
place, or event. Most digital camera users had no more than one 
layer  of  folders,  with  folders  given  a  descriptive  name  about 
place, event, or person: e.g., “Mexico,” “vacation,” “family.” 
4.4.3.1  Archiving 
Everyone  who  had  prints  had  what  we  came  to  refer  to  as  “the 
box,”  often  multiple  boxes,  drawers,  and  sometimes  bags:  the 

 

place(s)  where  most  prints  ended  up,  not  in  albums  but  in  the 
envelopes  from  which  they  came  back  from  processing.  While 
participants  differed  in  their propensity to throw away photos—
which seemed to correlate with their overall habits of collecting 
versus  discarding—many  found  it  much  harder  to  throw  away 
prints than to delete digital images. Some talked about preserving 
the  integrity  of  a  “roll”  as  a  record  of  an  event,  throwing  away 
only the greatest failures. Many surprisingly saw prints as more 
appropriate than digital media for archiving images. Some spoke 
of  computer  failures  and  losing  image  files.  (Although  one  re-
ported having her house burn down with all her prints and another 
participant lost a prized envelope of selected photos of her family 
on an international flight.) Since some had old family photos that 
had been handed down in paper form (none reported having old 
negatives), their sense of paper as an appropriate archival medium 
is based in part on experience. Some of the digital users worried 
about the obsolescence of digital storage media. Some of the digi-
tal users stored digital images on CDs, not on their hard drives. 
4.4.3.2  Annotation and Metadata 
Most participants reported minimal annotation, most commonly a 
scribble on the outside of an envelope of prints noting date, loca-
tion, or event, and maybe people: e.g., “Yosemite, Summer 2002, 
with Jeff.” Digital photos are sometimes given descriptive names 
if and when users edit and save a photo: e.g., “girls&santa.jpg.” A 
few participants—working with paper prints—do extensive anno-
tation about the photo and its circumstances, in essence telling a 
story about the photo, in the margins of photo albums or on the 
pages of a scrapbook. 
Most participants tended to rely on their own memories concern-
ing the content of photos. They generally wanted the photos dated 
and appreciated prints with the dates on the back, while they uni-
versally hated digital images with dates embedded in the image. 
They were less concerned with recording other metadata, gener-
ally saying that they knew the people and places. This reliance on 
memory  instead  of  metadata  had  several  possible  reasons.  First, 
participants complained about the time and effort required to an-
notate photos (and organize them in albums or folders). Second, 
as we discuss below, the act of face-to-face oral storytelling with 
photos was important. We asked people if they would like a way 
to record audio clips with pictures—annotations and stories, simi-
lar to the current “talking frames.” Reactions were mixed. In es-
sence, people did not want to do the recording. But another reason 
that seemed to be more potent was the preference for face-to-face 
storytelling outweighed any perceived benefits of recorded audio. 
4.4.4  Sharing Photos 
When we asked with whom they shared photos, the answer was, 
understandably  enough,  mostly  family  and  friends.  When  we 
asked  which  photos  they  shared,  the  prevailing  answer  was  im-
ages of people or events of significance to the recipient. One per-
son said that the grandparents wanted photos of the grandchildren, 
not of the family’s vacation; they were interested in the people, 
not the place A few participants maintained photoblogs and had 
the added dimension of sharing photos with “fans” of their blogs 
who included known and unknown people.  
People  often  shared  photos  by  simply  passing around envelopes 
of prints. Some left prints lying around in high-traffic areas of the 
house for people to look at as they wished. A recurring artifact in 
our  visits  to  people's  homes  was  a  wall,  shelf,  or  mantlepiece 
covered with photos of family and friends (See Figure 2). These 

photos  are  always  on  view,  and  act  as  a  continual,  passive  re-
minder of persons and events. 

 

Figure 2. Home Display of Family Photos 

Some participants used digital cameras or cameraphones to trans-
port,  display,  and  share  digital  images.  Sometimes  they  simply 
looked at images on the camera, in other cases, they plugged the 
camera into the TV. Most participants were not opposed to view-
ing images on computers, and some even commented on the qual-
ity of the image (larger and crisper than a print). Some did view 
images on their computer with family and friends. Those whose 
images were mainly digital used the computer for their own view-
ing  and  others’.  But  the  sociality  of  viewing  images  together 
seemed to be associated in most participants’ experience with the 
act of viewing prints, especially in photo albums. 
Some talked about emailing photos, especially to distant family, 
and  some  used  or  wanted  to  use  photo  sharing  websites.  Many 
received images as attachments or URLs. People were much more 
inclined  to  delete  email  attachments  than  to  throw  away  photos 
received  in  the  mail.  Several  said  that  when  they  share  photos 
they prefer giving (and receiving) photos hand-to-hand rather than 
mailing  them.  One  person  wanted  to  be  able  to  “squirt”  photos 
from her PDA to another’s using infrared, because she spoke of 
the other as being in the same room, not distant. Particularly good 
photos may be framed as gifts. Photos, ranging from loose snap-
shots to framed portraits, have a clear connotation of gift [25].  

Figure 3. Photo Sharing with a Photo Album 

 

The most striking finding was the connection between prints and 
sharing.  Everyone  we  talked  with  had  images  displayed  around 
the house—the mantlepiece or shelf of family photos (See Figure 
2), or the accretion of pictures on the refrigerator. Photo albums 
have  a  particular  place  in  photo  sharing.  The  act  of  looking  at 
(and, more rarely, making) an album is a social act, two or three 
people sitting together to look at the pictures and tell stories (See 
Figure 3). While many albums are chronological, some represent 
special events (vacations, anniversaries) and some are of people—
one respondent is making an album for each of the children in her 
life, with photos showing them over time. Interestingly, while all 
participants enjoyed sharing prints with other outside of the home, 
people  didn’t  take  their  photo  albums  to  other  people’s  houses, 
but would show them to visitors. 
4.4.5  What features do they want? 
When we asked people what features they would want on a cam-
era,  the  most  commonly  named  was  zoom.  Many  didn't  explain 
why—we gathered three major reasons. First, zoom gives people 
more control over the image itself, the ability to, in essence, crop 
an  image  in  the  camera.  Second,  zoom  gives  some  control  over 
place or location: the photographer can move closer without mov-
ing, for example, when taking a picture across water. Third, zoom 
allows a difference between social and physical space. One per-
son commented on wanting to take a picture of someone sitting at 
a sidewalk café in a gorilla suit, but was reluctant to pull out her 
zoomless camera so close to the subject. Interestingly, most cur-
rent cameraphones (which lack zoom and have fairly wide angle 
lenses) require that their users enter a subject’s intimate space of 
physical  proximity  (2  feet  away  or  less  [18])  in  order  to  get  a 
close-up  shot  of  a  person’s  face.  Another  highly  valued  feature 
was  flash,  which  gave  people  more  independence  from  lighting 
constraints and enabled night photography. Digital camera users 
often  wished  for  better  resolution—most  had  moderately-priced 
cameras  with  similarly  moderate  resolution.  Several  people 
wanted  a  digital  camera  for  children:  inexpensive  and  rugged. 
Several  told  us  that  their  children  did  take  pictures,  even  fairly 
young children. Digital cameras would provide the instant gratifi-
cation of seeing their image, and avoid the cost of printing.  
5.  DISCUSSION 
5.1  The Social Uses of Personal Photos  
Our findings provide a catalog of what our participants said and 
demonstrated about their personal photography practices. Under-
lying these various actions are social uses that these actions sat-
isfy.  We  identified  a  set  of  social  uses  which  seem  to  motivate 
and  shape  the  imaging  practices  we  observed  and  documented: 
memory (both personal and collective), relationships (both creat-
ing  and  maintaining),  and  self-expression.  These  social  uses 
interact  with  material  aspects  of 
technology, 
embodied social communication, and narrative activity employed 
by  our  participants.  Specifically,  these  uses  help  to  explain  the 
high value placed on the materiality of photographic artifacts, the 
surprising  centrality  of  unmediated  oral  communication  in  our 
subjects’ use of photos, and the recurrent use of photos in story-
telling, which calls upon and serves all of these factors: memory, 
relationships, self expression, materiality, and orality. 
5.1.1  Memory  
A major theme in the interviews was the role of photographs in 
memory, personal and collective. Images have an ability to evoke 

imaging 

 

the 

memories,  including  sensual  memories.  One  respondent  who  is 
now seriously ill spoke of viewing pictures from earlier parts of 
her life, especially travel, and remembering not just the sights but 
the tastes and smells of other places. An active photoblogger real-
ized, after a year of photoblogging, that she had a record of her 
life during that period for herself and her as-yet unborn children. 
Photos are not only about one's own memories but others’. Our ill 
participant is preparing albums for each of the children in her life 
consisting of photos and written stories about times she spent with 
them.  She  says  that  this  is  not  only  so  that  they  will  remember 
her,  but  to  help  them  see  what  they  themselves  were  like.  We 
conjecture  that  people's  attitudes  toward  photo  annotation  are 
associated with issues of memory and mortality. The person de-
scribed  above  spoke  frankly  about  wanting  the  children  to  re-
member that she had been a part of their lives. On the other hand, 
we interviewed a 98½ year old great-grandmother and her daugh-
ter, neither of whom was especially worried that the older woman 
was the only person who knew the identities of many people in 
her extensive collection of family photos.  
The memory function of photo use has informational components, 
but is strongly emotional. Favored images were usually spoken of 
not in terms of the quality of the image but of the memories and 
emotions evoked. In this context, “the box” has a particular bene-
fit:  rummaging  through  a  box  of  photos  creates  unexpected  en-
counters  with  images  and  thus  with  memories,  an  unplanned, 
undirected  revisiting  of  events,  people,  and  emotions.  A  couple 
looking  through  their  images  with  us  exclaimed  with  pleasure 
when the found an image of an event they had forgotten. 
5.1.2  Relationships 
The strong presence of family and friends in people's photos high-
lights  the  importance  of  interpersonal  relationships  and  photos. 
Photos are used, not just to remember people and events, but to 
maintain existing relationships and even create new ones. Photos 
were  valuable  not  only  for  themselves  but  for  the  connections 
among them and among the people represented, and for the active 
role they played in relationships.  
People were often identified (by themselves or others) as the fam-
ily/group  photographer  or  the  family  archivist.  These  people 
tended to see the task of maintaining the photographic record as 
critically  important,  especially  within  families.  One  interesting 
issue  is  how  the  photos  of  earlier  generations  migrate  forward. 
The informal family archivists keep track of who has which old 
family photos and try to acquire and consolidate the collection. A 
student whose family is now spread across at least two continents 
brought back old family pictures from a recent trip to his family’s 
homeland and is now trying to identify the subjects and their fam-
ily  relationships.  He  is  scanning  the  images  to  create  CDs  for 
family members. These photos were not simply informative, but 
were material traces of the continuity of the family over time and 
place.  Other  participants  are  the  photographers  for  a  family  or 
social group who count on them to take pictures. Unfortunately, 
the photographer is rarely in the picture: the person who cares the 
most about documenting events and keeping track of friends and 
family is often the least visible.  
Many people spoke of sharing photos to keep people up on what's 
going  on  in  one's  life,  as  a  form  of  reporting  or  journaling,  but 
also as a way of connecting to loved ones. We spoke with a cou-
ple  who  had  spent  a  year  living  on  separate  continents,  during 
which they used photoblogging extensively and would send pho-

tos  to  their  private  photoblog  in  near  real-time  “like  a  kiss  or  a 
hug.”  Cameraphone  users  talked  of  sending  photos  sporadically 
throughout the day just to make the other laugh. The sense of real 
time capture and sharing (i.e., the “Power of Now” we identified 
in our focus group studies [40]) was important to the senders. One 
way  that  online  images  help  maintain  relationships  is  when  a 
viewer  finds  that  a  photographer  has  posted  an  image  of  the 
viewer—an indication that one is important to the other. A young 
person’s photoblog had a section labeled “friends” with the nota-
tion, “If you’re here you know you’re loved.” 
While traditional photo sharing served largely to maintain exist-
ing relationships, the photobloggers also used their blogs to create 
new  social  relationships.  One  person  discovered  that  her  blog 
helped  her  to  make  connections  in  a  city  where  she  knew  few 
people: her blog had readers, some of whom she connected with 
via the blog, some of whom she met, including one who recog-
nized from her images that they lived in the same neighborhood.  
Photos—especially  photoblogs—are  also  a 
form  of  self-
presentation [15], which is about managing others' impressions of 
oneself.  Like  personal  webpages  [27],  photoblogs  are  a  way  of 
creating an online identity. Photobloggers did not want to incor-
porate other people’s photos in their blogs since their blogs were 
about “their own point of view on the world.” 
5.1.3  Self-Expression 
Photos  are  also  used  as  self-expression,  including  art  and  fun 
images.  Although  self-presentation  and  self-expression  are  re-
lated, self-presentation is about influencing others' views of one-
self (which may include deception), while self-expression is about 
giving  expression  to  our  “authentic”  self.  Several  participants 
clearly  distinguished  their  picturetaking  that  was  for  recording 
family  events  from  their  photos  for  self-expression.  Two  of  our 
participants worked in black and white at least part of the time for 
their art photography, and color for other photography. One nota-
ble  finding  from  our  review  of  online  personal  photos  was  how 
many seemed to be intended to be artistic or beautiful images. 
5.2  Media and Resistance 
Understanding image-related activity helps to explain two surpris-
ing findings from our empirical work: participants’ attachment to 
printed images, and their resistance to recording metadata. These 
two areas of resistance, which might have been seen as unreason-
able  or  ill-informed,  are  understandable  when  we  consider  the 
social uses to which people put images. 
5.2.1  Materiality 
A major theme in our interviews was the ways in which people 
used the affordances of the materiality of printed images. Many 
participants relied heavily on prints, even of digital images. The 
exception seemed to be users who had access to web-based tools 
for sharing photos, or photoblogs, who printed less. 
The social theoretical approaches to activity and distributed action 
that  we  described  in  Section  3  stress  the  importance  of artifacts 
and  their  particularity  as  shaping  behavior  and  carrying  prior 
understandings  and  practices.  They  also  stress  the  interpretive 
flexibility of technology and how people find ways to align arti-
facts and practices to accomplish their goals. 
The materiality of prints interacts with the social uses of images 
and the practices of creating, using, and sharing them in striking 
ways. Displayed and casually scattered prints enabled unplanned 

and  repeated  encounters  with  images.  Participants  generally 
treated  prints  as  more  precious  and  less  easily  discarded  than 
electronic  images.  The  sharing  of  prints  also  had  clear  connota-
tions of gift. People generally preferred sharing them face to face 
if possible. Even when the image wasn’t of interest, the fact of the 
gift of a photo was considered to be a significant part of relation-
ship maintenance and an expression of caring and connection.  
Participants  expressed  the  greatest  sense  of  obligation  and  of 
dereliction around the creation of photo albums. Those who regu-
larly  put  prints  in  albums  spoke  of  being  “behind,”  and  some 
could  even  tell  us  how  far  behind  they  were  (“four  months;” 
“those envelopes on the shelf”). People repeatedly used the self-
judging word “lazy” to describe their lack of annotation and al-
buming.  People  who  didn’t  create  albums  said  that  they  “ought 
to” and “definitely planned to.” Albums appear to be sociotechni-
cal artifacts for which people feel a responsibility toward others. 
There are norms of behavior associated with pictures, especially 
albums,  such  that  people  felt  a  responsibility  to  be  maintaining 
albums.  
5.2.2  Orality 
A surprising and significant finding in our study was the central 
role of face-to-face oral communication in our participants’ use of 
photos, and their overall lack of interest in assigning metadata and 
making annotations. The act of sharing photos in a photo album 
was  as  much  (if  not  more  so)  about  talking  with  family  and 
friends  as  it  was  about  looking  at  the  photographs.  Oral 
communication seemed to serve first and foremost the function of 
maintaining  social  relationships,  but  also  was  often  the  primary 
mode  of  intergenerational  transmission  of  memory  and  identity. 
The vast majority of contextual and content metadata (i.e., who, 
what,  where,  when,  why,  etc.)  of  photos  was  stored  in  human 
memory and transmitted through intimate speech.  
To  better  understand  the  function  of  oral  communication  in  our 
participants'  use  of  photos,  we  refer  to  the  work  of  Walter  Ong 
[31]. Ong’s classic study of orality identified three main phases in 
the evolution of communication and media: “orality” (or “primary 
orality”) is the phase of oral culture prior to the advent of writing; 
“literacy” is the phase from the invention of writing through the 
invention of the printing press up to before the advent of the first 
electronic communications technology; “secondary orality” is the 
phase from the invention of the telegraph to the present day and 
recovers  some  aspects  of  orality  by  connecting  people  across 
space  through  mediated  interaction  (e.g.,  the  telephone).  In  the 
modern  day,  aspects  of  primary  orality,  literacy  and  secondary 
orality  all  intertwine  in  our  social  uses  of  communication  and 
communications technologies.  
According  to  Ong,  orality  has  seven  main  aspects,  six  of which 
speak  directly  to  our  findings  as  to  how  and  why  people  talked 
about their photos to one another: orality is (1) “evanescent” (i.e., 
it  produces  sounds  which  have  no  record);  (2)  “additive  rather 
than  subordinative,  aggregative  rather  than  analytic”  (i.e.,  it  has 
different  organizing  principles  than  written  communication);  (3) 
“close  to  the  human  lifeworld,”  rather  than  about  abstract  con-
cepts); (4) “agonistically toned” (this aspect of orality was absent 
from  the  intimate  social  structures  we  studied);  (5)  “empathetic 
and  participatory  rather  than  objectively  distanced”  (6)  socially 
cohesive and knits people together into community; and (7) “ho-
meostatic”  (oral  cultures  change  slowly  and  yet  are  continually 
renewed in each generation). 

The centrality of orality in our subjects’ use of photos appears in 
interesting contradistinction to the emphasis on the materiality of 
prints. The combination of orality and materiality makes sense in 
terms  of  the  social  theoretic  emphasis  on  objects  as  organizing 
activity.  The  photo  needed  to  be  an  object;  the  photo’s  detailed 
metadata existed (with few exceptions) primarily as interpersonal 
and intergenerational conversations that were evanescent, additive 
and  aggregative,  close  to  the  human  lifeworld,  empathetic  and 
participatory rather than objectively distanced, functioned to bring 
about social cohesion and community maintenance, and aspired to 
homeostasis by trying to both renew and preserve the memories 
and  experiences  of  individuals  and  groups.  While  participants 
acknowledged  that  relying  on  oral  transmission  of  personal  and 
family knowledge often resulted in tragic loss of information, in 
their daily lives the affordances of text or recorded audio for cap-
turing photo metadata did not seem to satisfy their deep needs for 
intimacy, immediacy, and connection that face-to-face oral com-
munication offers. This resistance presents an intriguing and im-
portant challenge to digital imaging application designers. 
Seeing our participants' social uses of photos as an admixture of 
orality  and  literacy,  we  can  understand  the  process  of  photo  al-
buming within the context of a similar practice born in early oral 
culture, that of the rhetorical memory palace. Frances Yates de-
scribes the memory palace of classical rhetoric [41], a cognitive 
device  used  in  oral  culture  to  remember  and  deliver  long 
speeches.  The  rhetor  would  visualize  a  familiar  architectural 
structure like a palace and to remember parts of a speech would 
imagine a series of highly evocative images placed in the alcoves 
of the palace. To deliver a speech the rhetor would in the mind’s 
eye  stroll  through  the  memory  palace  stopping  at  alcoves  to 
unpack  the  discourse  that  had  been  condensed  in  the  highly 
evocative, often allegorical ima
In  the  oral  process  of  storytelling  with  photos  we  see  striking 
similarities  to  the  rhetorical  memory  palace:  images  and  image 
sequences  that  are  evocative  and  condensed,  i.e.,  that  can  elicit 
narrative discourse, are selected for inclusion in the photo album 
and the arrangement of images in the album is designed to facili-
tate the oral production of a narrative. It is in the narrative func-
tion of photos and photo sharing that we see all of the preceding 
social uses of memory, relationships, self-expression, materiality, 
and orality come together.  
5.2.3  Storytelling 
Personal photos are used as an occasion for storytelling: “this is 
when we went here and did this and so-and-so was with us.” Sto-
ries  are  for  both  the  people  who  were  there  (“remember  when 
we…”) and those who weren't (“this is your Aunt Mary who…”). 
Personal  photos  support  the  oral  transmission  of  family  stories 
and intergenerational experience and knowledge. Storytelling is a 
recurring  use  of  photos  deeply  connected  to  the  social  use  of 
memory as well as a fundamental cognitive process for organizing 
and remembering experience. As Endel Tulving points out, “epi-
sodic” memory is a fundamental way we remember events in our 
own and other’s lives [36]. Jerome Bruner describes narrative as a 
basic  mode  of  cognition  that  enables  us  to  organize  our  experi-
ence as narrative events in order to be able to better understand 
and remember them [9]. “Narrative Intelligence” researchers see 
narrative  as  a  fundamental  form  of  human  intelligence  which 
many  seek  to  represent,  manage,  and  produce  computationally 
[3]. The narrative use of photos among our participants serves to 

ges.  

structure and transmit personal, interpersonal, and especially int-
ergenerational  memory,  to  replay,  share,  and  deepen  social  ex-
perience and relationship, to express personal and group identity, 
relies on the materiality of the photographic artifact as a conden-
sation and elicitor of story, and functions through, and enables to 
function, intimate oral discourse. 
5.3  Conclusions 
Social  theoretical  perspectives,  especially  the  SCOT  approach, 
caution  us  to  ask  what  culturally  and  historically  conditioned 
motives,  intentions,  and  practices—in  our  terminology,  social 
uses—shape  both  the  content  and  the  form  of  people’s  actions. 
Artifacts (like photos) carry a prior history of practice and under-
standings  and  shape  people’s  actions.  Photos—specifically 
prints—are deeply implicated in memory, relationships, and self-
expression.  The  tangible  photo  and  associated  material  artifacts 
like photo albums are almost inextricably part of the practices of 
orality and storytelling. Digital images can of course be printed. 
Beyond that, digital images also support new practices aimed at 
prior and emerging social uses, as shown by the popularity of the 
photoblog—a technology situated within the social uses of mem-
ory, creating and maintaining relationships, and self-expression. 
Our point is not to be pessimistic about digital imaging—which is 
overtaking film—nor to insist that digital technology will simply 
replace  film  as  a  capture  medium  for  producing  printed  photos. 
Rather, our point is that examining the social uses and associated 
long-established practices, the deep, mutual constitution of social 
uses,  practices,  technology,  and  artifacts,  we  get  a  much  more 
complex, complete, and nuanced understanding of the domain for 
which  we  are  designing,  which  can  only  improve  our  designs 
(while nonetheless complicating our task). 
6.  IMPLICATIONS 
This study aims to provide a new approach to the design of digital 
imaging, especially cameraphone, technology by arguing for the 
investigation  of  the  social  uses  of  personal  photography  as  a 
foundation for design. By uncovering the underlying social uses 
that digital imaging technologies can address, we can design tech-
nologies that people actually want and use.  
This paper is intended to demonstrate the value of this approach 
for  anticipating  future  uses  and  users  of  a  new  technology.  Sci-
ence and technology studies (STS) and the SCOT approach gen-
erally deal with explaining technology success after the fact as an 
interaction of technology and the social, with an emphasis on the 
problems, goals, and activities of potential technology users. This 
paper demonstrates that the analysis can also go the opposite di-
rection, asking whether and how an emerging technology may be 
aligned with pre-existing activities, goals, and needs. It also dem-
onstrates  the  value  of  social  science  approaches  concerned  with 
understanding human action, not just in relation to technology.  
More specifically, this paper shows that social uses are an essen-
tial construct for user-centered inquiry. To design technology to 
be useful and used, we need to understand not only what users are 
doing but also why. Like Activity Theory, our approach employs 
a level of abstraction above the specifics of actions to understand 
the larger situated goals and intentions behind them. We also need 
to understand how artifacts of all kinds carry history and culture, 
and shape as well as reflect understanding and action. We cannot 
understand how users will respond to new or redesigned artifacts 
without understanding the meaning that they have for the users. 

Several implications seem clear: the resistances that users express 
in relation to technology may not simply be matters of “ease-of-
use”  but  of  more  profound  resistances  to  the  mismatch  between 
the  technological  medium  and  existing  social  uses.  The  social 
uses  of  memory  and  relationships  rely  on  the  importance of the 
materiality  of  photographic  artifacts  and  the  orality  of  narrative 
discourse  around  these  artifacts.  These  findings  mean  that  the 
immateriality of the digital medium itself on the one hand and the 
mediation of digital recordings (whether textual or verbal) on the 
other  face  resistance  in  relation  to  the  primary  modes  in  which 
people currently address basic social concerns.  
We do not yet have answers about what to design in light of these 
findings, but argue that they are significant in their influence on 
the resistances and affordances of current and future digital imag-
ing technology. Moreover, if we can serve multiple social uses of 
multiple  social  groups  simultaneously,  we  can  design  technolo-
gies that may achieve easier and more widespread adoption.  
This  social  use  framework  can  also  help  explain  why  some 
emerging  technologies  are  encountering  resistance  or  gaining 
acceptance. Photoblogging is increasing in popularity we believe 
due in large part to its ability to serve the social uses of memory, 
creating and maintaining relationships, and self-expression. Anno-
tation  software  (such  as  Adobe  PhotoShop  Album  or  our  own 
Mobile Media Metadata prototype for photo annotation on cam-
eraphones [11, 35, 40]) face consumer resistance not merely due 
to the complexity or difficulty of annotation, but because of the 
primarily  social  function  of  photo  sharing.  Possible  solutions  to 
this resistance include greater automation, incorporating metadata 
into  the  flow  of  social  uses  surrounding  personal  photos,  and 
seamlessly creating metadata as a byproduct of these uses. 
6.1  Future Work 
The  social  uses  of  personal  photography  outlined  in  our  study 
lead us to think about new ways to design digital imaging applica-
tions.  We  plan  further  interviews  with  photo  users,  particularly 
more  discussions  with  current  cameraphone  users.  We  will  ad-
dress  a  more  diverse  group,  including  more  non-family  photo 
users.  We  will  also  continue  our  examination  of  public  photo 
sites. We intend to ground our future technological design process 
in the continued study of social uses. We will use our studies of 
social uses to inform our design methods and technology devel-
opment of our next-generation Mobile Media Metadata prototypes 
and use our technology prototyping to help us uncover and better 
understand  the  underlying  social  uses  for  imaging  technology. 
Finally,  we  will  continue  work  on  the  theoretical  framework, 
investigating the uses of social theoretical work, especially from 
STS, for the design of technology. 
7.  ACKNOWLEDGMENTS 
Our thanks to the many people who invited us into their homes, 
showed  us  their  photo  collections,  and  admitted  how  far  behind 
they are in putting their prints into albums. 
8.  REFERENCES 
1.  Nardi, B. (ed.) Context and Consciousness: Activity Theory 
and Human-Computer Interaction. MIT Press: Cambridge, 
MA, 1996. 

2.  Bowker, G. C. et al. (eds.) Social Science, Technical Sys-

tems, and Cooperative Work: Beyond the Great Divide. Law-
rence Erlbaum Associates, Inc.: Mahwah, NJ, 1997. 

3.  Mateas, M., Sengers, P. (eds.) Narrative Intelligence. John 

Benjamins: Amsterdam, 2003. 

4.  Balabanovic, Marko et al. Storytelling with digital photo-
graphs, in Proc. of the Conference on Human Factors in 
Computing Systems (April 1-6, 2000), ACM Press, 564-571. 

5.  Bederson, B. B. PhotoMesa: a zoomable image browser 
using quantum treemaps and bubblemaps, in Proc. of the 
14th Annual ACM Symposium on User Interface Software 
(November 11-14, 2001), ACM Press, 71-80. 

6.  Bijker, W. E. Of Bicycles, Bakelites, and Bulbs: Toward a 
Theory of Sociotechnical Change. MIT Press: Cambridge, 
MA, 1995. 

7.  Bijker, W. E., et al. The Social Construction of Technologi-
cal Systems: New Directions in the Sociology and History of 
Technology. MIT Press: Cambridge, 1987. 

8.  Blomberg, J. et al. An ethnographic approach to design. In 
Jacko J. A., Sears A. (eds.). The Human-Computer Interac-
tion Handbook: Fundamentals, Evolving Technologies and 
Emerging Applications. Lawrence Erlbaum Associates, Inc.: 
Mahwah, NJ, 2003. 

9.  Bruner, J. Actual Minds, Possible Worlds. Harvard Univer-

sity Press: Cambridge, MA, 1986. 

10.  Chalfen, R. Snapshot Versions of Life. Bowling Green State 

University Popular Press: Bowling Green, Ohio, 1987. 

11.  Davis, M. and Sarvas, R. Mobile Media Metadata for Mobile 
Imaging, in Proc. of IEEE International Conference on Mul-
timedia and Expo(June 27-30, 2004), IEEE Press. 

12.  Dourish, P. Where the Action Is: The Foundations of Em-

bodied Interaction. MIT Press: Cambridge, MA, 2001. 

13.  Dourish, P.  What we talk about when we talk about context. 
Personal and Ubiquitous Computing, 8, 1 (Feb 2004) 19-30. 
14.  Frohlich, D., Kuchinsky, A., Pering, C., Don, A., and Ariss, 

S. Requirements for Photoware, in ACM Conference on 
Computer Supported Cooperative Work (Nov 16-20, 2002), 
ACM Press, 166-175. 

15.  Goffman, E. The Presentation of Self in Everyday Life. Dou-

bleday: Garden City, NY, 1955. 

16.  Graham, A. et al. Time as essence for photo browsing 

through personal digital libraries, in Proc. of the Second 
ACM/IEEE-CS Joint Conference on Digital Libraries (July 
14-18, 2002), ACM Press, 326-335. 

17.  Greenhill, P. So We Can Remember: Showing Family Photo-

graphs. National Museums of Canada: Ottawa, 1981. 

18.  Hall, E. T. The Hidden Dimension. Doubleday: Garden City, 

NY, 1966. 

21. 

19.  Halverson, C. A.  Activity Theory and Distributed 
Cognition: Or What Does CSCW Need to DO with 
Theories? Computer Supported Cooperative Work, 11 (2002) 
243-255. 
20.  Hollan, J. et al. Distributed cognition: Toward a new founda-
tion for human-computer interaction research. ACM 
Transactions on Computer-Human Interaction, 7, 2 (2000)
 
174-1
Iacucc
i, G. et al. Imagining and experiencing in design, the 
96. 
role of performances, in Proc. of the Second Nordic Confer-
ence on Human-Computer Interaction (October 19-23, 
2002), ACM Press, 167-176. 
Kline, R. Resisting Consumer Technology
The Telephone and Electrification. In Oudshoorn N., Pinch 
T. (eds.). How Users Matter: The Co-Construction of Users 
and Technology. MIT Press: Cambridge, MA, 2003. 

 in Rural America: 

22. 

23.  Kuchinsky, A. et al. FotoFile: A Consumer Multimedia Or-

ganization and Retrieval System, in Proc. of the SIGCHI 
Conference on Human Factors in Computing Systems (May 
15-20, 1999), ACM Press, 496-503. 

24.  Marshall, C. C. and Bly, S. Sharing encountered information: 

digital libraries get a social life. 2004.  

25.  Mauss, M. The Gift; Forms and Functions of Exchange in 

Archaic Societies. Cohen & West: London, 1954. 

26.  Mawston, N. Camera Phones Outsell Digital Still Cameras in 

H1 2003 and Beyond. Newton Centre, MA, Strategy Ana-
lytics, 2003. 

27.  Miller, H. The Presentation of Self in Electronic Life: Goff-
man on the Internet. Presented at the Embodied Knowledge 
and Virtual Space Conference (June 19-20, 1995). 

28.  Mynatt, E. et al.  Network communities: something old, 

something new, something borrowed... Computer Supported 
Cooperative Work, 7 (1998) 123-156. 

29.  Nardi, B. A. Activity theory and human-computer interac-

tion. In Nardi B. (ed.). Context and Consciousness: Activity 
Theory and Human-Computer Interaction. MIT Press: Cam-
bridge, MA, 1996. 

30.  Okabe, D. and Ito, M.  Camera phones changing the defini-
tion of picture-worthy. Japan Media Review (Aug 2003) . 
31.  Ong, W. J. Orality and Literacy: The Technologizing of the 

Word. Methuen: London, 1982. 

32.  Platt, J. C. et al. PhotoTOC: Automatic Clustering for 

Browsing Personal Photographs (Technical Report MSR-TR-
2002-17).  2002. Redmond, WA, Microsoft Research.  

33.  Redmiles, D.  Introduction to the Special Issue on Activity 
Theory and the Practice of Design. Computer Supported Co-
operative Work, 11, 1-2 (2002) 1-11. 

34.  Rodden, K. and Wood, K. R. How do people manage their 
digital photographs?, in CHI 2003: Proc. of the Conference 
on Human Factors in Computing Systems (April 5-10, 2003), 
ACM, 409-416. 

35.  Sarvas, R., Herrarte, E., Wilhelm, A., and Davis, M. Meta-

data Creation System for Mobile Images, in Proc. of the Sec-
ond International Conference on Mobile Systems, Applica-
tions, and Services (June 6-9, 2004), ACM Press. 

36.  Tulving, E.  What is Episodic Memory? Current Directions 

in Psychological Science, 2, 3 (1993) 67-70. 

37.  Van House, N. A. Digital libraries and collaborative knowl-

edge construction. In Bishop A. P., Buttenfield B., Van 
House N. A. (eds.). Digital Library Use: Social Practice in 
Design and Evaluation. MIT Press: Cambridge, MA, 2003. 
38.  Van House, N. A. Science and technology studies and infor-
mation studies. In Cronin B. (ed.). Annual Review of Infor-
mation Science and Technology, V. 38. American Society for 
Information Science and Technology: Washington, DC, 
2003. 

39.  Van House, N. A. Epistemic Machineries of Environmental 
Online Communication. In Scharl A. (ed.). Environmental 
Online Communication. Springer: 2004. 

40.  Wilhelm, A., Takhteyev, Y., Sarvas, R., Van House, N. A., 

and Davis, M. Photo Annotation on a Camera Phone, in 
Proc. of the Conference on Human Factors in Computing 
Systems (CHI 2004) (April 24-29, 2004), ACM Press. 
41.  Yates, F. A. The Art of Memory. University of Chicago 

Press: Chicago, 1966.  

 

View publication stats
View publication stats

",False,2004.0,{},False,False,conferencePaper,False,EGS763MR,[],self.user,False,False,False,False,,,From “what?” to “why?”: the social uses of personal photos,EGS763MR,False,False
5KBVCXMY,XZ9KC2ZB,"Mastering the Information Age

Solving Problems with

Visual Analytics

Edited by Daniel Keim, Jörn Kohlhammer,

Geoﬀrey Ellis and Florian Mansmann

This work is subject to copyright.
All rights reserved, whether the whole or part of the material is concerned, speciﬁcally those
of translation, reprinting, re-use of illustrations, broadcasting, reproduction by photocopying
machines or similar means, and storage in data banks.
Copyright c(cid:13) 2010 by the authors
Published by the Eurographics Association
–Postfach 8043, 38621 Goslar, Germany–
Printed in Germany, Druckhaus “Thomas Müntzer” GmbH, Bad Langensalza
Cover image: c(cid:13) iStockphoto.com/FotoMak
ISBN 978-3-905673-77-7
The electronic version of this book is available from the Eurographics Digital Library at
http://diglib.eg.org

In Memoriam of Jim Thomas1,

a visionary and inspiring person, innovative researcher, enthusiastic leader

and excellent promoter of visual analytics.

1Jim Thomas passed away on August 6, 2010.

Preface

Today, in many spheres of human activity, massive sets of data are collected and
stored. As the volumes of data available to lawmakers, civil servants, business
people and scientists increase, their eﬀective use becomes more challenging.
Keeping up to date with the ﬂood of data, using standard tools for data
management and analysis, is fraught with diﬃculty. The ﬁeld of visual analytics
seeks to provide people with better and more eﬀective ways to understand
and analyse these large datasets, while also enabling them to act upon their
ﬁndings immediately, in real-time. Visual analytics integrates the analytic
capabilities of the computer and the abilities of the human analyst, thus allowing
novel discoveries and empowering individuals to take control of the analytical
process. Visual analytics sheds light on unexpected and hidden insights, which
may lead to beneﬁcial and proﬁtable innovation.

This book is one of the outcomes of a two-year project called VisMaster CA,
a coordination action funded by the European Commission from August 2008
to September 2010. The goal of VisMaster was to join European academic
and industrial R&D excellence from several individual disciplines, forming a
strong visual analytics research community. An array of thematic working
groups was set up by the consortium, which focused on advancing the state
of the art in visual analytics. These working groups joined research excel-
lence in the ﬁelds of data management, data analysis, spatial-temporal data,
and human visual perception research with the wider visualisation research
community.

This Coordination Action successfully formed and shaped a strong European
visual analytics community, deﬁned the research roadmap, exposed public and
private stake-holders to visual analytics technology and set the stage for larger
follow-up visual analytics research initiatives. While there is still much work
ahead to realise the visions described in this book, Europe’s most prestigious
visual analytics researchers have combined their expertise to determine the next
steps.

This research roadmap is the ﬁnal delivery of VisMaster. It presents a detailed
review of all aspects of visual analytics, indicating open areas and strategies for
the research in the coming years. The primary sources for this book are the
ﬁnal reports of the working groups, the cross-community reports as well as the
resources built up on the Web platform2.

The VisMaster consortium is conﬁdent that the research agenda presented in
this book, and especially the recommendations in the ﬁnal chapter, will help
to support a sustainable visual analytics community well beyond the duration
of VisMaster CA, and also serves as the reference for researchers in related

2http://www.vismaster.eu

scientiﬁc disciplines, which are interested to join and strengthen the community.
This research roadmap does not only cover issues that correspond to scientiﬁc
challenges:
it also outlines the connections to sciences, technologies, and
industries for which visual analytics can become an ’enabling technology’.
Hence, it serves as a reference for research program committees and researchers
of related ﬁelds in the ICT theme and beyond, to assess the possible implications
for their respective ﬁeld.

Structure

Chapter 1 motivates the topic of visual analytics and presents a brief history
of the domain. Chapter 2 deals with the basis of visual analytics including its
current application areas, the visual analytics process, its building blocks, and
its inherent scientiﬁc challenges.

The following Chapters 3 to 8 were written by respective working groups in
VisMaster, assisted by additional partners of the consortium and community
partners. Each of these chapters introduces the speciﬁc community that is
linked to visual analytics (e.g., data mining). It then outlines the state of the
art and the speciﬁc challenges and opportunities that lie ahead for this ﬁeld
with respect to visual analytics research. In particular, Chapter 3 deals with
data management for visual analytics, Chapter 4 covers aspects of data mining,
Chapter 5 outlines the application of visual analytics to problems with spatial
and temporal components, Chapter 6 considers infra-structural issues, Chapter
7 looks at human aspects and Chapter 8 discusses evaluation methodologies for
visual analytics.

The ﬁnal chapter presents a summary of challenges for the visual analytics
community and sets out speciﬁc recommendations to advance visual analytics
research. These recommendations are a collaborative eﬀort of all working
groups and speciﬁcally address diﬀerent target groups: the European Commis-
sion, the visual analytics research community, the broader research community,
industry and governments, together with other potential users of visual analytics
technology.

Acknowledgements

We would like to thank all the partners of VisMaster (including community
partners) who have contributed to creating this book. Whilst some have
produced chapters (authors of each chapter are shown overleaf), others have
been involved with the reviewing process and/or coordinating their work
groups.

Special thanks goes to Bob Spence and Devina Ramduny-Ellis for their most
helpful comments and contributions.

We are appreciative of the excellent technical and creative support given by
Florian Stoﬀel, Juri Buchmüller and Michael Regenscheit. We are truly grateful

once more for the excellent support of Eurographics, and in particular Stefanie
Behnke, for publishing this work.
Last but not least, we are indebted to the European Commission, and especially,
the project oﬃcer of VisMaster CA, Dr. Teresa de Martino, for supporting
us throughout; her eﬀorts have contributed appreciably to the success of this
project.
This project was funded by the Future and Emerging Technologies (FET) pro-
gramme within the Seventh Framework Programme for Research of the Euro-
pean Commission, under FET-Open grant number: 225924.
We hope that the content of this book will inspire you to apply current visual
analytics technology to solve your real-world data problems, and to engage in
the community eﬀort to deﬁne and develop visual analytics technologies to meet
future challenges.

Daniel Keim (Scientiﬁc Coordinator of VisMaster), Jörn Kohlhammer (Co-
ordinator of VisMaster), Geoﬀrey Ellis, and Florian Mansmann

September 2010

Contents

1 Introduction

1.1 Motivation .
. . . . . . . . . . . . . .
1.2 An Historical Perspective on Visual Analytics . . . . . . . . . .
1.3 Overview .
. . . . . . . . . . . . . .

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

1
1
3
4

2 Visual Analytics

7
2.1 Application of Visual Analytics .
.
7
2.2 The Visual Analytics Process .
.
. . . . . . . . . . . . . . 10
2.3 Building Blocks of Visual Analytics Research . . . . . . . . . . 11

. . . . . . . . . . . . .

.
.

.
.

.
.

3 Data Management
.
.
3.1 Motivation .
3.2 State of the Art
.
3.3 Challenges and Opportunities .
3.4 Next Steps .
.

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

.

.

.

.

.

.

.

.

.

4 Data Mining

.

4.1 Motivation .
.
4.2 State of the Art
4.3 Challenges .
.
.
4.4 Opportunities .
.
4.5 Next Steps .

.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

19
. . . . . . . . . . . . . . 19
. . . . . . . . . . . . . . 22
. . . . . . . . . . . . . . 32
. . . . . . . . . . . . . . 36

39
. . . . . . . . . . . . . . 39
. . . . . . . . . . . . . . 44
. . . . . . . . . . . . . . 49
. . . . . . . . . . . . . . 54
. . . . . . . . . . . . . . 55

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.

.

.

.

.

.

.

.

5 Space and Time
57
5.1 Motivation .
. . . . . . . . . . . . . . 57
5.2 A Scenario for Spatio-Temporal Visual Analytics . . . . . . . . 59
.
5.3 Speciﬁcs of Time and Space .
. . . . . . . . . . . . . 62
. . . . . . . . . . . . . . 68
5.4 State of the Art
.
5.5 Challenges and Opportunities .
. . . . . . . . . . . . . . 81
. . . . . . . . . . . . . . 86
.
5.6 Next Steps .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

.

6 Infrastructure

.

.
6.1 Motivation .
6.2 State of the Art
6.3 Challenges .
.
.
6.4 Opportunities .
6.5 Next Steps .
.

.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

7 Perception and Cognitive Aspects
.
.
.

.
7.1 Motivation .
.
7.2 State of the Art
.
7.3 Challenges and Opportunities .

.
.

.
.

.
.

.
.

.
.

.
.

.
.

.

87
. . . . . . . . . . . . . . 87
. . . . . . . . . . . . . . 91
. . . . . . . . . . . . . . 102
. . . . . . . . . . . . . . 107
. . . . . . . . . . . . . . 108

109
. . . . . . . . . . . . . . 109
. . . . . . . . . . . . . . 110
. . . . . . . . . . . . . . 123

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

.
.
.
.
.

.
.
.

7.4 Next Steps .

.

.

8 Evaluation

8.1 Motivation .
.
8.2 State of the Art
8.3 Next Steps .
.

.

.

.

.
.
.

.

.
.
.

. .

. . . . . . . . . . . . . . . . . . . . . . 130

131
. .
. . . . . . . . . . . . . . . . . . . . . . 131
. . . . . . . . . . . . . . . . . . . . . . . . 134
. .
. . . . . . . . . . . . . . . . . . . . . . 141

9 Recommendations

145
9.1 The Challenges .
. . . . . . . . . . . . . . . . . . . . . . 145
9.2 Meeting the Challenges . . . . . . . . . . . . . . . . . . . . . . 148
9.3 Future Directions .
. . . . . . . . . . . . . . . . . . . . . . 153

. .

.

. .

Bibliography

List of Figures

Glossary of Terms

155

164

167

List of Authors

Chapters 1 & 2 Daniel A. Keim

Jörn Kohlhammer
Florian Mansmann
Thorsten May
Franz Wanner

University of Konstanz
Fraunhofer IGD
University of Konstanz
Fraunhofer IGD
University of Konstanz

Chapter 3

Chapter 4

Chapter 5

Chapter 6

Chapter 7

Chapter 8

Chapter 9

Giuseppe Santucci
Helwig Hauser

Sapienza Università di Roma
University of Bergen

Kai Puolamäki
Alessio Bertone
Roberto Therón
Otto Huisman
Jimmy Johansson
Silvia Miksch
Panagiotis Papapetrou Aalto University
Salvo Rinzivillo

Aalto University
Danube University Krems
Universidad de Salamanca
University of Twente
Linköping University
Danube University Krems

Consiglio Nazionale delle Ricerche

Gennady Andrienko
Natalia Andrienko
Heidrun Schumann
Christian Tominski
Urska Demsar
Doris Dransch
Jason Dykes
Sara Fabrikant
Mikael Jern
Menno-Jan Kraak

Fraunhofer IAIS
Fraunhofer IAIS
University of Rostock
University of Rostock
National University of Ireland
German Research Centre for Geosciences
City University London
University of Zurich
Linköping University
University of Twente

Jean-Daniel Fekete

INRIA

Alan Dix
Margit Pohl
Geoﬀrey Ellis

Jarke van Wijk
Tobias Isenberg
Jos B.T.M. Roerdink
Alexandru C. Telea
Michel Westenberg

Geoﬀrey Ellis
Daniel A. Keim
Jörn Kohlhammer

Lancaster University
Vienna University of Technology
Lancaster University

Eindhoven University of Technology
University of Groningen
University of Groningen
University of Groningen
Eindhoven University of Technology

University of Konstanz
University of Konstanz
Fraunhofer IGD

1 Introduction

1.1 Motivation

We are living in a world which faces a rapidly increasing amount of data to be
dealt with on a daily basis. In the last decade, the steady improvement of data
storage devices and means to create and collect data along the way, inﬂuenced
the manner in which we deal with information. Most of the time, data is stored
without ﬁltering and reﬁnement for later use. Virtually every branch of industry
or business, and any political or personal activity, nowadays generates vast
amounts of data. Making matters worse, the possibilities to collect and store
data increase at a faster rate than our ability to use it for making decisions.
However, in most applications, raw data has no value in itself; instead, we want
to extract the information contained in it.

The information overload problem refers to the danger of getting lost in data,
which may be:

- irrelevant to the current task at hand,
- processed in an inappropriate way, or
- presented in an inappropriate way.

Raw data has no value in
itself, only the extracted
information has value

Due to information overload,
time and money are wasted, scientiﬁc and
industrial opportunities are lost because we still lack the ability to deal with
the enormous data volumes properly. People in both their business and private
lives, decision-makers, analysts, engineers, emergency response teams alike,
are often confronted with large amounts of disparate, conﬂicting and dynamic
information, which are available from multiple heterogeneous sources. There
is a need for eﬀective methods to exploit and use the hidden opportunities and
knowledge resting in unexplored data resources.

In many application areas, success depends on the right information being
available at the right time. Nowadays, the acquisition of raw data is no longer
the main problem.
Instead, it is the ability to identify methods and models,
which can turn the data into reliable and comprehensible knowledge. Any
technology, that claims to overcome the information overload problem, should
answer the following questions:

Time and money are
wasted and opportunities
are lost

Success depends on
availability of the right
information

- Who or what deﬁnes the ’relevance of information’ for a given task?
- How can inappropriate procedures in a complex decision making process be

- How can the resulting information be presented in a decision-oriented or task-

identiﬁed?

oriented way?

2

Introduction

Visual analytics aims at
making data and
information processing
transparent

Visual analytics combines
the strengths of humans
and computers

With every new application, processes are put to the test, possibly under
circumstances totally diﬀerent from the ones they have been designed for.
The awareness of the problem of how to understand and analyse our data has
greatly increased in the last decade. Even though we implement more powerful
tools for automated data analysis, we still face the problem of understanding
and ’analysing our analyses’ in the future – fully automated search, ﬁlter and
analysis only work reliably for well-deﬁned and well-understood problems.
The path from data to decision is typically fairly complex. Fully automated
data processing methods may represent the knowledge of their creators, but
they lack the ability to communicate their knowledge. This ability is crucial.
If decisions that emerge from the results of these methods turn out to be
wrong, it is especially important to be able to examine the processes that are
responsible.

The overarching driving vision of visual analytics is to turn the information
overload into an opportunity: just as information visualisation has changed our
view on databases, the goal of visual analytics is to make our way of processing
data and information transparent for an analytic discourse. The visualisation of
these processes will provide the means of examining the actual processes in-
stead of just the results. Visual analytics will foster the constructive evaluation,
correction and rapid improvement of our processes and models and ultimately
the improvement of our knowledge and our decisions.

On a grand scale, visual analytics provides technology that combines the
strengths of human and electronic data processing. Visualisation becomes the
medium of a semi-automated analytical process, where humans and machines
cooperate using their respective, distinct capabilities for the most eﬀective
results. The user has to be the ultimate authority in directing the analysis. In
addition, the system has to provide eﬀective means of interaction to focus on
their speciﬁc task. In many applications, several people may work along the
processing path from data to decision. A visual representation will sketch this
path and provide a reference for their collaboration across diﬀerent tasks and at
diﬀerent levels of detail.

The diversity of these tasks cannot be tackled with a single theory. Visual
analytics research is highly interdisciplinary and combines various related
research areas such as visualisation, data mining, data management, data fusion,
statistics and cognition science (among others). One key idea of visual analytics
is that integration of all these diverse areas is a scientiﬁc discipline in its
own right. Application domain experts are becoming increasingly aware that
visualisation is useful and valuable, but often ad hoc solutions are used, which
rarely match the state of the art in interactive visualisation science, much less
the full complexity of the problems, for which visual analytics aims to seek
answers. Even if the awareness exists, that scientiﬁc analysis and results
must be visualised in one way or the other.
In fact, all related research
areas in the context of visual analytics research conduct rigorous science,
each in their vibrant research communities. One main goal of this book is to
demonstrate that collaboration can lead to novel, highly eﬀective analysis tools,
contributing solutions to the information overload problem in many important
domains.

1.2 An Historical Perspective on Visual Analytics

3

Because visual analytics is an integrating discipline, application speciﬁc re-
search areas can contribute existing procedures and models. Emerging from
highly application-oriented research, research communities often work on
speciﬁc solutions using the tools and standards of their speciﬁc ﬁelds. The
requirements of visual analytics introduce new dependencies between these
ﬁelds.

The integration of the previously mentioned disciplines into visual analytics
will result in a set of well-established and agreed upon concepts and theories,
allowing any scientiﬁc breakthrough in a single discipline to have a potential
impact on the whole visual analytics ﬁeld. In return, combining and upgrading
these multiple technologies onto a new general level will have a great impact
on a large number of application domains.

1.2 An Historical Perspective on Visual Analytics

Automatic analysis techniques such as statistics and data mining developed
independently from visualisation and interaction techniques. However, some
key thoughts changed the rather limited scope of the ﬁelds into what is
today called visual analytics research. One of the most important steps in
this direction was the need to move from conﬁrmatory data analysis (using
charts and other visual representations to just present results) to exploratory
data analysis (interacting with the data/results), which was ﬁrst stated in the
statistics research community by John W. Tukey in his 1977 book, Exploratory
Data Analysis[116].

Early visual analytics:
exploratory data analysis

With improvements in graphical user interfaces and interaction devices, a re-
search community devoted their eﬀorts to information visualisation[25, 27, 104, 122].
At some stage, this community recognised the potential of integrating the user in Visual data exploration
and visual data mining
the knowledge discovery and data mining process through eﬀective and eﬃcient
visualisation techniques, interaction capabilities and knowledge transfer. This
led to visual data exploration and visual data mining[64]. This integration
considerably widened the scope of both the information visualisation and
the data mining ﬁelds, resulting in new techniques and many interesting and
important research opportunities.
Two of the early uses of the term visual analytics were in 2004[125] and a year
later in the research and development agenda, Illuminating the Path[111]. More
recently, the term is used in a wider context, describing a new multidisciplinary
ﬁeld that combines various research areas including visualisation, human-
computer interaction, data analysis, data management, geo-spatial and temporal
data processing, spatial decision support and statistics[67, 5].

Since 2004: visual
analytics

Despite the relatively recent use of the term visual analytics, characteristics
of visual analytics applications were already apparent in earlier systems, such
as the CoCo system created in the early 1990s to achieve improvement in the
design of a silicon chip[32]. In this system, numerical optimisation algorithms
alone were acknowledged to have serious disadvantages, and it was found that
some of these could be ameliorated if an experienced chip designer continually

Some earlier systems
exhibited the
characteristics of visual
analytics

4

Introduction

monitored and guided the algorithm when appropriate. The Cockpit interface
supported this activity by showing, dynamically, hierarchically related and
meaningful indications of chip performance and sensitivity information, as well
as on-the-ﬂy advice by an artiﬁcial intelligence system, all of which information
could be managed to interactively.

1.3 Overview

This book is the result of a community eﬀort of the partners of the VisMaster
Coordinated Action funded by the European Union. The overarching aim of
this project was to create a research roadmap that outlines the current state of
visual analytics across many disciplines, and to describe the next steps to take
in order to form a strong visual analytics community, enabling the development
of advanced visual analytic applications. The ﬁrst two chapters introduce the
problem space and deﬁne visual analytics. Chapters 3 to 8 present the work of
the specialised working groups within the VisMaster consortium. Each of these
chapters follow a similar structure – the motivation section gives an outline
of the problem and relevant background information; the next section presents
an overview of the state of the art in the particular domain, with reference to
visual analytics; challenges and opportunities are then identiﬁed; and ﬁnally
in the next steps section, suggestions, pertinent to the subject of the chapter,
are put forward for discussion. Higher level recommendations for the direction
for future research in visual analytics, put forward by the chapter authors are
collectively summarised in the ﬁnal chapter. We now outline the chapters in
more detail.
Chapter 2 describes some application areas for visual analytics and puts
the size of the problem into context, and elaborates on the deﬁnition of
visual analytics. The interdisciplinary nature of this area is demonstrated
by considering the scientiﬁc ﬁelds that are an integral part of visual analyt-
ics.
Chapter 3 reviews the ﬁeld of data management with respect to visual analytics
and reviews current database technology.
It then summarises the problems
that can arise when dealing with large, complex and heterogeneous datasets
or data streams. A scenario is given, which illustrates tight integration of
data management and visual analytics. The state of the art section also
considers techniques for the integration of data and issues relating to data
reduction, including visual data reduction techniques and the related topic
of visual quality metrics. The challenges section identiﬁes important issues,
such as dealing with uncertainties in the data and the integrity of the results,
the management of semantics (i.e., data which adds meaning to the data
values), the emerging area of data streaming, interactive visualisation of large
databases and database issues concerning distributed and collaborative visual
analytics.

Daniel A. Keim
Jörn Kohlhammer
Florian Mansmann
Thorsten May
Franz Wanner

Giuseppe Santucci
Helwig Hauser

1.3 Overview

5

Chapter 4 considers data mining, which is seen as fundamental to the automated Kai Puolamäki
Alessio Bertone
analysis components of visual analytics. Since today’s datasets are often
Roberto Therón
extremely large and complex, the combination of human and automatic analysis
Otto Huisman
is key to solving many information gathering tasks. Some case studies are
Jimmy Johansson
Silvia Miksch
presented which illustrate the use of knowledge discovery and data mining
Panagiotis Papapetrou
(KDD) in bioinformatics and climate change. The authors then pose the
Salvo Rinzivillo
question of whether industry is ready for visual analytics, citing examples of
the pharmaceutical, software and marketing industries. The state of the art
section gives a comprehensive review of data mining/analysis tools such as
statistical and mathematical tools, visual data mining tools, Web tools and
packages. Some current data mining/visual analytics approaches are then
described with examples from the bioinformatics and graph visualisation ﬁelds.
Technical challenges speciﬁc to data mining are described such as achieving
data cleaning, integration, data fusion etc.
in real-time and providing the
necessary infrastructure to support data mining. The challenge of integrating
the human into the data process to go towards a visual analytics approach is
discussed together with issues regarding its evaluation. Several opportunities
are then identiﬁed, such as the need for generic tools and methods, visualisation
of models and collaboration between the KDD and visualisation communi-
ties.

Chapter 5 describes the requirements of visual analytics for spatio-temporal Gennady Andrienko
applications. Space (as in for example maps) and time (values change over
time) are essential components of many data analysis problems; hence there is
a strong need for visual analytics tools speciﬁcally designed to deal with the par-
ticular characteristics of these dimensions. Using a sizeable ﬁctitious scenario,
the authors guide the reader towards the speciﬁcs of time and space, illustrating
the involvement of various people and agencies, and the many dependencies
and problems associated with scale and uncertainties in the data. The current
state of the art is described with a review of maps, geographic information
systems, the representation of time, interactive and collaborative issues, and the
implication of dealing with massive datasets. Challenges are then identiﬁed,
such as dealing with diverse data at multiple scales, and supporting a varied set
of users, including non-experts.

Natalia Andrienko
Heidrun Schumann
Christian Tominski
Urska Demsar
Doris Dransch
Jason Dykes
Sara Fabrikan
Mikael Jern
Menno-Jan Kraak

Chapter 6 highlights the fact that currently most visual analytics application
are custom-built stand-alone applications, using for instance, in-memory data
storage rather than database management systems.
In addition, many other
common components of visual analytics applications can be identiﬁed and po-
tentially built into a unifying framework to support a range of applications. The
author of this chapter reviews architectural models of visualisation, data man-
agement, analysis, dissemination and communication components and outlines
the inherent challenges. Opportunities and next steps for current research are
subsequently identiﬁed which encourage a collaborative multidisciplinary eﬀort
to provide a much needed ﬂexible infrastructure.

Jean-Daniel Fekete

Chapter 7 discusses visual perception and cognitive issues - human aspects Alan Dix
of visual analytics. Following a review of the psychology of perception
and cognition, distributed cognition, problem solving, particular interaction
issues, the authors suggest that we can learn much from early application

Margit Pohl
Geoﬀrey Ellis

6

Introduction

examples. Challenges identiﬁed, include the provision of appropriate design
methodologies and design guidelines, suitable for the expert analyst as well as
the naive users; understanding the analysis process, giving the user conﬁdence
in the results, dealing with a wide range of devices and how to evaluate new
designs.
Chapter 8 explains the basic concept of evaluation for visual analytics, high-
lighting the complexities for evaluating systems that involve the close coupling
of the user and semi-automatic analytical processes through a highly interactive
interface. The exploratory tasks associated with visual analytics are often
open ended and hence it is diﬃcult to assess the eﬀectiveness and eﬃciency
of a particular method, let alone make comparisons between methods. The
state of the art section outlines empirical evaluation methodologies, shows
some examples of evaluation and describes the development of contests in
diﬀerent sub-communities to evaluate visual analytics approaches on common
datasets. The authors then argue that a solid evaluation infrastructure for
visual analytics is required and put forward some recommendations on how
to achieved this.
Chapter 9 summarises the challenges of visual analytics applications as identi-
ﬁed by the chapter authors and presents concrete recommendations for funding
agencies, the visual analytics community, the broader research community
and potential users of visual analytics technology in order to ensure the rapid
advancement of the science of visual analytics.

Jarke van Wijk
Tobias Isenberg
Jos B.T.M. Roerdink
Alexandru C. Telea
Michel Westenberg

Geoﬀrey Ellis
Daniel A. Keim
Jörn Kohlhammer

",False,2010.0,{},False,False,book,False,5KBVCXMY,[],self.user,False,False,False,False,,,Mastering the information age: solving problems with visual analytics,5KBVCXMY,False,False
SGQEGYSD,UK75SXZX,"Sun GD, Wu YC, Liang RH et al. A survey of visual analytics techniques and applications: State-of-the-art research and
future challenges. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 28(5): 852–867 Sept. 2013. DOI
10.1007/s11390-013-1383-8

A Survey of Visual Analytics Techniques and Applications:
State-of-the-Art Research and Future Challenges

Guo-Dao Sun1,2 ((cid:3556)(cid:1875)(cid:1441)), Ying-Cai Wu1,∗ ((cid:3814)(cid:4231)(cid:1164)), Member, IEEE
Rong-Hua Liang2 ((cid:2563)(cid:3252)(cid:1983)), Member, CCF, IEEE
and Shi-Xia Liu1 ((cid:2622)(cid:3415)(cid:3881)), Member, CCF, Senior Member, IEEE

1Internet Graphics Group, Microsoft Research Asia, Beijing 100080, China
2College of Information Engineering, Zhejiang University of Technology, Hangzhou 310023, China
E-mail: {v-gusun, yingcai.wu}@microsoft.com; rhliang@zjut.edu.cn; Shixia.Liu@microsoft.com

Received August 6, 2013; revised August 17, 2013.

Abstract Visual analytics employs interactive visualizations to integrate users’ knowledge and inference capability into
numerical/algorithmic data analysis processes.
It is an active research ﬁeld that has applications in many sectors, such
as security, ﬁnance, and business. The growing popularity of visual analytics in recent years creates the need for a broad
survey that reviews and assesses the recent developments in the ﬁeld. This report reviews and classiﬁes recent work into a
set of application categories including space and time, multivariate, text, graph and network, and other applications. More
importantly, this report presents analytics space, inspired by design space, which relates each application category to the
key steps in visual analytics, including visual mapping, model-based analysis, and user interactions. We explore and discuss
the analytics space to add the current understanding and better understand research trends in the ﬁeld.

Keywords

visual analytics, information visualization, data analysis, user interaction

1 Introduction

Recent advances in computing and storage technolo-
gies have made it possible to create, collect, and store
huge volumes of data in a variety of data formats, lan-
guages, and cultures[1]. Eﬀective analysis of the data to
derive valuable insights enables analysts to design suc-
cessful strategies and make informed decisions. Various
numerical/algorithmic approaches such as data mining
and machine learning methods have been used to auto-
matically analyze the data. Although these approaches
have proven their usefulness in many practical applica-
tions, they still face signiﬁcant challenges such as algo-
rithm scalability, increasing data dimensions, and data
heterogeneity. Furthermore, these methods may not be
perfect under all analysis scenarios. Users often have
to provide their knowledge to iteratively reﬁne the met-
hods. If complex, interesting patterns are discovered,
it is usually diﬃcult to understand and interpret the
ﬁndings in an intuitive and meaningful manner[2].

To address these challenges, visual analytics has
been developed in recent years through a proper com-
bination of automated analysis with interactive visua-
lizations. The emergence of visual analytics can be
largely attributed to the strong need of homeland se-
curity of the United States to analyze complex data,
such as incomplete, inconsistent, or potentially decep-
tive information, since the September 11, 2001 terrorist
attacks[3]. The analysis requires that humans should
become involved to evaluate the data to respond in a
timely manner.

Thomas and Cook presented the ﬁrst widely-
accepted roadmap for visual analytics to meet the prac-
tical requirement in their seminal book[3]. In the book,
visual analytics is deﬁned as “The science of analyti-
cal reasoning assisted by interactive visual interfaces”.
Later, the VisMaster Coordinated Action community,
funded by the European Union, updated the roadmap
and provided a more speciﬁc deﬁnition of visual analy-
tics: “Visual analytics combines automated analysis

Survey
The work is partly supported by the National Natural Science Foundation of China under Grant No. 61070114, the Program for
New Century Excellent Talents in University of China under Grant No. NCET-12-1087, and the Zhejiang Provincial Qianjiang Talents
of China under Grant No. 2013R10054.

∗Corresponding Author
	2013 Springer Science + Business Media, LLC & Science Press, China

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

853

with interactive visualizations for eﬀective understand-
ing, reasoning and decision making on the basis of a
very large and complex dataset”[2].

These pioneering researches[2-3] deﬁne the scope of
the ﬁeld and discuss future research challenges that the
ﬁeld will face. Subsequently, a large number of visual
analysis techniques have been developed. The rapid
technical developments in the ﬁeld have greatly pro-
moted the use of visual analysis techniques in diﬀerent
domains to solve real-world problems, such as network
traﬃc analysis[4], engaging education[5-6], concepts[5],
sport analysis[7], database analysis[8], and biological
data analysis[9-11]. As a result, visual analytics has
been gaining more and more attention from both in-
dustry and academia. With the growing popularity of
visual analytics, there is an increasing need for a com-
prehensive survey covering the recent advances of the
ﬁeld.

Our motivations in conducting this survey are
twofold. First, we aim to review the most recent deve-
lopments of visual analysis techniques and applications
and provide a concise but broad review of the ﬁeld. To
the best of our knowledge, the surveys of visual analy-
tics published in the last few years mainly focus on some
narrow topics of visual analytics, such as visual analysis
of time-oriented data[12], spatio-temporal data[13], or
network data[14]. A comprehensive survey that reviews
the current research of visual analytics is still absent.
Second, this survey aims to organize, classify, and
compare recent research to provide a critical assessment
of the research and understand current research trends.
We introduce analytics space to organize and classify
the current visual analytics research in a novel way
using key VA (Visual Analytics) steps and application
categories. As visual analytics is an application-driven
research ﬁeld[3], we classify the papers into diﬀerent ap-
plication categories: space and time, multivariate, text,
graph and network, and other applications. The key VA
steps refer to the key steps in the classic visual ana-
lytics process[2] including visual mapping, model-based
analysis, and user interactions which have been com-
monly accepted in the ﬁeld. We analyze the analytics
space to discuss and explore the research trends.

The contributions of this paper are as follows. First,
the paper presents a comprehensive survey of recent
developments of visual analytics research. Second, it
provides a novel classiﬁcation of the results and iden-
tiﬁes new research trends, which can help enhance the
understanding of the ﬁeld.

The structure of the paper is as follows. In Section 2,
we introduce recent models and theories of visual ana-
lytics. Section 3 discusses the paper classiﬁcation and
analyzes the research trends. In Sections 4, 5, 6, and

7, we review current researches in diﬀerent application
categories. Finally, Section 8 concludes the paper and
outlines future challenges in this research domain.

2 Theories, Models, and Frameworks

Visual analytics focuses on analytical reasoning
using interactive visualizations.
Shneiderman et
al.[15] proposed a famous information seeking mantra:
“Overview ﬁrst, zoom/ﬁlter, details on demand” to
facilitate visual data exploration. Keim et al.[1]
in-
dicated that only displaying the data using a visual
metaphor rarely provides any insight. They extended
the mantra[15] for visual analysis to gain profound in-
sights: “Analyze ﬁrst, show the important, zoom/ﬁlter,
analyze further, details on demands”. Compared with
the information visualization mantra[15], the visual ana-
lysis mantra[1] highlights the combination of nume-
rical/algorithmic data analysis and interactive visual
interfaces.

Keim et al.[2] also introduced a seminal framework
to depict the visual analytics process. Fig.1 illustrates
the entire visual analysis process. The process starts by
transforming the data (such as ﬁltering and sampling)
for further exploration. After that, a visual or an auto-
matic analysis method is adopted separately. When au-
tomatic analysis methods are applied, approaches such
as data mining methods are used to estimate models for
characterizing the data. When visual data exploration
is used, users directly interact with the visual interface
to analyze and explore the data.

Fig.1. Visual analytics process by Keim et al.[2]

The combination and interaction between visual and
automatic analysis methods are the key feature of visual
analytics, which helps distinguish the visual analytics
process from other data analysis processes.
It allows
for progressive reﬁnement and evaluation of the ana-
lysis results. For instance, patterns discovered by the
visual method can help reﬁne the automatic analysis

854

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

model. Thus, visual data exploration together with au-
tomatic model-based analysis can often lead to better
analysis results.

Recently,

researchers have introduced diﬀerent
means to enhance the classic information visualization
process[16].
Bertini et al.[17] proposed overlaying
the Quality-Metrics-Driven Automation on the classic
pipeline[16]. The quality metrics can be integrated into
diﬀerent steps of the pipeline to automate the numer-
ical/algorithmic data analysis and better support vi-
sual analysis and exploration. In addition, Crouser et
al.[18] emphasized the importance of human-computer
collaboration in the visual analytics process. Simoﬀ et
al.[19] suggested the importance of user interactions in
the visual analysis process.

Some other new models and guidelines for visual
analytics have also emerged in recent years, greatly
boosting the advancement of the ﬁeld[20-26]. Munzner
et al.[27] divided the visual analysis design into four lay-
ers: domain problem characterization, data/operation
abstraction design, encoding/interaction technique de-
sign, and algorithm design. Sedlmair et al.[28] intro-
duced a methodology with nine stages (learn, winnow,
cast, discover, design, implement, deploy, reﬂect, and
write) for conducting an eﬀective design study. Lam et
al.[29] reviewed a large number of visualization publi-
cations and derived seven evaluation scenarios in visual
analytics, thus providing a useful guidance for designing
an eﬀective evaluation procedure. An interaction model
called semantic interaction [30] has been introduced re-
cently. It allows users to interact with high-dimensional
data in a two-dimensional (2D) view, in which the dis-
tances between data items in the view represent the
similarity between the items.

A few visual analysis frameworks have also been in-
troduced to facilitate the development of visual ana-
lytics systems. Data-Driven Documents (D3)[31] is a
representation-transparent framework for rapid deve-

lopment of online data visualizations. It allows for di-
rect manipulation and modiﬁcation of any document el-
ements and enables smooth animation and user intera-
ctions. WebCharts[32] is a new visualization platform
that enables an application to host Javascript code. It
allows for easy reuse of existing code and fast system
deployment.

3 Analytics Space

In this section, we organize the papers from a
novel perspective, which considers the application cate-
gories and the key steps of the visual analytics pro-
cess. The key steps include data transformation, visual
mapping/layout, model-based analysis, and user intera-
ctions according to the widely-accepted visual analy-
tics model[2]. These key steps form the foundation
of eﬀective visual analytics systems. We do not con-
sider data transformation in our classiﬁcation since it
is straightforward and commonly-used. We also care-
fully examined the sections of papers from the premier
conferences of visual analytics such as IEEE InfoVis and
IEEE VAST. Five categories of applications have been
identiﬁed: space and time, multivariate, text, graph and
network, and other applications. The categories not
only provide a broad overview of visual analytics appli-
cations, but also diﬀerentiate recent research.

We have come up with analytics space, inspired by
design space, to better understand the relationships be-
tween these key steps and diﬀerent application cate-
gories. It relates each application category to speciﬁc
visual analytics steps. Fig.2 illustrates the analytics
space using a heatmap. Each row of the ﬁgure rep-
resents a key step of the visual analytics process and
each column stands for an application category. Each
cell contains one or more surveyed papers. A paper in a
certain cell means that the work belongs to an applica-
tion category and the techniques used can be classiﬁed
into a particular key step. We also use color to visually

Fig.2. Analytics space for diﬀerent applications in the visual analytics process.

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

855

encode the number of papers. The darker a cell, the
more papers it contains.

Fig.2 clearly indicates the imbalanced distribution of
recent research in diﬀerent key steps across diﬀerent ap-
plications. Obviously, the second row of the ﬁgure, rep-
resenting model-based analysis, looks lighter than the
other rows. That is recent research mainly improves
the visual mappings/layouts of existing algorithms and
designs intuitive user interactions to solve real-world
analysis problems. It is possible that the traditional in-
formation visualization research still plays an important
role in the ﬁeld of visual analytics. In the future, the re-
search of visual analytics needs to be conducted towards
a seamless integration of interactive visualizations and
model-based analysis.

Fig.2 also reveals that the second and third columns
of the ﬁgure look overall darker than the other columns.
That is the research in the text and multivariate cat-
egories exhibits a more balanced structure of the vi-
sual analytics process. We speculate that text data is
more complex, unstructured free text, which is diﬃcult
to analyze directly. Mining algorithms are needed to
transform the unstructured data to structured informa-
tion to facilitate the analysis. The multivariate data is
often high dimensional. Without model-based analysis
such as dimension reduction techniques, it would be al-
most impossible to derive any insight. In contrast, the
papers in the categories of space & time and graph still
mostly focus on visual mappings and user interactions.
One possible reason is that the data used by space &
time and graph is usually structured data. Thus, the
techniques without model-based analysis suﬃce for the
applications.

4 Space and Time

With advances in technologies, geospatial, tempo-
ral, and spatio-temporal data have been one of the
most prominent and ubiquitous data types in visual
analytics[121]. Finding spatial and temporal relation-
ships and patterns in the data is needed in many ana-
lysis tasks[2]. However, the scalability and complexity
of the data pose signiﬁcant challenges for eﬀective ana-
lysis, which requires both advanced computational and
visualization techniques.

4.1 Analysis of Geospatial Data

Visual analytics often plays a key role in analysis
of geospatial data[36]. Recent research has brought
some new developments in this ﬁeld[34,51,55]. Slingsby
et al.[51] presented an interactive visual analysis system
to explore and examine the results of OAC — a geode-
mographic classiﬁer. The work uses OAC to classify the
UK population with 41 demographic variables into a set

of geographical areas that are organized in a three-level
hierarchy. A set of coordinated views such as dot maps,
barcharts, treemaps, and parallel coordinates plots are
employed to visually analyze the OAC categories with
uncertainty information. The treemaps used with spa-
tial ordering[56] relate the node positions in the treemap
to the corresponding real geographic regions.

BallotMaps[55] is an interesting interactive graphics
tool based on hierarchically organized charts to facili-
tate analysis of spatial and non-spatial data. The tool
was used to study the relationship between the num-
ber of votes received by a candidate and the position
of his name on the ballot paper, and examine the as-
sociated geographical patterns. Some interesting pat-
terns related to the 2010 local government elections in
the Greater London area were discovered using the tool
(see Fig.3). However, the method does not consider the
voting bias patterns for diﬀerent parties over time.

Fig.3. BallotMap of 2010 local government elections in the
Greater London area[55].

4.2 Analysis of Temporal Data

Visual analytics of temporal data has attracted in-
creasing interest in many analysis tasks and has been
widely used in a variety of applications such as ana-
lysis of environmental time series[50]. This subsection
reviews only recent research. For other related work,
interested readers can refer to a book on visualization
of time-oriented data[12].
uses

visual
metaphor to visualize time series in limited space.
ChronoLenses[57] provides diﬀerent types of lenses to
explore regions of interest in time series data. Users are
allowed to interact with the lenses to build analytical
pipelines to facilitate exploratory analysis.

CloudLines[44]

a

new compact

High-dimensional time series data, such as multiva-
riate ﬁnancial and economic data, is commonly found
in our daily lives but is challenging for analysis.
TimeSeer[39] is a useful visualization tool for explor-
ing the high-dimensional time series data. The tool
employs a set of measures, such as density, skewness,

856

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

and outliers, called scagnostics to capture the charac-
teristics of the data. TimeSeer displays the estimated
scagnostics using a scatterplot matrix, line charts, and
a set of small multiples (see Fig.4). It supports various
interactions such as ﬁltering, brushing, and drill-down.

al.[52] proposed visualizing trajectories using a hybrid
2D/3D display. This display stacks 2D trajectory bands
on top of a 2D map in 3D space, such that trajectories
can be displayed in their spatial context, as shown in
Fig.6. Density-based methods with kernel density es-
timation techniques[33,35,47-48] are used for visualizing
a large number of trajectories on a map. Scheepens
et al.[47] proposed using composite density maps for
multivariate trajectories. Their approach uses a ﬂexible
architecture with six diﬀerent operators to create, com-
pose, and enhance density ﬁelds. Fig.7 shows a com-
posite density map displaying multivariate trajectories
of diﬀerent vessel types in front of Rotterdam harbor.

Fig.4. Visualization of a series of US Employment data using
TimeSeer[39].

RankExplorer[49] is a novel visual analysis technique
that combines ThemeRiver[122], color bars, and glyphs
to explore ranking changes in large time series data.
RankExplorer ﬁrst segments the time series data into
diﬀerent segments. A ThemeRiver layout is used to
visualize the temporal variation of each segment and
the total variation of all the segments. Color bars and
glyphs are embedded in the ThemeRiver layout to dis-
play inner ranking changes inside a segment and outer
ranking changes between segments, respectively. The
tool was used to analyze the ranking changes of tempo-
ral search queries (see Fig.5).

Fig.6. Stacking-based visualization of trajectories in 3D space[52].

Fig.5. Visualization of the top 2000 Bing search queries using
RankExplorer[49].

4.3 Analysis of Spatio-Temporal Data

Spatio-temporal visual analytics has attracted a
great deal of attention. Spatio-temporal data refers to
the data with both spatial and temporal information.
Various methods have been used to solve real-world
problems[37,43,46,58]. Nevertheless, visual analytics of
spatio-temporal data remains diﬃcult.

Trajectory visualization is a very important appli-
cation of spatio-temporal visual analytics. Tominski et

Fig.7. Composite density maps of vessels[47].

Many applications can beneﬁt from interactive
spatio-temporal visual analytics. Maciejewski et al.[45]

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

857

presented a visual analytics approach to forecast
hotspots, namely, unusual spatio-temporal regions.
BirdVis[40]
interactive spatio-temporal
visualization system with coordinated views to under-
stand bird populations.

is a typical

4.4 Summary

The recent developments in spatial, temporal, and
spatio-temporal visual analysis approaches indicate
that this research area is growing rapidly. Nevertheless,
there are still quite a few research challenges that must
be addressed. One challenge is to eﬀectively visualize
realtime streaming data with a large number of time se-
ries. Additionally, eﬀective modeling, characterization,
and visualization of the uncertainty information arising
from spatial-temporal data collection and transforma-
tion must also be investigated.

5 Multivariate Data

Visual analytics of multivariate data is an active
research area. Numerous methods are used to ex-
plore and understand the distributions and correlations
among diﬀerent data dimensions[62,67,69,119,123]. These
approaches can be generally classiﬁed into two broad
categories: projection-based methods based on dimen-
sion reduction techniques and visual methods based on
visual layouts.

5.1 Projection-Based Methods

Projection-based techniques (or dimension reduc-
tion) ﬁnd “interesting” projections of high-dimensional
data in low-dimensional space[123]. The techniques
transform high-dimensional data to low-dimensional
data while preserving some important features of the
original data. Dimension reduction can help avoid the
eﬀects of “the curse of dimensionality”[124] for subse-
quent data analysis.

Multidimensional scaling (MDS) is widely used in
this area to reduce data dimensionality. Traditional
MDS uses the Euclidean distance to compute data simi-
larity. Lee et al.[64] argued that the Euclidean distance
cannot characterize the inter-cluster distances, thus re-
sulting in poor data projections. They introduced a
structure-based distance metric to overcome this prob-
lem in high-dimensional space to produce good pro-
jections. This method was used to explore a variety
of multidimensional datasets, such as aerosol particles
data and operating system data.

Heterogeneous relationships among the dimensions
in high-dimensional data space are ignored in most ana-
lysis methods. Turkay et al.[69] proposed using rep-
resentative factors to capture the grouping relation-

ships among the data dimensions. Their method care-
fully chooses a set of factors including projection fac-
tors based on MDS and principal component analysis
(PCA), medoid factors, and distribution model factors
to represent the relationships among the data dimen-
sions. The representative factors are integrated into
the visual analytics pipeline to facilitate exploration of
high-dimensional data. The method was used to ana-
lyze the data from a healthy brain aging study with 315
dimensions and successfully discovered diﬀerent subsets
of individuals.

Local aﬃne multidimensional projection (LAMP)[62]
is a new projection method based on an orthogonal
mapping theory for handling high dimensional data.
LAMP is eﬃcient and allows users to progressively re-
ﬁne the results with their knowledge. The experiments
provided demonstrate that LAMP outperforms other
projection methods. A system developed using this
method was used to correlate images and music (see
Fig.8).

Fig.8. Visualization of
LAMP[62].

image and music correlation using

Paiva et al.[65] described an improved similarity tree
technique for visual analysis of high-dimensional data.
It is an alternative to traditional multidimensional pro-
jections. A platform called VisPipeline was developed
to apply the technique to three image datasets and over-
come the diﬃculty in traditional data analysis through
visual feedback.

Turkay et al. [68] presented an interactive visual ana-
lysis approach that is performed iteratively over two
spaces: the items space and the dimensions space, thus
allowing for joint analysis of both items and dimen-
sions. The approach uses PCA to map the dimensions

858

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

space to items space. This technique was tested on
the “Boston Neighborhood Housing Prices” dataset for
understanding the relationships between diﬀerent data
dimensions.

5.2 Visual Methods

Visual methods leverage visualization layout algo-
rithms such as pixel-oriented methods and parallel co-
ordinate plots (PCPs)[25] to directly draw multivariate
data for analysis.

Pixel-oriented methods visually map each multiva-
riate data item to a pixel or block with visual attributes
such as color, size, and position[63,80]. A typical re-
cent example is DICON[59], an icon-based solution that
helps compare and interpret clusters of multidimen-
sional data. The icons representing the clusters can
be embedded into various visualizations.

Traditional multivariate data visualizations such
as scatterplot matrices and PCPs[125] can also be
viewed as projection-based techniques, since they draw
high-dimensional data in a two-dimensional space.
Researchers have recently introduced ﬂexible linked
axes[60], which links a set of scatterplot matrices and/or
PCPs together for analyzing high-dimensional data.
This technique allows users to draw and drag axes
freely, which is useful for diﬀerent applications. Fig.9
shows a visualization of high-dimensional demographic
data of diﬀerent countries using the technique.

Fig.9. Flexible linked axes with scatterplots matrices and parallel
coordinated plots[60].

Although PCPs are widely used in the ﬁeld, they still
suﬀer from the problems of over-plotting and clutter.
Angular histograms and attribute curves were recently
introduced by Geng et al.[61] to overcome these prob-
lems. They are able to explore the correlation in the
data by investigating the density and slopes of the his-
togram. This work was evaluated on real-world animal
tracking datasets and was compared with traditional
parallel coordinates plots and histograms.

5.3 Summary

This section reviewed and discussed recent ap-
proaches to visual analysis of multivariate data. The
approaches are categorized into two classes, namely,
projection-based methods and visual methods. Al-
though notable successes have been achieved,
it is
still diﬃcult to understand data with a large number
of dimensions due to the “curse of dimensions”[124].
Projection-based approaches based on dimension re-
duction can deal with data that has many dimensions,
but understanding the projected data is often challeng-
ing. On the other hand, visualization approaches can-
not handle data with many dimensions, but the results
created by these approaches are intuitive to understand
and interpret. Towards this end, Yuan et al.[70] made
an early attempt to combine PCPs and MDS. A seam-
less integration of two kinds of methods is an interesting
direction.

6 Text Data

Text can be found almost everywhere in billboards,
newspapers, books, social media sites, and so on. With
the advance of technologies, a tremendous amount of
text data is being produced, collected, and stored each
day. However, eﬀective analysis of the text data is chal-
lenging for two reasons. First, the text data is often
free, unstructured text corpora. The data is inherently
ambiguous due to natural language ambiguity. Second,
the volume of the text data is usually huge. This pre-
vents analysts from reading the entire text corpora.

Many visual analytics techniques and applications
have been developed in recent years to address these
problems. They often leverage model-based analysis al-
gorithms such as topic modeling methods[126-127] from
natural language processing (NLP) to turn unstruc-
tured text into structured information, which can be
used readily by subsequent interactive visualization
approaches[75,79,84-85,120].

6.1 Topic-Based Methods

Topic-based methods extract topics or events from
text corpora and visually explore the extracted in-
formation using diﬀerent visualization techniques.
It
has been reported that the temporal information as-
sociated with the documents in text corpora is very
important for investigative analysis of the data[74].
Recent
such as EventRiver[79], Visual
Backchannel[77], and TextFlow[75], mostly analyze and
track on the temporal evolution and diﬀusion of events,
topics, or activities.

researches,

TextFlow[75] integrates topic mining techniques into
interactive visualizations to visually analyze the evo-

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

859

lution of topics over time (see Fig.10).
It uses a few
text mining algorithms to model topic evolution trends,
detect critical events, and ﬁnd keyword correlations.
Three visual views including a topic ﬂow view, a time-
line view, and a word cloud are employed to interac-
tively visualize the mining results and gain insights into
the massive text data.

Fig.10. Visualization of topic evolution illustrating the merging
and splitting patterns of topics over time by Textﬂow[75].

Whisper[72] is a system for visual analysis of informa-
tion diﬀusion. It uses a visual metaphor, “sunﬂower”,
to design a hierarchical social-spatial layout for visuali-
zing the propagation of a typical event over time on
Twitter.

More recently, Xu et al.[85] studied the competi-
tion among topics through information diﬀusion on
social media as well as the impact of opinion lead-
ers on competition. They developed a system with
three views: a timeline visualization with an integra-
tion of ThemeRiver and storyline visualization[128] to
visualize the competition, radial visualizations of word
clouds to summarize the relevant tweets, and a detailed
view to list all relevant tweets. The system was used
to illustrate the competition among six major topics,
such as economy, election, and welfare, during the 2012
United States presidential election on Twitter. This
work found that diﬀerent groups of opinion leaders such
as the media and grassroots played diﬀerent roles in the
competition (see Fig.11).

6.2 Feature-Based Methods

Feature-based methods use various features such as
word-level features[83] and document-level features[82]
to visualize text.

Word clouds are a commonly used method and have
received a great deal of attention in the last few years.
This method provides an intuitive visual summary of
document collections by displaying the keywords in a
compact layout. Keywords that appear more frequent
in the source text are drawn larger. A variety of al-
gorithms such as Wordle[83] and ManiWordle[78] have
been proposed to create good word cloud layouts. How-
ever, the semantic relationships between the keywords
in the original text are lost in the layouts. To handle

Fig.11. Visualization of topic competition and the impacts of
opinion leaders on the competition on social media[85].

this issue, researchers introduced methods such as the
force-based algorithm[76] (see Fig.12) and the seam-
carving algorithm[84] to produce semantic-preserving
word clouds. This can ensure that the keywords that
co-occur frequently in the source text are placed close
to one another in the word clouds.

Fig.12. Visualization of dynamic text corpora using a context-
preserving word cloud visualization[76].

FacetAtlas[73] integrates a node-link diagram into a
density map to visually analyze the multifaceted re-
lations of documents. The tool was used to explore a
document collection with over 1 500 articles. Interesting
multifaceted relations between diﬀerent diseases were
discovered. DAViewer[86] was designed to help linguis-
tics researchers study the discourse of language through
a tree layout using interactive visualization.

Oelke et al.[80] described an interesting visual ana-
lysis application for answering “How to make your writ-
ings easier to read”. Their work uses a semi-automatic
method to choose proper features from 141 candidate
readability features. They developed a visual analysis
system called VisRA with three views, including the
corpus view, the block view, and the detail view, to

860

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

explore the feature values of text corpora at diﬀerent
levels of detail.

6.3 Summary

This section mainly introduced recent topic-based
and feature-based methods for visual analysis of text
data. Both kinds of methods are commonly used to
solve practical problems. Although some successes have
been achieved, visual analytics of text data still faces a
few challenges. It is still diﬃcult, if not impossible, for
current methods to handle large amounts of text data.
More eﬃcient text mining and NLP algorithms, as well
as scalable interactive visualizations, are needed to ad-
dress this issue. Another challenge is to handle the
natural language ambiguity and the uncertainty that
arises from the text mining algorithms. Finally, text
data is often accompanied by multimedia data such as
images and videos, which are even more challenging
for analysis. Heterogeneous text data with images and
videos can be complementary.
It may allow users to
explore the data from diﬀerent perspectives. Thus, ef-
fective analysis of the heterogeneous text data is worth
further study.

7 Graph and Network

Visual analysis of graphs is an important application
of visual analytics. This section covers only the research
published in the last few years and classiﬁes the work
into two general categories: graph layout methods and
clutter reduction methods. Interested readers can refer
to a complete survey[14] for more details about the past
research.

7.1 Graph Layout Methods

Graphs

can be visually represented by ma-
trix visualization[100],
or
hybrid views of node-link diagrams and matrix
visualization[101].

node-link diagrams[92],

Matrix visualization is widely used to represent
networks[100]. For instance, RelEx[107] employs ma-
trix visualization to help car engineers visually analyze
information communication in in-car networks. Nev-
ertheless, matrix visualization does not work well for
sparse networks. Compressed matrices[93] explore the
characteristics of a network and rearrange the matrix
visualization for a compact layout. It was used to dis-
cover subnetworks in a large network. Quilts[88] is also a
matrix-based method for visualizing very large layered
graphs such as ﬂow charts.

Node-link diagrams are one of the most prevalent
visual representations for graphs[91-92,106,112]. In node-
link diagrams, nodes are linked with directed or undi-

rected edges to indicate the relationships of the nodes.
Node-link diagrams have been successfully used to ex-
plore and understand diﬀerent kinds of traditional net-
work data such as social networks[91,114] and paper ci-
tation networks[94].

TreeNetViz[98] draws a node-link diagram in a radial
layout to visualize both the hierarchical structure and
network relationships in a social network. Fig.13 shows
TreeNetViz that displays both the hierarchical struc-
ture (such as schools and departments) and the network
relationship at diﬀerent scales. Apart from traditional
networks, researchers have also employed node-link di-
agrams to visually analyze some other interesting data
such as set data[87] and interaction networks[105,109].

Fig.13. Visualization of a compound graph by TreeNetViz[87].

Analysis of elements of sets and their relationships
is an important task. LineSets[87] is a new visual ana-
lysis technique for set data. It uses curves to link the
elements across diﬀerent sets to intuitively reveal the
element relationships. Compared with traditional met-
hods such as Euler diagrams, LineSets can reduce clut-
tered information and handle complex situations when
many sets overlap. The technique was used to visualize
sets of geospatial elements such as restaurants on a map
to facilitate visual search tasks. It was also employed
to analyze communities in social networks (see Fig.14).
StoryLine visualization has emerged recently as a
new and eﬀective means to analyze dynamic relation-
ships such as the temporal interactions among the cha-
racters in a movie[105,109].
It is a new form of node-
link diagrams. In a storyline visualization, a character
is represented by a line, and the temporal interactions
between characters are encoded by the convergence and
divergence relationships of the corresponding lines over
time.

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

861

of networks, reducing visual clutter has become even
more important for visual analysis of large networks.
Edge bundling[102-103] is an eﬀective technique to re-
duce visual clutter and improve the readability of node-
link diagrams by bundling related edges along an adja-
cent path. Hierarchical edge bundling[102] takes advan-
tage of the hierarchy information in compound graphs
to bundle edges of the graphs. The technique was used
to explore and understand a software system (with hie-
rarchically organized components) and the call graph
between the components. The work was extended to
bundle edges in a graph using a force-directed technique
without the need of hierarchy information[103]. Selassie
et al.[108]
improved the force-directed edge bundling
method to take into account the directional informa-
tion, such that high-level directional edge patterns can
be revealed intuitively.

Apart from forced-based methods, recent research
introduces other methods for edge bundling such as
the geometry-based technique[92] (see Fig.16) and the
skeleton-based technique[96] (see Fig.17). The geome-
try-based technique uses a control mesh to attract
edges to some control points on the mesh, thus gen-
erating edge bundles. In contrast, the skeleton-based
method extracts the skeleton of a graph and forces its
edges to be close to the skeleton. Compared with the
geometry-based technique, the skeleton-based method
can generate smoother bundling results while maintain-
ing the graph structure[96], and can be easily accel-
erated by graphics hardware as it is an image-based
method. Both methods were applied to understand US
migrations network data (see Fig.16 and Fig.17). Luo et
al.[106] introduced a new method to reduce ambiguity
in edge-bundling results and enable detail-on-demand
visualization. The system was evaluated using a co-
authorship network.

Parallel edge splatting[90] is a new clutter reduction
technique for visual analysis of large graphs. It over-
comes the over-plotting problem by rearranging graph
nodes on diﬀerent parallel vertical axes, and connect-
ing the nodes between the axes using directed, colored
edges. This technique is capable of visualizing the evo-

Fig.14. Visualization of a co-authorship network by Linesets[87].

Tanahashi and Ma[109] described a set of design prin-
ciples, such as the principles for reducing line crossings
and wiggles, for creating proper storyline layouts, and
used a genetic optimization algorithm to automate the
layout generation process. Although the approach is
eﬀective for creating aesthetically-appealing, compact
layouts, the layout generation process is time consum-
ing. Thus, it does not support user interactions.

Recently, StoryFlow[105] was developed to create
good storyline layouts quickly for interactive visua-
lization (see Fig.15).
It uses an eﬃcient hybrid opti-
mization framework with an integration of discrete and
continuous optimization. The eﬃcient framework en-
ables a set of useful real-time user interactions such as
bundling and straightening. Furthermore, the approach
can faithfully convey the hierarchical relationships be-
tween entities in the created layouts. StoryFlow was
successfully used to study the dynamic interactions be-
tween opinion leaders on social media in the context of
2012 US presidential election.

7.2 Clutter Reduction Methods

Visual clutter is a commonly-found problem in in-
formation visualization[95]. With ever increasing sizes

Fig.15. Storyline visualization of movie The Lord of the Rings by StoryFlow[105].

862

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

prehensive overview of many advances in visual analy-
tics techniques and applications to gain a better un-
derstanding of the cutting-edge research in the ﬁeld.
In particular, the report classiﬁed the work of visual
analytics research in a novel and systematic manner
according to the types of the applications and the steps
in the visual analytics process that the work focuses
on. Additionally, through the analysis and comparison
across diﬀerent paper categories, this report identiﬁed
the trends and recent developments in visual analytics.
Furthermore, we divided the literature review into seve-
ral broad application categories such as space and time
analysis, text analysis, and network analysis. Next, we
discuss and summarize the key challenges of the future
visual analytics research.

Scalability. The explosion of data in recent years
presents a signiﬁcant challenge to existing techniques
for visualizing big data interactively. While re-
cent visual analytics techniques can handle small or
intermediate-size data, most of them are not scalable
to extreme-scale data. With the advance of parallel
computing technologies, researchers have started to em-
ploy powerful computational hardware such as GPUs
to accelerate the performance of visualization layout
algorithms[47,112]. Nevertheless, the hardware-based
parallel acceleration cannot keep pace with the data
explosion rate. To overcome these issues, a variety
of new visual analytics mechanisms such as bottom-up
methods[91] and in-situ analysis[99] have been proposed
in recent years. It is expected that scalable visual ana-
lytics techniques and methodologies will continue to at-
tract substantial interest in the future.

Storytelling.

Storytelling methods have received
a great deal of attention over the past several years
in visualization[105,109,129-130]. Narrative,
interactive
visualizations are also widely used in data-driven
journalism to engage more users and reach a wider
audience[81]. Typical visual analytics applications usu-
ally include a step for creating reports on the ﬁndings of
the analysis. Interactive, storytelling visualizations can
beneﬁt the reports by communicating the ﬁndings more
eﬀectively for sensemaking, as narrative visualizations
can convey the entire story behind the patterns found
in the analysis. For example, storytelling visualizations
can provide in-depth insights into why there are such
patterns. Nevertheless, storytelling (or narrative) vi-
sual analytics is still in its infancy. The basic deﬁni-
tion and the usage guidelines of storytelling techniques
are heuristics and subjective. The fundamental theo-
ries for storytelling visual analytics are worth further
study, and may involve multi-discipline research of hu-
man perception and cognition, human computer inter-
action, and visualization.

Fig.16. Geometry-based edge clustering[92].

Fig.17. Skeleton-based edge bundling[96].

lution of a dynamic graph. Zinsmaier et al.[112] pre-
sented a fast hardware-assisted layout technique using
the information of edge cumulation and node density to
reduce visual clutter. It also enables interactive level-
of-detail rendering of large graphs. The technique was
used to visually analyze large real world graphs to de-
tect patterns.

7.3 Summary

This section reviewed recent research of visual ana-
lysis of graphs. Although we have witnessed rapid
developments in the research area, it is still very dif-
ﬁcult to visually analyze and explore large graphs, let
alone extreme-scale graphs with billions of edges. For
instance, current edge-bundling techniques usually han-
dle thousands of edges, but they may not work well for
larger graphs. Clutter reduction in large graphs needs
to be studied in the future.

Another possible direction is to combine model-
based analysis methods, such as graph partition and
frequent pattern detection methods, with interactive
visualizations. Model-based analysis can help ﬁlter out
a great deal of irrelevant information while preserving
interesting patterns in the graphs.
Interactive visua-
lizations, on the other hand, allow analysts to work
closely with the model-based analysis process to evalu-
ate the results for sensemaking.

8 Conclusions and Future Challenges

This state-of-the-art report reviewed recent research
in the ﬁeld of visual analytics. It represented a com-

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

863

Trustworthiness. Uncertainty information may arise
and spread in diﬀerent steps of an analytics process[26].
Uncertainty modeling and visualization play a criti-
cal role in ensuring the reliability and trustworthiness
of the analytics process. Trustworthy visual analytics
with eﬀective uncertainty modeling and visualization
enables users to explicitly consider the uncertainty in-
formation, so that informed decisions can be made[26].
Many techniques have been proposed to quantitatively
characterize the uncertainty information and intuitively
display the information[26,51,131]. However, due to the
complexity of diﬀerent visual analytics applications,
there are still no widely accepted techniques.
In the
future, research on trustworthiness will be extended.

Evaluation. It is always important to assess the ef-
fectiveness of visual analytics systems[29]. Visual ana-
lytics practitioners use various approaches such as case
studies, expert review, or formal/informal user stu-
dies to evaluate the usability and eﬀectiveness of the
systems[132]. Each method has its own strengths and
weaknesses. For instance, a well-designed formal user
study can provide robust and valuable user feedback to
identify potential problems with the systems. However,
it is time-consuming to conduct a formal study and it
may be diﬃcult to provide high-level insights. A typi-
cal visual analytics system is rather complex and may
involve multiple data analysis and visualization com-
ponents, which poses a great challenge to evaluate the
system. Eﬀective evaluation of a visual analytics sys-
tem is expected to gain more interest in the ﬁeld.

Provenance. Keeping track of a visual analytics pro-
cess has become prominent in the ﬁeld, as the records
allow analysts to be informed of where they have been
and where they are now[133]. One straightforward usage
of provenance information is to allow for redo/undo user
interactions, or to avoid repeated analysis processes.
Furthermore, the provenance information of the gained
insight can facilitate the review and evaluation of the
knowledge or ﬁndings. The advance of collaborative
visual analytics highlights the importance of an eﬀec-
tive mechanism for recording insight provenance, such
that collaborating users can share and exchange their
knowledge and insight judiciously. Nevertheless, exist-
ing simple history mechanisms, such as the Photoshop-
style history mechanism, may not work well in compli-
cated, collaborative scenarios, for instance, when users
work remotely on the same problem and need to fre-
quently exchange their ﬁndings. It is foreseeable that
research into this topic will need to continue.

References

[1] Keim D A, Mansmann F, Schneidewind J, Ziegler H. Chal-
lenges in visual data analysis. In Proc. the IEEE Conference
on Information Visualization, Oct. 2006, pp.9-16.

[2] Keim D A, Kohlhammer J, Ellis G, Mansmann F. Mastering
the Information Age: Solving Problems with Visual Analy-
tics. Florian Mansmann, 2010.

[3] Thomas J J, Cook K A. Illuminating the Path: The Re-
search and Development Agenda for Visual Analytics. Na-
tional Visualization and Analytics Ctr, 2005.

[4] Landge A G, Levine J A, Bhatele A, Isaacs K E, Gamblin T,
Schulz M, Langer S H, Bremer P, Pascucci V. Visualizing net-
work traﬃc to understand the performance of massively par-
allel simulations.
IEEE Transactions on Visualization and
Computer Graphics, 2012, 18(12): 2467-2476.

[5] Block F, Horn M S, Phillips B C, Diamond J, Evans E M,
Shen C. The DeepTree exhibit: Visualizing the tree of life
to facilitate informal learning. IEEE Transactions on Visua-
lization and Computer Graphics, 2012, 18(12): 2789-2798.

[6] Ma J, Liao I, Ma K L, Frazier J. Living Liquid: Design and
evaluation of an exploratory visualization tool for museum
visitors. IEEE Transactions on Visualization and Computer
Graphics, 2012, 18(12): 2799-2808.

[7] Pileggi H, Stolper C D, Boyle J M, Stasko J T. Snapshot:
Visualization to propel ice hockey analytics. IEEE Transac-
tions on Visualization and Computer Graphics, 2012, 18(12):
2819-2828.

[8] Wang X, Zhou X, Wang S. Summarizing large-scale database
schema using community detection. Journal of Computer
Science and Technology, 2012, 27(3): 515-526.

[9] Albers D, Dewey C, Gleicher M. Sequence Surveyor: Lever-
aging overview for scalable genomic alignment visualization.
IEEE Transactions on Visualization and Computer Graph-
ics, 2011, 17(12): 2392-2401.

[10] Pretorius A J, Bray M A, Carpenter A E, Ruddle R A. Visua-
lization of parameter space for image analysis. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(12):
2402-2411.

[11] Das K, Majumder A, Siegenthaler M, Keirstead H, Gopi M.
Automated cell classiﬁcation and visualization for analyzing
remyelination therapy. The Visual Computer, 2011, 27(12):
1055-1069.

[12] Aigner W, Miksch S, Schumann H, Tominski C. Visualization

of Time-Oriented Data. Springer, 2011.

[13] Andrienko G, Andrienko N, Demsar U, Dransch D, Dykes J,
Fabrikant S I, Jern M, Kraak M J, Schumann H, Tominski
C. Space, time and visual analytics. International Journal of
Geographical Information Science, 2010, 24(10): 1577-1600.

[14] von Landesberger T, Kuijper A, Schreck T, Kohlhammer J,
van Wijk J J, Fekete J D, Fellner D W. Visual analysis of
large graphs: State-of-the-art and future research challenges.
Computer Graphics Forum, 2011, 30(6): 1719-1749.

[15] Shneiderman B. The eyes have it: A task by data type tax-
the IEEE

onomy for information visualizations.
Symposium on Visual Languages, Sept. 1996, pp.336-343.

In Proc.

[16] Card S K, Mackinlay J D, Schneiderman B. Readings in Infor-
mation Visualization: Using Vision to Think. Morgan Kauf-
mann, 1999.

[17] Bertini E, Tatu A, Keim D A. Quality metrics in high-
dimensional data visualization: An overview and systemati-
zation. IEEE Transactions on Visualization and Computer
Graphics, 2011, 17(12): 2203-2212.

[18] Crouser R J, Chang R. An aﬀordance-based framework for hu-
man computation and human-computer collaboration. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2859-2868.

[19] Simoﬀ S J, B¨ohlen M H, Mazeika A (eds.). Visual Data Min-

ing. Springer, 2008.

[20] Dasgupta A, Kosara R. Adaptive privacy-preserving visua-
IEEE Transactions on

lization using parallel coordinates.

864

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

Visualization and Computer Graphics, 2011, 17(12): 2241-
2248.

[21] Hullman J, Adar E, Shah P. Beneﬁtting InfoVis with visual
diﬃculties. IEEE Transactions on Visualization and Com-
puter Graphics, 2011, 17(12): 2213-2222.

[22] Kandel S, Paepcke A, Hellerstein J M, Heer J. Enterprise
data analysis and visualization: An interview study. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2917-2926.

[23] Wickham H, Hofmann H. Product plots. IEEE Transactions
on Visualization and Computer Graphics, 2011, 17(12): 2223-
2230.

[24] Wu Y, Liu X, Liu S, Ma K L. ViSizer: A visualization re-
sizing framework. IEEE Transactions on Visualization and
Computer Graphics, 2013, 19(2): 278-290.

[25] Wu Y, Wei F, Liu S et al. Opinionseer: Interactive visua-
lization of hotel customer feedback. IEEE Transactions on
Visualization and Computer Graphics, 2010, 16(6): 1109-
1118.

[26] Wu Y, Yuan G, Ma K L. Visualizing ﬂow of uncertainty
through analytical processes. IEEE Transactions on Visua-
lization and Computer Graphics, 2012, 18(12): 2526-2535.

[27] Munzner T. A nested model for visualization design and val-
idation. IEEE Transactions on Visualization and Computer
Graphics, 2009, 15(6): 921-928.

[28] Sedlmair M, Meyer M, Munzner T. Design study method-
ology: Reﬂections from the trenches and the stacks. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2431-2440.

[29] Lam H, Bertini E, Isenberg P, Plaisant C, Carpendale S. Em-
pirical studies in information visualization: Seven scenarios.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(9): 1520-1536.

[30] Endert A, Fiaux P, North C. Semantic interaction for sense-
making:
Inferring analytical reasoning for model steering.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2879-2888.

[31] Bostock M, Ogievetsky V, Heer J. D3 data-driven documents.
IEEE Transactions on Visualization and Computer Graphics,
2011, 17(12): 2301-2309.

[32] Fisher D, Drucker S M, Fernandez R, Ruble S. Visualizations
everywhere: A multiplatform infrastructure for linked visua-
lizations. IEEE Transactions on Visualization and Computer
Graphics, 2010, 16(6): 1157-1163.

[33] Adrienko N, Adrienko G. Spatial generalization and aggre-
gation of massive movement data.
IEEE Transactions on
Visualization and Computer Graphics, 2011, 17(2): 205-219.
[34] Afzal S, Maciejewski R, Jang Y, Elmqvist N, Ebert D S.
Spatial text visualization using automatic typographic maps.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2556-2564.

[35] Andrienko G, Andrienko N, Hurter C, Rinzivillo S, Wrobel S.
Scalable analysis of movement data for extracting and explor-
ing signiﬁcant places. IEEE Transactions on Visualization
and Computer Graphics, 2013, 19(7): 1078-1094.

[36] Andrienko N, Andrienko G. Exploratory Analysis of Spatial

and Temporal Data. Springer Berlin, 2006.

[37] Bak P, Mansmann F, Janetzko H, Keim D A. Spatiotem-
poral analysis of sensor logs using growth ring maps. IEEE
Transactions on Visualization and Computer Graphics, 2009,
15(6): 913-920.

[38] Buchin K, Speckmann B, Verbeek K. Flow map layout via spi-
ral trees. IEEE Transactions on Visualization and Computer
Graphics, 2011, 17(12): 2536-2544.

[39] Dang T N, Anand A, Wilkinson L. TimeSeer: Scagnostics for
high-dimensional time series. IEEE Transactions on Visua-
lization and Computer Graphics, 2013, 19(3): 470-483.

[40] Ferreira N, Lins L, Fink D, Kelling S, Wood C, Freire J, Silva
C. BirdVis: Visualizing and understanding bird populations.
IEEE Transactions on Visualization and Computer Graph-
ics, 2011, 17(12): 2374-2383.

[41] Haunert J H, Sering L. Algorithms for labeling focus regions.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2583-2592.

[42] Haunert J H, Sering L. Drawing road networks with focus
regions. IEEE Transactions on Visualization and Computer
Graphics, 2011, 17(12): 2555–2562.

[43] Kim S, Maciejewski R, Malik A, Jang Y, Ebert D S, Isen-
berg T. Bristle Maps: A multivariate abstraction technique
for geovisualization. IEEE Transactions on Visualization and
Computer Graphics, 2013, 19(9): 1438-1454.

[44] Kr´stajic M, Bertini E, Keim D A. CloudLines: Compact dis-
play of event episodes in multiple time-series. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(12):
2432-2439.

[45] Maciejewski R, Hafen R, Rudolph S et al. Forecasting
hotspots — A predictive analytics approach. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(4):
440-453.

[46] Maciejewski R, Rudolph S, Hafen R et al. A visual analytics
approach to understanding spatiotemporal hotspots.
IEEE
Transactions on Visualization and Computer Graphics, 2010,
16(2): 205-220.

[47] Scheepens R, Willems N, van de Wetering H et al. Compos-
ite density maps for multivariate trajectories. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(12):
2518-2527.

[48] Scheepens R, Willems N, van de Wetering H, van Wijk J J.
Interactive visualization of multivariate trajectory data with
density maps.
the IEEE Symposium on Paciﬁc
Visualization, Mar. 2011, pp.147-154.

In Proc.

[49] Shi C, Cui W, Liu S, Xu P, Chen W, Qu H. RankExplorer:
Visualization of ranking changes in large time series data.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2669-2678.

[50] Sips M, Kothur P, Unger A, Hege H C, Dransch D. A visual
analytics approach to multiscale exploration of environmental
time series. IEEE Transactions on Visualization and Com-
puter Graphics, 2012, 18(12): 2899-2907.

[51] Slingsby A, Dykes J, Wood J. Exploring uncertainty in geode-
mographics with interactive graphics. IEEE Transactions on
Visualization and Computer Graphics, 2011, 17(12): 2545-
2554.

[52] Tominski C, Schumann H, Andrienko G, Andrienko N.
Stacking-based visualization of trajectory attribute data.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2565-2574.

[53] Wang Y S, Chi M T. Focus+context metro maps.

IEEE
Transactions on Visualization and Computer Graphics, 2011,
17(12): 2528-2535.

[54] Wongsuphasawat K, Gotz D. Exploring ﬂow, factors, and out-
comes of temporal event sequences with the outﬂow visua-
lization. IEEE Transactions on Visualization and Computer
Graphics, 2012, 18(12): 2659-2668.

[55] Wood J, Badawood D, Dykes J, Slingsby A. BallotMaps: De-
tecting name bias in alphabetically ordered ballot papers.
IEEE Transactions on Visualization and Computer Graph-
ics, 2011, 17(12): 2384-2391.

[56] Wood J, Dykes J. Spatially ordered treemaps. IEEE Transac-
tions on Visualization and Computer Graphics, 2008, 14(6):
1348-1355.

[57] Zhao J, Chevalier F, Pietriga E, Balakrishnan R. Exploratory
analysis of time-series with chronolenses. IEEE Transactions

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

865

on Visualization and Computer Graphics, 2011, 17(12): 2422-
2431.

Visualization and Computer Graphics, 2011, 17(12): 2412-
2421.

[58] Sun G, Liang R, Wu F, Qu H. A web-based visual analy-
tics system for real estate data. Science China Information
Sciences, 2013, 56(5): 052112(13).

[76] Cui W, Wu Y, Liu S et al. Context-preserving dynamic word
cloud visualization. IEEE Computer Graphics and Applica-
tions, 2010, 30(6): 42-53.

[59] Cao N, Gotz D, Sun J, Qu H. Dicon: Interactive visual ana-
lysis of multidimensional clusters.
IEEE Transactions on
Visualization and Computer Graphics, 2011, 17(12): 2581-
2590.

[77] Dork M, Gruen D, Williamson C, Carpendale S. A visual
backchannel for large-scale events.
IEEE Transactions on
Visualization and Computer Graphics, 2010, 16(6): 1129-
1138.

[60] Claessen J H, van Wijk J J. Flexible linked axes for multiva-
riate data visualization. IEEE Transactions on Visualization
and Computer Graphics, 2011, 17(12): 2310-2316.

[78] Koh K, Lee B, Kim B, Seo J. Maniwordle: Providing ﬂexible
control over wordle. IEEE Transactions on Visualization and
Computer Graphics, 2010, 16(6): 1190-1197.

[61] Geng Z, Peng Z, Laramee R S, Walker R, Roberts J C. Angu-
lar histograms: Frequency-based visualizations for large, high
dimensional data. IEEE Transactions on Visualization and
Computer Graphics, 2011, 17(12): 2572-2580.

[79] Luo D, Yang J, Krstajic M, Ribarsky W, Keim D A. Even-
tRiver: Visually exploring text collections with temporal ref-
erences. IEEE Transactions on Visualization and Computer
Graphics, 2012, 18(1): 93-105.

[62] Joia P, Paulovich F V, Coimbra D et al. Local aﬃne multi-
dimensional projection. IEEE Transactions on Visualization
and Computer Graphics, 2011, 17(12): 2563-2571.

[63] Keim D A, Hao M C, Dayal U. Hierarchical pixel bar charts.
IEEE Transactions on Visualization and Computer Graph-
ics, 2002, 8(3): 255-269.

[64] Lee J H, McDonnell K T, Zelenyuk A, Imre D, Mueller K.
A structure-based distance metric for high-dimensional space
exploration with multi-dimensional scaling. IEEE Transac-
tions on Visualization and Computer Graphics, 2013, to be
appeared.

[65] Paiva J G, Florian L, Pedrini H, Telles G, Minghim R. Im-
proved similarity trees and their application to visual data
classiﬁcation. IEEE Transactions on Visualization and Com-
puter Graphics, 2011, 17(12): 2459-2468.

[66] Steinberger M, Waldner M, Streit M et al. Context-preserving
visual links. IEEE Transactions on Visualization and Com-
puter Graphics, 2011, 17(12): 2249-2258.

[67] Tatu A, Albuquerque G, Eisemann M et al. Automated
analytical methods to support visual exploration of high-
dimensional data. IEEE Transactions on Visualization and
Computer Graphics, 2011, 17(5): 584-597.

[68] Turkay C, Filzmoser P, Hauser H. Brushing dimensions — A
dual visual analysis model for high-dimensional data. IEEE
Transactions on Visualization and Computer Graphics, 2011,
17(12): 2591-2599.

[69] Turkay C, Lundervold A, Lundervold A J, Hauser H. Rep-
resentative factor generation for the interactive visual ana-
lysis of high-dimensional data. IEEE Transactions on Visua-
lization and Computer Graphics, 2012, 18(12): 2621-2630.

[70] Yuan X, Guo P, Xiao H, Zhou H, Qu H. Scattering points
in parallel coordinates. IEEE Transactions on Visualization
and Computer Graphics, 2009, 15(6): 1001-1008.

[71] Lex A, Schulz H J, Streit M et al. VisBricks: Multiform
visualization of large, inhomogeneous data. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(12):
2291-2300.

[72] Cao N, Lin Y R, Sun X et al. Whisper: Tracing the spatiotem-
poral process of information diﬀusion in real time.
IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2649-2658.

[73] Cao N, Sun J, Lin Y R et al. Facetatlas: Multifaceted visua-
lization for rich text corpora. IEEE Transactions on Visua-
lization and Computer Graphics, 2010, 16(6): 1172-1181.

[74] chul Kwon B, Javed W, Ghani S et al. Evaluating the role of
time in investigative analysis of document collections. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(11): 1992-2004.

[75] Cui W, Liu S, Tan L et al. TextFlow: Towards better un-
derstanding of evolving topics in text. IEEE Transactions on

[80] Oelke D, Spretke D, Stoﬀel A, Keim D A. Visual readabil-
ity analysis: How to make your writings easier to read. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(5): 662-674.

[81] Segel E, Heer J. Narrative visualization: Telling stories with
IEEE Transactions on Visualization and Computer

data.
Graphics, 2010, 16(6): 1139-1148.

[82] Strobelt H, Oelke D, Rohrdantz C et al. Document cards:
A top trumps visualization for documents. IEEE Transac-
tions on Visualization and Computer Graphics, 2009, 15(6):
1145-1152.

[83] Viegas F B, Wattenberg M, Feinberg J. Participatory visua-
IEEE Transactions on Visualization

lization with Wordle.
and Computer Graphics, 2009, 15(6): 1137-1144.

[84] Wu Y, Provan T, Wei F, Liu S, Ma K L. Semantic-preserving
word clouds by seam carving. Computer Graphics Forum,
2011, 30(3): 741-750.

[85] Xu P, Wu Y, Wei E, Peng T Q, Liu S, Zhu J H, Qu H. Visual
analysis of topic competition on social media. IEEE Transac-
tions on Visualization and Computer Graphics, 2013, 19(12):
to be appeared.

[86] Zhao J, Chevalier F, Collins C, Balakrishnan R. Facilitat-
IEEE
ing discourse analysis with interactive visualization.
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2639-2648.

[87] Alper B, Riche N, Ramos G, Czerwinski M. Design study of
linesets, a novel set visualization technique. IEEE Transac-
tions on Visualization and Computer Graphics, 2011, 17(12):
2259-2267.

[88] Bae J, Watson B. Developing and evaluating quilts for the de-
piction of large layered graphs. IEEE Transactions on Visua-
lization and Computer Graphics, 2011, 17(12): 2268-2275.

[89] Brandes U, Nick B. Asymmetric relations in longitudinal so-
cial networks. IEEE Transactions on Visualization and Com-
puter Graphics, 2011, 17(12): 2283-2290.

[90] Burch M, Vehlow C, Beck F, Diehl S, Weiskopf D. Parallel
edge splatting for scalable dynamic graph visualization. IEEE
Transactions on Visualization and Computer Graphics, 2011,
17(12): 2344-2353.

[91] Crnovrsanin T, Liao I, Wu Y, Ma K L. Visual recommen-
dations for network navigation. Computer Graphics Forum,
2011, 30(3): 1081-1090.

[92] Cui W, Zhou H, Qu H, Wong P C, Li X. Geometry-based
edge clustering for graph visualization. IEEE Transactions
on Visualization and Computer Graphics, 2008, 14(6): 1277-
1284.

[93] Dinkla K, Westenberg M A, van Wijk J J. Compressed adja-
cency matrices: Untangling gene regulatory networks. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(12): 2457-2466.

866

J. Comput. Sci. & Technol., Sept. 2013, Vol.28, No.5

[94] Dork M, Riche N H, Ramos G, Dumais S. PivotPaths:
Strolling through faceted information spaces. IEEE Transac-
tions on Visualization and Computer Graphics, 2012, 18(12):
2709-2718.

[113] Correa C D, Crnovrsanin T, Ma K L. Visual reasoning about
social networks using centrality sensitivity. IEEE Transac-
tions on Visualization and Computer Graphics, 2012, 18(1):
106-120.

[95] Ellis G, Dix A. A taxonomy of clutter reduction for informa-
tion visualisation. IEEE Transactions on Visualization and
Computer Graphics, 2007, 13(6): 1216-1223.

[96] Ersoy O, Hurter C, Paulovich F V, Cantareiro G, Telea A.
Skeleton-based edge bundling for graph visualization. IEEE
Transactions on Visualization and Computer Graphics, 2011,
17(12): 2364-2373.

[97] Feng K C, Wang C, Shen H W, Lee T Y. Coherent time-
varying graph drawing with multifocus+context interaction.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(8): 1330-1342.

[98] Gou L, Zhang X. TreeNetViz: Revealing patterns of networks
over tree structures. IEEE Transactions on Visualization and
Computer Graphics, 2011, 17(12): 2449-2458.

[99] Hadlak S, Schulz H J, Schumann H. In situ exploration of large
dynamic networks. IEEE Transactions on Visualization and
Computer Graphics, 2011, 17(12): 2334-2343.

[100] Henry N, Fekete J D. MatrixExplorer: A dual-representation
system to explore social networks.
IEEE Transactions on
Visualization and Computer Graphics, 2006, 12(5): 677-684.
[101] Henry N, Fekete J D, McGuﬃn M J. Nodetrix: A hy-
brid visualization of social networks. IEEE Transactions on
Visualization and Computer Graphics, 2007, 13(6): 1302-
1309.

[102] Holten D. Hierarchical edge bundles: Visualization of adja-
cency relations in hierarchical data. IEEE Transactions on
Visualization and Computer Graphics, 2006, 12(5): 741-748.
[103] Holten D, van Wijk J J. Force-directed edge bundling for
graph visualization. Computer Graphics Forum, 2009, 28(3):
983-990.

[104] Hurter C, Telea A, Ersoy O. MoleView: An attribute and
structure-based semantic lens for large element-based plots.
IEEE Transactions on Visualization and Computer Graph-
ics, 2011, 17(12): 2600-2609.

[105] Liu S, Wu Y, Wei E, Liu M, Liu Y. StoryFlow: Tracking the
evolution of stories. IEEE Transactions on Visualization and
Computer Graphics, 2013, 19(12): to be appeared.

[106] Luo S J, Liu C L, Chen B Y, Ma K L. Ambiguity-free edge-
bundling for interactive graph visualization. IEEE Transac-
tions on Visualization and Computer Graphics, 2012, 18(5):
810-821.

[107] Sedlmair M, Frank A, Munzner T, Butz A. RelEx: Visua-
lization for actively changing overlay network speciﬁcations.
IEEE Transactions on Visualization and Computer Graph-
ics, 2012, 18(12): 2729-2738.

[108] Selassie D, Heller B, Heer J. Divided edge bundling for di-
rectional network data. IEEE Transactions on Visualization
and Computer Graphics, 2011, 17(12): 2354-2363.

[109] Tanahashi Y, Ma K L. Design considerations for optimizing
storyline visualizations. IEEE Transactions on Visualization
and Computer Graphics, 2012, 18(12): 2679-2688.

[110] Yang J, Liu Y, Zhang X, Yuan X, Zhao Y, Barlowe S, Liu S.
PIWI: Visually exploring graphs based on their community
structure.
IEEE Transactions on Visualization and Com-
puter Graphics, 2013, 19(6): 1034-1047.

[111] Yuan X, Che L, Hu Y, Zhang X. Intelligent graph layout us-
ing many users’ input. IEEE Transactions on Visualization
and Computer Graphics, 2012, 18(12): 2699-2708.

[112] Zinsmaier M, Brandes U, Deussen O, Strobelt H. Interactive
level-of-detail rendering of large graphs. IEEE Transactions
on Visualization and Computer Graphics, 2012, 18(12): 2486-
2495.

[114] Martins R M, Andery G F, Heberle H, Paulovich F V, de An-
drade Lopes A, Pedrini H, Minghim R. Multidimensional pro-
jections for visual analysis of social networks. Journal of
Computer Science and Technology, 2012, 27(4): 791-810.

[115] Bowman B, Elmqvist N, Jankun-Kelly T J. Toward visua-
lization for games: Theory, design space, and patterns. IEEE
Transactions on Visualization and Computer Graphics, 2012,
18(11): 1956-1968.

[116] Trimm D, Rheingans P, desJardins M. Visualizing student
IEEE Transac-
histories using clustering and composition.
tions on Visualization and Computer Graphics, 2012, 18(12):
2809-2818.

[117] Nocaj A, Brandes U. Organizing search results with a refer-
IEEE Transactions on Visualization and Com-

ence map.
puter Graphics, 2012, 18(12): 2546-2555.

[118] Albuquerque G, Lowe T, Magnor M. Synthetic generation
of high-dimensional datasets. IEEE Transactions on Visua-
lization and Computer Graphics, 2011, 17(12): 2317-2324.

[119] Lu Z, Liu C, Zhang Q, Zhang C, Fan D, Yang P. Visual ana-
lytics for the clustering capability of data. Science China
Information Sciences, 2013, 56(5): 052110(14).

[120] Heimerl F, Koch S, Bosch H, Ertl T. Visual classiﬁer training
for text document retrieval. IEEE Transactions on Visua-
lization and Computer Graphics, 2012, 18(12): 2839-2848.

[121] Keim D A, Andrienko G, Fekete J et al. Visual analytics:
Deﬁnition, process, and challenges.
In Information Visua-
lization: Human-Centered Issues and Perspectives, Kerren A,
Stasko J, Fekete J et al. (eds.), Springer, 2008, pp.154-175.

[122] Havre S, Hetzler B, Nowell L. Themeriver: Visualizing theme
changes over time. In Proc. the IEEE Symposium on Infor-
mation Visualization, Oct. 2000, pp.115-123.

[123] Keim D A, Kriegel H P. Visualization techniques for min-
ing large databases: A comparison. IEEE Transactions on
Knowledge and Data Engineering, 1996, 8(6): 923-938.

[124] Bellman R E. Dynamic Programming. Courier Dover Publi-

cations, 2003.

In Proc.

[125] Inselberg A, Dimsdale B. Parallel coordinates: A tool for
the 1st

visualizing multi-dimensional geometry.
IEEE Symposium On Visualization, Oct. 1990, pp.361-378.
[126] Song Y, Pan S, Liu S, Wei F, Zhou M X, Qian W. Constrained
co-clustering for textual documents. In Proc. the 24th AAAI
Conference on Artiﬁcial Intelligence, July 2010, pp.581-586.
[127] Zhang J, Song Y, Zhang C, Liu S. Evolutionary hierarchical
dirichlet processes for multiple correlated time-varying cor-
pora.
the ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining, July 2010,
pp.1079-1088.

In Proc.

[128] Munroe R. Xkcd

657:

Movie

narrative

charts.

http://xkcd.com/657, Aug. 2013.

[129] Hullman J, Drucker S, Riche N H, Lee B, Fisher D, Adar E. A
deeper understanding of sequence in narrative visualization.
IEEE Transactions on Visualization and Computer Graph-
ics, 2013, to be appeared.

[130] Lee B, Kazi R H, Smith G. SketchStory: Telling more en-
gaging stories with data through freeform sketching. IEEE
Transactions on Visualization and Computer Graphics, 2013,
to be appeared.

[131] Correa C D, Chan Y H, Ma K L. A framework for uncertainty-
aware visual analytics. In Proc. the IEEE Symposium on Vi-
sual Analytics Science and Technology, Oct. 2009, pp.51-58.
[132] Tory M, Moeller T. Evaluating visualizations: Do expert re-
IEEE Computer Graphics and Applications,

views work?
2005, 25(5): 8-11.

Guo-Dao Sun et al.: A Survey of Visual Analytics Techniques and Applications

867

[133] Jankun-Kelly T, Ma K L, Gertz M. A model and framework
for visualization exploration. IEEE Transactions on Visua-
lization and Computer Graphics, 2007, 13(2): 357-369.

Guo-Dao Sun is a Ph.D. candi-
date at Zhejiang University of Tech-
nology, Hangzhou.
He is a re-
search intern at the Internet Graph-
ics Group, Microsoft Research Asia,
Beijing, from 2013 to 2014. His re-
search interests include information
visualization and visual analytics.

Ying-Cai Wu is a researcher in
the Internet Graphics Group at Mi-
crosoft Research Asia. His main re-
search interests include visual analy-
tics of social media data, uncertainty
data visualization and modeling, and
visual analytics of large-scale user log
data. He received the B.Eng. degree
in computer science and technology
from the South China University of
Technology in 2004 and the Ph.D. degree in computer sci-
ence from the Hong Kong University of Science and Tech-
nology in 2009. Prior to his current position, he was a
postdoctoral researcher at the Visualization and Interface
Design Innovation (VIDi) research group in the University
of California, Davis, from June 2010 to March 2012.

Rong-Hua Liang received the
B.Sc.
degree in computer science
from Hangzhou Dianzi University in
1996, and the Ph.D. degree in com-
puter science from Zhejiang Univer-
sity in 2003. He worked as a research
fellow at the University of Bedford-
shire, UK, from April 2004 to July
2005 and as a visiting scholar at the
University of California, Davis, US,
from March 2010 to March 2011. He is currently a profes-
sor of computer science, and vice dean of the College of In-
formation Engineering, Zhejiang University of Technology,
Hangzhou. His research interests include computer vision,
information visualization and visual analytics.

Shi-Xia Liu

is the lead re-
searcher in the Internet Graphics
Group at Microsoft Research Asia
(MSRA), Beijing. Her research in-
terest mainly focuses on interactive,
visual text analytics and interactive,
visual graph analytics. She received
her B.S. and M.S. degrees in com-
putational mathematics from Harbin
Institute of Technology, a Ph.D. de-
gree in computer-aided design and computer graphics from
Tsinghua University. Before she joined MSRA, she worked
as a research staﬀ member and research manager at IBM
China Research Lab, where she managed the Departments
of Smart Visual Analytics and User Experience.

",False,2013.0,{},False,False,journalArticle,False,SGQEGYSD,[],self.user,False,False,False,False,http://link.springer.com/10.1007/s11390-013-1383-8,,A Survey of Visual Analytics Techniques and Applications: State-of-the-Art Research and Future Challenges,SGQEGYSD,False,False
769UTJ9U,L2ETAE4Q,"Visual Analytics for the Big Data Era – A Comparative Review of

State-of-the-Art Commercial Systems

Leishi Zhang∗

University of Konstanz, Germany

Andreas Stoffel†

Sebastian Mittelst¨adt§

Tobias Schreck¶

University of Konstanz, Germany

University of Konstanz, Germany

Ren´e Pompl(cid:3)
Siemens AG

University of Konstanz, Germany

University of Konstanz, Germany

Michael Behrisch‡
Stefan Weber∗∗
Siemens AG

Holger Last††
Siemens AG

Daniel Keim‡‡

University of Konstanz, Germany

ABSTRACT
Visual analytics (VA) system development started in academic re-
search institutions where novel visualization techniques and open
source toolkits were developed. Simultaneously, small software
companies, sometimes spin-offs from academic research institu-
tions, built solutions for speciﬁc application domains.
In recent
years we observed the following trend: some small VA companies
grew exponentially; at the same time some big software vendors
such as IBM and SAP started to acquire successful VA compa-
nies and integrated the acquired VA components into their existing
frameworks. Generally the application domains of VA systems have
broadened substantially. This phenomenon is driven by the genera-
tion of more and more data of high volume and complexity, which
leads to an increasing demand for VA solutions from many applica-
tion domains. In this paper we survey a selection of state-of-the-art
commercial VA frameworks, complementary to an existing survey
on open source VA tools. From the survey results we identify sev-
eral improvement opportunities as future research directions.
Index Terms: H.4 [Information Systems]: INFORMATION SYS-
TEMS APPLICATIONS, K.1 [Computing Milieux]: THE COM-
PUTER INDUSTRY—Markets

1 INTRODUCTION
We are at the beginning of a big data era when data is generated at
an incredible speed everywhere — from satellite images to social
media posts, from online transaction records to high-throughput
biological experiment results, and from mobile phone GPS sig-
nals to digital pictures and videos posted online [3]. According
to IBM [9] 2.5 quintillion bytes of data are generated every day.
Thus, 90% of todays data has been created in the last two years
alone. This phenomenon leads to an increasing interest and effort
from both academia and industry towards developing VA solutions
with improved performance. On the academic side, a number of
advanced VA techniques and open source toolkits have been de-
veloped [21]. On the industrial side, a large variety of companies,
ranging from specialized data discovery vendors such as Tableau,

∗e-mail:leishi.zhang@uni-konstanz.de
†e-mail:andreas.stoffel@uni-konstanz.de
‡e-mail:michael.behrisch@uni-konstanz.de
§e-mail:sebastian.mittelst¨adt@uni-konstanz.de
¶e-mail:tobias.schreck@uni-konstanz.de
(cid:3)e-mail:rene.pompl.ext@siemens.com
∗∗e-mail:stefan hagen.weber@siemens.com
††e-mail:holger.last@siemens.com
‡‡e-mail:keim@uni-konstanz.de

QlikTech, and TIBCO, to multinational corporations such as IBM,
Microsoft, Oracle and SAP, have all devoted much effort to develop
their own commercial products for analyzing data of increasing vol-
ume and variety that arrives ever quicker.

Stakeholders from both academia and industry are well-aware of
the importance of gaining an overview of the state-of-the-art solu-
tions to stimulate innovative ideas and avoid redundant effort. Such
overview enables people to understand limitations of existing solu-
tions and thus to identify space for improvement. In the last couple
of years, effort has been made to survey and compare the function-
ality of existing open-source VA toolkits [21] as well as commercial
Business Intelligence (BI) applications [19, 28]. Such studies are
important to assess what tools are available, what techniques they
implement, and how good they are with respect to certain applica-
tion tasks. However, a thorough survey of speciﬁc visual analysis
functionality of existing commercial VA tools is still lacking, given
that the range of tools in existing surveys is restricted to BI appli-
cations and focuses on the usability aspects of a product. Towards
this end, we conducted a survey on a wider range of commercial
VA tools including not only BI VA products but also a number of
general purpose VA tools, and put our focus on evaluating their
capability of handling data of large volume and variety efﬁciently.
While existing surveys are largely based on user surveys, we devote
much effort to evaluate the system performance and functionality
by installing the software and testing with reference datasets.

We conducted our survey by ﬁrst building an encompassing list
of 15 relevant commercial systems. The choice is made by in-
vestigating current market share. A wide range of systems were
selected, covering software that falls into different categories, for
example, data discovery and visualization software, enterprise BI
systems, network analysis toolkits, innovative and niche products;
some products fall into more than one category. We assigned each
system a priority level to make sure that we can focus on a smaller
number of “core” systems without losing the whole picture. In the
second phase, a structured questionnaire was designed for evalu-
ating the functionality of each product from different perspectives,
including data management, visualization, automatic analysis, and
system and performance. We then contacted all vendors to get their
answers to our questionnaire. Although many vendors responded
with detailed answers, we did not manage to get responses from all
of them.

In this paper we report the results for those ten systems whose
vendors answered our questionnaire, including Tableau [14], Spot-
ﬁre [4], QlikView [13], JMP (SAS) [11], Jaspersoft [10], ADVIZOR
Solutions [6], Board [7], Centrifuge [8], Visual Analytics [15], and
Visual Mining [16]. For the remaining systems in our initial list,
some of which are regarded as key products in the market (Cognos
(IBM), SQL Server BI (Microsoft), Business Objects (SAP), Tera-
data, and PowerPivot (Microsoft), we managed to ﬁnd many an-
swers to the questionnaire by ourselves, which allows us to gain a
better understanding and overview of state-of-the-art VA systems.

173

But to provide a fair comparison we do not include our ﬁndings
about those ﬁve tools in the survey. This means unfortunately all the
systems that support linguistic analysis on text documents (Busi-
ness Objects, Cognos and Teradata) fall out of the comparison ta-
bles. However some of the relevant ﬁndings are used to support
the analysis and discussion in this paper. To provide further ref-
erences, we also investigated a number of analytical tools that are
known for their text analysis functionality, including nSpace (Ocu-
lus) [12], Palentir [2], and In-Spire (PNNL) [1] and integrate some
of our ﬁndings in the discussion.

In the last phase, further evaluation was carried out on the sys-
tems in the top priority list. After installing all the systems on the
same machine under the same conﬁguration, we performed a se-
ries of loading stress test to check the scalability of each system.
The analytical and visualization capability of the selected systems
is further tested using two benchmark dataset provided by different
research communities representing real-world data analysis chal-
lenges.

The main contributions of this paper are: (1) we complement the
existing survey of open-source toolkits [21] and user surveys of BI
tools [19, 28] by conducting an encompassing survey of commer-
cial VA tools; (2) we structure a comparison of the tools along a
harmonized schema; and (3) we draw some careful conclusion and
give recommendations to potential users on which tools are applica-
ble for what types of applications. (4) We identify future directions
for developing VA systems. The remainder of this paper is orga-
nized as follows: In the next section, we discuss related work. In
Section 3, we analyze the functionality of each product. In Sec-
tion 4, we show the result of our data evaluation. We summarize
our key ﬁndings in Section 5, before drawing conclusion and dis-
cussing space for improvement in current commercial products and
identifying interesting future directions in Section 6.

2 RELATED WORK
In this section, we review work on the deﬁnition of VA, existing VA
systems and surveys on the market for commercial products.

Visual Analytics Methodology. The VA methodology is
based on combining data visualization, data analytics, and human-
computer interaction to solve application problems.
Its general
approach, application examples, and research challenges are de-
tailed in [27, 26]. Recently, the infrastructure working group within
the EU VisMaster project [5] identiﬁed a number of shortcom-
ings of the current state of application of VA technology in prac-
tice [26] (Chapter 6). The lack of standardization in software com-
ponents, functionality and interfaces was regarded as a major prob-
lem, leading to a loss in efﬁciency and scalability due to massive
re-implementation of software components. Hence, standardization
was proposed as the key approach to enable a market for software
components which eventually should lead to streamlined produc-
tion of application-oriented VA systems.

Open Source Toolkits. A number of open-source VA toolk-
its exist; each covers a speciﬁc set of functionalities for visualiza-
tion, analysis and interaction. For example, InfoVis Toolkit [18],
Prefuse [23], Improvise [29], and JUNG [24]. Using existing toolk-
its for required functionality instead of implementing from scratch
provides much efﬁciency while developing new VA solutions, al-
though the level of maintenance, development and user community
support of open source toolkits can vary drastically. Besides, a rel-
atively high amount of programming expertise and effort is often
required to integrate these components into a new system. In [21],
a survey of 21 existing open source toolkits is presented. The func-
tionality of these toolkits is compared along three criteria: (1) vi-
sualization functions, (2) analysis capabilities, and (3) supported
development environment. The aim of the survey is to provide a
reference to developers for choosing a base framework for a given
problem.

174

Commercial VA Systems. An alternative is to resort to soft-
ware suites which integrate required functionality in software sys-
tems which work either standalone, or integrate, more or less seam-
lessly, into an existing information infrastructure. Example systems
include Tableau [14], Spotﬁre [4], and QlikView [13]. Commer-
cial toolkits typically require no or only limited conﬁgurations or
program adjustments, to become operational. They may provide,
subject to the business policy of the vendor, speciﬁc levels of main-
tenance, development and user support. As part of the software
market for (corporate) information systems, the BI market segment
provides commercial tools for analyzing business data. The BI soft-
ware market consists of long-standing software suites, which have
developed out of core database or statistical data analysis suites.
Other products are developed and marketed as standalone tools or
add-ons to existing information systems. Common tasks of BI sys-
tems include reporting of historic and current data, analysis (intel-
ligence) of data, and prediction including what-if-analysis.

BI System User Surveys. Gartner Research surveys the BI
software market annually and publish their result online [19]. They
maintain a set of 14 functional requirements that BI tools aim at,
structured along three categories: (1) integration into existing en-
vironments, (2) information delivery and (3) information analysis
functionality. A set of 21 products is included in the 2012 sur-
vey which outlines the strengths and possible risks of each selected
product, relative to the market and product history. A characteriza-
tion of the 21 products as challengers (2 products), market leaders
(8 products), niche solutions (11 products), and visionaries (0) is
provided.

In another report, a detailed survey of 16 current BI products is
provided by Passionned Group [28]. Eight evaluation criteria are
deﬁned by the study, ranging from software architecture, function-
ality, to usability and analytic capabilities. The products are catego-
rized into (1) standalone enterprise-level solutions, (2) BI products
which come integrated with database systems software, (3) data dis-
covery and visualization tools, and (4) innovative and niche prod-
ucts. A scoring scheme is deﬁned to compare product along these
criteria individually. Also, an all-against-all comparison along ag-
gregated scores is provided.

Open Source and Commercial Tool Landscape. There is a
wide spectrum of tools from which VA applications can be built. In
general, the open source domain provides state-of-the-art function-
ality, which may include early and sometimes prototypical tech-
niques. Often a library has to be embedded into a front-end and
connected to a back-end data infrastructure, to obtain an end-user
application. However we also see exceptions. For example, Gephi,
an open source graph visualization tool, also features a rich user
front-end interface. Open source tools are mainly developed and
maintained on a voluntary basis.

On the other hand, in the commercial sector, we see more conser-
vative visualization techniques, which in most instances are already
integrated with user front ends and data back end infrastructure.
Whereas in the open source market, development takes place in an
open, sometimes unpredictable manner, development in the com-
mercial area takes place under competition, in a closed way, often
involving pilot users. Intermediate results are not discussed with
the larger public.

Open source tools are freely available, whereas commercial
products generally require costly licensing. Licensing fees vary
drastically. For an industrial investment decision, the total cost of
ownership is relevant, which includes roll out, development and
adaption, life cycle management, and user training, among other
factors. It depends also on the environment in which the tools are
deployed. The discussion of this is beyond the scope of this work.
To determine the total costs a consultancy process is required, in-
volving users, vendors, and business process specialists.

Table 1: Data Handling Functionality

,


	


	

//-0-		'-

*,	
	

//-0-		'-

*,	
	

0

//-0-		'-

*,	
	
	



*	

+3	'
	

(4


	


	

-	

-	
-	



		

	 !
""#	$%

- -

- -
- -

- -

- 

- 
- 

- 

 
	

		/5



&'



((
)		*	

- %12

- 

- %12

- 

+	,
 ,-

-!		--

.	%

- - - - 

- - - - 
- - - - 

- - - - 

In this paper we concentrate on a functional comparison of a
selected number of tools. We relate our work with the existing
surveys as follows. Gartner reports and Passionned survey aim at
providing an overview of functionality of major BI products as a
reference to potential customers and market analysts. The result is
largely based on feedback from current users, although the vendors
are contacted to supply additional information (business strategy,
vision, etc.). We take a rather different perspective and approach
- we survey the identiﬁed vendors with a structured questionnaire
consisting of questions covering different aspect of system perfor-
mance and functionality, and test-driving the selected toolkits in
a standardized environment and on benchmark datasets. We also
extend the scope of the tool selection by including a number of
characteristic VA tools which provide solutions to speciﬁc problem
domains that are not included in BI tools. The main objective of
our survey is to provide an comparative review of the state-of-the-
art VA systems and highlight possible technical advances for future
research and development.

3 FUNCTIONAL COMPARISON
Typically, there are three main actions in a VA system work ﬂow,
data management, data modeling and visualization [26]. First of
all, heterogeneous data sources need to be processed and integrated.
Automated analysis techniques can then be applied to generate
models of the original data. These models can be visualized for
evaluation and reﬁnement. In addition to checking the models, vi-
sual representations can be abstracted from the data using a variety
of interactive visualization techniques that are best suited for the
speciﬁc data type, structure, and dimensionality. In the VA process,
knowledge can be gained from visualization, automatic analysis, as
well as the interactions between visualization, models and the hu-
man analysts.

Based on the evaluation strategy described in section 1, a struc-
tured questionnaire consisting of 52 questions was designed to eval-
uate the functionality of each system (see Appendix 1). Questions
are categorized into 4 classes in order to cover the three main ac-
tions in a system work ﬂow as well as the system performance:
data management, automatic analysis, visualization, and system
and performance. The questionnaire was sent to 15 different ven-
dors and 10 answers were received.

Among the 10 systems, 4 fall into the top priority list: Tableau,
Spotﬁre, QlikView, and JMP. We managed to acquire academic or
evaluation licenses from each vendor and evaluated the functional-
ity and performance of the four systems further by installing each
system and testing with real data. In addition, we veriﬁed the infor-
mation provided by vendors wherever possible. Next we detail our
results.

3.1 Data Management
Following the Knowledge Discovery in Databases pipeline deﬁned
by Fayyad et al. [17], the primary steps for VA tools are data load-

ing, integration, preprocessing, transformation, data mining, and
data interpretation. In a data management related functional com-
parison of commercial VA tools one can subsume all data loading,
integration, and exporting options under data management func-
tionality. Operational steps, such as data preprocessing or transfor-
mation, as well as their relation to usability aspects can be classiﬁed
as data handling functionality.

Regarding data management, all VA systems allow connecting to
relational database systems, such as SQL, PostgreSQL, and Oracle.
But only a few tools allow access to vertically scalable storage sys-
tem, such as Hadoop, Vertica (Column-oriented), and MongoDB
(Document-oriented), or web-based on-demand database systems,
such as Amazon S3 and Salesforce Database System (None-SQL,
Object-oriented).

The import of raw (structured or unstructured) data ﬁles was as-
sessed too. The most prominent data ﬁle formats, which are Mi-
crosoft Excel and plain text ﬁle (CSV), are supported by all assessed
tools. Yet, only a few tools import dedicated geo-related ﬁles, such
as ESRI or Google’s KML, or allow to process the content of Adobe
PDF or Microsoft Word ﬁles.

Another data management aspect is related to the simultaneous
access to multiple data sources. In a data warehouse scenario, the
analyst often needs to access various distributed databases. In most
systems, multiple data connections can be maintained. However,
to use some of the dashboarding facilities, a data uniﬁcation batch
needs to be processed to consolidate the data sources.

The data/result exporting is the ﬁnal step in the data analysis
pipeline. It serves the purpose of presenting results to a broader
audience or save intermediate results. In the latter case, it is often
necessary to write results back into the databases. Yet, this data
handling mechanism is rarely implemented. Only Tableau, JMP,
and Visual Analytics support a direct database write-back. The ob-
vious standard way to present results is via (interactive) dashboards
either hosted on-premise (on a company’s secured local server) or
on the VA producer’s public gallery, via HTML or Adobe Flash
Websites.

Mobility is one of the hot topics for commercial VA systems.
Tableau, Spotﬁre, QlikView, and JMP take advantage of their un-
derlying presentation platform and offer Apple iPad apps for ac-
cessing interactive dashboards in meetings, at customer sites and
at operation centers. Another approach towards mobility is the
presentation through HTML5-capable browser engines (e.g. An-
droid/BlackBerry/Nokia built-in browsers support HTML5).

The next functional comparison is related to all mandatory data
handling steps during data transformation. Table 1 emphasizes two
aspects. First, it depicts a use case oriented data handling compar-
ison of the four tools that fall into our top priority list (Tableau,
QlikView, Spotﬁre, and JMP). And second, it gives an insight into
the data handling usability and feature richness.

After the loading procedure, a data cleaning and transformation
step is often needed. For example, handling missing/null values and

175

Table 2: Automatic Analysis Methods

 ( 

+&*

/(&


 '&'  

-&. . . ) &* )  (&

	
+&*

+(.
&'  











































	



  

!""#$

 &'( 
)&*

 &&*
+



	
!""#$## %& '
*+)











% , 










,



























	





%$








)

































 ()
)+- ,%

#














    

normalizing data over one or more dimensions. Most commercial
VA systems provide the user the option of manipulating data with
a proprietary expression language. For example, Tableau patented
in 2003 VizQL [20], a structured, declarative query language that
translates user-actions into database queries and handles the map-
ping of the results to their visual representations.

Since data preprocessing can range from data sampling or ﬁl-
tering, to more sophisticated approaches such as binning or outlier
detection, we decided to derive different data handling tasks that
occur in most data analytics tasks. The ﬁrst one, called Column
calculations, describes a batch modiﬁcation of every row record in
a selected column, for example, string to date conversion or numer-
ical columns scaling. Combining columns or rows, into a single
column/row, is another required data handling step. More related
to the analytical part of data analysis is the task Joins/Joins on Fil-
tered Tables. Most of the commercial VA systems have difﬁculties
in combining tables that are ﬁltered according to the user’s needs.
Accordingly, the user has to overcome these problems by exporting
the ﬁltered table, reloading it from ﬁle, and doing the join operation
as a distinctive intermediate step.

3.2 Automatic Analysis Methods
Various techniques for automatic analysis of data exist, ranging
from simple aggregation to advanced data modeling algorithms. In
our survey, we divide automated analysis functions that are imple-
mented by the investigated systems into four categories: statistics,
data modeling, dimensionality reduction, and visual query analysis.
The ﬁrst category includes statistics functions for: 1) univari-
ate analysis that operates on one dimensional data, for example the
calculation of the mean, minimum and maximum, and standard de-
viation; 2) bivariate analysis that reveals interrelations of two di-
mensions, for example, Pearson correlation and Spearman’s rank
correlation coefﬁcient; and 3) multivariate analysis that models the
relations over multiple dimensions, for example, discriminant anal-
ysis and variance analysis. These functions provide different levels
of statistical analysis and allow the user to explore the data and re-
lations from different perspectives. As shown in Table 2, all the
systems provide some simple statistics methods for univariate and
bivariate analysis, but multivariate analysis is only supported by
Spotﬁre, JMP and ADVIZOR.

Methods in the second category allow the user to model the data
and ﬁnd patterns using various data mining algorithms. Most com-
monly implemented algorithms include: 1) clustering algorithms
that group data items based on their similarities; 2) classiﬁcation al-
gorithms that assign data items into different classes based on train-
ing data with class labels for each data item; 3) network modeling
techniques that model the relationships between data items as a net-
work (graph), where nodes represent entities (e.g. persons, organi-

176

zations) and links represent relationships (e.g. co-authors, friends);
4) predictive modeling techniques that analyze current and histor-
ical facts to make predictions about future events. Note that with
Spotﬁre some of the automatic analysis methods are only available
with additional upgrades.

The third category describes dimension reduction techniques that
can be applied to transform high dimensional data into lower di-
mensional space. Such transformation leverages the dimensionality
problem by reducing the number of dimensions prior to analysis or
visualization while keeping the essence of the data intact. The re-
sult is often used to generate 2D or 3D projections (typically scatter
plots) of the data. The commonly used dimension reduction tech-
niques are Principle Component Analysis (PCA), Multidimensional
Scaling (MDS) and Self Organizing Map (SOM).

Among all the systems, Visual Analytics and Centrifuge are the
only two that support network modeling. Both systems also sup-
port cluster analysis on the networks. JMP and Spotﬁre appear to
cover all the other data modeling functionalities. They are also the
only two systems that implement dimension reduction techniques
for handling high-dimensional data.

Another useful feature for automatic data analysis is pattern
search. Given a target pattern, an automatic searching mechanism
can be designed to look for similar patterns in the data. Some sys-
tems enable the user to deﬁne a target pattern with the help of the
graphical user interface. Once a pattern is deﬁned, the system will
automatically search for similar patterns and visualize the results
accordingly. We call such functionality visual query analysis and
use it as the fourth category. Such functionality is favorable to many
users as it provides a fast and intuitive means of pattern analysis.
Surprisingly only half of the system we surveyed support the visual
query analysis (see Table 2).

3.3 Visualization Techniques
To analyze the visualization functionality of each system, we di-
vide visualization techniques into graphical representations of data
and interaction techniques. The former refer to the visual form in
which the data or model is displayed, for example, a bar chart or
a line chart. Graphical representations are often also called “vi-
sualizations” by the tools, and often refer to the static graphical
models representing the data. Interaction techniques describe how
the user can interact with the graphical models, for example, zoom-
ing or panning, and has to be implemented on top of one or more
graphical representation to provide users with more freedom and
ﬂexibility while exploring graphical representations of the data. In
this section we analyze which of these two types of visualization
techniques are supported by each surveyed product and detail our
ﬁndings.

On a high level, we classify the visualization techniques by the

Table 3: Visualization techniques

	
	

,'*

)*	!

	


,	*


	

(	

	


-

+			(!

)	!

.	


		*


$	+















































































	
	

	


	
 !""#$%
 &
	'	
'
(














	







	




type of visualized data: 1) numerical data; 2) text/web; 3) geo-
related data; and 4) network data (graph). On a lower level, we
investigate individual graphical representations implemented by the
surveyed systems to visualize different types of data. For example,
for visualizing numerical data, a large number of techniques exist,
from bar chart, line chart, pie chart and scatter plots, which are
often used to visualize numerical data with few dimensions, to par-
allel coordinates, heatmaps, and scatter plot matrix, which are used
for displaying data with higher dimensionality.

Text/web data visualization is a relatively new ﬁeld, with tech-
niques such as word cloud [25] and theme river [22] having been
developed in recent years. The generation of more and more geo-
tagged data increases the demand for geo-spatial data visualization.
Often the analyst wants to see geo-related information projected on
a conventional 2D map or 3D globe.

Another important branch are graph visualizations, which are
widely used for displaying relationships in data and which are ap-
plied in emerging ﬁelds such as social network analysis and bio-
logical regulatory network analysis. Depending on whether there
is a hierarchical relation in the graph data, the ﬁeld can be further
divided into hierarchical and non-hierarchical graph visualization.
While many force-directed placement techniques can be applied to
visualize graphs in general, a number of techniques exist for visual-
izing graphs with a hierarchical structure, for example, the treemap
and the hyperbolic view.

Surprisingly, the number of visualization techniques that are im-
plemented by the surveyed VA systems is rather small compared to
the number of techniques that are available from research. Table 3
shows the main visualization techniques that are implemented by
(at least one of) the products we surveyed.

As we can see from the result, all products implement standard
visualization techniques such as line charts, bar charts, pie charts
and histograms. These techniques are commonly used to analyze
data with very few dimensions. Scatterplot, scatterplot matrices and
heatmaps can be found in most of the tools for analyzing data with
higher dimensionality. But to our surprise only few products im-
plement the parallel coordinates visualization, which is considered
to be effective for visualizing high dimensional data. Also none
of the systems provide functionality for textual data visualization
(therefore we removed the column from the comparison table).

In terms of network analysis, only QlikView, Spotﬁre, JMP, Vi-
sual Analytics and Centrifuge provide functionality for visualizing
network data. In addition, functionality for visualizing geo-related
data is rather limited in many systems, although most of them do
allow the user to project data on top of a static map.

Both Tableau and JMP implement recommendation facilities

which suggest suitable visualizations for the input data. This is very
helpful in the initial analysis, especially for people who are not fa-
miliar with visualization techniques or the data. These products are
marked with * in Table 3.

For most visual analytics tasks it is essential to interact with the
data and visualization models. For example, to ﬁlter the data, to
drill down to a subset of the dimensions or data items, to zoom and
pan the view to see the visualization model at different levels of de-
tail, to interactively change the focus of the view without losing the
whole picture (focus+context distortion techniques), and to link and
brush different views to see the data from different perspectives.

Most of the tools we surveyed support interactive ﬁltering and
zooming as well as the distortion of views (e.g. logarithmic scale).
Providing multiple views simultaneously connected by linking-and-
brushing functionality is one of the most effective approaches and
a major strength of some tools.

3.4 System and Architecture
In addition to the functional characteristics of the VA tools, sev-
eral non-functional features determine its usability. For exam-
ple, platform, scalability and architecture. Another important non-
functional characteristic is security with respect to data transmis-
sion, collaborative working environment, anonymization and role-
based content access. Table 4 depicts the system, architecture and
security features of the surveyed systems.

According to our ﬁndings, VA systems can be subdivided into
stand-alone desktop programs and server-sided dashboarding tools.
However, the architecture has direct impact on the scalability and
performance. In case of client-server architectures, dedicated com-
puting server machines can be added to scale to the given process-
ing needs. Tableau, QlikView and Spotﬁre support this so-called
vertical scalability. Of all tools, only QlikView and Jaspersoft(cid:4)s
cloud-based Platform-as-a-Service (PaaS) offering adapts ﬂexibly
to the task’s processing needs.

The deployment platform is another aspect to consider, espe-
cially for medium and large-sized organizations. Most tools sup-
port on the client-side Microsoft Windows XP, Vista, and 7. On
the server side Microsoft Windows Server 2003/2008 dominate the
platform installation environment. Only a few tools allow an in-
stallation on Apple MacOS, Linux distributions or are JVM-based
(Java Virtual Machine) applications.

As external viewers, browser-based access to HTML5 or Flash-
based dashboards are popular. Tableau, Spotﬁre, QlikView, JMP
and Board go even one step further and offer a dedicated iPad app
to take advantage of the underlying mobile platform.

The memory concept also plays an important role for the perfor-

177

Table 4: Scalability and Performance Functionality

7#

	

$8
 

 #





%	

	


	



	






	

 

 

 

 

  

 




() 


*	


2""

2*
	

. 40

 #

 


%


 ""%

 

 

	 !



+,,3+,,-

	 !



+,,-

	 !



+,,-

	 !

""0

	 !



+,,3+,,-
'8""0

	 !



+,,3+,,-

2

	
 !'8

	 !



+,,3+,,-

	

 

	 !



+,,3+,,-

	

**


	


	

**

	

**


	


	


	


	


	

	

**

0






















""
	#	
*
""
	#$%


.

	*/



""
	#$%


.

	*/



""
	#$%


.

	*/





1*#

0








	
/	

""





""
	#$%






$

""
	#$%


.



""

.



""
	#$%


.



""

.



""
	#$%


.

	*/



.
























#

	

#*	
&'

'.""1/

&'

'.""1/

&'

'.""1/

&'

0.


&'

'.""1/

&'

""1/

$+56	&+56








&'

&'

	#9		
*





















mance and scalability in terms of processable data size. Nearly all
vendors acknowledge this fact and come up with a proprietary in-
memory data engine. For example, QlikView(cid:4)s patented in-memory
data analysis engine assumes a star schema in the data and thus as-
sociates ﬁelds with the same name in a global and fast array-like
data structure. The indexes are determined by parallelized scans,
taking advantage of todays multi-core processors. Moreover, it han-
dles caching and query prediction intelligently by taking the cost of
a query reconstruction into account, too. Other vendors, such as
Tableau, Spotﬁre, Jaspersoft, Board and ADVIZOR have their own
approaches to the topic. However, their common point is the capa-
bility of handling big amounts of data. Despite the great advances
in this ﬁeld one has to acknowledge the fact that sophisticated cal-
culations, especially with a lot of data joins, are still limited by the
RAM size and lead to paging.

Security considerations have also to be taken into account. Se-
curity is not only regarded as plain transmission security, but also
content-wise access security. Role-based content access, which
restricts or permits well-deﬁned data views, is implemented in
all systems.
If the data needs to be published openly, automatic
anonymization features are required. In our test it was therefore not
assessed, whether the systems allow to modify one or more name
columns (e.g. by a hashing algorithm) manually and then create a
new anonymized view (ﬁle), but rather if this publishing function-
ality is supported by a built-in export functionality.

4 BENCHMARKING THE SYSTEM PERFORMANCE

In addition to surveying the vendors, we further evaluated the func-
tionality and performance of the four systems in our top priority
list, Tableau, QlikView, Spotﬁre, and JMP. First we installed the
four systems on our local computer under the same system conﬁg-
uration. A use case study is then carried out on the systems using
two benchmark datasets 1) the “Practice Fusion Medical Research
Data” provided by Microsoft Azure Marketplace representing real-
world challenge in health data analysis, and 2) the “Geospatial and
Microblogging Data” provided by VAST challenge 2011 represent-
ing challenges in spatial-temporal data analysis. The essential idea
is to test the analytical and visualization capability of each system.
Besides, a series of loading stress tests are applied to test the scala-
bility of each system. Next we detail our ﬁndings.

178

Figure 1: Histogram and treemap visualizations of the pregnancy
diagnosis in the “Practice Fusion Medical Research Data” in JMP.
The age is mapped into colors in both visualizations. The shares of
pregnancy are mapped to the height of bars in the histogram, and
the area in the treemap.

4.1 Use Case Study

Practice Fusion Medical Research Data contains a sample
of 15,000 de-identiﬁed health records stored in 7 different tables,
recording information about patients, diagnosis, medications, pre-
scriptions, allergies, immunizations, and vitals respectively. All the
tables share a common ﬁeld PatientGuide, which means informa-
tion in different tables can be linked and aggregated across the dif-
ferent tables.

In our study, we use the data to test the data handling capability
of each system, as well as some basic analytical capability with re-
spect to answering simple analysis questions and visualize related
information. To achieve this we started from a simple question
“What is the distribution of pregnancy age?” and try to ﬁnd out
how easy it is to get the answer using different systems and what
type of visualization each system provides.

To answer the question, the data set has to be preprocessed be-
fore further analysis. First of all, tables containing patient and di-

agnosis records have to be joined. Next the age of the patient at the
moment of the diagnosis need to be calculated based on the year of
the diagnosis and the patient’s birth year. The last step is to ﬁlter
out non pregnancy related diagnosis and patients with invalid ages.
We had no problem with all the systems during the preprocessing
stage. After the ﬁltering, 91 pregnancy diagnoses with a valid age
were found among the 77,400 diagnoses in the data.

Using the pregnancy diagnoses records, we tried the basic vi-
sualization functionality of each system. First we try to see if we
could generate a histogram from the data to show the age distri-
bution over pregnancy. While all the systems were able to render
histograms from the data with absolution values (number of preg-
nancies), creating histograms with percentage values seemed to be
more challenging in Spotﬁre and QlikView - both systems require
additional effort to convert absolute values to percentage before ren-
dering. Tableau offers a wizard for creating calculated columns in
the visualization, and JMP includes a similar aggregation function
in the visualization wizard. It is not difﬁcult to ﬁnd out the answer
to our question in the result histograms - the pregnancy age ranges
between 18 and 44, and 22 is the peak age that has the highest preg-
nancy rate.

We further checked the ﬂexibility of customizing visualizations
by trying to assign data values to different visual parameters (e.g.
color, size) in each system. We tested the possibility of double
coding the data values to both height and color of the bars in the
histogram. Although this is possible with all the systems, it is rel-
atively easier in Tableau and Spotﬁre because the user can change
the settings directly on top of the interactive visualization or via
menu functions. With JMP is less easy, because the system tends
to automatically assign the colors of the data column to the corre-
sponding bars in the histogram, and once a visualization is created,
it is not possible to change the color encodings unless the user re-
sets the colors in the data column and generates a new histogram.
With QlikView the user has to deﬁne customized functions for as-
signing colors to bars. This is undesirable to non-programmers, but
for users with more programming experience, the system provides
much freedom to customize their visualizations. For example, a
user deﬁned bi-polar colormap can be generated using some func-
tions in the program library. One slight disadvantage with the cur-
rent implementation is the fact that the colormap cannot to saved.

Last we try to see the possibility of generating a slightly more
“advanced” visualization technique - Treemap with the systems.
Except Tableau, all the other systems support treemap visualiza-
tion. The implementation in both Spotﬁre and QlikView orders the
rectangle in lexical order of the visualized data columns by default.
The conﬁguration of the treemap visualization in all cases are sim-
ilar to the corresponding histogram visualization: while the visual-
izations in Tableau and Spotﬁre are easily conﬁgurable, QlikView
provides less ﬂexibility, although the system does allow the user to
write their own functions for changing conﬁgurations. With JMP,
once a visualization is created, modiﬁcation is restricted. For in-
stance, it is not possible to change the mapping of dimensions in
X and Y axes, however it is easy to create the same visualization
with different settings. Figure 1 shows a histogram and a treemap
visualization generated by JMP as example outputs.

Geospatial and Microblogging Data encodes the character-
ization of an epidemic spread. Two datasets are included, the
ﬁrst one contains geo-tagged microblogging messages with time
stamps, the second one contains map information for the artiﬁcial
“Vastopolis” metropolitan area. We use the data in our second use
case to see how geo-temporal data can be analyzed and visualized
in different systems.

As a preprocessing step we transformed each of the 1,023,057
messages into a tabular
form containing the timestamp, x-
geolocation, y-geolocation and the message text. We store this data
in a CSV ﬁle for further analysis. In all tools, the overarching anal-

ysis goal is to visualize the geo-referenced disease outbreaks over
the given time span.

Importing the 185 MB CSV ﬁle into the tools worked without
any problem. However, only Tableau and Spotﬁre recognized the
standard date format correctly. QlikView and JMP required us to
deﬁne a conversion to their proprietary date format. After load-
ing, the data extraction step requires the calculation of two speciﬁc
columns: (1) the inversion of the y-coordinate (due to the differ-
ent notions of origin in the image and the standard Cartesian co-
ordinate system) and (2) the extraction of interesting disease key-
words, including “breath”, “chest”, “diarrhea”, “cough”, “fever”,
“ﬂu”, “pneumonia”, and “sick”, in the text. All the tools were able
to extract the disease indicators with an if-then-else statement that
checks whether the keywords are present or not. However, more so-
phisticated text analysis/mining features, such as sentiment analy-
sis, stemming or stop word removal, are not present in our packaged
versions of the Visual Analytics tools.

In order to visualize the results, we decided on a small mul-
tiple map presentation that takes the geo-spatial, as well as the
temporal information into account. Each line in the small multi-
ple view should represent the development of one disease indicator
over time. As Figure 2 shows, all tools allowed us to load the data
into a 2D scatterplot and set a user-determined background image
(the Vastopolis map). Furthermore, none of the tools showed prob-
lems with the image space geo-location parameters given in the data
set. While the standard interaction paradigm for exploring the data
is an on-demand time interval ﬁltering, only Spotﬁre and Tableau
have a built-in functionality to visualize a series of small multi-
ples with different ﬁltering parameters each. JMP and QlikView,
on the other hand, let the user explore the content differences on
a single screen. From a visualization perspective, a small multiple
view is one of the best solutions to get an all-embracing overview
of the data. However, the high number of interactive screens has
an impact on the system’s performance. Spotﬁre renders the small
multiple screens fast and allows sufﬁciently fast brushing and link-
ing. JMP and QlikView also render the single screen fast, but vary
greatly in the time needed by brushing and linking.

Some of the known VAST Challenge 2011 ﬁndings can be easily
retrieved from the map visualizations. For example, in Figure 2 all
tools clearly showed the uncorrelatedness of the disease indicators
“diarrhea” and “fever”, thus leading to the hypothesis of two disease
patterns. However, while the small multiple views (a) and (c) give
the user the ability to perceive the delayed outbreaks of the two
diseases on one screen, (b) and (d) leave the user with the problem
of choosing the correct ﬁlter predicate to make this observation.
Another example: Figure 2 (a) and (c) let the user hypothesize that
the wind direction is from west to east, which can be seen in the
“breath” outbreak occurrences. Also, Figure 2 (a) and (c) let the
user hypothesize about the location of the hospitals in Cornertown,
Suburbia, Southville and Lakeside.

4.2 System Performance
Scalability with respect to the size of the analyzed data sets is an im-
portant aspect of a system’s performance. In practice, big data ﬁles
are often held on sophisticated database storage systems, which
themselves can manage operations such as ﬁltering and grouping.
Many VA systems can work with DBMSs and it was not our goal
to test the capacity and connection speed for any particular DBMS.
Instead we experimentally tested the upper boundary of data load
that a VA system can handle on its own.

We generated a series of test data sets of increasing size. Our
test data are uniformly generated records of 50 dimensions, con-
taining 3 categorical and 47 numerical values. We provided our
test data as CSV ﬁles of 100 MB (204,683 records), 200 MB
(409,358 records), 500 MB (1,023,348 records), 1 GB (2,095,847
records), 10 GB (20,957,918 records), 20 GB (41,915,609 records),

179

(a) Tableau

(b) QlikView

(c) Spotﬁre

(d) JMP

Figure 2: Visualization of Spatial-temporal Data in Tableau, QlikView, Spotﬁre and JMP

Tableau
Spotfire
QlikView
JMP

0
0
6
3

0
0
6

0
8
1

0
6

0
1

s
d
n
o
c
e
S

100 MB

200 MB

500 MB

1 GB

10 GB

20 GB

50 GB

Figure 3: Loading Stress Test

and 50 GB (104,789,361). The evaluation was conducted on a
workstation with an Intel Core i7-2600 CPU, and 16 GB of main
memory. The operating system and the tools are installed on a
128 GB SSD drive. In addition, the workstation has a 1 TB HDD
storage for user data, which we used for storing the workbooks cre-
ated with the tools.

For each system we measured the time required for loading the
data set into a project and displaying the data table. Figure 3 shows
for each VA system the time to load the data. Only Spotﬁre was able
to handle a data size of 50 GB. QlikView failed to load the 10 GB ﬁle
on our test system. Tableau and JMP reached their limits at 20 GB.
At 10 GB Tableau was not able to display the data table anymore.
In all other cases the times taken for displaying the data table was
negligible. Spotﬁre was even able to show the data table instantly
for the 50 GB test after the data was loaded.

5 SUMMARY OF KEY FINDINGS
Generally speaking, the tasks supported by all investigated VA sys-
tems fall into four categories: exploration, dashboards, reporting,
and alerting. Exploration allows users to generate and verify hy-
potheses. The advantage is the ability to easily create and mod-

180

ify visualizations and statistical models. The result of the explo-
ration is usually additional knowledge or statistical models. In con-
trast, dashboards are either used to communicate ﬁndings or to pro-
vide standardized interfaces for regularly occurring analysis prob-
lems. Usually a dashboard consists of a ﬁxed set of visualizations
and controls, allowing interactions such as selection, ﬁltering, and
drilling down. The reporting task generates a static summary of
information from the data sources. Reports are either generated on
demand or on a regular basis. The representation of the information
in the reports is standardized, allowing easy comparison of differ-
ent reports. The alerting task provides automatic notiﬁcation when
the data sources reach predeﬁned states. These states are typically
thresholds or indicators, but more complex ones may incorporate
evaluations of statistical models. Alerts are used to inform users
about unusual events that need attention.

Among all the systems we surveyed, a number have roots back
in academic research, for example Tableau from Stanford Univer-
sity, Spotﬁre from University of Maryland, and ADVIZOR from Bell
Labs. These vendors appear to be leaders in interactive visualiza-
tion and automatic analysis, and put effort in integrating innovative
visualization techniques. For example, Tableau beneﬁts from its
unique visual query language, VizQL, that translates user actions
into a database query and then expresses the response graphically.
Spotﬁre provides powerful automatic analysis functionality and is
regarded as a pioneer in predictive analysis. ADVIZOR implements
different types of interactive charts, some of which are not included
in many other VA systems.

Tableau is still expanding its statistics and automatic analysis
functionality over the latest releases. Spotﬁre already has advanced
its functionality in all aspects we investigated - from automatic ana-
lytics, to interactive visualization, from system architecture to data
management. However, some advanced data analysis components
are only available with additional upgrades and cost. (see Table 2).
QlikView appears advanced regarding data compression and
memory optimization. It has strong interactive drill-down capabil-

ities and fast response time because of its in-memory architecture.
The system accesses information from standard database applica-
tions and displays data associatively using highlighting colors. But
not many statistics and automatic analyses are included in the sys-
tem.

Several other systems, such as JMP and Cognos (which is not
included in our study) also provide strong analytical capabilities by
integrating their own VA components. For example, JMP integrates
SAS, and Cognos integrates SPSS. In particular, the integration
of interactive visualization with automatic analysis functionalities
makes JMP an advanced data discovery system for data modeling
and predictive analysis.

Systems more oriented towards BI, such as Centrifuge, Board,
Visual Mining and Jaspersoft put much focus on presentation-
oriented features (e.g. dashboards, reports), which allow the user
to generate in a straightforward way graphical representation of
standard data. Among those, Jaspersoft is one of the least costly
BI products on the market, although it appears to be a little be-
hind other BI systems in terms of functionality and infrastructure.
BOARD earns the name of an innovative product by integrating BI
and Corporate Performance Management (called Management In-
telligence by the tool’s advocates). One issue we noticed is that the
interactivity of most of the dashboard facilities is rather limited.

While network analysis is still not a fully developed functional-
ity in many VA systems, Centrifuge and Visual Analytics put much
focus on applying interactive network visualizations and automatic
analysis methods to help understanding hidden relations in data.
Visual Analytics is widely used in ﬁnancial transaction data analy-
sis and fraud detection. A range of reactive and proactive analyses
is supported, including entity extraction, social network analysis,
geo-spatial analysis, etc.

Linguistic analysis on text documents is not supported by many
VA systems, despite the increasing amount of text documents gen-
erated on- and off-line and need to analyze them. To our knowl-
edge, only three systems in our initial list (Business Objects, Cog-
nos and Teradata) have text mining functionality. However, for
more speciﬁc text mining tasks, Oculus provides a nice open source
toolkit nSpace [12] which includes a number of useful functions in-
cluding faceted search, faceted trends, and evidence marshalling.
Besides, Palentir [2], and In-Spire [1] are also known for their text
analysis capabilities.

6 CONCLUDING REMARKS
VA system development is a fast moving ﬁeld with effort been made
by multiple disciplines including statistics, machine learning, in-
formation visualization, human computer interaction, data manage-
ment, and memory optimization. Besides open source toolkits, a
large number of commercial products were developed, marketed,
and employed, relying in practice on corporate IT as well as IT
consulting services. In the past ten years, on the one hand some
existing VA software companies expanded rapidly (e.g. Tableau
Software, QlikTech (QlikView)) due to the growing market. On the
other hand, big software vendors such as IBM, Oracle and Microsoft
started to either acquire successful VA software companies and in-
tegrate acquired VA components into their own framework (e.g.
IBM bought Cognos, Oracle acquired Siebel and Hyperion, SAP
purchased Business Objects, and TIBCO acquired Spotﬁre) or to
develop their own VA components (e.g. SAS developed JMP, Mi-
crosoft developed Sharepoint and PowerPivot). Such phenomena
are not surprising in a dynamic market where the trend is led by
the practical need in application domains. The trend is most likely
going to continue if we look at the increasing volume, velocity and
variety of data that are generated in different application domains
nowadays.

In this paper, we report our survey on a selection of state-of-the-
art VA systems as a basis for analyzing current market and trend,

discussing space for improvement and identifying future research
directions. We evaluate the functionality and performance of each
system by surveying the vendor with a structured questionnaire as
well as testing with real world data. We detail our ﬁndings and out-
line the main characteristic of each system. Our survey provides a
comparative review of ten products on the market. We also inves-
tigate a larger number of systems, including Cognos, SQL Server
BI, Business Objects, Teradata, PowerPivot, Panopticon, KNIME,
Oculus, Palentir and in-Spire to gain a better overview of the VA
software market. Future work will include harmonizing ﬁndings of
the latter tools, which are still being collected, with the presented
systems.

Through our study, we identify a number of challenges which

may lead to possible future directions:

Semi- and Unstructured Data. The increasing speed of data
generation brings both opportunity and challenge.
In particular,
more and more semi- or unstructured data are generated on- or off-
line. A large number of data analysis and visualization techniques
are available for analyzing structured data, but methods for mod-
eling and visualizing semi- or unstructured data are still underrep-
resented. An effective VA system often needs to be able to handle
both, and ideally integrate the analysis of both types of data for
supporting decision making.

Advanced Visualization. Compared to open source VA sys-
tems, it seems that commercial products take longer time to inte-
grate innovative visualization techniques. In particular, some big
software vendors tend to focus on only a small number of “stan-
dard” visualization techniques such as line charts, bar charts and
scatter plots, which have limited capability in handling large com-
plex data. The success of Tableau, Spotﬁre and ADVIZOR demon-
strate the possibility and beneﬁt of transferring technical advances
developed by academic research into industrial products.

Customizable Visualization. One useful feature which is of-
ten ignored in visualization function design is customizable visu-
alization. Given the same data and visualization technique, differ-
ent parameter settings may lead to totally different visual repre-
sentations and give people different visual impressions. Designing
customizable visualization functions leaves the user the freedom of
changing visual parameter setting and more opportunity to gain in-
sight from the visualization.

Real Time Analysis. More and more data are generated in
real-time on the Internet (e.g. online news streams, twitter streams,
weblogs) or by modern equipment or devices (e.g. sensors, GPS,
satellite cameras). If analysis is applied appropriately, these data
provide rich information resources to many tasks. Therefore, im-
proving analytical capability to handle such data is a development
opportunity in current commercial products. We expect to see more
functionality in this respect in the future.

Predictive Analysis. The demand of predictive modeling is
increasing, especially in the business domain, but only very few
systems support predictive analysis. Even with those systems that
support predictive analysis, not many predictive modeling methods
are implemented.

ACKNOWLEDGEMENTS
This work was partially funded by the German Research Founda-
tion (DFG) under grant GK-1042 ”Explorative Analysis and Visu-
alization of Large Information Spaces” and by the European Com-
mission (FP7) under the grant ”Modeling and Simulation of the Im-
pact of Public Policies on SMEs (MOSIPS)”. The authors wish to
thank Christine Jacob for her work on testing the different applica-
tions.

181

REFERENCES
[1] http://in-spire.pnnl.gov/.
[2] http://palantir.com/.
[3] http://radar.oreilly.com/2012/01/what-is-big-data.html.
[4] http://spotﬁre.tibco.com/.
[5] http://vismaster.eu/.
[6] http://www.advizorsolutions.com/.
[7] http://www.board.com/.
[8] http://www.centrifugesystems.com/.
[9] http://www.ibm.com/software/data/bigdata/.
[10] http://www.jaspersoft.com/.
[11] http://www.jmp.com/.
[12] http://www.oculusinfo.com/nspace/.
[13] http://www.qlikview.com/.
[14] http://www.tableausoftware.com/.
[15] http://www.visualanalytics.com/.
[16] http://www.visualmining.com/.
[17] U. M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. From data mining
to knowledge discovery: an overview. In U. M. Fayyad, G. Piatetsky-
Shapiro, P. Smyth, and R. Uthurusamy, editors, Advances in knowl-
edge discovery and data mining, chapter From data mining to knowl-
edge discovery: an overview, pages 1–34. American Association for
Artiﬁcial Intelligence, Menlo Park, CA, USA, 1996.

[18] J.-D. Fekete. The infovis toolkit. In INFOVIS, pages 167–174, 2004.
[19] J. Hagerty, R. Sallam, and J. Richardson. Magic quadrant for busi-
ness intelligence platforms. Technical report, Gartner Technology Re-
search, 2012.

[20] P. Hanrahan. Vizql: a language for query, analysis and visualization.
In Proceedings of the 2006 ACM SIGMOD international conference
on Management of data, SIGMOD ’06, pages 721–721, New York,
NY, USA, 2006. ACM.

[21] J. R. Harger and P. J. Crossno. Comparison of open-source visual ana-
lytics toolkits. In Proceedings of the SPIE Conference on Visualization
and Data Analysis, 2012.

[22] S. Havre, B. Hetzler, and L. Nowell. Themeriver: Visualizing theme
changes over time. In Proc. IEEE Symposium on Information Visual-
ization, pages 115–123, 2000.

[23] J. Heer, S. K. Card, and J. A. Landay. prefuse: a toolkit for interactive
information visualization. In Proceedings of the SIGCHI conference
on Human factors in computing systems, CHI ’05, pages 421–430,
New York, NY, USA, 2005. ACM.

[24] Java Universal Network/Graph Framework.

http://jung.-

sourceforge.net/, 2012.

[25] O. Kaser and D. Lemire. Tag-cloud drawing: Algorithms for cloud

visualization. CoRR, abs/cs/0703109, 2007.

[26] D. Keim, J. Kohlhammer, G. Ellis, and F. Mansmann, editors. Mas-
tering The Information Age - Solving Problems with Visual Analytics.
Eurographics, 2010.

[27] J. Thomas and K. Cook.

Illuminating the Path: The Research and
Development Agenda for Visual Analytics. IEEE Computer Society,
2005.

[28] D. van Beek and N. Manley. The business intelligence product survey.

Technical report, Passionned Group, 2012.

[29] C. Weaver. Building highly-coordinated visualizations in improvise.

In INFOVIS, pages 159–166, 2004.

182

",False,2012.0,{},False,False,conferencePaper,False,769UTJ9U,[],self.user,False,False,False,False,http://ieeexplore.ieee.org/document/6400554/,,Visual analytics for the big data era &#x2014; A comparative review of state-of-the-art commercial systems,769UTJ9U,False,False
FGKT2K5L,P952WXP9,"Visual Analytics: Definition, Process and Challenges
Daniel Keim, Gennady Andrienko, Jean-Daniel Fekete, Carsten Görg, Jörn

Kohlhammer, Guy Melançon

To cite this version:
Daniel Keim, Gennady Andrienko, Jean-Daniel Fekete, Carsten Görg, Jörn Kohlhammer, et al.. Vi-
sual Analytics: Definition, Process and Challenges. Andreas Kerren and John T. Stasko and Jean-
Daniel Fekete and Chris North. Information Visualization - Human-Centered Issues and Perspectives,
Springer, pp.154-175, 2008, LNCS. <lirmm-00272779>

HAL Id: lirmm-00272779

https://hal-lirmm.ccsd.cnrs.fr/lirmm-00272779

Submitted on 11 Apr 2008

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.

Visual Analytics:

Deﬁnition, Process, and Challenges

Daniel Keim1, Gennady Andrienko2, Jean-Daniel Fekete3, Carsten G¨org4,

J¨orn Kohlhammer5, and Guy Melan¸con6

1 Department of Computer and Information Science, University of Konstanz,

78457 Konstanz, Germany,

2 Fraunhofer Institute for Intelligent Analysis and Information Systems(IAIS),

keim@informatik.uni-konstanz.de

Schloss Birlinghoven 53754 Sankt Augustin, Germany,

gennady.andrienko@iais.fraunhofer.de
3 Universit´e Paris-Sud, INRIA, Bˆat 490,

4 School of Interactive Computing & GVU Center, Georgia Institute of Technology,

F-91405 Orsay Cedex, France,
Jean-Daniel.Fekete@inria.fr

85 5th St., NW, Atlanta, GA 30332-0760, USA,

5 Fraunhofer Institute for Computer Graphics Research,

goerg@cc.gatech.edu

Fraunhoferstraße 5, D-64283 Darmstadt, Germany,

joern.kohlhammer@igd.fraunhofer.de

6 INRIA Bordeaux – Sud-Ouest, CNRS UMR 5800 LaBRI,

Campus Universit´e Bordeaux I,

351 Cours de la lib´eration, 33405 Talence Cedex, France,

Guy.Melancon@labri.fr

1 Introduction and Motivation

We are living in a world which faces a rapidly increasing amount of data to be
dealt with on a daily basis. In the last decade, the steady improvement of data
storage devices and means to create and collect data along the way inﬂuenced our
way of dealing with information: Most of the time, data is stored without ﬁltering
and reﬁnement for later use. Virtually every branch of industry or business,
and any political or personal activity nowadays generate vast amounts of data.
Making matters worse, the possibilities to collect and store data increase at a
faster rate than our ability to use it for making decisions. However, in most
applications, raw data has no value in itself; instead we want to extract the
information contained in it.

The information overload problem refers to the danger of getting lost in

data which may be

– irrelevant to the current task at hand
– processed in an inappropriate way
– presented in an inappropriate way

A. Kerren et al. (Eds.): Information Visualization, LNCS 4950, pp. 154–175, 2008.
c(cid:1) Springer-Verlag Berlin Heidelberg 2008

Visual Analytics: Deﬁnition, Process, and Challenges

155

Due to information overload, time and money are wasted, scientiﬁc and in-
dustrial opportunities are lost because we still lack the ability to deal with the
enormous data volumes properly. People in both their business and private lives,
decision-makers, analysts, engineers, emergency response teams alike, are often
confronted with massive amounts of disparate, conﬂicting and dynamic infor-
mation, which are available from multiple heterogeneous sources. We want to
simply and eﬀectively exploit and use the hidden opportunities and knowledge
resting in unexplored data sources.

In many application areas success depends on the right information being
available at the right time. Nowadays, the acquisition of raw data is no longer
the driving problem: It is the ability to identify methods and models, which can
turn the data into reliable and provable knowledge. Any technology, that claims
to overcome the information overload problem, has to provide answers for the
following problems:

– Who or what deﬁnes the “relevance of information” for a given task?
– How can appropriate procedures in a complex decision making process be

identiﬁed?

way?

ing?

– How can the resulting information be presented in a decision- or task-oriented

– What kinds of interaction can facilitate problem solving and decision mak-

With every new “real-life” application, procedures are put to the test possibly
under circumstances completely diﬀerent from the ones under which they have
been established. The awareness of the problem how to understand and analyse
our data has been greatly increased in the last decade. Even as we implement
more powerful tools for automated data analysis, we still face the problem of un-
derstanding and “analysing our analyses” in the future: Fully-automated search,
ﬁlter and analysis only work reliably for well-deﬁned and well-understood prob-
lems. The path from data to decision is typically quite complex. Even as fully-
automated data processing methods represent the knowledge of their creators,
they lack the ability to communicate their knowledge. This ability is crucial: If
decisions that emerge from the results of these methods turn out to be wrong,
it is especially important to examine the procedures.

The overarching driving vision of visual analytics is to turn the information
overload into an opportunity: Just as information visualization has changed our
view on databases, the goal of Visual Analytics is to make our way of processing
data and information transparent for an analytic discourse. The visualization of
these processes will provide the means of communicating about them, instead
of being left with the results. Visual Analytics will foster the constructive eval-
uation, correction and rapid improvement of our processes and models and -
ultimately - the improvement of our knowledge and our decisions (see Figure 1).
On a grand scale, visual analytics solutions provide technology that combines
the strengths of human and electronic data processing. Visualization becomes
the medium of a semi-automated analytical process, where humans and machines
cooperate using their respective distinct capabilities for the most eﬀective results.

156

D. Keim et al.

Fig. 1. Tight integration of visual and automatic data analysis methods with database
technology for a scalable interactive decision support.

The user has to be the ultimate authority in giving the direction of the analysis
along his or her speciﬁc task. At the same time, the system has to provide
eﬀective means of interaction to concentrate on this speciﬁc task. On top of
that, in many applications diﬀerent people work along the path from data to
decision. A visual representation will sketch this path and provide a reference
for their collaboration across diﬀerent tasks and abstraction levels.

The diversity of these tasks can not be tackled with a single theory. Visual
analytics research is highly interdisciplinary and combines various related re-
search areas such as visualization, data mining, data management, data fusion,
statistics and cognition science (among others). Visualization has to continuously
challenge the perception by many of the applying sciences that visualization is
not a scientiﬁc discipline in its own right. Even if the awareness exists, that
scientiﬁc analysis and results must be visualized in one way or the other, this
often results in ad hoc solutions by application scientists, which rarely match
the state of the art in interactive visualization science, much less the full com-
plexity of the problems. In fact, all related research areas in the context of visual
analytics research conduct rigorous, serious science each in a vibrant research
community. To increase the awareness of their work and their implications for
visual analytics research clearly emerges as one main goal of the international
visual analytics community (see Figure 2).

Because visual analytics research can be regarded as an integrating discipline,
application speciﬁc research areas should contribute with their existing proce-
dures and models. Emerging from highly application-oriented research, dispersed
research communities worked on speciﬁc solutions using the repertoire and stan-
dards of their speciﬁc ﬁelds. The requirements of visual analytics introduce new
dependencies between these ﬁelds.

Visual Analytics: Deﬁnition, Process, and Challenges

157

Fig. 2. Visual analytics integrates scientiﬁc disciplines to improve the division of labor
between human and machine.

2 Deﬁnition of Visual Analytics

In “Illuminating the Path” [39], Thomas and Cook deﬁne visual analytics as the
science of analytical reasoning facilitated by interactive visual interfaces. In this
chapter, however, we would like to give a more speciﬁc deﬁnition:

Visual analytics combines automated analysis techniques with interactive
visualizations for an eﬀective understanding, reasoning and decision making on
the basis of very large and complex data sets.

The goal of visual analytics is the creation of tools and techniques to enable

people to:

– Synthesize information and derive insight from massive, dynamic, ambigu-

ous, and often conﬂicting data.

– Detect the expected and discover the unexpected.
– Provide timely, defensible, and understandable assessments.
– Communicate assessment eﬀectively for action.

By integrating selected science and technology from the above discussed disci-
plines and as illustrated in Figure 2, there is the promising opportunity to form
the unique and productive ﬁeld of visual analytics. Work in each of the partici-
pating areas focuses on diﬀerent theoretical and practical aspects of users solving
real-world problems using Information Technology in an eﬀective and eﬃcient
way. These areas have in common similar scientiﬁc challenges and signiﬁcant sci-
entiﬁc added-value from establishing close collaboration can be identiﬁed. Beneﬁt
of collaboration between the ﬁelds is identiﬁed to be two-fold:

– Jointly tackling common problems will arrive at better results on the local

level of each discipline, in a more eﬃcient way.

– Integrating appropriate results from each of the disciplines will lay the fun-
dament for signiﬁcantly improved solutions in many important data analysis
applications.

158

D. Keim et al.

Visual Analytics versus Information Visualization

Many people are confused by the new term visual analytics and do not see a dif-
ference between the two areas. While there is certainly some overlay and some of
the information visualization work is certainly highly related to visual analytics,
traditional visualization work does not necessarily deal with an analysis tasks
nor does it always also use advanced data analysis algorithms.

Visual analytics is more than just visualization. It can rather be seen as an
integral approach to decision-making, combining visualization, human factors
and data analysis. The challenge is to identify the best automated algorithm for
the analysis task at hand, identify its limits which can not be further automated,
and then develop a tightly integrated solution with adequately integrates the best
automated analysis algorithms with appropriate visualization and interaction
techniques.

While some of such research has been done within the visualization commu-
nity in the past, the degree to which advanced knowledge discovery algorithms
have been employed is quite limited. The idea of visual analytics is to funda-
mentally change that. This will help to focus on the right part of the problem,
i.e. the parts that can not be solved automatically, and will provide solutions to
problems that we were not able to solve before.

One important remark should be made here. Most research eﬀorts in Infor-
mation Visualization have concentrated on the process of producing views and
creating valuable interaction techniques for a given class of data (social network,
multi-dimensional data, etc.). However, much less has been suggested as to how
user interactions on the data can be turned into intelligence to tune underlying
analytical processes. A system might for instance observe that most of the user’s
attention concern only a subpart of an ontology (through queries or by repeated
direct manipulations of the same graphical elements, for instance). Why not then
use this knowledge about the user’s interest and update various parameters by
the system (trying to systematically place elements or components of interest in
center view, even taking this fact into account when driving a clustering algo-
rithm with a modularity quality criteria, for instance).

This is one place where Visual Analytics maybe diﬀers most from Information
Visualization, giving higher priority to data analytics from the start and through
all iterations of the sense making loop. Creativity is then needed to understand
how perception issues can help bring more intelligence into the analytical process
by “learning” from users’ behavior and eﬀective use of the visualization.

3 Areas Related to Visual Analytics

Visual analytics builds on a variety of related scientiﬁc ﬁelds. At its heart, Visual
Analytics integrates Information and Scientiﬁc Visualization with Data Manage-
ment and Data Analysis Technology, as well as Human Perception and Cognition
research. For eﬀective research, Visual Analytics also requires an appropriate In-
frastructure in terms of software and data sets and related analytical problems
repositories, and to develop reliable Evaluation methodology (see Figure 3).

Visual Analytics: Deﬁnition, Process, and Challenges

159

Fig. 3. Visual Analytics integrates Scientiﬁc and Information Visualization with core
adjacent disciplines: Data management and analysis, spatio-temporal data, and human
perception and cognition. Successful Visual Analytics research also depends on the
availability of appropriate infrastructure and evaluation facilities.

An example for a common problem in several of the disciplines is that of scalabil-
ity with data size. The larger the data set to be handled gets, the more diﬃcult
it gets to manage, analyze, and visualize these data eﬀectively. Researching ap-
propriate forms to represent large data volumes by smaller volumes containing
the most relevant information beneﬁts each of the data management, analy-
sis, and visualization ﬁelds. On top of these individual progresses, a synergetic
collaboration of all these ﬁelds may lead to signiﬁcantly improved processing
results. Consider a very large data stream. Appropriate data management tech-
nology gives eﬃcient access to the stream, which is intelligently processed and
abstracted by an automatic analysis algorithm which has an interface to the
data management layer. On top of the analysis output, an interactive visual-
ization which is optimized for eﬃcient human perception of the relevant infor-
mation allows the analyst to consume the analysis results, and adapt relevant
parameters of the data aggregation an analysis engines as appropriate. The com-
bination of the individual data handling steps into a Visual Analytics pipeline
leads to improved results and makes data domains accessible which are not ef-
fectively accessible by any of the individual data handling disciplines. Similar
argumentations apply to other related ﬁelds and disciplines. In many ﬁelds, vi-
sualization is already used and developed independently as a means for analyzing
the problems at hand. However, a uniﬁed, interdisciplinary perspective on using
visualization for analytical problem-solving will show beneﬁcial for all involved
disciplines. As common principles, best practices, and theories will be developed,
these will become usable in the individual disciplines and application domains,
providing economies of scale, avoiding replication of work or application of only
sub-optimal techniques.

160

D. Keim et al.

3.1 Visualization

Visualization has emerged as a new research discipline during the last two dec-
ades. It can be broadly classiﬁed into Scientiﬁc and Information Visualization.
In Scientiﬁc Visualization, the data entities to be visualized are typically 3D
geometries or can be understood as scalar, vectorial, or tensorial ﬁelds with ex-
plicit references to time and space. A survey of current visualization techniques
can be found in [22,35,23]. Often, 3D scalar ﬁelds are visualized by isosurfaces or
semi-transparent point clouds (direct volume rendering) [15]. To this end, meth-
ods based on optical emission- or absorption models are used which visualize the
volume by ray-tracing or projection. Also, in the recent years signiﬁcant work
focused on the visualization of complex 3-dimensional ﬂow data relevant e.g.,
in aerospace engineering [40]. While current research has focused mainly on eﬃ-
ciency of the visualization techniques to enable interactive exploration, more and
more methods to automatically derive relevant visualization parameters come
into focus of research. Also, interaction techniques such as focus&context [28]
gain importance in scientiﬁc visualization.

Information Visualization during the last decade has developed methods
for the visualization of abstract data where no explicit spatial references are
given [38,8,24,41]. Typical examples include business data, demographics data,
network graphs and scientiﬁc data from e.g., molecular biology. The data con-
sidered often comprises hundreds of dimensions and does not have a natural
mapping to display space, and renders standard visualization techniques such as
(x, y) plots, line- and bar-charts ineﬀective. Therefore, novel visualization tech-
niques are being developed by employing e.g., Parallel Coordinates and their
numerous extensions [20], Treemaps [36], and Glyph [17]- and Pixel-based [25]
visual data representations. Data with inherent network structure may be visual-
ized using graph-based approaches. In many Visualization application areas, the
typically huge volumes of data require the appropriate usage of automatic data
analysis techniques such as clustering or classiﬁcation as preprocessing prior to
visualization. Research in this direction is just emerging.

3.2 Data Management

An eﬃcient management of data of various types and qualities is a key com-
ponent of Visual Analytics as this technology typically provides the input of
the data which are to be analyzed. Generally, a necessary precondition to per-
form any kind of data analysis is an integrated and consistent data basis [18,19].
Database research has until the last decade focused mainly on aspects of eﬃ-
ciency and scalability of exact queries on homogeneous, structured data. With
the advent of the Internet and the easy access it provides to all kinds of hetero-
geneous data sources, the database research focus has shifted toward integration
of heterogeneous data. Finding integrated representation of diﬀerent data types
such as numeric data, graphs, text, audio and video signals, semi-structured
data, semantic representations and so on is a key problem of modern database

Visual Analytics: Deﬁnition, Process, and Challenges

161

technology. But the availability of heterogeneous data not only requires the map-
ping of database schemata but includes also the cleaning and harmonization of
uncertainty and missing data in the volumes of heterogeneous data. Modern ap-
plications require such intelligent data fusion to be feasible in near real-time and
as automatically as possible [32]. New forms of information sources such as data
streams [11], sensor networks [30] or automatic extraction of information from
large document collections (e.g., text, HTML) result in a diﬃcult data analysis
problem which to support is currently in the focus of database research [43].
The relationship between Data Management, Data Analysis and Visualization
is characterized such that Data Management techniques developed increasingly
rely on intelligent data analysis techniques, and also interaction and visualiza-
tion to arrive at optimal results. On the other hand, modern database systems
provide the input data sources which are to be visually analyzed.

3.3 Data Analysis
Data Analysis (also known as Data Mining or Knowledge Discovery) researches
methods to automatically extract valuable information from raw data by means
of automatic analysis algorithms [29,16,31]. Approaches developed in this area
can be best described by the addressed analysis tasks. A prominent such task
is supervised learning from examples: Based on a set of training samples, deter-
ministic or probabilistic algorithms are used to learn models for the classiﬁcation
(or prediction) of previously unseen data samples [13]. A huge number of algo-
rithms have been developed to this end such as Decision Trees, Support Vector
Machines, Neuronal Networks, and so on. A second prominent analysis task is
that of cluster analysis [18,19], which aims to extract structure from data with-
out prior knowledge being available. Solutions in this class are employed to au-
tomatically group data instances into classes based on mutual similarity, and to
identify outliers in noisy data during data preprocessing for subsequent analysis
steps. Further data analysis tasks include tasks such as association rule mining
(analysis of co-occurrence of data items) and dimensionality reduction. While
data analysis initially was developed for structured data, recent research aims at
analyzing also semi-structured and complex data types such as web documents
or multimedia data [34].

It has recently been recognized that visualization and interaction are highly
beneﬁcial in arriving at optimal analysis results [9]. In almost all data analysis
algorithms a variety of parameters needs to be speciﬁed, a problem which is
usually not trivial and often needs supervision by a human expert. Visualization
is also a suitable means for appropriately communicating the results of the au-
tomatic analysis, which often is given in abstract representation, e.g., a decision
tree. Visual Data Mining methods [24] try to achieve exactly this.

3.4 Perception and Cognition
Eﬀective utilization of the powerful human perception system for visual analysis
tasks requires the careful design of appropriate human-computer interfaces. Psy-
chology, Sociology, Neurosciences and Design each contribute valuable results to

162

D. Keim et al.

the implementation of eﬀective visual information systems. Research in this area
focuses on user-centered analysis and modeling (Requirement Engineering), the
development of principles, methods and tools for design of perception-driven,
multimodal interaction techniques for visualization and exploration of large in-
formation spaces, as well as usability evaluation of such systems [21,12]. On the
technical side, research in this area is inﬂuenced by two main factors: (1.) The
availability of improved display resources (hardware), and (2.) Development of
novel interaction algorithms incorporating machine recognition of the actual user
intent and appropriate adaptation of main display parameters such as the level
of detail, data selection and aggregation, etc. by which the data is presented[44].
Important problems addressed in this area include the research of perceptual,
cognitive and graphical principles which in combination lead to improved visual
communication of data and analysis results; The development of perception-
theory-based solutions for the graphical representation of static and dynamic
structures; And development of visual representation of information at several
levels of abstraction, and optimization of existing focus-and-context techniques.

3.5 Human-Computer Interaction

Human-computer interaction is the research area that studies the interaction
between people and computers. It involves the design, implementation and eval-
uation of interactive systems in the context of the user’s task and work [12].
Like visual analytics itself, human-computer interaction is a multi-disciplinary
research area that draws on many other disciplines: computer science, system
design, and behavioral science are some of them. The basic underlying research
goal is to improve the interaction between users and computers: how to make
computers more receptive to the users’ intentions and needs. Thus, the research
areas discussed in the previous section about perception and cognition are also
much related to human-computer interaction [21].

As pointed out in the introduction, visual analytics aims to combine and
integrate the strengths of computers and humans into an interactive process to
extract knowledge from data. To eﬀectively switch back and forth between tasks
for the computer and tasks for the human it is crucial to develop an eﬀective
user interface that minimizes the barrier between the human’s cognitive model
of what they want to accomplish and the computer’s understanding of the hu-
man’s task. The design of user interfaces focuses on human factors of interactive
software, methods to develop and assess interfaces, interaction styles, and design
considerations such as eﬀective messages and appropriate color choice [37].

3.6 Infrastructure and Evaluation

The above described research disciplines require cross-discipline support regard-
ing the evaluation of the found solutions, and need certain infrastructure and
standardization grounding to build on eﬀectively. In the ﬁeld of information vi-
sualization, standardization and evaluation came into the focus of research only
recently. It has been realized that a general understanding of the taxonomies

Visual Analytics: Deﬁnition, Process, and Challenges

163

regarding the main data types and user tasks [2] to be supported are highly de-
sirable for shaping visual analytics research. A common understanding of data
and problem dimensions and structure, and acceptance of evaluation standards
will make research results better comparable, optimizing research productivity.
Also, there is an obvious need to build repositories of available analysis and vi-
sualization algorithms, which researchers can build upon in their work, without
having to re-implement already proven solutions.

How to assess the value of visualization is a topic of lively debate [42,33]. A
common ground that can be used to position and compare future developments
in the ﬁeld of data analysis is needed. The current diversiﬁcation and dispersion
of visual analytics research and development resulted from its focus onto speciﬁc
application areas. While this approach may suit the requirements of each of
these applications, a more rigorous and overall scientiﬁc perspective will lead to
a better understanding of the ﬁeld and a more eﬀective and eﬃcient development
of innovative methods and techniques.

3.7 Sub-communities

Spatio-Temporal Data: While many diﬀerent data types exist, one of the
most prominent and ubiquitous data types is data with references to time and
space. The importance of this data type has been recognized by a research
community which formed around spatio-temporal data management and anal-
ysis [14]. In geospatial data research, data with references in the real world
coming from e.g., geographic measurements, GPS position data, remote sensing
applications, and so on is considered. Finding spatial relationships and patterns
among this data is of special interest, requiring the development of appropriate
management, representation and analysis functions. E.g., developing eﬃcient
data structures or deﬁning distance and similarity functions is in the focus of re-
search. Visualization often plays a key role in the successful analysis of geospatial
data [6,26].

In temporal data, the data elements can be regarded as a function of time.
Important analysis tasks here include the identiﬁcation of patterns (either lin-
ear or periodical), trends and correlations of the data elements over time, and
application-dependent analysis functions and similarity metrics have been pro-
posed in ﬁelds such as ﬁnance, science, engineering, etc. Again, visualization of
time-related data is important to arrive at good analysis results [1].

The analysis of data with references both in space and in time is a chal-
lenging research topic. Major research challenges include [4]: scale, as it is often
necessary to consider spatio-temporal data at diﬀerent spatio-temporal scales;
the uncertainty of the data as data are often incomplete, interpolated, collected
at diﬀerent times, or based upon diﬀerent assumptions; complexity of geograph-
ical space and time, since in addition to metric properties of space and time
and topological/temporal relations between objects, it is necessary to take into
account the heterogeneity of the space and structure of time; and complexity of
spatial decision making processes, because a decision process may involve hetero-

164

D. Keim et al.

geneous actors with diﬀerent roles, interests, levels of knowledge of the problem
domain and the territory.

Network and Graph Data: Graphs appear as ﬂexible and powerful math-
ematical tools to model real-life situations. They naturally map to transporta-
tion networks, electric power grids, and they are also used as artifacts to study
complex data such as observed interactions between people, or induced interac-
tions between various biological entities. Graphs are successful at turning seman-
tic proximity into topological connectivity, making it possible to address issues
based on algorithmics and combinatorial analysis.

Graphs appear as essential modeling and analytical objects, and as eﬀective
visual analytics paradigms. Major research challenges are to produce scalable
analytical methods to identify key components both structurally and visually.
Eﬀorts are needed to design process capable of dealing with large datasets while
producing readable and usable graphical representations, allowing proper user
interaction. Special eﬀorts are required to deal with dynamically changing net-
works, in order to assess of structural changes at various scales.

4 The Visual Analytics Process

A number of systems for information visualization, as well as speciﬁc visual-
ization techniques, motivate their design choice from Shneiderman’s celebrated
mantra “Overview ﬁrst, Filter and zoom, Details on demand”. As is, the mantra
clearly emphasizes the role of visualization in the knowledge discovery process.
Recently, Keim adjusted the mantra to bring its focus toward Visual Analytics:
“Analyze ﬁrst, Show the Important, Zoom, ﬁlter and analyze further, Details
on demand”. In other words, this mantra is calling for astute combinations of
analytical approaches together with advanced visualization techniques.

The computation of any visual representation and/or geometrical embedding
of large and complex datasets requires some analysis to start with. Many scalable
graph drawing algorithms try to take advantage of any knowledge on topology
to optimize the drawing in terms of readability. Other approaches oﬀer repre-
sentations composed of visual abstractions of clusters to improve readability.
The challenge then is to try to come up with a representation that is as faithful
as possible to avoid introducing uncertainty. We must not fall into the na¨ıve
assumption that visualization can oﬀer a virgin view on the data: any represen-
tation will inevitably favor an interpretation over all possible ones. The solution
oﬀered by Visual Analytics is then to let the user enter into a loop where data
can be interactively manipulated to help gain insight both on the data and the
representation itself.

The sense-making loop structures the whole knowledge discovery process
supported through Visual Analytics. A generic scenario can be given following a
schema developed by van Wijk [42], which furthermore admits to be evaluated
and measured in terms of eﬃciency or knowledge gained. A choice for an initial
representation and adequate interactions can be made after applying diﬀerent

Visual Analytics: Deﬁnition, Process, and Challenges

165

statistical and mathematical techniques, such as spatio-temporal data analysis or
link mining depending on the nature of the dataset under study. The process then
enters a loop where the user can gain knowledge on the data, ideally driving the
system toward more focused and more adequate analytical techniques. Dually,
interacting on the visual representation, the user will gain a better understanding
of the visualization itself commanding for diﬀerent views helping him or her to
go beyond the visual and ultimately conﬁrm hypotheses built from previous
iterations (see Figure 4).

Fig. 4. The sense-making loop for Visual Analytics based on the simple model of
visualization by Wijk [42].

5 Application Challenges

Visual Analytics is a highly application oriented discipline driven by practical
requirements in important domains. Without attempting a complete survey over
all possible application areas, we sketch the potential applicability of Visual
Analytics technology in a few key domains.

In the Engineering domain, Visual Analytics can contribute to speed-up de-
velopment time for products, materials, tools and production methods by oﬀering
more eﬀective, intelligent access to the wealth of complex information resulting
from prototype development, experimental test series, customers’ feedback, and
many other performance metrics. One key goal of applied Visual Analytics in
the engineering domain will be the analysis of the complexity of the production
systems in correlation with the achieved output, for an eﬃcient and eﬀective
improvement of the production environments.

Financial Analysis is a prototypical promising application area for Visual
Analytics. Analysts in this domain are confronted with streams of heterogeneous
information from diﬀerent sources available at high update rates, and of varying

166

D. Keim et al.

reliability. Arriving at a unifying, task-centered view on diverse streams of data
is a central goal in ﬁnancial information systems. Integrated analysis and visu-
alization of heterogeneous data types such as news feeds, real-time trading data,
and fundamental economic indicators poses a challenge for developing advanced
analysis solutions in this area. Research based on results from Information Vi-
sualization is regarded as promising in this case.

Socio-economic considerations often form the basis of political decision
processes. A modern society can be regarded as a complex system of interre-
lationships between political decisions and economic, cultural and demographic
eﬀects. Analysis and Visualization of these interrelationships is promising in de-
veloping a better understanding of these phenomena, and to arrive at better
decisions. Successful Visual Analytics applications in this domain could start
being developed based on currently existing Geo-Spatial analysis frameworks.

Public Safety & Security is another important application area where Vi-
sual Analytics may contribute with advanced solutions. Analysts need to con-
stantly monitor huge amounts of heterogeneous information streams, correlating
information of varying degrees of abstraction and reliability, assessing the cur-
rent level of public safety, triggering alert in case of alarming situations being
detected. Data integration and correlation combined with appropriate analysis
and interactive visualization is promising to develop more eﬃcient tools for the
analysis in this area.

The study of Environment and Climate change often requires the ex-
amination of long term weather records and logs of various sensors, in a search
for patterns that can be related to observations such as changes in animal pop-
ulations, or in meteorological and climatic processes for instance. These require-
ments call for the development of systems allowing visual and graphical access
to historical monitoring data and predictions from various models in search for
or in order to validate patterns building over time.

These diverse ﬁelds of applications share many problems on an abstract level,
most of which are addressed by Visual Analytics. The actual (software) solution
must be adapted to the speciﬁc needs and terminologies of the application area
and consequently, many researchers currently focus on a speciﬁc customer seg-
ment. Much can be achieved, if the European research infrastructure in this ﬁeld
becomes strong enough to encourage the exchange of ideas on a broad scale, to
foster development of solutions applicable to multiple domains, achieving syn-
ergy eﬀects.

6 Technical Challenges

The primary goal of Visual Analytics is the analysis of vast amounts of data to
identify and visually distill the most valuable and relevant information content.
The visual representation should reveal structural patterns and relevant data
properties for easy perception by the analyst. A number of key requirements
need to be addressed by advanced Visual Analytics solutions. We next outline
important scientiﬁc challenges in this context.

Visual Analytics: Deﬁnition, Process, and Challenges

167

Scalability with Data Volumes and Data Dimensionality: Visual Ana-
lytics techniques need to be able to scale with the size and dimensionality of
the input data space. Techniques need to accommodate and graphically repre-
sent high-resolution input data as well as continuous input data streams of high
bandwidth. In many applications, data from multiple, heterogeneous sources
need to be integrated and processed jointly. In these cases, the methods need
to be able to scale with a range of diﬀerent data types, data sources, and levels
of quality. The visual representation algorithms need to be eﬃcient enough for
implementation in interactive systems.

Quality of Data and Graphical Representation: A central issue in Visual
Analytics is the avoidance of misinterpretations by the analyst. This may result
due to uncertainty and errors in the input data, or limitations of the chosen
analysis algorithm, and may produce misleading analysis results. To face this
problem, the notion of data quality, and the conﬁdence of the analysis algorithm
needs to be appropriately represented in the Visual Analytics solutions. The user
needs to be aware of these data and analysis quality properties at any stage in
the data analysis process.

Visual Representation and Level of Detail: To accommodate vast streams
of data, appropriate solutions need to intelligently combine visualizations of
selected analysis details on the one hand, and a global overview on the other
hand. The relevant data patterns and relationships need to be visualized on
several levels of detail, and with appropriate levels of data and visual abstraction.

User Interfaces, and Interaction Styles and Metaphors: Visual Analytics
systems need to be easily used and interacted with by the analyst. The analyst
needs to be able to fully focus on the task at hand, not on overly technical or
complex user interfaces, which potentially distract. To this end, novel interaction
techniques need to be developed which fully support the seamless, intuitive visual
communication with the system. User feedback should be taken as intelligently
as possible, requiring as little manual user input as possible, which guarantees
the full support of the user in navigating and analyzing the data, memorizing
insights and making informed decisions.

Display Devices: In addition to high-resolution desktop displays, advanced
display devices such as large-scale power walls and small portable personal assis-
tant, graphically-enabled devices need to be supported. Visual Analytics systems
should adapt to the characteristics of the available output devices, supporting
the Visual Analytics workﬂow on all levels of operation.

Evaluation: Due to the complex and heterogeneous problem domains addressed
by Visual Analytics, so far it has been diﬃcult to perform encompassing evalua-
tion work. A theoretically founded evaluation framework needs to be developed
which allows assessing the contribution of any Visual Analytics system toward
the level of eﬀectiveness and eﬃciency achieved regarding their requirements.

168

D. Keim et al.

Infrastructure: Managing large amounts of data for visualization or analysis
requires special data structures and mechanisms, both in memory and disks.
Achieving interactivity means refreshing the display in 100ms at worst whereas
analyzing data with standard techniques such as clustering can take hours to
complete. Achieving the smooth interaction required by the analysts to perform
their tasks while providing high-quality analytical algorithms need the combi-
nation of asynchronous computation with hybrid analytical algorithms that can
trade time with quality. Moreover, to fully support the analytical process, the
history of the analysis should also be recorded and interactively edited and an-
notated. Altogether, these requirements call for a novel software infrastructure,
built upon well understood technologies such as databases, software components
and visualization but augmented with asynchronous processing, history manage-
ments and annotations.

7 Examples for Visual Analytics Applications

7.1 Visual Analytics Tools for Analysis of Movement Data

With widespread availability of low cost GPS devices, it is becoming possible to
record data about the movement of people and objects at a large scale. While
these data hide important knowledge for the optimization of location and mobil-
ity oriented infrastructures and services, by themselves they lack the necessary
semantic embedding which would make fully automatic algorithmic analysis pos-
sible. At the same time, making the semantic link is easy for humans who however
cannot deal well with massive amounts of data. In [5] we argue that by using
the right visual analytics tools for the analysis of massive collections of move-
ment data, it is possible to eﬀectively support human analysts in understanding
movement behaviors and mobility patterns.

Figure 5 shows a subset of raw GPS measurements presented in so-called
space-time cube. The large amount of position records referring to the same
territory over a long time period makes it virtually impossible to do the analysis
by purely visual methods.

The paper [5] proposes a framework where interactive visual interfaces are
synergistically combined with database operations and computational process-
ing. The generic database techniques are used for basic data processing and ex-
traction of relevant objects and features. The computational techniques, which
are specially devised for movement data, aggregate and summarize these objects
and features and thereby enable the visualization of large amounts of informa-
tion. The visualization enables human cognition and reasoning, which, in turn,
direct and control the further analysis by means of the database, computational,
and visual techniques. Interactive visual interfaces embrace all the tools.

Thus, in order to detect and interpret signiﬁcant places visited by the mov-
ing entities, the positions of stops are extracted from the data by means of
appropriate database queries. Then, clustering methods are applied to detect
frequently visited places. Interactive visual displays put the results in the spa-
tial and temporal contexts. The spatial positions of the stops can be observed on

Visual Analytics: Deﬁnition, Process, and Challenges

169

Fig. 5. A visual display of a large amount of position records is unreadable and not
suitable for analysis.

Fig. 6. Positions of stops have been extracted from the database. By means of cluster-
ing, frequently visited places have been detected.

170

D. Keim et al.

Fig. 7. The temporal histograms show the distribution of the stops in the frequently
visited places (Figure 6) with respect to the weekly (left) and daily (right) cycles.

a map (Figure 6) or 3D spatial view. Temporal histograms (Figure 7) are used
to explore the temporal distribution of the stops throughout the time period and
within various temporal cycles (daily, weekly, etc.). These complementary views
allow a human analyst to understand the meanings or roles of the frequently
visited places.

In order to detect and interpret typical routes of the movement between the
signiﬁcant places, the analyst ﬁrst applies a database query to extract sequences
of position records between the stops, from which trajectories (time-referenced
lines) are constructed. Then, clustering is applied with the use of specially de-
vised similarity measures. The results are computationally generalized and sum-
marized and displayed in the spatial context (Figure 8).

7.2 Multilevel Visualization of the Worldwide Air

Transportation Network

The air transportation network has now become more dense and more complex
at all geographical levels. Its dynamic no more rests on simple territorial logics.
The challenge is to gain insightful understandings on how the routes carrying the
densest traﬃc organize themselves and impact the organization of the network
into sub-communities at lower levels. At the same time, subnetworks grow on
their own logic, involving tourism, economy or territorial control, and inﬂuence
or ﬁght against each other. Because of the network size and complexity, its study
can no more rely on traditional world map and requires novel visualization. A
careful analysis of the network structural properties, requiring recent results on
small world phenomenon, reveals its multilevel community structure.

The original network is organized into a top level network of communi-
ties (Figure 9(a)). Each component can then be further decomposed into sub-
communities. Capitals such as New York, Chicago, Paris or London (Figure 9(b))
clearly attract most of the international traﬃc and impose routes to ﬂy the world
around because of airline partnerships (economical logic). Asia (Figure 9(c))
clearly stands apart from these core hubs because of strong territorial ties en-
dorsed by national Asian airline companies (territorial logic). Visualization of
social networks such as the worldwide air transportation is challenged by the
necessity to scale with the growing size of network data while being able to oﬀer

Visual Analytics: Deﬁnition, Process, and Challenges

171

Fig. 8. A result of clustering and summarization of movement data: the routes between
the signiﬁcant places.

readable visual representations and ﬂuid interaction. Visualization today brings
the ﬁeld of social sciences close to the study of complex systems and promises
to deliver new knowledge across these disciplines [7,3,10].

8 Conclusions

The problems addressed by Visual Analytics are generic. Virtually all sciences
and many industries rely on the ability to identify methods and models, which
can turn data into reliable and provable knowledge. Ever since the dawn of mod-
ern science, researchers needed to ﬁnd methodologies to create new hypotheses,
to compare them with alternative hypotheses, and to validate their results. In
a collaborative environment this process includes a large number of specialized
people each having a diﬀerent educational background. The ability to commu-
nicate results to peers will become crucial for scientiﬁc discourse.

Currently, no technological approach can claim to give answers to all three

key questions that have been outlined in the ﬁrst section, regarding the

– relevance of a speciﬁc information
– adequacy of data processing methods and validity of results
– acceptability of the presentation of results for a given task

172

D. Keim et al.

(a) World air transportation network.

(b) USA and world hubs.

(c) Asia.

Fig. 9. Multilevel Visualization of the Worldwide Air Transportation Network

Visual Analytics: Deﬁnition, Process, and Challenges

173

Visual Analytics research does not focus on speciﬁc methods to address these
questions in a single “best-practice”. Each speciﬁc domain contributes a reper-
toire of approaches to initiate an interdisciplinary creation of solutions.

Visual Analytics literally maps the connection between diﬀerent alternative
solutions, leaving the opportunity for the human user to view these options in
the context of the complete knowledge generation process and to discuss these
options with peers on common ground.

References

1. Aigner, W., Miksch, S., M¨uller, W., Schumann, H., Tominski, C.: Visual meth-
ods for analyzing time-oriented data. IEEE Transactions on Visualization and
Computer Graphics 14(1), 47–60 (2008)

2. Amar, R.A., Eagan, J., Stasko, J.T.: Low-level components of analytic activity in

information visualization. In: INFOVIS, p. 15 (2005)

3. Amiel, M., Melan¸con, G., Rozenblat, C.: R´eseaux multi-niveaux: l’exemple des

´echanges a´eriens mondiaux. M@ppemonde 79(3) (2005)

4. Andrienko, G., Andrienko, N., Jankowski, P., Keim, D., Kraak, M.-J.,
MacEachren, A., Wrobel, S.: Geovisual analytics for spatial decision support:
Setting the research agenda. Special issue of the International Journal of Geo-
graphical Information Science 21(8), 839–857 (2007)

5. Andrienko, G., Andrienko, N., Wrobel, S.: Visual analytics tools for analysis of

movement data. ACM SIGKDD Explorations 9(2) (2007)

6. Andrienko, N., Andrienko, G.: Exploratory Analysis of Spatial and Temporal

Data. Springer, Heidelberg (2005)

7. Auber, D., Chiricota, Y., Jourdan, F., Melan¸con, G.: Multiscale visualization of

small world networks. In: INFOVIS (2003)

8. Card, S.K., Mackinlay, J., Shneiderman, B.: Readings in Information Visualiza-

tion: Using Vision to Think. Morgan Kaufmann, San Francisco (1999)

9. Ceglar, A., Roddick, J.F., Calder, P.: Guiding knowledge discovery through in-

teractive data mining, pp. 45–87. IGI Publishing, Hershey (2003)

10. Chiricota, Y., Melan¸con, G.: Visually mining relational data. International Review

on Computers and Software (2005)

11. Das, A.: Semantic approximation of data stream joins. IEEE Transactions on
Knowledge and Data Engineering 17(1), 44–59 (2005), Member-Johannes Gehrke
and Member-Mirek Riedewald

12. Dix, A., Finlay, J.E., Abowd, G.D., Beale, R.: Human-Computer Interaction (.),

3rd edn. Prentice-Hall, Inc., Upper Saddle River (2003)

13. Duda, R., Hart, P., Stock, D.: Pattern Classiﬁcation. John Wiley and Sons Inc,

Chichester (2000)

14. Dykes, J., MacEachren, A., Kraak, M.-J.: Exploring geovisualization. Elsevier

Science, Amsterdam (2005)

15. Engel, K., Hadwiger, M., Kniss, J.M., Rezk-salama, C., Weiskopf, D.: Real-time

Volume Graphics. A. K. Peters, Ltd., Natick (2006)

16. Ester, M., Sander, J.: Knowledge Discovery in Databases - Techniken und An-

wendungen. Springer, Heidelberg (2000)

17. Forsell, C., Seipel, S., Lind, M.: Simple 3d glyphs for spatial multivariate data.

In: INFOVIS, p. 16 (2005)

174

D. Keim et al.

18. Han, J., Kamber, M. (eds.): Data Mining: Concepts and Techniques. Morgan

Kaufmann, San Francisco (2000)

19. Hand, D., Mannila, H., Smyth, P. (eds.): Principles of Data Mining. MIT Press,

Cambridge (2001)

20. Inselberg, A., Dimsdale, B.: Parallel Coordinates: A Tool for Visualizing Multi-
variate Relations (chapter 9), pp. 199–233. Plenum Publishing Corporation, New
York (1991)

21. Jacko, J.A., Sears, A.: The Handbook for Human Computer Interaction. Lawrence

Erlbaum & Associates, Mahwah (2003)

22. Johnson, C., Hanson, C. (eds.): Visualization Handbook. Kolam Publishing (2004)
23. Keim, D., Ertl, T.: Scientiﬁc visualization (in german). Information Technol-

ogy 46(3), 148–153 (2004)

24. Keim, D., Ward, M.: Visual Data Mining Techniques (chapter 11). Springer, New

York (2003)

25. Keim, D.A., Ankerst, M., Kriegel, H.-P.: Recursive pattern: A technique for visu-
alizing very large amounts of data. In: VIS ’95: Proceedings of the 6th conference
on Visualization ’95, Washington, DC, USA, p. 279. IEEE Computer Society
Press, Los Alamitos (1995)

26. Keim, D.A., Panse, C., Sips, M., North, S.C.: Pixel based visual data mining of

geo-spatial data. Computers &Graphics 28(3), 327–344 (2004)

27. Kerren, A., Stasko, J.T., Fekete, J.-D., North, C.J. (eds.): Information Visualiza-

tion. LNCS, vol. 4950. Springer, Heidelberg (2008)

28. Kr´uger, J., Schneider, J., Westermann, R.: Clearview: An interactive context pre-
serving hotspot visualization technique. IEEE Transactions on Visualization and
Computer Graphics 12(5), 941–948 (2006)

29. Maimon, O., Rokach, L. (eds.): The Data Mining and Knowledge Discovery Hand-

book. Springer, Heidelberg (2005)

30. Meliou, A., Chu, D., Guestrin, C., Hellerstein, J., Hong, W.: Data gathering tours

in sensor networks. In: IPSN (2006)

31. Mitchell, T.M.: Machine Learning. McGraw-Hill, Berkeley (1997)
32. Naumann, F., Bilke, A., Bleiholder, J., Weis, M.: Data fusion in three steps:
Resolving schema, tuple, and value inconsistencies. IEEE Data Eng. Bull. 29(2),
21–31 (2006)

33. North, C.: Toward measuring visualization insight. IEEE Comput. Graph.

Appl. 26(3), 6–9 (2006)

34. Perner, P. (ed.): Data Mining on Multimedia Data. LNCS, vol. 2558. Springer,

Heidelberg (2002)

35. Schumann, H., M¨uller, W.: Visualisierung - Grundlagen und allgemeine Metho-

den. Springer, Heidelberg (2000)

36. Shneiderman, B.: Tree visualization with tree-maps: 2-d space-ﬁlling approach.

ACM Trans. Graph. 11(1), 92–99 (1992)

37. Shneiderman, B., Plaisant, C.: Designing the User Interface. Addison-Wesley,

Reading (2004)

38. Spence, R.: Information Visualization. ACM Press, New York (2001)
39. Thomas, J.J., Cook, K.A.: Illuminating the Path. IEEE Computer Society Press,

Los Alamitos (2005)

40. Tricoche, X., Scheuermann, G., Hagen, H.: Tensor topology tracking: A visual-
ization method for time-dependent 2d symmetric tensor ﬁelds. Comput. Graph.
Forum 20(3) (2001)

41. Unwin, A., Theus, M., Hofmann, H.: Graphics of Large Datasets: Visualizing a

Million (Statistics and Computing). Springer, New York (2006)

Visual Analytics: Deﬁnition, Process, and Challenges

175

42. van Wijk, J.J.: The value of visualization. In: IEEE Visualization, p. 11 (2005)
43. Widom, J.: Trio: A system for integrated management of data, accuracy, and

lineage. In: CIDR, pp. 262–276 (2005)

44. Yi, J.S., Kang, Y.a., Stasko, J.T., Jacko, J.A.: Toward a deeper understanding
of the role of interaction in information visualization. IEEE Trans. Vis. Comput.
Graph. 13(6), 1224–1231 (2007)

",False,2008.0,{},False,False,bookSection,False,FGKT2K5L,[],self.user,False,False,False,False,http://link.springer.com/10.1007/978-3-540-70956-5_7,,"Visual Analytics: Definition, Process, and Challenges",FGKT2K5L,False,False
G9DSVLIY,K6HMLFX8,"Principles of Data Mining 
by David Hand, Heikki Mannila and Padhraic Smyth 
The MIT Press © 2001 (546 pages) 
A comprehensive, highly technical look at the math and science behind 
extracting useful information from large databases.  

ISBN: 026208290x 

 

 

 

 

Table of Contents  

 Principles of Data Mining  
 Series Foreword  
 Preface  
 Chapter 1  - Introduction 
 Chapter 2  - Measurement and Data 
 Chapter 3  - Visualizing and Exploring Data 
 Chapter 4  - Data Analysis and Uncertainty 
 Chapter 5  - A Systematic Overview of Data Mining Algorithms 
 Chapter 6  - Models and Patterns 
 Chapter 7  - Score Functions for Data Mining Algorithms 
 Chapter 8  - Search and Optimization Methods 
 Chapter 9  - Descriptive Modeling 
 Chapter 10 - Predictive Modeling for Classification 
 Chapter 11 - Predictive Modeling for Regression 
 Chapter 12 - Data Organization and Databases 
 Chapter 13 - Finding Patterns and Rules 
 Chapter 14 - Retrieval by Content 
 Appendix 
 References  
 Index  
 List of Figures  
 List of Tables  
 List of Examples  
 
 

- Random Variables 

Principles of Data Mining  
David Hand  
Heikki Mannila  
Padhraic Smyth 
A Bradford Book The MIT Press 
Cambridge, Massachusetts LondonEngland  

Copyright © 2001 Massachusetts Institute of Technology 

All rights reserved. No part of this book may be reproduced in any form by any electronic 
or mechanical means (including photocopying, recording, or information storage and 
retrieval) without permission in writing from the publisher. 

This book was typeset in Palatino by the authors and was printed and bound in the 
United States of America. 
Library of Congress Cataloging-in-Publication Data 
 

Hand, D. J. 
 Principles of data mining / David Hand, Heikki Mannila, Padhraic Smyth. 
   p. cm.—(Adaptive computation and machine learning) 
 Includes bibliographical references and index. 

ISBN 0-262-08290-X (hc. : alk. paper) 
1. Data Mining. I. Mannila, Heikki. II. Smyth, Padhraic. III. Title. IV. Series.  

 
 

 
 

QA76.9.D343 H38 2001 

006.3—dc21 2001032620  
To Crista, Aidan, and Cian  
To Paula and Elsa  
To Shelley, Rachel, and Emily  

Series Foreword 
The rapid growth and integration of databases provides scientists, engineers, and 
business people with a vast new resource that can be analyzed to make scientific 
discoveries, optimize industrial systems, and uncover financially valuable patterns. To 
undertake these large data analysis projects, researchers and practitioners have 
adopted established algorithms from statistics, machine learning, neural networks, and 
databases and have also developed new methods targeted at large data mining 
problems. Principles of Data Mining by David Hand, Heikki Mannila, and Padhraic Smyth 
provides practioners and students with an introduction to the wide range of algorithms 
and methodologies in this exciting area. The interdisciplinary nature of the field is 
matched by these three authors, whose expertise spans statistics, databases, and 
computer science. The result is a book that not only provides the technical details and 
the mathematical principles underlying data mining methods, but also provides a 
valuable perspective on the entire enterprise. 

Data mining is one component of the exciting area of machine learning and adaptive 
computation. The goal of building computer systems that can adapt to their 
envirionments and learn from their experience has attracted researchers from many 
fields, including computer science, engineering, mathematics, physics, neuroscience, 
and cognitive science. Out of this research has come a wide variety of learning 
techniques that have the potential to transform many scientific and industrial fields. 
Several research communities have converged on a common set of issues surrounding 
supervised, unsupervised, and reinforcement learning problems. The MIT Press series 
on Adaptive Computation and Machine Learning seeks to unify the many diverse strands 
of machine learning research and to foster high quality research and innovative 
applications. 

Thomas Dietterich  

Preface 

The science of extracting useful information from large data sets or databases is known 
as data mining. It is a new discipline, lying at the intersection of statistics, machine 
learning, data management and databases, pattern recognition, artificial intelligence, and 
other areas. All of these are concerned with certain aspects of data analysis, so they 
have much in common—but each also has its own distinct flavor, emphasizing particular 
problems and types of solution. 

Because data mining encompasses a wide variety of topics in computer science and 
statistics it is impossible to cover all the potentially relevant material in a single text. 
Given this, we have focused on the topics that we believe are the most fundamental. 

From a teaching viewpoint the text is intended for undergraduate students at the senior 
(final year) level, or first or second-year graduate level, who wish to learn about the basic 
principles of data mining. The text should also be of value to researchers and 
practitioners who are interested in gaining a better understanding of data mining 
methods and techniques. A familiarity with the very basic concepts in probability, 
calculus, linear algebra, and optimization is assumed—in other words, an undergraduate 
background in any quantitative discipline such as engineering, computer science, 
mathematics, economics, etc., should provide a good background for reading and 
understanding this text. 
There are already many other books on data mining on the market. Many are targeted at 
the business community directly and emphasize specific methods and algorithms (such 
as decision tree classifiers) rather than general principles (such as parameter estimation 
or computational complexity). These texts are quite useful in providing general context 
and case studies, but have limitations in a classroom setting, since the underlying 
foundational principles are often missing. There are other texts on data mining that have 
a more academic flavor, but to date these have been written largely from a computer 
science viewpoint, specifically from either a database viewpoint (Han and Kamber, 
2000), or from a machine learning viewpoint (Witten and Franke, 2000). 

This text has a different bias. We have attempted to provide a foundational vi ew of data 
mining. Rather than discuss specific data mining applications at length (such as, say, 
collaborative filtering, credit scoring, and fraud detection), we have instead focused on 
the underlying theory and algorithms that provide the ""glue"" for such applications. This is 
not to say that we do not pay attention to the applications. Data mining is fundamentally 
an applied discipline, and with this in mind we make frequent references to case studies 
and specific applications where the basic theory can (or has been) applied. 
In our view a mastery of data mining requires an understanding of both statistical and 
computational issues. This requirement to master two different areas of expertise 
presents quite a challenge for student and teacher alike. For the typical computer 
scientist, the statistics literature is relatively impenetrable: a litany of jargon, implicit 
assumptions, asymptotic arguments, and lack of details on how the theoretical and 
mathematical concepts are actually realized in the form of a  data analysis algorithm. The 
situation is effectively reversed for statisticians: the computer science literature on 
machine learning and data mining is replete with discussions of algorithms, pseudocode, 
computational efficiency, and so forth, often with little reference to an underlying model 
or inference procedure. An important point is that both approaches are nonetheless 
essential when dealing with large data sets. An understanding of both the ""mathematical 
modeling"" view, and the ""computational algorithm"" view are essential to properly grasp 
the complexities of data mining. 

In this text we make an attempt to bridge these two worlds and to explicitly link the notion 
of statistical modeling (with attendant assumptions, mathematics, and notation) with the 
""real world"" of actual computational methods and algorithms. 

With this in mind, we have structured the text in a somewhat unusual manner. We begin 
with a discussion of the very basic principles of modeling and inference, then introduce a 
systematic framework that connects models to data via computational methods and 
algorithms, and finally instantiate these ideas in the context of specific techniques such 
as classification and regression. Thus, the text can be divided into three general 
sections: 

1.  Fundamentals: Chapters 1 through 4 focus on the fundamental aspects of 

data and data analysis: introduction to data mining (chapter 1), measurement 
(chapter 2), summarizing and visualizing data (chapter 3), and uncertainty 
and inference (chapter 4). 

2.  Data Mining Components: Chapters 5 through 8 focus on what we term the 

""components"" of data mining algorithms: these are the building blocks that 
can be used to systematically create and analyze data mining algorithms. In 
chapter 5 we discuss this systematic approach to algorithm analysis, and 
argue that this ""component-wise"" view can provide a useful systematic 
perspective on what is often a very confusing landscape of data analysis 

algorithms to the novice student of the topic. In this context, we then delve 
into broad discussions of each component: model representations in chapter 
6, score functions for fitting the models to data in chapter 7, and optimization 
and search techniques in chapter 8. (Discussion of data management is 
deferred until chapter 12.) 

3.  Data Mining Tasks and Algorithms: Having discussed the fundamental 

components in the first 8 chapters of the text, the remainder of the chapters 
(from 9 through 14) are then devoted to specific data mining tasks and the 
algorithms used to address them. We organize the basic tasks into density 
estimation and clustering (chapter 9), classification (chapter 10), regression 
(chapter 11), pattern discovery (chapter 13), and retrieval by content (chapter 
14). In each of these chapters we use the framework of the earlier chapters to 
provide a general context for the discussion of specific algorithms for each 
task. For example, for classification we ask: what models and representations 
are plausible and useful? what score functions should we, or can we, use to 
train a classifier? what optimization and search techniques are necessary? 
what is the computational complexity of each approach once we implement it 
as an actual algorithm? Our hope is that this general approach will provide the 
reader with a ""roadmap"" to an understanding that data mining algorithms are 
based on some very general and systematic principles, rather than simply a 
cornucopia of seemingly unrelated and exotic algorithms. 

In terms of using the text for teaching, as mentioned earlier the target audience for the 
text is students with a quantitative undergraduate background, such as in computer 
science, engineering, mathematics, the sciences, and more quantitative business-
oriented degrees such as economics. From the instructor's viewpoint, how much of the 
text should be covered in a course will depend on both the length of the course (e.g., 10 
weeks versus 15 weeks) and the familiarity of the students with basic concepts in 
statistics and machine learning. For example, for a 10-week course with first-year 
graduate students who have some exposure to basic statistical concepts, the instructor 
might wish to move quickly through the early chapters: perhaps covering chapters 3, 4, 5 
and 7 fairly rapidly; assigning chapters 1, 2, 6 and 8 as background/review reading; and 
then spending the majority of the 10 weeks covering chapters 9 through 14 in some 
depth. 
Conversely many students and readers of this text may have little or no formal statistical 
background. It is unfortunate that in many quantitative disciplines (such as computer 
science) students at both undergraduate and graduate levels often get only a very limited 
exposure to statistical thinking in many modern degree programs. Since we take a fairly 
strong statistical view of data mining in this text, our experience in using draft versions of 
the text in computer science departments has taught us that mastery of the entire text in 
a 10-week or 15-week course presents quite a challenge to many students, since to fully 
absorb the material they must master quite a broad range of statistical, mathematical, 
and algorithmic concepts in chapters 2 through 8. In this light, a less arduous path is 
often desirable. For example, chapter 11 on regression is probably the most 
mathematically challenging in the text and can be omitted without affecting 
understanding of any of the remaining material. Similarly some of the material in chapter 
9 (on mixture models for example) could also be omitted, as could the Bayesian 
estimation framework in chapter 4. In terms of what is essential reading, most of the 
material in chapters 1 through 5 and in chapters 7, 8 and  12 we consider to be essential 
for the students to be able to grasp the modeling and algorithmic ideas that come in the 
later chapters (chapter 6 contains much useful material on the general concepts of 
modeling but is quite long and could be skipped in the interests of time). The more ""task-
specific"" chapters of 9, 10, 11, 13, and  14 can be chosen in a ""menu-based"" fashion, i.e., 
each can be covered somewhat independently of the others (but they do assume that 
the student has a good working knowledge of the material in chapters 1 through 8). 
An additional suggestion for students with limited statistical exposure is to have them 
review some of the basic concepts in probability and statistics before they get to chapter 
4 (on uncertainty) in the text. Unless students are comfortable with basic concepts such 
as conditional probability and expectation, they will have difficulty following chapter 4 and 
much of what follows in later chapters. We have included a brief appendix on basic 
probability and definitions of common distributions, but some students will probably want 

to go back and review their undergraduate texts on probability and statistics before 
venturing further. 
On the other side of the coin, for readers with substantial statistical background (e.g., 
statistics students or statisticians with an interest in data mining) much of this text will 
look quite familiar and the statistical reader may be inclined to say ""well, this data mining 
material seems very similar in many ways to a course in applied statistics!"" And this is 
indeed somewhat correct, in that data mining (as we view it) relies very heavily on 
statistical models and methodologies. However, there are portions of the text that 
statisticians will likely find quite informative: the overview of chapter 1, the algorithmic 
viewpoint of chapter 5, the score function viewpoint of chapter 7, and all of chapters 12 
through 14 on database principles, pattern finding, and retrieval by content. In addition, 
we have tried to include in our presentation of many of the traditional statistical concepts 
(such as classification, clustering, regression, etc.) additional material on algorithmic and 
computational issues that would not typically be presented in a statistical textbook. 
These include statements on computational complexity and brief discussions on how the 
techniques can be used in various data mining applications. Nonetheless, statisticians 
will find much familiar material in this text. For views of data mining that are more 
oriented towards computational and data-management issues see, for example, Han and 
Kamber (2000), and for a business focus see, for example, Berry and Linoff (2000). 
These texts could well serve as complementary reading in a course environment. 

In summary, this book describes tools for data mining, splitting the tools into their 
component parts, so that their structure and their relationships to each other can be 
seen. Not only does this give insight into what the tools are designed to achieve, but it 
also enables the reader to design tools of their own, suited to the particular problems and 
opportunities facing them. The book also shows how data mining is a process—not 
something which one does, and then finishes, but an ongoing voyage of discovery, 
interpretation, and re-investigation. The book is liberally illustrated with real data 
applications, many arising from the authors' own research and applications work. For 
didactic reasons, not all of the data sets discussed are large—it is easier to explain what 
is going on in a ""small"" data set. Once the idea has been communicated, it can readily 
be applied in a realistically large context. 

Data mining is, above all, an exciting discipline. Certainly, as with any scientific 
enterprise, much of the effort will be unrewarded (it is a rare and perhaps rather dull 
undertaking which gives a guaranteed return). But this is more than compensated for by 
the times when an exciting discovery—a gem or nugget of valuable information—is 
unearthed. We hope that you as a reader of this text will be inspired to go forth and 
discover your own gems! 
We would like to gratefully acknowledge Christine McLaren for granting permission to 
use the red blood cell data as an illustrative example in chapters 9 and 10. Padhraic 
Smyth's work on this text was supported in part by the National Science Foundation 
under Grant IRI-9703120. 

We would also like to thank Niall Adams for help in producing some of the diagrams, 
Tom Benton for assisting with proof corrections, and  Xianping Ge for formatting the 
references. Naturally, any mistakes which remain are the responsibility of the authors 
(though each of the three of us reserves the right to blame the other two). 

Finally we would each like to thank our respective wives and families for providing 
excellent encouragement and support throughout the long and seemingly never-ending 
saga of ""the book""! 

Chapter 1: Introduction 
1.1 Introduction to Data Mining 
Progress in digital data acquisition and storage technology has resulted in the growth of 
huge databases. This has occurred in all areas of human endeavor, from the mundane 
(such as supermarket transaction data, credit card usage records, telephone call details, 

 
 

and government statistics) to the more exotic (such as images of astronomical bodies, 
molecular databases, and medical records). Little wonder, then, that interest has grown 
in the possibility of tapping these data, of extracting from them information that might be 
of value to the owner of the database. The discipline concerned with this task has 
become known as data mining. 

Defining a scientific discipline is always a controversial task; researchers often disagree 
about the precise range and limits of their field of study. Bearing this in mind, and 
accepting that others might disagree about the details, we shall adopt as our working 
definition of data mining: 

Data mining is the analysis of (often large) observational data sets to find unsuspected 
relationships and to summarize the data in novel ways that are both understandable and 
useful to the data owner. 
The relationships and summaries derived through a data mining exercise are often 
referred to as models or patterns. Examples include linear equations, rules, clusters, 
graphs, tree structures, and recurrent patterns in time series. 

The definition above refers to ""observational data,"" as opposed to ""experimental data."" 
Data mining typically deals with data that have already been collected for some purpose 
other than the data mining analysis (for example, they may have been collected in order 
to maintain an up-to-date record of all the transactions in a bank). This means that the 
objectives of the data mining exercise play no role in the data collection strategy. This is 
one way in which data mining differs from much of statistics, in which data are often 
collected by using efficient strategies to answer specific questions. For this reason, data 
mining is often referred to as ""secondary"" data analysis. 
The definition also mentions that the data sets examined in data mining are often large. If 
only small data sets were involved, we would merely be discussing classical exploratory 
data analysis as practiced by statisticians. When we are faced with large bodies of data, 
new problems arise. Some of these relate to housekeeping issues of how to store or 
access the data, but others relate to more fundamental issues, such as how to determine 
the representativeness of the data, how to analyze the data in a reasonable period of 
time, and how to decide whether an apparent relationship is merely a chance occurrence 
not reflecting any underlying reality. Often the available data comprise only a sample 
from the complete population (or, perhaps, from a hypothetical superpopulation); the aim 
may be to generalize from the sample to the population. For example, we might wish to 
predict how future customers are likely to behave or to determine the properties of 
protein structures that we have not yet seen. Such generalizations may not be 
achievable through standard statistical approaches because  often the data are not 
(classical statistical) ""random samples,"" but rather ""convenience"" or ""opportunity"" 
samples. Sometimes we may want to summarize or compress a very large data set in 
such a way that the result is more comprehensible, without any notion of generalization. 
This issue would arise, for example, if we had complete census data for a particular 
country or a database recording millions of individual retail transactions. 

The relationships and structures found within a set of data must, of course, be novel. 
There is little point in regurgitating well-established relationships (unless, the exercise is 
aimed at ""hypothesis"" confirmation, in which one was seeking to determine whether 
established pattern also exists in a new data set) or necessary relationships (that, for 
example, all pregnant patients are female). Clearly, novelty must be measured relative to 
the user's prior knowledge. Unfortunately few data mining algorithms take into account a 
user's prior knowledge. For this reason we will not say very much about novelty in this 
text. It remains an open research problem. 

While novelty is an important property of the relationships we seek, it is not sufficient to 
qualify a relationship as being worth finding. In particular, the relationships must also be 
understandable. For instance simple relationships are more readily understood than 
complicated ones, and may well be preferred, all else being equal.  
Data mining is often set in the broader context of knowledge discovery in databases, or 
KDD. This term originated in the artificial intelligence (AI) research field. The KDD 

process involves several stages: selecting the target data, preprocessing the data, 
transforming them if necessary, performing data mining to extract patterns and 
relationships, and then interpreting and assessing the discovered structures. Once again 
the precise boundaries of the data mining part of the process are not easy to state; for 
example, to many people data transformation is an intrinsic part of data mining. In this 
text we will focus primarily on data mining algorithms rather than the overall process. For 
example, we will not spend much time discussing data preprocessing issues such as 
data cleaning, data verification, and defining variables. Instead we focus on the basic 
principles for modeling data and for constructing algorithmic processes to fit these 
models to data. 

The process of seeking relationships within a data set— of seeking accurate, convenient, 
and useful summary representations of some aspect of the data—involves a number of 
steps: 

§ determining the nature and structure of the representation to be used; 
§ deciding how to quantify and compare how well different representations fit 

the data (that is, choosing a ""score"" function); 

§ choosing an algorithmic process to optimize the score function; and 
§ deciding what principles of data management are required to implement the 

algorithms efficiently. 

The goal of this text is to discuss these issues in a systematic and detailed manner. We 
will look at both the fundamental principles (chapters 2 to 8) and the ways these 
principles can be applied to construct and evaluate specific data mining algorithms 
(chapters 9 to 14). 

Example 1.1  

 
Regression analysis is a tool with which many readers will be familiar. In its simplest form, 
it involves building a predictive model to relate a predictor variable, X, to a response 
variable, Y , through a relationship of the form Y = aX + b. For example, we might build a 
model which would allow us to predict a person's annual credit-card spending given their 
annual income. Clearly the model would not be perfect, but since spending typically 
increases with income, the model might well be adequate as a rough characterization. In 
terms of the above steps listed, we would have the following scenario: 

§  The representation is a model in which the response variable, spending, 

is linearly related to the predictor variable, income. 

§  The score function most commonly used in this situation is the sum of 

squared discrepancies between the predicted spending from the model 
and observed spending in the group of people described by the data. 
The smaller this sum is, the better the model fits the data. 

§  The optimization algorithm is quite simple in the case of linear 

regression: a and b can be expressed as explicit functions of the 
observed values of spending and income. We describe the algebraic 
details in chapter 11. 

§  Unless the data set is very large, few data management problems arise 

with regression algorithms. Simple summaries of the data (the sums, 
sums of squares, and sums of products of the X and Y values) are 
sufficient to compute estimates of a and b. This means that a single pass 
through the data will yield estimates. 

 

 

Data mining is an interdisciplinary exercise. Statistics, database technology, machine 
learning, pattern recognition, artificial intelligence, and visualization, all play a role. And 
just as it is difficult to define sharp boundaries between these disciplines, so it is difficult 
to define sharp boundaries between each of them and data mining. At the boundaries, 
one person's data mining is another's statistics, database, or machine learning problem. 

1.2 The Nature of Data Sets 

We begin by discussing at a high level the basic nature of data sets. 
A data set is a set of measurements taken from some environment or process. In the 
simplest case, we have a collection of objects, and for each object we have a set of the 
same p measurements. In this case, we can think of the collection of the measurements 
on n objects as a form of n × p data matrix. The n rows represent the n objects on which 
measurements were taken (for example, medical patients, credit card customers, or 
individual objects observed in the night sky, such as stars and galaxies). Such rows may 
be referred to as individuals, entities, cases, objects, or records depending on the 
context. 
The other dimension of our data matrix contains the set of p measurements made on 
each object. Typically we assume that the same p measurements are made on each 
individual although this need not be the case (for example, different medical tests could 
be performed on different patients). The p columns of the data matrix may be referred to 
as variables, features, attributes, or fields; again, the language depends on the research 
context. In all situations the idea is the same: these names refer to the measurement that 
is represented by each column. In chapter 2 we will discuss the notion of measurement 
in much more detail.  

Example 1.2  

 
The U.S. Census Bureau collects information about the U.S. population every 10 years. 
Some of this information is made available for public use, once information that could be 
used to identify a particular individual has been removed. These data sets are called 
PUMS, for Public Use Microdata Samples, and they are available in 5 % and 1 % sample 
sizes. Note that even a 1 % sample of the U.S. population contains about 2.7 million 
records. Such a data set can contain tens of variables, such as the age of the person, 
gross income, occupation, capital gains and losses, education level, and so on. Consider 
the simple data matrix shown in table 1.1. Note that the data contains different types of 
variables, some with continuous values and some with categorical. Note also that some 
values are missing—for example, the Age of person 249, and the Marital Status of person 
255. Missing measurements are very common in large real-world data sets. A more 
insidious problem is that of measurement noise. For example, is person 248's income really 
$100,000 or is this just a rough guess on his part? 
Table 1.1: Examples of Data in Public Use Microdata Sample Data Sets.   

ID 

248 

249 

250 

251 

252 

253 

Age 

Sex 

Male 

54 

?? 

29 

9 

85 

40 

Marital 
Status 

Married 

Female 

Married 

Male 

Male 

Female 

Married 

Not 
married 

Not 
married 

Male 

Married 

Education 

Income 

High 
school 
graduate 

High 
school 
graduate 

Some 
college 

Child 

High 
school 
graduate 

High 
school 
graduate 

100000 

12000 

23000 

0 

19798 

40100 

Table 1.1: Examples of Data in Public Use Microdata Sample Data Sets.   

ID 

254 

255 

256 

257 

Age 

Sex 

38 

7 

49 

76 

Female 

Male 

Male 

Male 

Marital 
Status 

Not 
married 

?? 

Married 

Married 

Education 

Income 

Less than 
1st grade 

Child 

11th grade 

Doctorate 
degree 

2691 

0 

30000 

30686 

A typical task for this type of data would be finding relationships between different 
variables. For example, we might want to see how well a person's income could be 
predicted from the other variables. We might also be interested in seeing if there are 
naturally distinct groups of people, or in finding values at which variables often coincide. A 
subset of variables and records is available online at the Machine Learning Repository of 
the University of California, Irvine , www.ics.uci.edu/~mlearn/MLSummary.html. 

 

 

Data come in many forms and this is not the place to develop a complete taxonomy. 
Indeed, it is not even clear that a complete taxonomy can be developed, since an 
important aspect of data in one situation may be unimportant in another. However there 
are certain basic distinctions to which we should draw attention. One is the difference 
between quantitative and categorical measurements (different names are sometimes 
used for these). A quantitative variable is measured on a numerical scale and can, at 
least in principle, take any value. The columns Age and Income in table 1.1 are 
examples of quantitative variables. In contrast, categorical variables such as Sex, Marital 
Status and Education in 1.1 can take only certain, discrete values. The common three 
point severity scale used in medicine (mild, moderate, severe) is another example. 
Categorical variables may be ordinal (possessing a natural order, as in the Education 
scale) or nominal (simply naming the categories, as in the Marital Status case). A data 
analytic technique appropriate for one type of scale might not be appropriate for another 
(although it does depend on the objective—see Hand (1996) for a detailed discussion). 
For example, were marital status represented by integers (e.g., 1 for single, 2 for 
married, 3 for widowed, and so forth) it would generally not be meaningful or appropriate 
to calculate the arithmetic mean of a sample of such scores using this scale. Similarly, 
simple linear regression (predicting one quantitative variable as a function of others) will 
usually be appropriate to apply to quantitative data, but applying it to categorical data 
may not be wise; other techniques, that have similar objectives (to the extent that the 
objectives can be similar when the data types differ), might be more appropriate with 
categorical scales. 

Measurement scales, however defined, lie at the bottom of any data taxonomy. Moving 
up the taxonomy, we find that data can occur in various relationships and structures. 
Data may arise sequentially in time series, and the data mining exercise might address 
entire time series or particular segments of those time series. Data might also describe 
spatial relationships, so that individual records take on their full significance only when 
considered in the context of others. 
Consider a data set on medical patients. It might include multiple measurements on the 
same variable (e.g., blood pressure), each measurement taken at different times on 
different days. Some patients might have extensive image data (e.g., X-rays or magnetic 
resonance images), others not. One might also have data in the form of text, recording a 
specialist's comments and diagnosis for each patient. In addition, there might be a 
hierarchy of relationships between patients in terms of doctors, hospitals, and 
geographic locations. The more complex the data structures, the more complex the data 
mining models, algorithms, and tools we need to apply. 

For all of the reasons discussed above, the n × p data matrix is often an 
oversimplification or idealization of what occurs in practice. Many data sets will not fit into 
this simple format. While much information can in principle be ""flattened"" into the n × p 
matrix (by suitable definition of the  p variables), this will often lose much of the structure 
embedded in the data. Nonetheless, when discussing the underlying principles of data 
analysis, it is often very convenient to assume that the observed data exist in an n × p 
data matrix; and we will do so unless otherwise indicated, keeping in mind that for data 
mining applications n and p may both be very large. It is perhaps worth remarking that 
the observed data matrix can also be referred to by a variety names including data set, 
training data, sample, database, (often the different terms arise from different 
disciplines). 
Example 1.3  

 
Text documents are important sources of information, and data mining methods can help in 
retrieving useful text from large collections of documents (such as the Web). Each 
document can be viewed as a sequence of words and punctuation. Typical tasks for mining 
text databases are classifying documents into predefined categories, clustering similar 
documents together, and finding documents that match the specifications of a query. A 
typical collection of documents is ""Reuters-21578, Distribution 1.0,"" located at 
http://www.research.att.com/~lewis. Each document in this collection is a short 
newswire article. 
A collection of text documents can also be viewed as a matrix, in which the rows represent 
documents and the columns represent words. The entry (d, w), corresponding to document 
d and word  w, can be the number of times w occurs in d, or simply 1 if  w occurs in d and 0 
otherwise. 
With this approach we lose the ordering of the words in the document (and, thus, much of 
the semantic content), but still retain a reasonably good representation of the document's 
contents. For a document collection, the number of rows is the number of documents, and 
the number of columns is the number of distinct words. Thus, large multilingual document 
collections may have millions of rows and hundreds of thousands of columns. Note that 
such a data matrix will be very sparse; that is, most of the entries will be zeroes. We 
discuss text data in more detail in chapter 14. 

 

 
Example 1.4  

 
Another common type of data is transaction data, such as a list of purchases in a store, 
where each purchase (or transaction) is described by the date, the customer ID, and a list 
of items and their prices. A similar example is a Web transaction log, in which a sequence 
of triples (user id, web page, time), denote the user accessing a particular page at a 
particular time. Designers and owners of Web sites often have great interest in 
understanding the patterns of how people navigate through their site.  

As with text documents, we can transform a set of transaction data into matrix form. 
Imagine a very large, sparse matrix in which each row corresponds to a particular individual 
and each column corresponds to a particular Web page or item. The entries in this matrix 
could be binary (e.g., indicating whether a user had ever visited a certain Web page) or 
integer-valued (e.g., indicating how many times a user had visited the page). 
Figure 1.1 shows a visual representation of a small portion of a large retail transaction data 
set displayed in matrix form. Rows correspond to individual customers and columns 
represent categories of items. Each black entry indicates that the customer corresponding 
to that row purchased the item corresponding to that column. We can see some obvious 
patterns even in this simple display. For example, there is considerable variability in terms 
of which categories of items customers purchased and how many items they purchased. In 
addition, while some categories were purchased by quite a few customers (e.g., columns 3, 

5, 11, 26) some were not purchased at all (e.g., columns 18 and 19). We can also see pairs 
of categories which that were frequently purchased together (e.g., columns 2 and 3). 

 

Figure 1.1: A Portion of a Retail Transaction Data Set Displayed as a Binary Image, With 100 
Individual Customers (Rows) and 40 Categories of Items (Columns).  
Note, however, that with this ""flat representation"" we may lose a significant portion of 
information including sequential and temporal information (e.g., in what order and at what 
times items were purchased), any information about structured relationships between 
individual items (such as product category hierarchies, links between Web pages, and so 
forth). Nonetheless, it is often useful to think of such data in a standard n × p matrix. For 
example, this allows us to define distances between users by comparing their p-
dimensional Web-page usage vectors, which in turn allows us to cluster users based on 
Web page patterns. We will look at clustering in much more detail in chapter 9. 
 
 

1.3 Types of Structure: Models and Patterns 
The different kinds of representations sought during a data mining exercise may be 
characterized in various ways. One such characterization is the distinction between a 
global model and a local pattern. 
A model structure, as defined here, is a global summary of a data set; it makes 
statements about any point in the full measurement space. Geometrically, if we consider 
the rows of the data matrix as corresponding to p-dimensional vectors (i.e., points in p-
dimensional space), the model can make a statement about any point in this space (and 
hence, any object). For example, it can assign a point to a cluster or predict the value of 
some other variable. Even when some of the measurements are missing (i.e., some of 
the components of the p-dimensional vector are unknown), a model can typically make 
some statement about the object represented by the (incomplete) vector. 
A simple model might take the form Y = aX + c, where Y and X are variables and a and c 
are parameters of the model (constants determined during the course of the data mining 
exercise). Here we would say that the functional form of the model is linear, since Y is a 
linear function of X. The conventional statistical use of the term is slightly different. In 
statistics, a model is linear if it is a linear function of the parameters. We will try to be 
clear in the text about which form of linearity we are assuming, but when we discuss the 
structure of a model (as we are doing here) it makes sense to consider linearity as a 
function of the variables of interest rather than the parameters. Thus, for example, the 
model structure Y = aX2 + bX + c, is considered a linear model in classic statistical 
terminology, but the functional form of the model relating Y and X is nonlinear (it is a 
second-degree polynomial). 

In contrast to the global nature of models, pattern structures make statements only about 
restricted regions of the space spanned by the variables. An example is a simple 
probabilistic statement of the form if X > x1 then prob(Y > y1) = p1. This structure 
consists of constraints on the values of the variables X and Y , related in the form of a 
probabilistic rule. Alternatively we could describe the relationship as the conditional 
probability p(Y > y1|X > x1) = p1, which is semantically equivalent. Or we might notice 
that certain classes of transaction records do not show the peaks and troughs shown by 
the vast majority, and look more closely to see why. (This sort of exercise led one bank 
to discover that it had several open accounts that belonged to people who had died.) 
Thus, in contrast to (global) models, a (local) pattern describes a structure relating to a 
relatively small part of the data or the space in which data could occur. Perhaps only 
some of the records behave in a certain way, and the pattern characterizes which they 
are. For example, a search through a database of mail order purchases may reveal that 
people who buy certain combinations of items are also likely to buy others. Or perhaps 
we identify a handful of ""outlying"" records that are very different from the majority (which 
might be thought of as a central cloud in p-dimensional space). This last example 
illustrates that global models and local patterns may sometimes be regarded as opposite 
sides of the same coin. In order to detect unusual behavior we need a description of 
usual behavior. There is a parallel here to the role of  diagnostics in statistical analysis; 
local pattern-detection methods have applications in anomaly detection, such as fault 
detection in industrial processes, fraud detection in banking and other commercial 
operations. 
Note that the model and pattern structures described above have parameters associated 
with them; a, b, c for the model and  x1, y1 and  p1 for the pattern. In general, once we 
have established the structural form we are interested in finding, the next step is to 
estimate its parameters from the available data. Procedures for doing this are discussed 
in detail in chapters 4, 7 and 8. Once the parameters have been assigned values, we 
refer to a particular model, such as y = 3:2x + 2:8, as a ""fitted model,"" or just ""model"" for 
short (and similarly for patterns). This distinction between model (or pattern) structures 
and the actual (fitted) model (or pattern) is quite important. The structures represent the 
general functional forms of the models (or patterns), with unspecified parameter values. 
A fitted model or pattern has specific values for its parameters. 

The distinction between models and patterns is useful in many situations. However, as 
with most divisions of nature into classes that are convenient for human comprehension, 
it is not hard and fast: sometimes it is not clear whether a particular structure should be 
regarded as a model or a pattern. In such cases, it is best not to be too concerned about 
which is appropriate; the distinction is intended to aid our discussion, not to be a 
proscriptive constraint. 

1.4 Data Mining Tasks 
It is convenient to categorize data mining into types of tasks, corresponding to different 
objectives for the person who is analyzing the data. The categorization below is not 
unique, and further division into finer tasks is possible, but it captures the types of data 
mining activities and previews the major types of data mining algorithms we will describe 
later in the text. 

1.  Exploratory Data Analysis (EDA) (chapter 3): As the name suggests, 

the goal here is simply to explore the data without any clear ideas of 
what we are looking for. Typically, EDA techniques are interactive and 
visual, and there are many effective graphical display methods for 
relatively small, low-dimensional data sets. As the dimensionality 
(number of variables, p) increases, it becomes much more difficult to 
visualize the cloud of points in p-space. For p higher than 3 or 4, 
projection techniques (such as principal components analysis) that 
produce informative low-dimensional projections of the data can be very 
useful. Large numbers of cases can be difficult to visualize effectively, 
however, and notions of scale and detail come into play: ""lower 
resolution"" data samples can be displayed or summarized at the cost of 

 
 

possibly missing important details. Some examples of EDA applications 
are: 

§  Like a pie chart, a coxcomb plot divides up a circle, but 

§ 

whereas in a pie chart the angles of the wedges differ, in 
a coxcomb plot the radii of the wedges differ. Florence 
Nightingale used such plots to display the mortality rates 
at military hospitals in and near London (Nightingale, 
1858). 
In 1856 John Bennett Lawes laid out a series of plots of 
land at Rothamsted Experimental Station in the UK, and 
these plots have remained untreated by fertilizers or 
other artificial means ever since. They provide a rich 
source of data on how different plant species develop 
and compete, when left uninfluenced. Principal 
components analysis has been used to display the data 
describing the relative yields of different species (Digby 
and Kempton, 1987, p. 59). 

§  More recently, Becker, Eick, and Wilks (1995) described 
a set of intricate spatial displays for visualization of time-
varying long-distance telephone network patterns (over 
12,000 links). 

2.  Descriptive Modeling (chapter 9): The goal of a descriptive model is 

describe all of the data (or the process generating the data). Examples of 
such descriptions include models for the overall probability distribution of 
the data (density estimation), partitioning of the p-dimensional space into 
groups (cluster analysis and segmentation), and models describing the 
relationship between variables (dependency modeling). In segmentation 
analysis, for example, the aim is to group together similar records, as in 
market segmentation of commercial databases. Here the goal is to split 
the records into homogeneous groups so that similar people (if the 
records refer to people) are put into the same group. This enables 
advertisers and marketers to efficiently direct their promotions to those 
most likely to respond. The number of groups here is chosen by the 
researcher; there is no ""right"" number. This contrasts with cluster 
analysis, in which the aim is to discover ""natural"" groups in data—in 
scientific databases, for example. Descriptive modelling has been used 
in a variety of ways. 

§  Segmentation has been extensively and successfully 

used in marketing to divide customers into homogeneous 
groups based on purchasing patterns and demographic 
data such as age, income, and so forth (Wedel and 
Kamakura, 1998). 

§  Cluster analysis has been used widely in psychiatric 

research to construct taxonomies of psychiatric illness. 
For example, Everitt, Gourlay and Kendell (1971) applied 
such methods to samples of psychiatric inpatients; they 
reported (among other findings) that ""all four analyses 
produced a cluster composed mainly of patients with 
psychotic depression."" 

§  Clustering techniques have been used to analyze the 

long-term climate variability in the upper atmosphere of 
the Earth's Northern hemisphere. This variability is 
dominated by three recurring spatial pressure patterns 
(clusters) identified from data recorded daily since 1948 
(see Cheng and Wallace [1993] and Smyth, Idea, and 
Ghil [1999] for further discussion). 

3.  Predictive Modeling: Classification and Regression (chapters 10 and 

11): The aim here is to build a model that will permit the value of one 
variable to be predicted from the known values of other variables. In 
classification, the variable being predicted is categorical, while in 

regression the variable is quantitative. The term ""prediction"" is used here 
in a general sense, and no notion of a time continuum is implied. So, for 
example, while we might want to predict the value of the stock market at 
some future date, or which horse will win a race, we might also want to 
determine the diagnosis of a patient, or the degree of brittleness of a 
weld. A large number of methods have been developed in statistics and 
machine learning to tackle predictive modeling problems, and work in this 
area has led to significant theoretical advances and improved 
understanding of deep issues of inference. The key distinction between 
prediction and description is that prediction has as its objective a unique 
variable (the market's value, the disease class, the brittleness, etc.), 
while in descriptive problems no single variable is central to the model. 
Examples of predictive models include the following: 

§  The SKICAT system of Fayyad, Djorgovski, and Weir 
(1996) used a tree-structured representation to learn a 
classification tree that can perform as well as human 
experts in classifying stars and galaxies from a 40-
dimensional feature vector. The system is in routine use 
for automatically cataloging millions of stars and galaxies 
from digital images of the sky. 

§  Researchers at AT&T developed a system that tracks the 

characteristics of all 350 million unique telephone 
numbers in the United States (Cortes and Pregibon, 
1998). Regression techniques are used to build models 
that estimate the probability that a telephone number is 
located at a business or a residence. 

4.  Discovering Patterns and Rules (chapter 13): The three types of tasks 

listed above are concerned with model building. Other data mining 
applications are concerned with pattern detection. One example is 
spotting fraudulent behavior by detecting regions of the space defining 
the different types of transactions where the data points significantly 
different from the rest. Another use is in astronomy, where detection of 
unusual stars or galaxies may lead to the discovery of previously 
unknown phenomena. Yet another is the task of finding combinations of 
items that occur frequently in transaction databases (e.g., grocery 
products that are often purchased together). This problem has been the 
focus of much attention in data mining and has been addressed using 
algorithmic techniques based on association rules. 

A significant challenge here, one that statisticians have traditionally dealt with 
in the context of outlier detection, is deciding what constitutes truly unusual 
behavior in the context of normal variability. In high dimensions, this can be 
particularly difficult. Background domain knowledge and human interpretation 
can be invaluable. Examples of data mining systems of pattern and rule 
discovery include the following: 

§  Professional basketball games in the United States are 

routinely annotated to provide a detailed log of every 
game, including time-stamped records of who took a 
particular type of shot, who scored, who passed to 
whom, and so on. The Advanced Scout system of 
Bhandari et al. (1997) searches for rule-like patterns from 
these logs to uncover interesting pieces of information 
which might otherwise go unnoticed by professional 
coaches (e.g., ""When Player X is on the floor, Player Y's 
shot accuracy decreases from 75% to 30%."") As of 1997 
the system was in use by several professional U.S. 
basketball teams. 

§  Fraudulent use of cellular telephones is estimated to cost 
the telephone industry several hundred million dollars per 
year in the United States. Fawcett and Provost (1997) 
described the application of rule-learning algorithms to 

discover characteristics of fraudulent behavior from a 
large database of customer transactions. The resulting 
system was reported to be more accurate than existing 
hand-crafted methods of fraud detection. 

5.  Retrieval by Content (chapter 14): Here the user has a pattern of 

interest and wishes to find similar patterns in the data set. This task is 
most commonly used for text and image data sets. For text, the pattern 
may be a set of keywords, and the user may wish to find relevant 
documents within a large set of possibly relevant documents (e.g., Web 
pages). For images, the user may have a sample image, a sketch of an 
image, or a description of an image, and wish to find similar images from 
a large set of images. In both cases the definition of similarity is critical, 
but so are the details of the search strategy.  

There are numerous large-scale applications of retrieval systems, including: 

§  Retrieval methods are used to locate documents on the 

Web, as in the Google system (www.google.com) of 
Brin and Page (1998), which uses a mathematical 
algorithm called PageRank to estimate the relative 
importance of individual Web pages based on link 
patterns. 

§  QBIC (""Query by Image Content""), a system developed 

by researchers at IBM, allows a user to interactively 
search a large database of images by posing queries in 
terms of content descriptors such as color, texture, and 
relative position information (Flickner et al., 1995). 

Although each of the above five tasks are clearly differentiated from each other, they 
share many common components. For example, shared by many tasks is the notion of 
similarity or  distance between any two data vectors. Also shared is the notion of score 
functions (used to assess how well a model or pattern fits the data), although the 
particular functions tend to be quite different across different categories of tasks. It is 
also obvious that different model and pattern structures are needed for different tasks, 
just as different structures may be needed for different kinds of data. 

1.5 Components of Data Mining Algorithms 

 

In the preceding sections we have listed the basic categories of tasks that may be 
undertaken in data mining. We now turn to the question of how one actually 
accomplishes these tasks. We will take the view that data mining algorithms that address 
these tasks have four basic components: 

1.  Model or Pattern Structure: determining the underlying structure or 

functional forms that we seek from the data (chapter 6). 

2.  Score Function: judging the quality of a fitted model (chapter 7). 
3.  Optimization and Search Method: optimizing the score function and 

searching over different model and pattern structures (chapter 8). 

4.  Data Management Strategy: handling data access efficiently during the 

search/optimization (chapter 12). 

We have already discussed the distinction between model and pattern structures. In the 
remainder of this section we briefly discuss the other three components of a data mining 
algorithm.  

1.5.1 Score Functions 
Score functions quantify how well a model or parameter structure fits a given data set. In 
an ideal world the choice of score function would precisely reflect the utility (i.e., the true 
expected benefit) of a particular predictive model. In practice, however, it is often difficult 
to specify precisely the true utility of a model's predictions. Hence, simple, ""generic"" 
score functions, such as least squares and classification accuracy are commonly used. 
Without some form of score function, we cannot tell whether one model is better than 
another or, indeed, how to choose a good set of values for the parameters of the model. 

Several score functions are widely used for this purpose; these include likelihood, sum of 
squared errors, and misclassification rate (the latter is used in supervised classification 
problems). For example, the well-known squared error score function is defined as 

(1.1)  

where we are predicting n ""target"" values y(i), 1 = i = n, and our predictions for each are 
denoted as y(i) (typically this is a function of some other ""input"" variable values for 
prediction and the parameters of the model). 
Any views we may have on the theoretical appropriateness of different criteria must be 
moderated by the practicality of applying them. The model that we consider to be most 
likely to have given rise to the data may be the ideal one, but if estimating its parameters 
will take months of computer time it is of little value. Likewise, a score function that is 
very susceptible to slight changes in the data may not be very useful (its utility will 
depend on the objectives of the study). For example if altering the values of a few 
extreme cases leads to a dramatic change in the estimates of some model parameters 
caution is warranted; a data set is usually chosen from a number of possible data sets, 
and it may be that in other data sets the value of these extreme cases would have 
differed. Problems like this can be avoided by using robust methods that are less 
sensitive to these extreme points. 

1.5.2 Optimization and Search Methods 

The score function is a measure of how well aspects of the data match proposed models 
or patterns. Usually, these models or patterns are described in terms of a structure, 
sometimes with unknown parameter values. The goal of optimization and search is to 
determine the structure and the parameter values that achieve a minimum (or maximum, 
depending on the context) value of the score function. The task of finding the ""best"" 
values of parameters in models is typically cast as an optimization (or estimation) 
problem. The task of finding interesting patterns (such as rules) from a large family of 
potential patterns is typically cast as a combinatorial search problem, and is often 
accomplished using heuristic search techniques. In linear regression, a prediction rule is 
usually found by minimizing a least squares score function (the sum of squared errors 
between the prediction from a model and the observed values of the predicted variable). 
Such a score function is amenable to mathematical manipulation, and the model that 
minimizes it can be found algebraically. In contrast, a score function such as 
misclassification rate in supervised classification is difficult to minimize analytically. For 
example, since it is intrinsically discontinuous the powerful tool of differential calculus 
cannot be brought to bear. 

Of course, while we can produce score functions to produce a good match between a 
model or pattern and the data, in many cases this is not really the objective. As noted 
above, we are often aiming to generalize to new data which might arise (new customers, 
new chemicals, etc.) and having too close a match to the data in the database may 
prevent one from predicting new cases accurately. We discuss this point later in the 
chapter. 

1.5.3 Data Management Strategies 

The final component in any data mining algorithm is the data management strategy: the 
ways in which the data are stored, indexed, and accessed. Most well-known dat a 
analysis algorithms in statistics and machine learning have been developed under the 
assumption that all individual data points can be accessed quickly and efficiently in 
random-access memory (RAM). While main memory technology has improved rapidly, 
there have been equally rapid improvements in secondary (disk) and tertiary (tape) 
storage technologies, to the extent that many massive data sets still reside largely on 
disk or tape and will not fit in available RAM. Thus, there will probably be a price to pay 
for accessing massive data sets, since not all data points can be simultaneously close to 
the main processor. 

 
 

Many data analysis algorithms have been developed without including any explicit 
specification of a data management strategy. While this has worked in the past on 
relatively small data sets, many algorithms (such as classification and regression tree 
algorithms) scale very poorly when the ""traditional version"" is applied directly to data that 
reside mainly in secondary storage. 

The field of databases is concerned with the development of indexing methods, data 
structures, and query algorithms for efficient and reliable data retrieval. Many of these 
techniques have been developed to support relatively simple counting (aggregating) 
operations on large data sets for reporting purposes. However, in recent years, 
development has begun on techniques that support the ""primitive"" data access 
operations necessary to implement efficient versions of data mining algorithms (for 
example, tree-structured indexing systems used to retrieve the neighbors of a point in 
multiple dimensions). 

1.6 The Interacting Roles of Statistics and Data Mining 

Statistical techniques alone may not be sufficient to address some of the more 
challenging issues in data mining, especially those arising from massive data sets. 
Nonetheless, statistics plays a very important role in data mining: it is a necessary 
component in any data mining enterprise. In this section we discuss some of the 
interplay between traditional statistics and data mining. 
With large data sets (and particularly with very large data sets) we may simply not know 
even straightforward facts about the data. Simple eye-balling of the data is not an option. 
This means that sophisticated search and examination methods may be required to 
illuminate features which would be readily apparent in small data sets. Moreover, as we 
commented above, often the object of data mining is to make some inferences beyond 
the available database. For example, in a database of astronomical objects, we may 
want to make a statement that ""all objects like this one behave thus,"" perhaps with an 
attached qualifying probability. Likewise, we may determine that particular regions of a 
country exhibit certain patterns of telephone calls. Again, it is probably not the calls in the 
database about which we want to make a statement. Rather it will probably be the 
pattern of future calls which we want to be able to predict. The database provides the set 
of objects which will be used to construct the model or search for a pattern, but the 
ultimate objective will not generally be to describe those data. In most cases the 
objective is to describe the general process by which the data arose, and other data sets 
which could have arisen by the same process. All of this means that it is necessary to 
avoid models or patterns which match the available database too closely: given that the 
available data set is merely one set from the sets of data which could have arisen, one 
does not want to model its idiosyncrasies too closely. Put another way, it is necessary to 
avoid overfitting the given data set; instead one wants to find models or patterns which 
generalize well to potential future data. In selecting a score function for model or pattern 
selection we need to take account of this. We will discuss these issues in more detail in 
chapter 7 and chapters 9 through 11. While we have described them in a data mining 
context, they are fundamental to statistics; indeed, some would take them as the defining 
characteristic of statistics as a discipline. 
Since statistical ideas and methods are so fundamental to data mining, it is legitimate to 
ask whether there are really any differences between the two enterprises. Is data mining 
merely exploratory statistics, albeit for potentially huge data sets, or is there more to data 
mining than exploratory data analysis? The answer is yes—there is more to data mining. 
The most fundamental difference between classical statistical applications and data 
mining is the size of the data set. To a conventional statistician, a ""large"" data set may 
contain a few hundred or a thousand data points. To someone concerned with data 
mining, however, many millions or even billions of data points is not unexpected—
gigabyte and even terabyte databases are by no means uncommon. Such large 
databases occur in all walks of life. For instance the American retailer Wal-Mart makes 
over 20 million transactions daily (Babcock, 1994), and constructed an 11 terabyte 
database of customer transactions in 1998 (Piatetsky-Shapiro, 1999). AT&T has 100 
million customers and carries on the order of 300 million calls a day on its long distance 

network. Characteristics of each call are used to update a database of models for every 
telephone number in the United States (Cortes and Pregibon, 1998). Harrison (1993) 
reports that Mobil Oil aims to store over 100 terabytes of data on oil exploration. Fayyad, 
Djorgovski, and Weir (1996) describe the Digital Palomar Observatory Sky Survey as 
involving three terabytes of data. The ongoing Sloan Digital Sky Survey will create a raw 
observational data set of 40 terabytes, eventually to be reduced to a mere 400 gigabyte 
catalog containing 3 × 108 individual sky objects (Szalay et al., 1999). The NASA Earth 
Observing System is projected to generate multiple gigabytes of raw data per hour 
(Fayyad, Piatetsky-Shapiro, and Smyth, 1996). And the human genome project to 
complete sequencing of the entire human genome will likely generate a data set of more 
than 3.3 × 109 nucleotides in the process (Salzberg, 1999). With data sets of this size 
come problems beyond those traditionally considered by statisticians. 
Massive data sets can be tackled by sampling (if the aim is modeling, but not necessarily 
if the aim is pattern detection) or by adaptive methods, or by summarizing the records in 
terms of sufficient statistics. For example, in standard least squares regression 
problems, we can replace the large numbers of scores on each variable by their sums, 
sums of squared values, and sums of products, summed over the records—these are 
sufficient for regression co-efficients to be calculated no matter how many records there 
are. It is also important to take account of the ways in which algorithms scale, in terms of 
computation time, as the number of records or variables increases. For example, 
exhaustive search through all subsets of variables to find the ""best"" subset (according to 
some score function), will be feasible only up to a point. With p variables there are 2p - 1 
possible subsets of variables to consider. Efficient search methods, mentioned in the 
previous section, are crucial in pushing back the boundaries here. 
Further difficulties arise when there are many variables. One that is important in some 
contexts is the curse of dimensionality; the exponential rate of growth of the number of 
unit cells in a space as the number of variables increases. Consider, for example, a 
single binary variable. To obtain reasonably accurate estimates of parameters within 
both of its cells we might wish to have 10 observations per cell; 20 in all. With two binary 
variables (and four cells) this becomes 40 observations. With 10 binary variables it 
becomes 10240 observations, and with 20 variables it becomes 10485760. The curse of 
dimensionality manifests itself in the difficulty of finding accurate estimates of probability 
densities in high dimensional spaces without astronomically large databases (so large, in 
fact, that the gigabytes available in data mining applications pale into insignificance). In 
high dimensional spaces, ""nearest"" points may be a long way away. These are not 
simply difficulties of manipulating the many variables involved, but more fundamental 
problems of what can actually be done. In such situations it becomes necessary to 
impose additional restrictions through one's prior choice of model (for example, by 
assuming linear models). 

Various problems arise from the difficulties of accessing very large data sets. The 
statistician's conventional viewpoint of a ""flat"" data file, in which rows represent objects 
and columns represent variables, may bear no resemblance to the way the data are 
stored (as in the text and Web transaction data sets described earlier). In many cases 
the data are distributed, and stored on many machines. Obtaining a random sample from 
data that are split up in this way is not a trivial matter. How to define the sampling frame 
and how long it takes to access data become important issues. 

Worse still, often the data set is constantly evolving—as with, for example, records of 
telephone calls or electricity usage. Distributed or evolving data can multiply the size of a 
data set many-fold as well as changing the nature of the problems requiring solution. 
While the size of a data set may lead to difficulties, so also may other properties not 
often found in standard statistical applications. We have already remarked that data 
mining is typically a secondary process of data analysis; that is, the data were originally 
collected for some other purpose. In contrast, much statistical work is concerned with 
primary analysis: the data are collected with particular questions in mind, and then are 
analyzed to answer those questions. Indeed, statistics includes subdisciplines of 
experimental design and survey design—entire domains of expertise concerned with the 
best ways to collect data in order to answer specific questions. When data are used to 
address problems beyond those for which they were originally collected, they may not be 

 
 

ideally suited to these problems. Sometimes the data sets are entire populations (e.g., of 
chemicals in a particular class of chemicals) and therefore the standard statistical notion 
of inference has no relevance. Even when they are not entire populations, they are often 
convenience or  opportunity samples, rather than random samples. (For instance,the 
records in question may have been collected because they were the most easily 
measured, or covered a particular period of time.) 
In addition to problems arising from the way the data have been collected, we expect 
other distortions to occur in large data sets—including missing values, contamination, 
and corrupted data points. It is a rare data set that does not have such problems. Indeed, 
some elaborate modeling methods include, as part of the model, a component describing 
the mechanism by which missing data or other distortions arise. Alternatively, an 
estimation method such as the EM algorithm (described in chapter 8) or an imputation 
method that aims to generate artificial data with the same general distributional 
properties as the missing data might be used. Of course, all of these problems also arise 
in standard statistical applications (though perhaps to a lesser degree with small, 
deliberately collected data sets) but basic statistical texts tend to gloss over them. 

In summary, while data mining does overlap considerably with the standard exploratory 
data analysis techniques of statistics, it also runs into new problems, many of which are 
consequences of size and the non traditional nature of the data sets involved. 

1.7 Data Mining: Dredging, Snooping, and Fishing 

An introductory chapter on data mining would not be complete without reference to the 
historical use of terms such as ""data mining,"" ""dredging,"" ""snooping,"" and ""fishing."" In the 
1960s, as computers were increasingly applied to data analysis problems, it was noted 
that if you searched long enough, you could always find some model to fit a data set 
arbitrarily well. There are two factors contributing to this situation: the complexity of the 
model and the size of the set of possible models. 

Clearly, if the class of models we adopt is very flexible (relative to the size of the 
available data set), then we will probably be able to fit the available data arbitrarily well. 
However, as we remarked above, the aim may be to generalize beyond the available 
data; a model that fits well may not be ideal for this purpose. Moreover, even if the aim is 
to fit the data (for example, when we wish to produce the most accurate summary of data 
describing a complete population) it is generally preferable to do this with a simple 
model. To take an extreme, a model of complexity equivalent to that of the raw data 
would certainly fit it perfectly, but would hardly be of interest or value. 
Even with a relatively simple model structure, if we consider enough different models 
with this basic structure, we can eventually expect to find a good fit. For example, 
consider predicting a response variable, Y from a predictor variable X which is chosen 
from a very large set of possible variables, X1, ..., Xp, none of which are related to Y. By 
virtue of random variation in the data generating process, although there are no 
underlying relationships between Y and any of the X variables, there will appear to be 
relationships in the data at hand. The search process will then find the X variable that 
appears to have the strongest relationship to Y. By this means, as a consequence of the 
large search space, an apparent pattern is found where none really exists. The situation 
is particularly bad when working with a small sample size n and a large number p of 
potential X variables. Familiar examples of this sort of problem include the spurious 
correlations which are popularized in the media, such as the ""discovery"" that over the 
past 30 years when the winner of the Super Bowl championship in American football is 
from a particular league, a leading stock market index historically goes up in the 
following months. Similar examples are plentiful in areas such as economics and the 
social sciences, fields in which data are often relatively sparse but models and theories 
to fit to the data are relatively plentiful. For instance, in economic time-series prediction, 
there may be a relatively short time-span of historical data available in conjunction with a 
large number of economic indicators (potential predictor variables). One particularly 
humorous example of this type of prediction was provided by Leinweber (personal 
communication) who achieved almost perfect prediction of annual values of the well-

known Standard and Poor 500 financial index as a function of annual values from 
previous years for butter production, cheese production, and sheep populations in 
Bangladesh and the United States. 
The danger of this sort of ""discovery"" is well known to statisticians, who have in the past 
labelled such extensive searches ""data mining"" or ""data dredging""—causing these terms 
to acquire derogatory connotations. The problem is less serious when the data sets are 
large, though dangers remain even then, if the space of potential structures examined is 
large enough. These risks are more pronounced in pattern detection than model fitting, 
since patterns, by definition, involve relatively few cases (i.e., small sample sizes): if we 
examine a billion data points, in search of an unusual configuration of just 50 points, we 
have a good chance of detecting this configuration. 

There are no easy technical solutions to this problem, though various strategies have 
been developed, including methods that split the data into subsamples so that models 
can be built and patterns can be detected using one part, and then their validity can be 
tested on another part. We say more about such methods in later chapters. The final 
answer, however, is to regard data mining not as a simple technical exercise, divorced 
from the meaning of the data. Any potential model or pattern should be presented to the 
data owner, who can then assess its interest, value, usefulness, and, perhaps above all, 
its potential reality in terms of what else is known  about the data. 

1.8 Summary 

Thanks to advances in computers and data capture technology, huge data sets—
containing gigabytes or even terabytes of data—have been and are being collected. 
These mountains of data contain potentially valuable information. Th e trick is to extract 
that valuable information from the surrounding mass of uninteresting numbers, so that 
the data owners can capitalize on it. Data mining is a new discipline that seeks to do just 
that: by sifting through these databases, summarizing them, and finding patterns. 

Data mining should not be seen as a simple one-time exercise. Huge data collections 
may be analyzed and examined in an unlimited number of ways. As time progresses, so 
new kinds of structures and patterns may attract interest, and may be worth seeking in 
the data. 

Data mining has, for good reason, recently attracted a lot of attention: it is a new 
technology, tackling new problems, with great potential for valuable commercial and 
scientific discoveries. However, we should not expect it to provide answers to all 
questions. Like all discovery processes, successful data mining has an element of 
serendipity. While data mining provides useful tools, that does not mean that it will 
inevitably lead to important, interesting, or valuable results. We must beware of over-
exaggerating the likely outcomes. But the potential is there. 

1.9 Further Reading 
Brief, general introductions to data mining are given in Fayyad, Piatetsky-Shapiro, and 
Smyth (1996), Glymour et al. (1997), and a special issue of the Communications of the 
ACM, Vol. 39, No. 11. Overviews of certain aspects of predictive data mining are given 
by Adriaans and Zantige (1996) and Weiss and Indurkhya (1998). Witten and Franke 
(2000) provide a very readable, applications-oriented account of data mining from a 
machine learning (artificial intelligence) perspective and Han and Kamber (2000) is an 
accessible textbook written from a database perspective data mining. Th ere are many 
texts on data mining aimed at business users, notably Berry and Linoff (1997, 2000) that 
contain extensive practical advice on potential business applications of data mining. 
Leamer (1978) provides a general discussion of the dangers of data dredging, and Lovell 
(1983) provides a general review of the topic. From a statistical perspective. Hendry 
(1995, section 15.1) provides an econometrician's view of data mining. Hand et al. 
(2000) and Smyth (2000) present comparative discussions of data mining and statistics. 

 
 

 
 

 
 

 
 

Casti (1990, 192–193 and 439) provides a briefly discusses ""common folklore"" stock 
market predictors and coincidences. 

Chapter 2: Measurement and Data 
2.1 Introduction 
Our aim is to discover relationships that exist in the ""real world,"" where this may be the 
physical world, the business world, the scientific world, or some other conceptual 
domain. However, in seeking such relationships, we do not go out and look at that 
domain firsthand. Rather, we study data describing it. So first we need to be clear about 
what we mean by data. 

Data are collected by mapping entities in the domain of interest to symbolic 
representation by means of some measurement procedure, which associates the value 
of a variable with a given property of an entity. The relationships between objects are 
represented by numerical relationships between variables. These numerical 
representations, the data items, are stored in the data set; it is these items that are the 
subjects of our data mining activities. 
Clearly the measurement process is crucial. It underlies all subsequent data analytic and 
data mining activities. We discuss this process in detail in section 2.2. 
We remarked in chapter 1 that the notion of ""distance"" between two objects is 
fundamental. Section 2.3 outlines distance measures between two objects, based on the 
vectors of measurements taken on those objects. The raw results of measurements may 
or may not be suitable for direct data mining. Section 2.4 briefly comments on how the 
data might be transformed before analysis. 
We have already noted that we do not want our data mining activities simply to discover 
relationships that are mere artifacts of the way the data were collected. Likewise, we do 
not want our findings to be properties of the way the data are defined: discovering that 
people with the same surname often live in the same household would not be a major 
breakthrough. In section 2.5 we briefly introduce notions of the schema of data—the a 
priori structure imposed on the data. 
No data set is perfect, and this is particularly true of large data sets. Measurement error, 
missing data, sampling distortion, human mistakes, and a host of other factors corrupt 
the data. Since data mining is concerned with detecting unsuspected patterns in data, it 
is very important to be aware of these imperfections—we do not want to base our 
conclusions on patterns that merely reflect flaws in data collection or of the recording 
processes. Section 2.6 discusses quality issues in the context of measurements on 
cases or records and individual variables or fields. Section 2.7 discusses the quality of 
aggregate collections of such individuals (i.e., samples). 
Section 2.8 presents concluding remarks, and section 2.9 gives pointers to more detailed 
reading. 

2.2 Types of Measurement 

Measurements may be categorized in many ways. Some of the distinctions arise from 
the nature of the properties the measurements represent, while others arise from the use 
to which the measurements are put. 

To illustrate, we will begin by considering how we might measure the property WEIGHT. 
In this discussion we will denote a property by using uppercase letters, and the variable 
corresponding to it (the result of the mapping to numbers induced by the measurement 
operation) by lowercase letters. Thus a measurement of WEIGHT yields a value of 
weight. For concreteness, let us imagine we have a collection of rocks. 

The first thing we observe is that we can rank the rocks according to the WEIGHT 
property. We could do this, for example, by placing a rock on each pan of a weighing 
scale and seeing which way the scale tipped. On this basis, we could assign a number to 

each rock so that larger numbers corresponded to heavier rocks. Note that here only the 
ordinal properties of these numbers are relevant. The fact that one rock was assigned 
the number 4 and another was assigned the number 2 would not imply that the first was 
in any sense twice as heavy as the second. We could equally have chosen some other 
number, provided it was greater than 2, to represent the WEIGHT of the first rock. In 
general, any monotonic (order preserving) transformation of the set of numbers we 
assigned would provide an equally legitimate assignment. We are only concerned with 
the order of the rocks in terms of their WEIGHT property.  

We can take the rocks example further. Suppose we find that, when we place a large 
rock on one pan of the weighing scale and two small rocks on the other pan, the pans 
balance. In some sense the WEIGHT property of the two small rocks has combined to be 
equal to the WEIGHT property of the large rock. It turns out (this will come as no 
surprise!) that we can assign numbers to the rocks in such a way that not only does the 
order of the numbers correspond to the order observed from the weighing scales, but the 
sum of the numbers assigned to the two smaller rocks equals the number assigned to 
the larger rock. That is, the total weight of the two smaller rocks equals the weight of the 
larger rock. Note that even now the assignment of numbers is not unique. Suppose we 
had assigned the numbers 2 and 3 to the smaller rocks, and the number 5 to the larger 
rock. This assignment satisfies the ordinal and additive property requirements, but so too 
would the assignment of 4, 6, and 10 respectively. There is still some freedom in how we 
define the variable weight corresponding to the WEIGHT property. 
The point of this example is that our numerical representation reflects the empirical 
properties of the system we are studying. Relationships between rocks in terms of their 
WEIGHT property correspond to relationships between values of the measured variable 
weight. This representation is useful because it allows us to make inferences about the 
physical system by studying the numerical system. Without juggling sacks of rocks, we 
can see which sack contains the largest rock, which sack has the heaviest rocks on 
average, and so on. 
The rocks example involves two empirical relationships: the order of the rocks, in terms 
of how they tip the scales, and their concatenation property—the way two rocks together 
balance a third. Other empirical systems might involve less than or more than two 
relationships. The order relationship is very common; typically, if an empirical system has 
only one relationship, it is an order relationship. Examples of the order relationship are 
provided by the SEVERITY property in medicine and the PREFERENCE property in 
psychology. 

Of course, not even an order relationship holds with some properties, for example, the 
properties HAIR COLOR, RELIGION, and RESIDENCE OF PROGRAMMER, do not 
have a natural order. Numbers can still be used to represent ""values"" of the properties, 
(blond = 1, black = 2, brown = 3, and so on), but the only empirical relationship being 
represented is that the colors are different (and so are represented by different 
numbers). It is perhaps even more obvious here that the particular set of numbers 
assigned is not unique. Any set in which different numbers correspond to different values 
of the property will do. 

Given that the assignment of numbers is not unique, we must find some way to restrict 
this freedom—or else problems might arise if different researchers use different 
assignments. The solution is to adopt some convention. For the rocks example, we 
would adopt a basic ""value"" of the property WEIGHT, corresponding to a basic value of 
the variable weight, and defined measured values in terms of how many copies of the 
basic value are required to balance them. Examples of such basic values for the 
WEIGHT/weight system are the gram and pound. 
Types of measurement may be categorized in terms of the empirical relationships they 
seek to preserve. However, an important alternative is to categorize them in terms of the 
transformations that lead to other equally legitimate numerical representations. Thus, a 
numerical severity scale, in which only order matters, may be represented equally well 
by any numbers that preserve the order—numbers derived through a monotonic or 
ordinal transformation of the original ones. For this reason, such scales are termed 
ordinal scales. 

In the rocks example, the only legitimate transformations involved multiplying by a 
constant (for example, converting from pounds to grams). Any other transformation 
(squaring the numbers, adding a constant, etc.) would destroy the ability of the numbers 
to represent the order and concatenation property by addition. (Of course, other 
transformations may enable the empirical relationships to be represented by different 
mathematical operations. For example, if we transformed the values 2, 3, and 5 in the 
rocks example to e2, e3, and e5, we could represent the empirical relationship by 
multiplication: e2e3 = e5. However, addition is the most basic operation and is a favored 
choice.) Since with this type of scale multiplying by a constant leaves the ratios of values 
unaffected, such scales are termed ratio scales. 
In the other case we outlined above (the hair color example) any transformation was 
legitimate, provided it preserved the unique identity of the different numbers—it did not 
matter which of two numbers was larger, and addition properties were irrelevant. 
Effectively, here, the numbers were simply used as labels or names; such scales are 
termed nominal scales. 
There are other scale types, corresponding to different families of legitimate (or 
admissible) transformations. One is the interval scale. Here the family of legitimate 
transformations permit changing the units of measurement by multiplying by a constant, 
plus adding an arbitrary constant. Thus, not only is the unit of measurement arbitrary, but 
so also is the origin. Classic examples of such scales are conventional measures of 
temperature (Fahrenheit, Centigrade, etc.) and calendar time. 

It is important to understand the basis for different kinds of measurement scale so we 
can be sure that any patterns discovered during mining operations are genuine. To 
illustrate the dangers, suppose that two groups of three patients record their pain on an 
ordinal scale that ranges from 1 (no pain) to 10 (severe pain); one group of patients 
yields scores of 1, 2, and 6, while the other yields 3, 4, and 5. The mean of the first three 
is (1 + 2 + 6)/3 = 3, while that of the second three is 4. The second group has the larger 
mean. However, since the scale is purely ordinal any order-preserving transformation will 
yield an equally legitimate numerical representation. For example, a transformation of 
the scale so that it ranged from 1 to 20, with (1, 2, 3, 4, 5, 6) transformed to (1, 2, 3, 4, 5, 
12) would preserve the order relationships between the different levels of pain—if a 
patient A had worse pain than a patient B using the first scale, then patient A would also 
have worse pain than patient B using the second scale. Now, however, the first group of 
patients would have a mean score (1 + 2 + 12)/3 = 5, while the second group would still 
have a mean score 4. Thus, two equally legitimate numerical representations have led to 
opposite conclusions. The pattern observed using the first scale (one mean being larger 
than the other) was an artifact of the numerical representation adopted, and did not 
correspond to any true relationship among the objects (if it had, two equally legitimate 
representations could not have led to opposite conclusions). To avoid such problems we 
must be sure to only make statistical statements for which the truth value will be invariant 
under legitimate transformations of the measurement scales. In this example, we could 
make the statement that the median of the scores of the second group is larger than the 
median of the scores of the first group; this would remain true, whatever order-preserving 
transformation we applied. 
Up to this point, we have focussed on measurements that provide mappings in which the 
relationships between numbers in the empirical system being studied correspond to 
relationships between numbers in a numerical system. Because the mapping serves to 
represent relationships in an empirical system, this type of measurement is called 
representational. 
However, not all measurement procedures fit easily into this framework. In some 
situations, it is more natural to regard the measurement procedure as defining a property 
in question, as well as assigning a number to it. For example, the property QUALITY OF 
LIFE in medicine is often measured by identifying those components of human life that 
one regards as important, and then defining a way of combining the scores 
corresponding to the separate components (e.g., a weighted sum). EFFORT in software 
engineering is sometimes defined in a similar way, combining measures of the number of 
program instructions, a complexity rating, the number of internal and external documents 
and so forth. Measurement procedures that define a property as well as measure it are 
called operational or nonrepresentational procedures. The operational perspective on 

measurement was originally conceived in physics, around the start of the century, amid 
uneasiness about the reality of concepts such as atoms. The approach has gone on to 
have larger practical implications for the social and behavioral sciences. Since in this 
method the measurement procedure also defines the property, no question of legitimate 
transformations arises. Since there are no alternative numerical representations any 
statistical statements are permissible. 

Example 2.1  

 
One early attempt at measuring programming effort is given by Halstead (1977). In a given 
program if a is the number of unique operators, b is the number of unique operands, n is 
the number of total operator occurrences, and m is the total number of operand 
occurrences, then the programming effort is 

e = am(n + m) log(a + b)/2b. 

This is a nonrepresentational measurement, since it defines programming effort, as well as 
providing a way to measure it. 

 

 

 
 

One way of describing the distinction between representational and operational 
measurement is that the former is concerned with understanding what is going on in a 
system, while the latter is concerned with  predicting what is going on. The difference 
between understanding (or describing) a system and predicting its behavior crops up 
elsewhere in this book. Of course, the two aims overlap, but the distinction is a useful 
one. We can construct effective and valuable predictive systems that make no reference 
to the mechanisms underlying the process. For instance most people successfully drive 
automobiles or operate video recorders, without any idea of their inner workings. 

In principle, the mappings defined by the representational approach to measurement, or 
the numbers assigned by the operational approach, can take any values from the 
continuum. For example, a mapping could tell us that the length of the diagonal of a unit 
square is the square root of 2. However, in practice, recorded data are only 
approximations to such mathematical ideals. First, there is often unavoidable error in 
measurement (e.g., if you repeatedly measure someone's height to the nearest 
millimeter you will observe a distribution of values). Second, data are recorded to a finite 
number of decimal places. We might record the length of the diagonal of a unit square as 
1.4, or 1.41, or 1.414, or 1.4142, and so on, but the measure will never be exact. 
Occasionally, this kind of approximation can have an impact on an analysis. The effect is 
most noticeable when the approximation is crude (when the data are recorded to only 
very few decimal places). 

The above discussion provides a theoretical basis for measurement issues. However, it 
does not cover all descriptive measurement terms that have been introduced. Many 
other taxonomies for measurement scales have been described, sometimes based not 
on the abstract mathematical properties of the scales but rather on the sorts of data 
analytic techniques used to manipulate them. Examples of such alternatives include 
counts versus measurements; nominal, ordinal, and numerical scales; qualitative versus 
quantitative measurements; metrical versus categorical measurements; and grades, 
ranks, counted fractions, counts, amounts, and balances. In most cases it is clear what is 
intended by these terms. Ranks, for example, correspond to an operational assignment 
of integers to the particular entities in a given collection on the basis of the relative ""size"" 
of the property in question: the ranks are integers which preserve the order property. 

In data mining applications (and in this text), the scale types that occur most frequently 
are categorical scales in which any one-to-one transformation is allowed (nominal 
scales), ordered categorical scales, and numerical (quantitative or real-valued) scales. 

2.3 Distance Measures 

Many data mining techniques (for example, nearest neighbor classification methods, 
cluster analysis, and multidimensional scaling methods) are based on similarity 
measures between objects. There are essentially two ways to obtain measures of 
similarity. First, they can be obtained directly from the objects. For example, a marketing 
survey may ask respondents to rate pairs of objects according to their similarity, or 
subjects in a food tasting experiment may be asked to state similarities between flavors 
of ice-cream. Alternatively, measures of similarity may be obtained indirectly from 
vectors of measurements or characteristics describing each object. In the second case it 
is necessary to define precisely what we mean by ""similar,"" so that we can calculate 
formal similarity measures. 
Instead of talking about how similar two objects are, we could talk about how dissimilar 
they are. Once we have a formal definition of either ""similar"" or ""dissimilar,"" we can 
easily define the other by applying a suitable monotonically decreasing transformation. 
For example, if s(i, j) denotes the similarity and d(i, j) denotes the dissimilarity between 
objects i and j, possible transformations include d(i, j) = 1 - s(i, j) and 

. The term proximity is often used as a general term to denote 

either a measure of similarity or dissimilarity. 
Two additional terms—distance and metric—are often used in this context. The term 
distance is often used informally to refer to a dissimilarity measure derived from the 
characteristics describing the objects—as in Euclidean distance, defined below. A metric, 
on the other hand, is a dissimilarity measure that satisfies three conditions: 

1.  d(i, j) = 0 for all i and j, and d(i, j) = 0 if and only if i = j; 
2.  d(i, j) = d(j, i) for all i and j; and 
3.  d(i, j) = d(i, k) + d(k, j) for all i, j, and k. 
The third condition is called the triangle inequality. 
Suppose we have n data objects with p real-valued measurements on each object. We 
denote the vector of observations for the ith object by x(i) = (x1(i), x2(i), . . . , xp(i)), 1 = i = 
n, where the value of the kth variable for the ith object is xk(i). The Euclidean distance 
between the ith and jth objects is defined as 

(2.1)  

This measure assumes some degree of commensurability between the different 
variables. Thus, it would be effective if each variable was a measure of length (with the 
number p of dimensions being 2 or 3, it would yield our standard physical measure of 
distance) or a measure of weight, with each variable measured using the same units. It 
makes less sense if the variables are noncommensurate. For example, if one variable 
were length and another were weight, there would be no obvious choice of units; by 
altering the choice of units we would change which variables were most important as far 
as the distance was concerned. 
Since we often have to deal with data sets in which the variables are not commensurate, 
we must find some way to overcome the arbitrariness of the choice of units. A common 
strategy is to standardize the data by dividing each of the variables by its sample 
standard deviation, so that they are all regarded as equally important. (But note that this 
does not resolve the issue—treating the variables as equally important in this sense is 
still making an arbitrary assumption.) The standard deviation for the kth variable Xk can 
be estimated as 

(2.2)  

where µk is the mean for variable Xk, which (if unknown) can be estimated using the 
sample mean 

removes the effect of scale as captured by 

. Thus, 

. 

In addition, if we have some idea of the relative importance that should be accorded to 
each variable, then we can weight them (after standardization), to yield the weighted 
Euclidean distance measure 

(2.3)  

The Euclidean and weighted Euclidean distances are both additive, in the sense that the 
variables contribute independently to the measure of distance. This property may not 
always be appropriate. To take an extreme case, suppose that we are measuring the 
heights and diameters of a number of cups. Using commensurate units, we could define 
similarities between the cups in terms of these two measurements. Now suppose that we 
measured the height of each cup 100 times, and the diameter only once (so that for any 
given cup we have 101 variables, 100 of which have almost identical values). If we 
combined these measurements in a standard Euclidean distance calculation, the height 
would dominate the apparent similarity between the cups. However, 99 of the height 
measurements do not contribute anything to what we really want to measure; they are 
very highly correlated (indeed, perfectly, apart from measurement error) with the first 
height measurement. To eliminate such redundancy we need a data-driven method. One 
approach is to standardize the data, not just in the direction of each variable, as with 
weighted Euclidean distance, but also taking into account the covariances between the 
variables. 

Example 2.2  

 
Consider two variables X and Y, and assume we have n objects, with X taking the values 
x(1), . . . , x(n) and Y taking the values y(1), . . . , y(n).  
Then the sample covariance between X and Y is defined as 
(2.4)  

where  is the sample mean of the X values and  is the sample mean of the Y values. 
The covariance is a measure of how X and Y vary together: it will have a large positive 
value if large values of X tend to be associated with large values of Y and small values of X 
with small values of Y. If large values of X tend to be associated with small values of Y, it 
will take a negative value. 
More generally, with p variables we can construct a p × p matrix of covariances, in which 
the element (k, l) is the covariance between the kth and lth variables. From the definition of 
covariance above, we can see that such a matrix (a co-variance matrix) must be 
symmetric. 
The value of the covariance depends on the ranges of X and Y. This dependence can be 
removed by standardizing, dividing the values of X by their standard deviation and the 
values of Y by their standard deviation. The result is the sample correlation coefficient ?(X, 
Y) between X and Y: 
(2.5)  

In the same way that a covariance matrix can be formed if there are p variables, a p × p 
correlation matrix can be formed in the same manner. Figure 2.1 shows a pixel image of a 
correlation matrices for an 11-dimensional data set on housing-related variables across 
different Boston suburbs. From the matrix we can clearly see structure in terms of how 
different variables are correlated. For example, variables 3 and 4 (relating to business 
acreage and presence of nitrous oxide) are each highly negatively correlated with variable 
2 (the percent of large residential lots in the suburb) and positively correlated with each 
other. Variable 5 (average number of rooms) is positively correlated with variable 11 
(median home value) (i.e., larger houses tend to be more valuable). Variables 8 and 9 (tax 
rates and highway accessibility) are also highly correlated. 

 

Figure 2.1: A Sample Correlation Matrix Plotted as a Pixel Image. White Corresponds to +1 
and Black to -1. The Three Rightmost Columns Contain Values of -1, 0, and +1 
(Respectively) to Provide a Reference for Pixel Intensities. The Remaining 11 × 11 Pixels 
Represent the 11 × 11 Correlation Matrix. The Data Come From a well-known Data Set in the 
Regression Research Literature, in Which Each Data Vector is a Suburb of Boston and Each 
Variable Represents a Certain General Characteristic of a Suburb. The Variable Names are 
(1) Per-Capita Crime Rate, (2) Proportion of Area Zoned for Large Residential Lots, (3) 
Proportion of Non-Retail Business Acres, (4) Nitric Oxide Concentration, (5) Average Number 
of Rooms Perdwelling, (6) Proportion of Pre-1940 Homes, (7) Distance to Retail Centers 
Index, (8) Accessibility to Highways Index, (9) Property Tax Rate, (10) Pupil-to-Teacher Ratio, 
and (11) Median Value of Owner-Occupied Homes.  
Note that covariance and correlation capture linear dependencies between variables (they 
are more accurately termed linear covariance and linear correlation). Consider data points 
that are uniformly distributed around a circle in two dimensions (X and Y), centered at the 
origin. The variables are clearly dependent, but in a nonlinear manner and they will have 
zero linear correlation. Thus, independence implies a lack of correlation, but the reverse is 
not generally true. We will have more to say about independence in chapter 4. 

 

 

Recall again our coffee cup example with 100 measurements of height and one 
measurement of width. We can discount the effect of the 100 correlated variables by 
incorporating the covariance matrix in our definition of distance. This leads to the 
Mahalanobis distance between two p-dimensional measurements x(i) and x(j), defined 
as: 
(2.6)  

where T represents the transpose, S is the p × p sample covariance matrix, and S-1 
standardizes the data relative to S. Note that although we have been thinking about our 
p-dimensional measurement vectors x(i) as rows in our data matrix, the convention in 
matrix algebra is to treat these as p × 1 column vectors (we can still visualize our data 
matrix as being an n × p matrix). Entry (k, l) of S is defined between variable Xk and Xl, 
as in equation 2.5. Thus, we have a  p × 1 vector transposed (to give a 1 × p vector), 
multiplied by the  p × p matrix S-1, multiplied by a p × 1 vector, yielding a scalar distance. 
Of course, other matrices could be used in place of S. Indeed, the statistical frameworks 
of canonical variates analysis and discriminant analysis use the average of the 
covariance matrices of different groups of cases. 
The Euclidean metric can also be generalized in other ways. For example, one obvious 
generalization is to the Minkowski or L? metric: 

(2.7)  

where ? = 1. Using this, the Euclidean distance is the special case of ? = 2. The  L1 metric 
(also called the  Manhattan or city-block metric) can be defined as 

(2.8)  

The case ? ? 8 yields the L8  metric 

 

There is a huge number of other metrics for quantitative measurements, so the problem 
is not so much defining one but rather deciding which is most appropriate for a particular 
situation. 
For multivariate binary data we can count the number of variables on which two objects 
take the same or take different values. Consider table 2.1, in which all p variables 
defined for objects i and j take values in {0, 1}; the entry n1, 1 in the box for i = 1 and j = 1 
denotes that there are n1, 1 variables such that i and j both have value 1. 
Table 2.1: A Cross-Classification of Two Binary Variables.   

  

i = 1 

i = 0 

j = 
1 
n1, 
1  
n0, 
1  

j = 
0 
n1, 
0  
n0, 
0  

With binary data, rather than measuring the dissimilarities between objects, we often 
measure the similarities. Perhaps the most obvious measure of similarity is the simple 
matching coefficient, defined as 

(2.9)  

the proportion of the variables on which the objects have the same value, where n1,1 + 
n1,0 + n0,1 + n0,0 = p, the total number of variables. Sometimes, however, it is 
inappropriate to include the (0,0) cell (or the (1,1) cell, depending on the meaning of 0 
and 1). For example, if the variables are scores of the presence (1) or absence (0) of 
certain properties, we may not care about all the irrelevant properties had by neither 
object. (For instance, in vector representations of text documents it may be not be 
relevant that two documents do not contain thousands of specific terms). This 
consideration leads to a modification of the matching coefficient, the Jaccard coefficient, 
defined as 

(2.10)  

The Dice coefficient extends this argument. If (0,0) matches are irrelevant, then (0,1) and 
(1,0) mismatches should lie between (1,1) matches and (0,0) matches in terms of 
relevance. For this reason the number of (0,1) and (1,0) mismatches should be multiplied 
by a half. This yields 2n1,1/(2n1,1 + n1,0 + n0,1). As with quantitative data, there are many 
different measures for multivariate binary data—again the problem is not so much 
defining such measures but choosing one that possesses properties that are desirable 
for the problem at hand. 
For categorical data in which the variables have more than two categories, we can score 
1 for variables on which the two objects agree and 0 otherwise, expressing the sum of 
these as a fraction of the possible total p. If we know about the categories, we might be 
able to define a matrix giving values for the different kinds of disagreement. 

Additive distance measures can be readily adapted to deal with mixed data types (e.g., 
some binary variables, some categorical, and some quantitative) since we can add the 
contributions from each variable. Of course, the question of relative standardization still 
arises. 

 

 

2.4 Transforming Data 
Sometimes raw data are not in the most convenient form and it can be advantageous to 
modify them prior to analysis. Note that there is a duality between the form of the model 
and the nature of the data. For example, if we speculate that a variable Y is a function of 
the square of a variable X, then we either could try to find a suitable function of X2, or we 
could square X first, to U = X2, and fit a function to U. The equivalence of the two 
approaches is obvious in this simple example, but sometimes one or other can be much 
more straightforward. 

Example 2.3  

 
Clearly variable V1 in figure 2.2 is nonlinearly related to variable V2. However, if we work 
with the reciprocal of V2, that is, V3 = 1/V2, we obtain the linear relationship shown in  figure 
2.3. 

Figure 2.2: A Simple Nonlinear Relationship between Variable V1 and V2. (In These and 
Subsequent Figures V1 and V2 are on the X and Y Axes Respectively).  

 

Figure 2.3: The Data of Figure 2.2 after the Simple Transformation of V2 to 1/V2.  

 

 

 

Sometimes, especially if we are concerned with formal statistical inferences in which the 
shape of a distribution is important (as when running statistical tests, or calculating 
confidence intervals), we might want to transform the data so that they approximate the 
requisite distribution more closely. For example, it is common to take logarithms of 
positively skewed data (such as bank account sizes or incomes) to make the distribution 

more symmetric (so that it more closely approximates a normal distribution, on which 
many inferential procedures are based). 

Example 2.4  

 
In figure 2.4 not only are the two variables nonlinearly related, but the variance of V2 
increases as V1 increases. Sometimes inferences are based on an assumption that the 
variance remains constant (for example, in the basic model for regression analysis). In the 
case of these (artificial) data, a square root transformation of V2 yields the transformed data 
shown in figure 2.5. 

Figure 2.4: Another Simple Nonlinear Relationship. Here the Variance of V2 Increases as V1 
Increases.  

 

Figure 2.5: The Data of Figure 2.4 after a Simple Square Root Transformation of V2. Now the 
Variance of V2 is Relatively Constant as V1 Increases.  

 

 

 

Since our fundamental aim in data mining is exploration, we must be prepared to 
contemplate and search for the unsuspected. Certain transformations of the data may 
lead to the discovery of structures that were not at all obvious on the original scale. On 
the other hand, it is possible to go too far in this direction: we must be wary of creating 
structures that are simply arti-facts of a peculiar transformation of the data (see the 
example of the ordinal pain scale in section 2.2). Presumably, when this happens in a 
data mining context, the domain expert responsible for evaluating an apparent discovery 
will soon reject the structure. 
Note also that in transforming data we may sacrifice the way it represents the underlying 
objects. As described in section 2.2 the standard mapping of rocks to weights maps a 
physical concatenation operation to addition. If we nonlinearly transform the numbers 
representing the weights, using logarithms or taking square roots for example, the 

 
 

physical concatenation operation is no longer preserved. Caution—and common 
sense—must be exercised.  
Common data transformations include taking square roots, reciprocals, logarithms, and 
raising variables to positive integral powers. For data expressed as proportions, the logit 
transformation, 

, is often used. 

Some classes of techniques assume that the variables are categorical—that only a few 
(ordered) responses are possible. At an extreme, some techniques assume that 
responses are binary, with only two possible outcome categories. Of course continuous 
variables (those that can, at least in principle, take any value within a given interval) can 
be split at various thresholds to reduce them to categories. This sacrifices information, 
with the information loss increasing as the number of categories is reduced, but in 
practice this loss can be quite small. 

2.5 The Form of Data 
We mentioned in chapter 1 that data sets come in different forms; these forms are known 
as schemas. The simplest form of data (and the only form we have discussed in any 
detail) is a set of vector measurements on objects o(1), . . . , o(n). For each object we 
have measurements of p variables X1, . . . , Xp. Thus, the data can be viewed as a matrix 
with n rows and p columns. We refer to this standard form of data as a data matrix, or 
simply standard data. We can also refer to the data set as a table. 
Often there are several types of objects we wish to analyze. For example, in a payroll 
database, we might have data both about employees, with variables name, department -
name, age, and salary, and about departments with variables department-name, budget 
and manager. These data matrices are connected to each other by the occurrence of the 
same (categorical) values in the department-name fields and in the fields name and 
manager. Data sets consisting of several such matrices or tables are called 
multirelational data. 

In many cases multirelational data can be mapped to a single data matrix or table. For 
example, we could join the two data tables using the values of the variable department-
name. This would give us a data matrix with the variables name, department -name, age, 
salary, budget (of the department), and manager (of the department). The possibility of 
such a transformation seems to suggest that there is no need to consider multirelational 
structures at all since in principle we could represent the data in one large table or 
matrix. However, this way of joining the data sets is not the only possibility: we could also 
create a table with as many rows as there are departments (this would be useful if we 
were interested in getting information about the departments, e.g., determining whether 
there was a dependence between the budget of a department and the age of the 
manager). Generally no single table best captures all the information in a multirelational 
data set. More important, from the point of view of efficiency in storage and data access, 
""flattening"" multirelational data to form a single large table may involve the needless 
replication of numerous values. 

Some data sets do not fit well into the matrix or table form. A typical example is a time 
series, in which consecutive values correspond to measurements taken at consecutive 
times, (e.g., measurements of signal strength in a waveform, or of responses of a patient 
at a series of times after receiving medical treatment). We can represent a time series 
using two variables, one for time and one for the measurement value at that time. This is 
actually the most natural representation to use for storing the time series in a database. 
However, representing the data as a two-variable matrix does not take into account the 
ordered aspect of the data. In analyzing such data, it is important to recognize that a 
natural order does exist. It is common, for example, to find that neighboring observations 
are more closely related (more highly correlated) than distant observations. Failure to 
account for this factor could lead to a poor model. 
A string is a sequence of symbols from some finite alphabet. A sequence of values from 
a categorical variable is a string, and so is standard English text, in which the values are 
alphanumeric characters, spaces, and punctuation marks. Protein and DNA/RNA 
sequences are other examples. Here the letters are individual proteins (note that a string 

representation of a protein sequence is a 2-dimensional view of a 3-dimensional 
structure). A string is another data type that is ordered and for which the standard matrix 
form is not necessarily suitable. 
A related ordered data type is the event-sequence. Given a finite alphabet of categorical 
event types, an event-sequence is a sequence of pairs of the form {event, occurrence 
time}. This is quite similar to a string, but here each item in the sequence is tagged with 
an occurrence time. An example of an event-sequence is a telecommunication alarm log, 
which includes a time of occurrence for each alarm. More complicated event-sequences 
include transaction data (such as records of retail or financial transactions), in which 
each transaction is time-stamped and the events themselves can be relatively complex 
(e.g., listing all purchases along with prices, department names, and so forth). 
Furthermore, there is no reason to restrict the concept of event sequences to categorical 
data; for example we could extend it to real-valued events occurring asynchronously, 
such as data from animal behavioral experiments or bursts of energy from objects in 
deep space. 

Of course, order may be imposed simply for logistic convenience: placing patient records 
in alphabetical order by name assists retrieval, but the fact that Jones precedes Smith is 
unlikely to have any impact on most data mining activities. Still, care must always be 
exercised in data mining. For example, records of members of the same family (with the 
same last name) would probably occur near one another in a data set, and they may 
have related properties. (We may find that a contagious disease tends to infect groups of 
people whose names are close together in the data set.) 
Ordered data are spread along a unidimensional continuum (per individual variable), but 
other data often lie in higher dimensions. Spatial, geographic, or image data are located 
in two and three dimensional spaces. It is important to recognize that some of the 
variables are part of the defining data schema in these examples: that is, some of the 
variables merely specify the coordinates of observations in the spaces. The discovery 
that geographical data lies in a two-dimensional continuum would not be very profound. 
A hierarchical structure is a more complex data schema. For example, a data set of 
children might be grouped into classes, which are grouped into years, which are grouped 
into schools, which are grouped into counties, and so on. This structure is obvious in a 
multirelational representation of the data, but can be harder to see in a single table. 
Ignoring this structure in data analysis can be very misleading. Research on statistical 
models for such multi-level data has been particularly active in recent years. A special 
case of hierarchical structures arises when responses to certain items on a questionnaire 
are contingent on answers to other questions: for instance the relevance of the question 
""Have you had a hysterectomy?"" depends on the answer to the question ""Are you male 
or female?"" 
To summarize, in any data mining application it is crucial to be aware of the schema of 
the data. Without such awareness, it is easy to miss important patterns in the data or, 
perhaps worse, to rediscover patterns that are part of the fundamental design of the 
data. In addition, we must be particularly careful about data schemas when sampling, as 
we will discuss in more detail in chapter 4. 

2.6 Data Quality for Individual Measurements 
The effectiveness of a data mining exercise depends critically on the quality of the data. 
In computing this idea is expressed in the familiar acronym GIGO—Garbage In, Garbage 
Out. Since data mining involves secondary analysis of large data sets, the dangers are 
multiplied. It is quite possible that the most interesting patterns we discover during a data 
mining exercise will have resulted from measurement inaccuracies, distorted samples or 
some other unsuspected difference between the reality of the data and our perception of 
it. 

It is convenient to characterize data quality in two ways: the quality of the individual 
records and fields, and the overall quality of the collection of data. We deal with each of 
these in turn. 

 
 

No measurement procedure is without the risk of error. The sources of error are infinite, 
ranging from human carelessness, and instrumentation failure, to inadequate definition 
of what it is that we are measuring. Measuring instruments can lead to errors in two 
ways: they can be inaccurate or they can be imprecise. This distinction is important, 
since different strategies are required for dealing with the different kinds of errors. 
A precise measurement procedure is one that has small variability (often measured by its 
variance). Using a precise process, repeated measurements on the same object under 
the same conditions will yield very similar values. Sometimes the word precision is taken 
to connote a large number of digits in a given recording. We do not adopt this 
interpretation, since such ""precision"" can all too easily be spurious, as anyone familiar 
with modern data analysis packages (which sometimes give results of calculations to 
eight or more decimal places) will know. 
An accurate measurement procedure, in contrast, not only possesses small variability, 
but also yields results close to what we think of as the true value. A measurement 
procedure may yield precise but inaccurate measurements. For example repeated 
measurements of someone's height may be precise, but if these were made while the 
subject was wearing shoes, the result would be inaccurate. In statistical terms, the 
difference between the mean of repeated measurements and the true value is the bias of 
a measurement procedure. Accurate procedures have small bias as well as small 
variance. 
Note that the concept of a ""true value"" is integral to the concept of accuracy. But this 
concept is rather more slippery than it might at first appear. Take a person's height, for 
example. Not only does it vary slightly from moment to moment —as the person breathes 
and as his or her heart beats— but it also varies over the course of a day (gravity pulls 
us down). Astronauts returning from extended tours in space, are significantly taller than 
when they set off (though they soon revert to their former height). Mosteller (1968) 
remarked that ""Today some scientists believe that true values do not exist separately 
from the measuring process to be used, and in much of social science this view can be 
amply supported. The issue is not limited to social science; in physics, complications 
arise from the different methods of measuring microscopic and macroscopic quantities 
such as lengths. On the other hand, because it suggests ways of improving 
measurement methods, the concept of true value is useful; since some methods come 
much nearer to being ideal than others, the better ones can provide substitutes for true 
values."" 
Other terms are also used to express these concepts. The reliability of a measurement 
procedure is the same as its precision. The former term is typically used in the social 
sciences whereas the latter is used in the physical sciences. This use of two different 
names for the same concept is not as unreasonable as it might seem, since the process 
of determining reliability is quite different from that of determining precision. In measuring 
the precision of an instrument, we can use that instrument repeatedly: assuming that 
during the course of the repeated applications the circumstances will not change much. 
Furthermore, we assume that the measurement process itself will not influence the 
system being measured. (Of course, there is a grey area here: as Mosteller noted, very 
small or delicate phenomena may indeed be perturbed by the measurement procedure.) 
In the social and behavioral sciences, however, such perturbation is almost inevitable: 
for instance a test asking a subject to memorize a list of words could not usefully be 
applied twice in quick succession. Effective retesting requires more subtle techniques, 
such as alternative-form testing (in which two alternative forms of the measuring 
instrument are used), split-halves testing (in which the items on a single test are split into 
two groups), and methods that assess internal consistency (giving the expected 
correlation of one test with another version that contains the same number of items). 
Earlier we described two factors contributing to the inaccuracy of a measurement. One 
was basic precision—the extent to which repeated measurements of the same object 
gave similar results. The other was the extent to which the distribution of measurements 
was centered on the true value. While precision corresponds to reliability, the other 
component corresponds to validity. Validity is the extent to which a measurement 
procedure measures what it is supposed to measure. In many areas—including software 
engineering and economics—careful thought is required to construct metrics that tap the 
underlying concepts we want to measure. If a measurement procedure has poor validity, 
any conclusions we draw from it about the target phenomena will be at best dubious and 

 
 

at worst positively misleading. This is especially true in feedback situations, where action 
is taken on the basis of measurements. If the measurements are not tapping the 
phenomenon of interest, such actions could lead the system to depart even further from 
its target state. 

2.7 Data Quality for Collections of Data 
In addition to the quality of individual observations, we need to consider the quality of 
collections of observations. Much of statistics and data mining is concerned with 
inference from a sample to a population, that is, how, on the basis of examining just a 
fraction of the objects in a collection, one can infer things about the entire population. 
Statisticians use the term parameter to refer to descriptive summaries of populations or 
distributions of objects (more generally, of course, a parameter is a value that indexes a 
family of mathematical functions). Values computed from a sample of objects are called 
statistics, and appropriately chosen statistics can be used as estimates of parameters. 
Thus, for example, we can use the average of a sample as an estimate of the mean 
(parameter) of an entire population or distribution. 

Such estimates are useful only if they are accurate. As we have just noted, inaccuracies 
can occur in two ways. Estimates from different samples might vary greatly, so that they 
are unreliable: using a different sample might have led to a very different estimate. Or 
the estimates might be biased, tending to be too large or too small. In general, the 
precision of an estimate (the extent to which it would vary from sample to sample) 
increases with increasing sample size; as resources permit, we can reduce this 
uncertainty to an acceptable value. Bias, on the other hand, is not so easily diminished. 
Some estimates are intrinsically biased, but do not cause a problem because the bias 
decreases with increasing sample size. Of more significance in data mining are biases 
arising from an inappropriate sample. If we wanted to calculate the average weight of 
people living in New York, it would obviously be inadvisable to restrict our sample to 
women. If we did this, we would probably underestimate the average. Clearly, in this 
case, the population from which our sample is drawn (women in New York) is not the 
population to which we wish to generalize (everyone in New York). Our sampling frame, 
the list of people from which we will draw our sample, does not match the population 
about which we want to make an inference. This is a simple example—we were able to 
clearly identify the population from which the sample was drawn (women in New York). 
Difficulties arise when it is less obvious what the effect of the incorrect sampling frame 
will be. Suppose, for example, that we drew our sample from people working in offices. 
Would this lead to biased estimates? Maybe the sexes are disproportionately 
represented in offices. Maybe office workers have a tendency to be heavier than average 
because of their sedentary occupation. There are many reasons why such a sample 
might not be representative of the population we aim to study. The concept of 
representativeness is key to the ability to make valid inferences, as is the concept of a 
random sample. We discuss the need for random samples, as well as strategies for 
drawing such samples, in chapter 4. 
Because we often have no control over the way the data are collected, quality issues are 
particularly important in data. Our data set may be a distorted sample of the population 
we wish to describe. If we know the nature of this distortion then we might be able to 
allow for it in our inferences, but in general this is not the case and inferences must be 
made with care. The terms opportunity sample and convenience sample are sometimes 
used to describe samples that are not properly drawn from the population of interest. The 
sample of office workers above would be a convenience sample—it is much more 
convenient to sample from them than to sample from the whole population of New York. 
Distortions of a sample can occur for many reasons, but the risk is especially grave when 
humans are involved. The effects can be subtle and unexpected: for instance, in large 
samples, the distribution of stated ages tends to cluster around integers ending with 0 or 
5—just the sort of pattern that data mining would detect as potentially interesting. 
Interesting it may be, but will probably be of no value in our analysis. 

A different kind of distortion occurs when customers are selected through a chain of 
selection steps. With bank loans, for example, an initial population of potential customers 

is contacted (some reply and some do not), those who reply are assessed for 
creditworthiness (some receive high scores and some do not), those with high scores 
are offered a loan (some accept and some do not), those who take out a loan are 
followed up (some are good customers, paying the installments on time, and others are 
not), and so on. A sample drawn at any particular stage would give a distorted 
perspective on the population at an earlier stage. 
In this example of candidates for bank loans, the selection criteria at each step are 
clearly and explicitly stated but, as noted above, this is not always the case. For 
example, in clinical trials samples of patients are selected from across the country, 
having been exposed to different diagnostic practices and perhaps different previous 
treatments in different primary care facilities. Here the notion of taking a ""random sample 
from a well-defined population"" makes no sense. This problem is compounded by the 
imposition of inclusion/exclusion criteria: perhaps the patients must be male, aged 
between 18 and 50, with a primary diagnosis of the disease in question made no longer 
than two years ago, and so on. (It is hardly surprising in this context, that the sizes of 
effects recorded in clinical trials are typically larger than those found when the treatments 
are applied more widely. On the other hand it is reassuring that the directions of the 
effects do normally generalize in this way.) 
In addition to sample distortion arising from a mismatch between the sample population 
and the population of interest other kinds of distortion arise. The aim of many data 
mining exercises is to make some prediction of what will happen in the future. In such 
cases it is important to remember that populations are not static. For instance the nature 
of a customers shopping at a certain store will change over time, perhaps because of 
changes in the social culture of the surrounding neighborhood, or in response to a 
marketing initiative, or for many other reasons. Much work on predictive methods has 
failed to take account of such population drift. Typically, the future performance of such 
methods is assessed using data collected at the same time as the data used to build the 
model—implicitly assuming that the distribution of objects used to construct the model is 
the same as that of future objects. Ideally, a more sophisticated model is required that 
can allow for evolution over time. In principle, population drift can be modeled, but in 
practice this may not be easy. 
An awareness of the risks of using distorted samples is vital to valid data mining, but not 
all data sets are samples from the population of interest. Often the data set comprises 
the entire population, but is so large that we wish to work with a sample from it. We can 
formulate valid descriptions of the population represented in such a data set, to any 
degree of accuracy, provided the sample is properly chosen. Of course, technical 
difficulties may arise, as we discuss in more detail in chapter 4, when working with data 
sets that have complex structures and that might be dispersed over many different 
databases. In chapter 4, we explain how to draw samples from a data set in such a way 
that we can make accurate inferences about the overall population of values in the data 
set, but we restrict our discussion to the cases in which the actual drawing of a sample is 
straightforward, once we know which cases should be included. 

Distortion of samples can be viewed as a special case of incomplete data, one in which 
entire records are missing from what would otherwise be a representative sample. Data 
can also be missing in other ways. In particular, individual fields may be missing from 
records. In some ways this is not as serious as the situation described above. (At least 
here, one can see that the data are missing!) Still, significant problems may arise from 
incomplete data. The fundamental question is ""Why are the data missing?"" Was there 
information in the missing data that is not present in the data that have been recorded? If 
so, inferences based on the observed data are likely to be biased. In any incomplete 
data problem, it is crucial to be clear about the objectives of the analysis. In particular, if 
the aim is to make an inference only about the cases that have complete records, 
inferences based only on the complete cases is entirely valid. 
Outliers or anomalous observations represent another, quite different aspect of data 
quality. In many situations the objective of the data mining exercise is to detect 
anomalies: in fraud detection and fault detection those records that differ from the 
majority are precisely the ones that are of interest. In such cases we would use a pattern 
detection process (see chapters 6 and 13). On the other hand, if the aim is model 
building—constructing a global model to aid understanding of, or prediction from, the 

data—outliers may simply obscure the main points of the model. In this case we might 
want to identify and remove them before building our model. 
When observing only one variable, we can detect outliers simply by plotting the data—as 
a histogram, for example. Points that are far from the others will lie out in the tails. 
However, the situation becomes more interesting—and challenging—when multiple 
variables are involved. In this case, it is possible that each variable for a particular record 
has perfectly normal values, but the overall pattern of scores is abnormal. Consider the 
distribution of points shown in figure 2.6. Clearly there is an unusual point here, one that 
would immediately arouse suspicion if such a distribution were observed in practice. But 
the point stands out only because we produced the two dimensional plot. A one 
dimensional examination of the data would indicate nothing unusual at all about the point 
in question. 

Figure 2.6: A Plot of 200 Points From Highly Positively Correlated Bivariate Data (From a 
Bivariate Normal Distribution), With a Single Easily Identifiable Outlier.  

 

Furthermore, there may be highly unusual cases whose abnormality becomes apparent 
only when large numbers of variables are examined simultaneously. In such cases, a 
computer is essential to detection. 
Every large data set includes suspect data. Rather than promoting relief, a large data set 
that appears untarnished by incompleteness, distortion, measurement error, or other 
problems should invite suspicion. Only when we recognize and understand the 
inadequacies of the data can we take steps to alleviate their impact. Only then can we be 
sure that the discovered structures and patterns reflect what is really going on in the 
world. Since data miners rarely have control over the data collection processes, an 
awareness of the dangers that can arise from poor data is crucial. Hunter (1980) stated 
the risks succinctly: 

Data of a poor quality are a pollutant of clear thinking and rational decisionmaking. 
Biased data, and the relationships derived from such data, can have serious 
consequences in the writing of laws and regulations. 

And, we might add, they can have serious consequences in developing scientific 
theories, in unearthing commercially valuable information, in improving quality of life, and 
so on. 

2.8 Conclusion 
In this chapter we have restricted our discussion to numeric data. However, other kinds 
of data also arise. For example, text data is an important class of non-numeric data, 
which we discuss further in chapter 14. Sometimes the definition of an individual data 
item (and hence whether it is numeric or non-numeric) depends on the objectives of our 
analysis: in economic contexts, in which hundreds of thousands of time series are stored 

 
 

 
 

 
 

in databases, the data items might be entire time series, rather than the individual 
numbers within those series. 

Even with non-numeric data, numeric data analysis plays a fundamental role. Often non-
numeric data items, or the relationships between them, are reduced to numeric 
descriptions, which are subject to standard methods of analysis. For example, in text 
processing we might measure the number of times a particular word occurs in each 
document, or the probability that certain pairs of words appear in documents. 

2.9 Further Reading 
The magnum opus on representational measurement theory is the three volume work of 
Krantz et al. (1971), Suppes et al. (1989), and Luce et al. (1990). Roberts (1979) also 
outlines this approach. Dawes and Smith (1985) and Michell (1986, 1990) describe 
alternative approaches, including the operational approach. Hand (1996) explores the 
relationship between measurement theory and statistics. Some authors place their 
discussions of software metrics in a formal measurement theoretical context—see, for 
example, Fenton (1991). Anderberg (1973) includes a good discussion of similarity and 
dissimilarity measures. 
Issues of reliability and validity are often discussed in treatments of measurement issues 
in the social, behavioral, and medical sciences—see, for example, Dunn (1989) and 
Streiner and Norman (1995). Carmines and Zeller (1979) also discuss such issues. A 
key work on incomplete data and different types of missing data mechanisms is Little 
and Rubin (1987). The bank loan example of distorted samples is taken from Hand, 
McConway, and Stanghellini (1997). Goldstein (1995) is a key work on multilevel 
modeling. 

Chapter 3: Visualizing and Exploring Data 
3.1 Introduction 

This chapter explores visual methods for finding structures in data. Visual methods have 
a special place in data exploration because of the power of the human eye/brain to 
detect structures—the product of aeons of evolution. Visual methods are used to display 
data in ways that capitalize upon the particular strengt hs of human pattern processing 
abilities. This approach lies at quite the opposite end of the spectrum from methods for 
formal model building and for testing to see whether observed data could have arisen 
from a hypothesized data generating structure. Visual methods are important in data 
mining because they are ideal for sifting through data to find unexpected relationships. 
On the other hand, they do have their limitations, particularly, as we illustrate below, with 
very large data sets. 
Exploratory data analysis can be described as data-driven hypothesis generation. We 
examine the data, in search of structures that may indicate deeper relationships between 
cases or variables. This process stands in contrast to hypothesis testing (we use the 
phrase here in an informal and general sense; more formal methods are described in 
chapter 4) which begins with a proposed model or hypothesis and undertakes statistical 
manipulations to determine the likelihood that the data arose from such a model. The 
phrase data based in the above description indicates that it is the patterns in the data 
that give rise to the hypotheses—in contrast to situations in which hypotheses are 
generated from theoretical arguments about underlying mechanisms. This distinction has 
implications for the legitimacy of subsequent testing of the hypotheses. It is closely 
related to the issues of overfitting discussed in chapter 7 (and again in 10 and 11). A 
simple example will illustrate the problem.  

If we take 10 random samples of size 20 from the same population, and measure the 
values of a single variable, the random samples will have different means (just by virtue 
of random variability). We could compare the means using formal tests. Suppose, 
however, we took only the two samples giving rise to the smallest and largest means, 

ignoring the others. A test of the difference between these means might well show 
significance. If we took 100 samples, instead of 10, then we would be even more likely to 
find a significant difference between the largest and the smallest means. By ignoring the 
fact that these are the largest and smallest in a set of 100, we are biasing the analysis 
toward detecting a difference—even though the samples were generated from the same 
population. 
In general, when searching for patterns, we cannot test whether a discovered pattern is a 
real property of the underlying distribution (as opposed to a chance property of the 
sample) without taking into account the size of the search—the number of possible 
patterns we have examined. The informal nature of exploratory data analysis makes this 
very difficult—it is often impossible to say how many patterns have been examined. For 
this reason researchers often use a separate data set, obtained from the same source as 
the first, to conduct formal testing for the existence of any pattern. (Alternatively, they 
may use some kind of sophisticated method such as cross-validation and sample re-use, 
as described in chapter 7.) 

This chapter examines informal graphical data exploration methods, which have been 
widely used in data analysis down through the ages. Early books on statistics contain 
many such methods. They were often more practical than lengthy, number crunching 
alternatives in the days before computers. However, something of a revolution has 
occurred in recent years, and now such methods are even more widely used. As with the 
bulk of the methods decribed in this book, the revolution has been driven by the 
computer: computers enable us to view data in many different ways, both quickly and 
easily, and have led to the development of extremely powerful data visualization tools. 
We begin the discussion in section 3.2 with a description of simple summary statistics for 
data. Section 3.3 discusses visualization methods for exploring distributions of values of 
single variables. Such tools, at least for small data sets, have been around for centuries, 
but even here progress in computer technology has led to the development of novel 
approaches. More-over, even when using univariate displays, we often want 
simultaneous univariate displays of many variables, so we need concise displays that 
readily convey the main features of distributions.  
Section 3.4 moves on to methods for displaying the relationships between pairs of 
variables. Perhaps the most basic form is the scatterplot. Due to the sizes of the data 
sets often encountered in data mining applications, scatterplots are not always 
enlightening—the diagram may be swamped by the data. Of course, this qualification 
can also apply to other graphical displays. 
Moving beyond variable pairs, section 3.5 describes some of the tools used to examine 
relationships between multiple variables. No method is perfect, of course: unless a very 
rare relationship holds in the data, the relationship between multiple variables cannot be 
completely displayed in two dimensions. 
Principal components analysis is illustrated in section 3.6. This method can be regarded 
as a special (indeed, the most basic) form of multidimensional scaling analysis. These 
are methods that seek to represent the important structure of the data in a reduced 
number of dimensions. Section 3.7 discusses additional multidimensional scaling 
methods. 
There are numerous books on data visualization (see section 3.8) and we could not hope 
to examine all of the possibilities thoroughly in a single chapter. There are also several 
software packages motivated by an awareness of the importance of data vi sualization 
that have very powerful and flexible graphics facilities. 

3.2 Summarizing Data: Some Simple Examples 
We mentioned in earlier chapters that the mean is a simple summary of the average of a 
collection of values. Suppose that x(1), ..., x(n) comprise a set of n data values. The 
sample mean is defined as 

 
 

(3.1)  

(Note that we use µ to refer to the true mean of the population, and 
based estimate of this mean). The sample mean has the property that it is the value that 

to refer a sample-

is ""central"" in the sense that it minimizes the sum of squared differences between it and 
the data values. Thus, if there are n data values, the mean is the value such that the sum 
of n copies of it equals the sum of the data values. 
The mean is a measure of location. Another important measure of location is the median, 
which is the value that has an equal number of data points above and below it. (Easy if n 
is an odd number. When there is an even number it is usually defined as halfway 
between the two middle values.)  
The most common value of the data is the mode. Sometimes distributions have more 
than one mode (for example, there may be 10 objects which take the value 3 on some 
variable, and another 10 which take the value 7, with all other values taken less often 
than 10 times) and are therefore called multimodal. 
Other measures of location focus on different parts of the distribution of data values. The 
first quartile is the value that is greater than a quarter of the data points. The third 
quartile is greater than three quarters. (We leave it to you to discover why we have not 
mentioned the second quartile.) Likewise, deciles and percentiles are sometimes used. 
Various measures of dispersion or  variability are also common. These include the 
standard deviation and its square, the variance. The variance is defined as the average 
of the squared differences between the mean and the individual data values: 

(3.2)  

Note that since the mean minimizes the sum of these squared differences, there is a 
close link between the mean and the variance. If µ is unknown, as is often the case in 
practice, we can replace µ above with 
, our data based estimate. When µ is replaced 
with  , to get an unbiased estimate (as discussed in chapter 4), the variance is estimated 
as 
(3.3)  

The standard deviation is the square root of the variance: 

(3.4)  

The interquartile range, common in some applications, is the difference between the third 
and first quartile. The range is the difference between the largest and smallest data 
point. 
Skewness measures whether or not a distribution has a single long tail and is commonly 
defined as 

(3.5)  

 
 

For example, the distribution of peoples' incomes typically shows the vast majority of 
people earning small to moderate amounts, and just a few people earning large sums, 
tailing off to the very few who earn astronomically large sums—the Bill Gateses of the 
world. A distribution is said to be right-skewed if the long tail extends in the direction of 
increasing values and left-skewed otherwise. Right-skewed distributions are more 
common. Symmetric distributions have zero skewness. 

3.3 Tools for Displaying Single Variables 

One of the most basic displays for univariate data is the histogram, showing the number 
of values of the variable that lie in consecutive intervals. With small data sets, histograms 
can be misleading: random fluctuations in the values or alternative choices for the ends 
of the intervals can give rise to very different diagrams. Apparent multimodality can arise, 
and then vanish for different choices of the intervals or for a different small sample. As 
the size of the data set increases, however, these effects diminish. With large data sets, 
even subtle features of the histogram can represent real aspects of the distribution. 
Figure 3.1 shows a histogram of the number of weeks during 1996 in which owners of a 
particular credit card used that card to make supermarket purchases (the label on the 
vertical axis has been removed to conceal commercially sensitive details). There is a 
large mode to the left of the diagram: most people did not use their card in a 

supermarket, or used it very rarely. The number of people who used the card a given 
number of times decreases rapidly with increases in the number of times. However, the 
relatively large number of people represented in this diagram allows us to detect another, 
much smaller mode toward the right hand end of the diagram. Apparently there is a 
tendency for people to make regular weekly trips to a supermarket, though this is 
reduced from 52 annual transactions, probably by interruptions such as holidays. 

Figure 3.1: Histogram of the Number of Weeks of the Year a Particular Brand of Credit Card 
was Used.  
Example 3.1  

 

 
Figure 3.2 shows a histogram of diastolic blood pressure for 768 females of Pima Indian 
heritage. This is one variable out of eight that were collected for the purpose of building 
classification models for forecasting the onset of diabetes. Th e documentation for this data 
set (available online at the UCI Machine Learning data archive) states that there are no 
missing values in the data. However, a cursory glance at the histogram reveals that about 
35 subjects have a blood pressure value of zero, which is clearly impossible if these 
subjects were alive when the measurements were taken (presumably they were). A 
plausible explanation is that the measurements for these 35 subjects are in fact missing, 
and that the value ""0"" was used in the collection of the data to code for ""missing."" This 
seems likely given that a number of the other variables (such as triceps-fold-skin-
thickness) also have zero-values that are physically impossible. 

Figure 3.2: Histogram of Diastolic Blood Pressure for 768 Females of Pima Indian Descent.  

 

The point here is that even though the histogram has limitations it is nonetheless often 
quite valuable to plot data before proceeding with more detailed modeling. In the case of 

the Pima Indians data, the histogram clearly reveals some suspicious values in the data 
that are incompatible with the physical interpretations of the variables being measured. 
Performing such simple checks on the data is always advisable before proceeding to use a 
data mining algorithm. Once we apply an algorithm it is unlikely that we will notice such 
data quality problems, and these problems may distort our analysis in an unpredictable 
manner.  

 

 

The disadvantages of histograms have also been tackled by smoothing estimates. One 
of the most widely used types is the kernel estimate. 
Kernel estimates smooth out the contribution of each observed data point over a local 
neighborhood of that point (we will revisit the kernel method again in chapter 9). 
Consider a single variable  X for which we have measured values {x(1), ..., x(n)}. The 
contribution of data point x(i) to the estimate at some point x* depends on how far apart 
x(i) and  x* are. The extent of this contribution is dependent upon on the shape of the 
kernel function adopted and the width accorded to it. Denoting the kernel function by K 
and its width (or bandwidth) by h, the estimated density at any point x is 

(3.6)  

where ?K(t)dt = 1 to ensure that the estimate ƒ(x) itself integrates to 1 (i.e., is a proper 
density) and where the kernel function K is usually chosen to be a smooth unimodal 
function with a peak at 0. The quality of a kernel estimate depends less on the shape of 
K than on the value of h. 
A common form for K is the Normal (Gaussian) curve, with h as its spread parameter 
(standard deviation), i.e., 

(3.7)  

where C is a normalization constant and t = x - x(i) is the distance of the query point x to 
data point x(i). The bandwidth h is equivalent to s, the standard deviation (or width) of 
the Gaussian kernel function. 
There are formal methods for optimizing the fit of these estimates to the unknown 
distribution that generated the data, but here our interest is in graphical procedures. For 
our purposes the attraction of such estimates is that by varying h, we can search for 
peculiarities in the shape of the sample distribution. Small values of h lead to very spiky 
estimates (not much smoothing at all), while large values lead to oversmoothing. The 
limits at each extreme of h are the empirical distribution of the data points (i.e., ""delta 
functions"" on each data point x(i)) as h ?  0, and a uniform flat distribution as  h ?  8 .  
These limits correspond to the extremes of total commitment to the data (with no mass 
anywhere except at the observed data points), versus completely ignoring the observed 
data. 
Figure 3.3 shows a kernel estimate of the density of the weights of 856 elderly women 
who took part in a study of osteoporosis. The distribution is clearly right skewed and 
there is a hint of multimodality. Certainly the assumption often made in classical 
statistical work that distributions are normal does not apply in this case. (This is not to 
say that statistical techniques nominally based on that assumption might not still be valid. 
Often the arguments are asymptotic—based on normality arising from the central limit 
theorem. In this case, the assumption that the sample mean of 856 subjects would vary 
from sample to sample according to a normal distribution would be reasonable for 
practical purposes.) 

Figure 3.3: Kernel Estimate of the Weights (in Kg) of 856 Elderly Women.  

Figure 3.4 shows what happens when a larger value is used for the smoothing 
parameter h. Which of the two kernel estimates is ""better"" is a difficult question to 
answer. Figure 3.4 is more conservative in that less credence is given to local 
(potentially random) fluctuations in the observed data values. 

 

Figure 3.4: As Figure 3.3, but with More Smoothing.  

 

Although this section focuses on displaying single variables, it is often desirable to 
display different groups of scores on a single variable separately, so that the groups may 
be compared. (Of course, we can think of this as a two-variable situation, in which one of 
the variables is the grouping factor.) Histograms, kernel plots, and other unidimensional 
displays can be used separately for each group. However, this can become unwieldy if 
there are more than two or three groups. In such cases a useful alternative display is the 
box and whisker plot. 
Although various versions of box and whisker plots exist, the essential ideas are the 
same. A box containing which the bulk of the data is defined—for example, the interval 
between the first and third quartiles. A line across this box indicates some measure of 
location—often the median of the data. Whiskers project from the ends of the box to 
indicate the spread of the tails of the empirical distribution. 
We illustrate the boxplot using a subset of the diabetes data set from figure 3.2. Figure 
3.5 shows four panels of box plots, each containing a separate boxplot for each of the 
two classes in the data, healthy (1) and  diabetic (2).The diagrams show clearly how 
mean, dispersion, and skewness vary with values of the grouping variable. 

 

Figure 3.5: Boxplots on Four Different Variables From the Pima Indians Diabetes Data Set. 
For Each Variable, a Separate Boxplot is Produced for the Healthy Subjects (Labeled 1) and 
the Diabetic Subjects (Labeled 2). The Upper and Lower Boundaries of Each Box Represent 
the Upper and Lower Quartiles of the Data Respectively. The Horizontal Line within Each Box 
Represents the Median of the Data. The Whiskers Extend 1.5 Times the Interquartile Range 
From the End of Each Box. All Data Points Outside the Whiskers are Plotted Individually 
(Although Some Overplotting is Present, e.g., for Values of 0).  
 

3.4 Tools for Displaying Relationships between Two 
Variables 
The scatterplot is a standard tool for displaying two variables at a time. Figure 3.6 shows 
the relationship between two variables describing credit card repayment patterns (the 
details are confidential). It is clear from this diagram that the variables are strongly 
correlated—when one value has a high (low) value, the other variable is likely to have a 
high (low) value. However, a significant number of people depart from this pattern; 
showing high values on one of the variables and low values on the other. It might be 
worth investigating these individuals to find out why they are unusual. 

Figure 3.6: A Standard Scatterplot for Two Banking Variables.  

Unfortunately, in data mining, scatterplots are not always so useful. If there are too many 
data points we will find ourselves looking at a purely black rectangle. Figure 3.7 

 

illustrates this sort of problem. This shows a scatterplot of 96,000 points from a study of 
bank loans. Little obvious structure is discernible, although it might appear that later 
applicants in general are older. On the other hand, the apparent greater vertical 
dispersion toward the right end of the diagram could equally be caused by a greater 
number of samples on the right side. In fact, the linear regression fit to these data has a 
very small but highly significant downward slope. 

Figure 3.7: A Scatterplot of 96,000 Cases, with Much Overprinting. Each Data Point 
Represents an Individual Applicant for a Loan. The Vertical Axis Shows the Age of the 
Applicant, and the Horizontal Axis Indicates the Day on Which the Application was Made.  

Even when the situation is not quite so extreme, scatterplots with large numbers of 
points can conceal more than they reveal. Figure 3.8 plots the number of weeks a 
particular credit card was used to buy petrol (gasoline) in a given year against the 
number of weeks the card was used in a supermarket (each data point represents an 
individual credit card). There is clearly some correlation, but the actual correlation 0.482 
is much higher than it appears here. The diagram is deceptive because it conceals a 
great deal of overprinting in the bottom left corner—there are 10,000 customers 
represented here altogether. The bimodality shown in  figure 3.1 can also be discerned in 
this figure, though not as easily as in figure 3.1. 

 

 

Figure 3.8: Overprinting Conceals the Actual Strength of the Correlation.  

Another curious phenomenon is also apparent in figure 3.8. The distribution of the 
number of weeks the card was used in a petrol station is skewed for low values of the 
supermarket variable, but fairly uniform for high values. What could explain this? (Of 
course, bearing in mind the point above, this apparent phenomenon needs to be 
checked for overprinting.)  
Contour plots can help overcome some of these problems. Note that creating a contour 
plot in two dimensions effectively requires us to construct a two-dimensional density 
estimate, using something like a two-dimensional generalization of the kernel method of 
equation 3.6, again raising the issue of bandwidth selection but now in a two-dimensional 
context. A contour plot of the 96,000 points shown in figure 3.7 is given in  figure 3.9. 
Certain trends are clear from this display that cannot be discerned in  figure 3.7. For 

instance the density of points increases toward the right side of the diagram; the 
apparent increasing dispersion of the vertical axis is due to there being a greater 
concentration of points in that area. The vertical skewness of the data is also very 
evident in this diagram. The unimodality of the data, and the position of the single mode 
cannot be seen at all in  figure 3.7 but is quite clear in  figure 3.9. Note that since the 
horizontal axis in these plots is time, an alternative way to display the data is to plot 
contours of constant conditional probability density, as time progresses. 

Figure 3.9: A Contour Plot of the Data from Figure 3.7.  

 

Other standard forms of display can be used when one of the two variables is time, to 
show the value of the other variable as time progresses. This can be a very effective way 
of detecting trends and departures from expected or standard behaviour. Figure 3.10 
shows a plot of the number of credit cards issued in the United Kingdom from 1985 to 
1993 inclusive. A smooth curve has been fitted to the data to place emphasis on the 
main features of the relationship. It is clear that around 1990 something caused a break 
in a growth pattern that had been linear up to that point. In fact, what happened was that 
in 1990 and 1991 annual fees were introduced for credit cards, and many users reduced 
their holding to a single card. 

 

Figure 3.10: A Plot of the Number of Credit Cards in Circulation in the United Kingdom, By 
Year.  

Figure 3.11 shows a plot of the number of miles flown by UK airlines, during each month 
from January 1963 to December 1970. There are several patterns immediately apparent 
from this display that conform with what one might expect to observe, such as the 
gradually increasing trend and the periodicity (with large peaks in the summer and small 
peaks around the new year). The plot also reveals an interesting bifurcation of the 
summer peak, suggesting a tendency for travelers to favor the early and late summer 
over the middle period. 

Figure 3.11: Patterns of Change over Time in the Number of Miles Flown by UK Airlines in 
the 1960s.  

 

Figure 3.12 provides a third example of the power of plots in which time is one of the two 
variables. From February to June 1930, an experiment was carried out in Lanarkshire, 
Scotland to investigate whether adding milk to children's diets had an effect on 
""physique, general health and increasing mental alertness"" (Leighton and McKinlay, 
1930). In this study 20,000 children were allocated to one of three groups; 5000 of the 
children received three-quarters of a pint of raw milk per day, 5000 received three-
quarters of a pint of pasteurized milk per day, and 10,000 formed a control group 
receiving no dietary milk supplement. The children were weighed at the start of the 
experiment and again four months later. Interest lay in whether there was differential 
growth between the three groups. 

Figure 3.12: Weight Changes Over Time in a Group of 10,000 School Children in the 1930s. 
The Steplike Pattern in the Data Highlights a Problem with the Measurement Process.  

 

Figure 3.12 plots the mean weight of the control group of girls against the mean age of 
the group they are in. The first point corresponds to the youngest age group (mean age 
5.5 years) at the start of the experiment, and the second point corresponds to this group 
four months later. The third and fourth points correspond to the second age group, and 
so on. The points are connected by lines to make the shape easier to discern. Similar 
shapes are apparent for all groups in the experiment. 

The plot immediately reveals an unexpected pattern that cannot be seen from a table of 
the data. We would expect a smooth plot, but there are clear steps evident here. It 
seems that each age group does not gain as much weight as expected. There are 
various possible explanations for this shape. Perhaps children grow less during the early 
months of the year than during the later ones. However, similar plots of heights show no 
such intermittent growth, so we need a more elaborate explanation in which height 
increases uniformly but weight increases in spurts. Another possible explanation arises 
from the fact that the children were weighed in their clothes. The report does say, ""All of 
the children were weighed without their boots or shoes and wearing only their ordinary 
outdoor clothing. The boys were made to turn out the miscellaneous collection of articles 

 
 

that is normally found in their pockets, and overcoats, mufflers, etc., were also discarded. 
Where a child was found to be wearing three or four jerseys—a not uncommon 
experience—all in excess of one were removed."" It still seems likely, however, that the 
summer garb was lighter than the winter garb. This example illustrates that the patterns 
discovered by data mining may not shed much light on the phenomena under 
investigation, but finding data anomalies and shortcomings may be just as valuable. 

3.5 Tools for Displaying More Than Two Variables 
Since sheets of paper and computer screens are flat, they are readily suited for 
displaying two-dimensional data, but are not effective for displaying higher dimensional 
data. We need some kind of projection, from the higher dimensional data to a two 
dimensional plane, with modifications to show (aspects of) the other dimensions. The 
most obvious approach along these lines is to examine the relationships between all 
pairs of variables, extending the basic scatterplot described in section 3.3 to a scatterplot 
matrix. 
Figure 3.13 illustrates a scatterplot matrix for characteristics, performance measures, 
and relative performance measures of 209 computer CPUs dating from over 10 years 
ago. The variables are cycle time, minimum memory (kb), maximum memory (kb), cache 
size (kb), minimum channels, maximum channels, relative performance, and estimated 
relative performance (relative to an IBM 370/158-3). While some pairs of variables 
appear to be unrelated, others are strongly related. Brushing allows us to highlight points 
in a scatterplot matrix in such a way that the points corresponding to the same objects in 
each scatterplot are highlighted. This is particularly useful in interactive exploration of 
data. 

Figure 3.13: A scatterplot Matrix for the Computer CPU Data.  

Of course, scatterplot matrices are not really multivariate solutions: they are multiple 
bivariate solutions, in which the multivariate data are projected into multiple two-
dimensional plots (and in each two-dimensional plot all other variables are ignored). 
Such projections necessarily sacrifice information. Picture a cube formed from eight 

 

smaller cubes. If data points are uniformly distributed in alternate subcubes, with the 
others being empty, all three one-dimensional and all three two-dimensional projections 
show uniform distributions. (This ""exclusive-or"" structure caused great difficulty with 
perceptrons—the precursors of today's neural networks which we will discuss in 
chapters 5 and 11.) 
Interactive graphics come into their own when more than two variables are involved, 
since then we can rotate (""spin"") the direction of projection in a search for structure. 
Some systems even let the software follow random rotations, while we watch and wait 
for interesting structures to become apparent. While this is a good idea in principle, the 
excitement of watching a cloud of points shift relative position as the direction of viewing 
changes can quickly pall, and more structured methods are desirable. Projection pursuit, 
described in chapter 11, is one such method. 

Trellis plotting also utilizes multiple bivariate plots. Here, however, rather than displaying 
a scatterplot for each pair of variables, they fix a particular pair of variables that is to be 
displayed and produce a series of scatterplots conditioned on levels of one or more other 
variables. 
Figure 3.14 shows a trellis plot for data on epileptic seizures. The horizontal axis of each 
plot gives the number of seizures that 58 patients experienced over a certain two week 
period, and the vertical axis gives the number of seizures experienced over a later two 
week period. The two left hand graphs show the figures for males, and the two right hand 
graphs the figures for females. The two upper graphs show ages 29 to 42 while the two 
lower graphs show ages 18 to 28. (The original data set included the record of another 
subject who had much higher counts. We have removed this subject here so that we can 
more clearly see the relationships between the scores of the other subjects.) From these 
plots, we can see that the younger group show lower average counts than the older 
group. The figures also hint at some possible differences between the slopes of the 
estimated best fitting lines relating the y and x axes, though we would need to carry out 
formal tests to be confident that these differences were real. 

Figure 3.14: A Trellis Plot for the Epileptic Seizures Data.  

 

Trellis plots can be produced with any kind of component graph. Instead of scatterplots 
in each cell, we could have histograms, time series plots, contour plots, or any other 
types of plots. 
An entirely different way to display multivariate data is through the use of icons, small 
diagrams in which the sizes of different features are determined by the values of 
particular variables. Star icons are among the most popular. In these, different directions 
from the origin correspond to different variables, and the lengths of radii projecting in 
these directions correspond to the magnitudes of the variables. Figure 3.15 shows an 
example. The data displayed here come from 12 chemical properties that were 
measured on 53 mineral samples equally spaced along a long drill into the Earth's 
surface. 

Figure 3.15: An Example of a Star Plot.  

 

Another type of icon plot, Chernoff's faces, is discussed frequently in introductory texts 
on the subject. In these plots, the sizes of features in cartoon faces (length of nose, 
degree of smile, shape of eyes, etc.) represent the values of the variables. The method 
is based on the principle that the human eye is particularly adept at recognizing and 
distinguishing between faces. Although they are entertaining, plots of this type are 
seldom used in serious data analysis since the idea does not work very well in practice 
with more than a handful of cartoon faces. In general, iconic representations are effective 
only for relatively small numbers of cases since they require the eye to scan each case 
separately. 
Parallel coordinates plots show variables as parallel axes, representing each case as a 
piecewise linear plot connecting the measured values for that case. Figure 3.16 shows 
such a plot for four repeated measurements of the number of epileptic seizures 
experienced by 58 patients during successive two week periods. The data are clearly 
skewed and might be modeled by a Poisson distribution (see Appendix). Since the data 
set is not too large, we can follow the trajectories of individual patients. 

Figure 3.16: A Parallel Coordinates Plot for the Epileptic Seizure Data.  

 

 
 

Another way of representing dimensions is through the use of color. Line styles, as in the 
parallel coordinates plot above, can serve the same purpose. 

No single method of representing multivariate data is a universal solution. Which method 
is most useful in a given situation will depend on the data and on the structures being 
sought. 

3.6 Principal Components Analysis 
Scatterplots project multivariate data into a two-dimensional space defined by just two of 
the variables. This allows us to examine pairwise relationships between variables, but 
such simple projections might conceal more complicated relationships. To detect these 
relationships we can use projections along different directions, defined by any weighted 
linear combination of variables (e.g., along the direction defined by 2x1 + 3x2 + x3). 
With only a few variables, it might be feasible to search for such interesting spaces 
manually, rotating the distribution of the data. With more than a few variables, however, it 
is best to let the computer loose to search by itself. To do this, we need to define what an 
""interesting"" projection might look like, so that the computer knows when it has found 
one. Projection pursuit methods are based on this general principle of allowing the 
computer to search for interesting directions. (Such techniques, however, are 
computationally quite intensive: we will return to projection pursuit in chapter 11 when we 
discuss regression.) 
However, in one special case—for one specific definition of what constitutes an 
""interesting"" direction—a computationally efficient explicit solution can be found. This is 
when we seek the projection onto the two-dimensional plane for which the sum of 
squared differences between the data points and their projections onto this plane is 
smaller than when any other plane is used. (We use two-dimensional projections here for 
convenience, but in general we can use any k-dimensional projection, 1 = k = p - 1). This 
two-dimensional plane can be shown to be spanned by (1) the linear combination of the 
variables that has maximum sample variance and (2) the linear combination that has 

maximum variance subject to being uncorrelated with the first linear combination. Thus 
""interesting"" here is defined in terms of the maximum variability in the data. 

Of course, we can take this process further, seeking additional linear combinations that 
maximize the variance subject to being uncorrelated with all those already selected. In 
general, if we are lucky, we find a set of just a few such linear combinations 
(""components"") that describes the data fairly accurately. The mathematics of this 
process is described below. Our aim here is to capture the intrinsic variability in the data. 
This is a useful way of reducing the dimensionality of a data set, either to ease 
interpretation or as a way to avoid overfitting and to prepare for subsequent analysis. 
Suppose that X is an n × p data matrix in which the rows represent the cases (each row 
is a data vector x(i)) and the columns represent the variables. Strictly speaking, the ith 
row of this matrix is actually the transpose xT of the ith data vector x(i), since the 
convention is to consider data vectors as being p × 1 column vectors rather than 1 × p 
row vectors. In addition, assume that X is mean-centered so that the value of each 
variable is relative to the sample mean for that variable (i.e., the estimated mean has 
been subtracted from each column).  
Let a be the p × 1 column vector of projection weights (unknown at this point) that result 
in the largest variance when the data X are projected along a. The projection of any 
particular data vector x is the linear combination 
. Note that we can express 
the projected values onto a of all data vectors in X as Xa (n × p by p × 1, yielding an n × 
1 column vector of projected values). Furthermore, we can define the variance along a 
as 
(3.8)  

where V = XTX is the p × p covariance matrix of the data (since X has zero mean), as 
defined in chapter 2. Thus, we can express 
scalar) that we wish to maximize) as a function of both a and the covariance matrix of the 
data V. 
Of course, maximizing  directly is not well-defined, since we can increase  without limit 
simply by increasing the size of the components of a. Some kind of constraint must be 
imposed, so we impose a normalization constraint on the a vectors such that aTa = 1. 

(the variance of the projected data (a 

With this normalization constraint we can rewrite our optimization problem as that of 
maximizing the quantity 

(3.9)  

where ? is a Lagrange multiplier. Differentiating with respect to a yields 

(3.10)  

which reduces to the familiar eigenvalue form of 

(3.11)  

Thus, the first principal component a is the eigenvector associated with the largest 
eigenvalue of the covariance matrix V. Furthermore, the second principal component 
(the direction orthogonal to the first component that has the largest projected variance) is 
the eigenvector corresponding to the second largest eigenvalue of V, and so on (the 
eigenvector for the kth largest eigenvalue corresponds to the kth principal component 
direction). 
In practice of course we may be interested in projecting to more than two-dimensions. A 
basic property of this projection scheme is that if the data are projected into the first k 
eigenvectors, the variance of the projected data can be expressed as 
the jth eigenvalue. Equivalently, the squared error in terms of approximating the true 
data matrix X using only the first k eigenvectors can be expressed as 

, where ?j is 

(3.12)  

Thus, in choosing an appropriate number k of principal components, one approach is to 
increase k until the squared error quantity above is smaller than some acceptable degree 
of squared error. For high-dimensional data sets, in which the variables are often 
relatively well-correlated, it is not uncommon for a relatively small number of principal 
components (say, 5 or 10) to capture 90% or more of the variance in the data. 
A useful visual aid in this context is the scree plot—which shows the amount of variance 
explained by each consecutive eigenvalue. This is necessarily nonincreasing with the 
number of the component, and the hope is that it demonstrates a sudden dramatic fall 
toward zero. A principal components analysis of the correlation matrix of the computer 
CPU data described earlier gives rise to eigenvalues proportional to 63.26, 10.70, 10.30, 
6.68, 5.23, 2.18, 1.31, and 0.34 (see figure 3.17). The fall from the first to the second 
eigenvalue is dramatic, but after that the decline is gradual. (The weights that the first 
component puts on the eight variables are (0.199, -0.365, -0.399, -0.336, -0.331, - 0.298, 
-0.421, -0.423). Note that, it gives them all roughly similar weights, but gives the first 
variable (cycle time) a weight opposite in sign to those of the other variables.) If, instead 
of the correlation matrix, we analyzed the covariance matrix, the variables with larger 
ranges of values would tend to dominate. In the case of these data, the values given for 
memory are much larger than those for the other variables. (This is because they are 
given in kilobytes. Had they been given in megabytes, this would not be the case—an 
example of the arbitrariness of the scaling of noncommensurate variables (see chapter 
2)). Principal components analysis of the covariance matrix gives proportions of variation 
attributable to the different components as 96.02, 3.93, 0.04, 0.01, 0.00, 0.00, 0.00, and 
0.00 (see figure 3.17). Here the fall from the first component is very striking—the 
variability in the data can, indeed, be explained almost entirely by the differences in 
memory capacity. Often, however, there is no obvious fall such as this—no point at 
which the  remaining variance in the data can be attributed to random variation. Then the 
choice of how many components to extract is fairly arbitrary. The proportion of the total 
variance that we regard as providing an adequate simplified description of the data 
depends on the field of application. In some cases it might be sufficient for the first few 
components to describe 60% of the variance, but in other fields one might hope for 95% 
or more. 

Figure 3.17: Scree Plots for the Computer CPU Data Set. The Upper Plot Displays the 
Eigenvalues From the Correlation Matrix, and the Lower Plot is for the Covariance Matrix.  

 

When conducting principal components analysis prior to further analyses, it is risky to 
choose a small number of components that fail to explain the variability in the data very 
well. Information is lost, and there is no guarantee that the sacrificed information is not 
relevant to the aims of further analyses. (Indeed, this is true even if the retained 
components do explain the variability well, short of 100%.) For example, we might 
perform principal components analysis prior to classifying our data. Since the aims of 
dimension reduction and classification are somewhat different, it is possible that the 
reduction to a few spanning components may lose valuable information about the 
differences between the classes—we will see an example of this at the end of chapter 9. 
Likewise, for many multivariate data sets in which the points fall into two (or more) 
classes, a prior principal components analysis may completely obliterate the differences 
between the distributions of the classes. On the other hand, in regression problems 
(chapter 11) with many explanatory variables, unless the data set is large, there may be 
problems of instability of the estimated coefficients. A principal components analysis is 
sometimes performed to reduce the large number of explanatory variables to a few linear 
combinations prior to carrying out the regression analysis. 
Despite the risks of failing to extract relevant information, principal components analysis 
is a powerful and valuable tool. Because it is based on linear projections and minimizing 
the variance (or sum of squared errors), numerical manipulations can be carried out 
explicitly, without any iterative searches. Computing the principal component solutions 
directly from the eigenvector equations will scale roughly as O(np2 + p3) (np2 to calculate 
V and p3 to solve the eigenvalue equations for the p×p matrix ). This means that it can be 
applied to data sets with large numbers of records n (but does not scale so well as a 
function of dimensionality p). As illustrated above when we applied principal components 
analysis to both correlation and covariance matrices, the method is not invariant under 
rescalings of the original variables. The appropriate steps to take will depend on the 
objectives of the analysis. Typically we rescale the data if different variables measure 
different attributes (e.g., height, weight, and lung capacity) since otherwise the results of 
a direct principal components analysis depend on the arbitrary choice of units used for 
each attribute. 
To illustrate the simple graphical use of principal components analysis, figure 3.18 shows 
the projections (indicated by the numbers) of 17 pills onto the space spanned by the first 
two principal components. The six measurements on each pill are the times at which a 
specified proportion (10%, 30%, 50%, 70%, 75%, and 90%) of the pill has dissolved. It is 
clear from this diagram that one of the pills is very different from the others, lying in the 
bottom right corner, far from the other points. 

 

Figure 3.18: Projection Onto the First Two Principal Components.  

Sometimes we can gain insights from the pattern of weights (or loadings, as they are 
sometimes called) defining the components of a principal components analysis. Huba et 
al. (1981) collected data on 1684 students in Los Angeles showing consumption of each 
of thirteen legal and illegal psychoactive substances: cigarettes, beer, wine, spirits, 
cocaine, tranquilizers, drug store medications used to get high, heroin and other opiates, 
marijuana, hashish, inhalants (such as glue), hallucinogenics, and amphetamines. They 

scored each as 1 (never tried), 2 (tried only once), 3 (tried a few times), 4 (tried many 
times), 5 (tried regularly). Taking these variables in order, the weights of the first 
component from a principal components analysis were (0.278, 0.286, 0.265, 0.318, 
0.208, 0.293, 0.176, 0.202, 0.339, 0.329, 0.276, 0.248, 0.329). This component assigns 
roughly equal weights to each of the  variables and can be regarded as a general 
measure of how often students use such substances. Thus, the biggest difference 
between the students is in terms of how often they use psychoactive substances, 
regardless of which substances they use. 

The second component had weights (0.280, 0.396, 0.392, 0.325, -0.288, -0.259, -0.189, 
-0.315, 0.163, -0.050, -0.169, -0.329, -0.232). This is interesting because it gives positive 
weights to the legal substances and negative weights to the illegal ones: therefore, once 
we have controlled for overall substance use, the major difference between the students 
lies in their use of legal versus illegal substances. This is just the sort of relationship one 
would hope to discover from a data mining exercise. 
Another statistical technique, factor analysis, is often confused with principal components 
analysis, but the two have very different aims. As described above, principal components 
analysis is a transformation of the data to new variables. We can then select just some of 
these as providing an adequate description of the data. Factor analysis, on the other 
hand, is a model for data, based on the notion that we can define the measured 
variables X1, ..., Xp as linear combinations of a smaller number m (m < p) of ""latent"" 
(unobserved) factors—variables that cannot be measured explicitly. The objective of 
factor analysis is to unearth information about these latent variables. 
We can define F = (F1, ..., Fm)T as the m × 1 column vector of unknown latent variables, 
taking values f = (ƒ1, ..., ƒm). Then a measured data vector x = (x1, ..., xp)T (defined here 
as a p × 1 column vector) is regarded as a linear function of f defined by 

Here ? is a p × m matrix of factor loadings giving the weights with which each factor 
contributes to each manifest variable. The components of the p × 1 vector e are 
uncorrelated random variables, sometimes termed specific factors since they contribute 
only to single manifest (observed) variables, Xj, 1 = j = p. Factor analysis is a special 
case of structural linear relational models described in chapter 9, so we will not dwell on 
estimation procedures here. However, since factor analysis was the earliest model 
structure of this form to be developed, it has a special place, not only because of its 
history, but also because it continues to be among the most widely used of such models. 
Factor analysis has not had an entirely uncontroversial history, partly because its 
solutions are not invariant to various transformations. It is easy to see that new factors 
can be defined from equation 3.13 via m × m orthogonal matrices M, such that x = (? M) 
(Mf) +e. This corresponds to rotating the factors in the space they span. Thus, the 
extracted factors are essentially nonunique, unless extra constraints are imposed. There 
are various constraints in general use, including methods that seek to extract factors for 
which the weights are as close to 0 or 1 as possible, defining the variables as clearly as 
possible in terms of a subset of the factors. 

3.7 Multidimensional Scaling 

In the preceding section we described how to use principal components analysis to 
project a multivariate data set onto the plane in which the data has maximum dispersion. 
This allows us to examine the data visually, while sacrificing the minimum amount of 
information. Such a method is effective only to the extent that the data lie in a two-
dimensional linear subspace of the area spanned by the measured variables. But what if 
the data forms a set that is intrinsically two-dimensional, but instead of being ""flat,"" is 
curved or otherwise distorted in the space spanned by the original variables? (Imagine a 
crumpled piece of paper, intrinsically two-dimensional, but occupying three dimensions.) 
In this event it is quite possible that principal components analysis might fail to detect the 
underlying two-dimensional structure. In such cases, multidimensional scaling can be 
helpful. Multidimensional scaling methods seek to represent data points in a lower 
dimensional space while preserving, as far as is possible, the distances between the 
data points. Since, we are mostly concerned with two-dimensional representations, we 

(3.13)  

 
 

shall restrict most of our discussion to such cases. The extension to higher dimensional 
representations is immediate. 

Many multidimensional scaling methods exist, differing in how they define the distances 
that are being preserved, the distances they map to, and how the calculations are 
performed. Principal components analysis may be regarded as a basic form. In this 
approach the distances between the data points are taken as Euclidean (or 
Pythagorean), and they are mapped to distances in a reduced space that are also 
measured using the Euclidean metric. The sum of squared distances between the 
original data points and their projections provides a measure of quality of the 
representation. Other methods of multidimensional scaling also have associated 
measures of the quality of the representation. 

Since multidimensional scaling methods seek to preserve interpoint distances, such 
distances can serve as the starting point for an analysis. That is, we do not need to know 
any measured values of variables for the objects being analyzed, only how similar the 
objects are, in terms of some distance measure. For example, the data may have been 
collected by asking respondents to rate the similarity between pairs of objects. (A classic 
example of this is a matrix showing the number of times the Morse codes for different 
letters are confused. Th ere are no ""variables"" here, simply a matrix of ""similarities"" 
measuring how often is letter was mistaken for another.) The end point of the process is 
the same—a configuration of data points in a two-dimensional space. In a sense, the 
objects and the raters are used to determine on what dimensions ""similarity"" is to be 
measured. Multidimensional scaling methods are widely used in areas such as 
psychometrics and market research, in attempts to understand perceptions of 
relationships and similarities between objects. 
From an n × p data matrix X we can compute an n × n matrix B = XXT. (Since this scales 
as O(n2) in both time and memory, it is clear that this approach is not practical for very 
large numbers of objects n). It is straightforward to see from this that the Euclidean 
distance between the ith and jth objects is given by 

(3.14)  

If we could invert this relationship, then, given a matrix of distances D (derived from 
original data points by computing Euclidean distances or obtained by other means), we 
could compute the elements of B. B could then be factorized to yield the coordinates of 
the points. One factorization of B would be in terms of the eigenvectors. If we chose 
those associated with the two largest eigenvalues, we would have a two-dimensional 
representation that preserved the structure of the data as well as possible. 
The feasibility of this procedure hinges upon our ability to invert equation 3.14. 
Unfortunately, this is not possible without imposing some extra constraints. Because 
shifting the mean and rotating a configuration of points does not affect the interpoint 
distances, for any given a set of distances there is an infinite number of possible 
solutions, differing in the location and orientation of the point configuration. 
A sufficient constraint to impose is the assumption that the means of all the variables are 
0. That is, we assume 
. Now, 
by summing equation 3.14 first over i, then over j, and finally over both i and j, we obtain 

for all k = 1, ..., p. This means that 

(3.15)  

 

where tr(B) is the trace of the matrix B. The third equation expresses tr(B) in terms of the 

, the first and second express bjj and bii in terms of 
alone. Plugging these into equation 3.14 expresses bij as a function of 

and tr(B), and hence in terms of 
, yielding the 

required inversion. 
This process is known as the principal coordinates method. It can be shown that the 
scores on the components calculated from a principal components analysis of a data 
matrix X (and hence a factorization of the matrix XT) are the same as the coordinates of 
the above scaling analysis. 

Of course, if the matrix B does not arise not as a product XXT, but by some other route 
(such as simple subjective differences between pairs of objects), then there is no 
guarantee that all the eigenvalues will be non-negative. If the negative eigenvalues are 
small in absolute value, they can be ignored. 

Classical multidimensional scaling into two dimensions finds the projection into two 
dimensions that is most accurate in the sense that it minimizes 

(3.16)  

where dij is the observed distance between points i and j in the p-dimensional space and 
dij is the distance between the points representing these objects in the two-dimensional 
space. Expressed this way the process permits ready generalization. Given distances or 
dissimilarities, derived in one way or another, we can seek a distribution of points in a 
two-dimensional space that minimizes the sum of squared differences ? i ? j (dij - dij)2. 
Thus, we relax the restriction that the configuration must be found by projection. With this 
relaxation an exact algebraic solution will generally not be possible, so numerical 
methods must be used: we simply have a function of 2n parameters (the coordinates of 
the points in the two-dimensional space) that is to be minimized. 
The score function ? i ? j(dij - dij)2, measuring how well the interpoint distances in the 
derived configuration match those originally provided, is invariant with respect to 
rotations and translations. However, it is not invariant to rescalings: if the dij were 
multiplied by a constant, we would end up with the same solution, but a different value of 
? i? j (dij - dij)2. To permit different situations to be properly compared we divide  ? i ? j(dij - 
dij)2 by, 
, yielding the standardized residual sum of squares. A common by score 
function is the square root of this quantity, the stress. A variant on the stress is the 
sstress, defined as 

(3.17)  

These measures effectively assume that the differences between the original 
dissimilarities and the distances in the two-dimensional configuration are due to random 
discrepancies and arbitrary distortions—that is, that dij = dij + ˛
models can also be built. For example, we might assume that dij = a + bdij + ˛
two-stage procedure is necessary. Beginning with a proposed configuration, we regress 
the distances dij in the two-dimensional space on the given dissimilarities, yielding 
estimates for a and b. We then find new values of the  dij that minimize the stress 

ij. More sophisticated 

ij. Now a 

(3.18)  

and repeat this process until we achieve satisfactory convergence. 
Multidimensional scaling methods such as the above, which attempt to model the 
dissimilarities as given, are called metric methods. Sometimes, however, a more general 
approach is required. For example, we may not be given the precise similarities, only 
their rank order (objects A and B are more similar than B and C, and so on); or we may 
not be prepared to assume that the relationship between dij and dij has a particular form, 
just that some monotonic relationship exists. This requires a two-stage approach similar 
to that described in the preceding paragraph, but with a technique known as monotonic 
regression replacing simple linear regression, yielding non-metric multidimensional 
scaling. The term non-metric here indicates that the method seeks to preserve only 
ordinal relationships. 
Multidimensional scaling is a powerful method for displaying data to reveal structure. 
However, as with the other graphical methods described in this chapter, if there are too 
many data points the structure becomes obscured. Moreover, since multidimensional 
scaling involves applying highly sophisticated transformations to the data (more so than 
a simple scatterplot or principal components analysis) there is a possibility that artifacts 
may be introduced. In particular, in some situations the dissimilarities between objects 
can be determined more accurately when the objects are similar than when they are 
quite different. Consider the evolution of the style of a manufactured object. Those 
objects that are produced within a short time of each other will probably have much in 

common, while those separated by a greater time gap may have very little in common. 
The consequence will be an induced curvature in the multidimensional scaling plot, 
where we might have hoped to achieve a more or less straight line. This phenomenon is 
known as the horseshoe effect. 
Figure 3.19 shows a plot produced using nonmetric scaling to minimize the sstress score 
function of equation 3.17. The data arose from a study of English dialects. Each pair of a 
group of 25 villages was rated according to the percentages of 60 items for which the 
villages used different words. The villages, and the counties in which they are located, 
are listed in table 3.1. The figure shows that villages from the same county (and hence 
that are relatively close geographically) tend to use the same words. 

Figure 3.19: A Multidimensional Scaling Plot of the Village Dialect Similarities Data.  

 

Table 3.1: Numerical Codes, Names, And Counties for the 25 Villages with Dialect 
Similarities Displayed in Figure 3.19.  

1 

2 

3 

4 

5 

6 

7 

8 

9 

10 

11 

12 

13 

14 

15 

16 

17 

North 
Wheatley 

South 
Clifton 

Oxton 

Eastoft 

Keelby 

Wiloughton 

Wragby 

Old 
Bolingbroke 

Fulbeck 

Sutterton 

Swinstead 

Crowland 

Harby 

Packington 

Goadby 

Ullesthorpe 

Nottinghamshire 

Nottinghamshire 

Nottinghamshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Lincolnshire 

Leicestershire 

Leicestershire 

Leicestershire 

Leicestershire 

Empingham 

Rutland 

18 

19 

20 

21 

22 

23 

24 

25 

Warmington 

Northamptonshire 

Little 
Harrowden 

Kislingbury 

Sulgrave 

Warboys 

Little 
Downham 

Tingewick 

Turvey 

Northamptonshire 

Northamptonshire 

Northamptonshire 

Huntingdonshire 

Cambridgeshire 

Buckinghamshire 

Bedfordshire 

 
 

Multidimensional scaling methods typically display the data points in a two-dimensional 
space. If the variables are also described in this space (provided the data are in vector 
form) the relationships between data points and variables may be clearly seen. Given the 
complicated nonlinear relationship between the space defined by the original variables 
and the space used to display the data, representing the original variables is a non-trivial 
task. Plots that display both data points and variables are known as biplots. The ""bi"" here 
signifies that there are two modes being displayed—the points and the variables—not 
that the display is two-dimensional. Indeed, three-dimensional biplots have also been 
developed. Forms of multidimensional scaling that involve nonlinear transformations 
produce nonlinear biplots. Biplots have even been produced for categorical data, and in 
this case the levels of the variables are represented by regions in the plot. Effective 
interpretation of multidimensional and biplot displays requires practice and experience. 

3.8 Further Reading 
Exploratory data analysis achieved an identity and respectability with the publication of 
John Tukey's book Exploratory Data Analysis (Tukey, 1977). Since then, as progress in 
computer technology facilitated rapid and straight-forward production of accurate 
graphical displays, such methods have blossomed. Modern data visualization techniques 
can be very powerful ways of discovering structure. Books on graphical methods include 
those of  Tufte (1983), Chambers et al. (1983), and  Jacoby (1997). Wilkinson (1999) is a 
particularly interesting recent addition to the visualization literature, introducing a novel 
and general purpose language for analyzing and synthesizing a wide variety of data 
visualization techniques. 
Interactive dynamic methods are emphasized by Asimov (1985), Becker, Cleveland, and 
Wilks (1987), Cleveland and McGill (1988), and Buja, Cook, and Swayne (1996). Books 
that describe smoothing approaches to displaying univariate distributions, as well as 
multivariate extensions, include those of Silverman (1986), Scott (1992), and Wand and 
Jones (1995). Carr et al. (1987) discuss scatterplot techniques for large data sets. 
Wegman (1990) discusses parallel coordinates. Categorical data is somewhat more 
difficult to visualize than quantitative real-valued data, and for this reason, visualization 
techniques for categorical data are not as widely developed or used. Still, Blasius and 
Greenacre (1998) provide a useful and broad review of recent developments in the 
visualization and exploratory data analysis of categorical data. Cook and Weisberg 
(1994) describe the use of graphical techniques for the task of regression modeling. 
Card, MacKinlay, and Shneiderman (1999) contains a collection of papers on a variety of 
topics entitled ""information visualization"" and describe a number of techniques for 
displaying complex heterogeneous data sets in a useful manner. Keim and Kriegel 
(1994) describe a system specifically designed for database exploration.  
Multidimensional scaling has become a large field in its own right. Books on this include 
those by Davidson (1983) and Cox and Cox (1994). Biplots are discussed in detail by 
Gower and Hand (1996). 
The CPU data is from Ein-Dor and Feldmesser (1987), and is reproduced in Hand et al. 
(1994), dataset 325. The data on English dialects is from Morgan (1981) and is 

 
 

 
 

reproduced in Hand et al. (1994), dataset 145. The data on epileptic seizures is given in 
Thall and Vail (1990) and also in  Hand et al. (1994). The mineral core data shown in the 
icon plot is described in Chernoff (1973). 

Chapter 4: Data Analysis and Uncertainty 
4.1 Introduction 
In this chapter, we focus on uncertainty and how to cope with it. Not only is the process 
of mapping from the real world to our databases seldom perfect, but the domain of the 
mapping—the real world itself—is beset with ambiguities and uncertainties. The basic 
tool for dealing with uncertainty is probability, and we begin by defining the concept and 
showing how it is used to construct statistical models. Section 4.2 provides a brief 
discussion of the distinction between probability calculus and the interpretation of 
probability, focusing on the two main interpretations: the frequentist and the subjective 
(Bayesian). Section 4.3 extends this discussion to define the concept of a random 
variable, with a particular focus on the relationships that can exist between multiple 
random variables. 
Fundamental to many data mining activities is the notion of a sample. Sometimes the 
database contains only a sample from the universe of possible records; section 4.4 
explores this situation, explaining why samples are often sufficient to work with. Section 
4.5 describes estimation, the process of moving beyond a data sample to develop 
parameter estimates for a model describing the data. In particular, we review in some 
detail the basic principles of the maximum likelihood and Bayesian approaches to 
estimation. Section 4.6 discusses the closely related topic of how to evaluate the quality 
of a hypothesis on the basis of observed data. Section 4.7 outlines various systematic 
methods for drawing samples from data. Section 4.8 presents some concluding remarks, 
and section 4.9 gives pointers to more detailed reading. 

4.2 Dealing with Uncertainty 
The ubiquity of the idea of uncertainty is illustrated by the rich variety of words used to 
describe it and related concepts. Probability, chance, randomness, luck, hazard, and fate 
are just a few examples. The omnipresence of uncertainty requires us to be able to cope 
with it: modeling uncertainty is a necessary component of almost all data analysis. 
Indeed, in some cases our primary aim is to model the uncertain or random aspects of 
data. It is one of the great achievements of science that we have developed a deep and 
powerful understanding of uncertainty. The capricious gods that were previously invoked 
to explain the lack of predictability in the world have been replaced by mathematical, 
statistical, and computer-based models that allow us to understand and manipulate 
uncertain events. We can even attempt the seemingly impossible and predict uncertain 
events, where prediction for a data miner either can mean the prediction of future events 
(where the notion of uncertainty is very familiar) or prediction in a nontemporal sense of 
a variable whose true value is somehow hidden from us (for example, diagnosing 
whether a person has cancer, based on only descriptive symptoms). 

We may be uncertain for various reasons. Our data may be only a sample from the 
population we wish to study, so that we are uncertain about the extent to which different 
samples differ from each other and  from the overall population. Perhaps our interest lies 
in making a prediction about tomorrow, based on the data we have today, so that our 
conclusions are subject to uncertainty about what the future will bring. Perhaps we are 
ignorant and cannot observe some value, and have to base our ideas on our ""best 
guess"" about it. And so on. 

Many conceptual bases have been formulated for handling uncertainty and ignorance. Of 
these, by far the most widely used is probability. Fuzzy logic is another that has a 
moderately large following, but this area—along with closely related areas such as 
possibility theory and rough sets—remains rather controversial: it lacks the sound 
theoretical backbone and widespread application and acceptance of probability. These 

ideas may one day develop solid foundations, and become widely used, but because of 
their current uncertain status we will not consider them further in this book. 
It is useful to distinguish between probability theory and probability calculus. The former 
is concerned with the interpretation of probability while the latter is concerned with the 
manipulation of the mathematical representation of probability. (Unfortunately, not all 
textbooks make this distinction between the two terms—often books on probability 
calculus are given titles such as ""Introduction to the Theory of Probability."") The 
distinction is an important one because it permits the separation of those areas about 
which there is universal agreement (the calculus) from those areas about which opinions 
differ (the theory). The calculus is a branch of mathematics, based on well-defined and 
generally accepted axioms (stated by the Russian mathematician Kolmogorov in the 
1930s); the aim is to explore the consequences of those axioms. (There are some areas 
in which different sets of axioms are used, but these are rather specialized and generally 
do not impinge on problems of data mining.) The theory, on the other hand, leaves scope 
for perspectives on the mapping from the real world to the mathematical 
representation—i.e., on what probability is. 
A study of the history and philosophy of probability theory reveals that there are as many 
perspectives on the meaning of probability as there are thinkers. However, the views can 
be grouped into variants of a few different types. Here we shall restrict ourselves to 
discussing the two most important types (in terms of their impact on data mining 
practice). More philosophically inclined readers may wish to consult section 4.9 for 
references to material containing broader discussions. 
The frequentist view of probability takes the perspective that probability is an objective 
concept. In particular, the probability of an event is defined as the limiting proportion of 
times that the event would occur in repetitions of essentially identical situations. A simple 
example is the proportion of times a head comes up in repeatedly tossing a coin. This 
interpretation restricts our application of probability: for instance we cannot assess the 
probability that a particular athlete will win a medal in the next Olympics because this is a 
one-off event, where the notion of a ""limiting proportion"" makes no sense. On the other 
hand, we can certainly assess the probability that a customer in a supermarket will 
purchase a certain item, since we can use a large number of similar customers as the 
basis for a limiting proportion argument. It is clear in this last example that some 
idealization is going on: different customers are not really the same as repetitions of a 
single customer. As in all scientific modeling we need to decide what aspects are 
important for our model to be sufficiently accurate. In predicting customer behavior we 
might decide that the differences between customers do not matter. 
The frequentist view was the dominant perspective on probability throughout most of the 
last century, and hence it underpins most widely used statistical software. However, in 
the last decade or so, a competing vi ew has acquired increasing importance. This view, 
that of subjective probability, has been around since people first started formalizing 
probabilistic notions, but until recently it was primarily of theoretical interest. What 
revived the approach was the development of the computer and of powerful algorithms 
for manipulating and processing subjective probabilities. The principles and 
methodologies for data analysis that derive from the subjective point of view are often 
referred to as Bayesian statistics. A central tenet of Bayesian statistics is the explicit 
characterization of all forms of uncertainty in a data analysis problem, including 
uncertainty about any parameters we estimate from the data, uncertainty as to which 
among a set of model structures are best or closest to ""truth,"" uncertainty in any forecast 
we might make, and so on. Subjective probability is a very flexible framework for 
modeling such uncertainty in different forms. 
From the perspective of subjective probability, probability is an individual degree of belief 
that a given event will occur. Thus, probability is not an objective property of the outside 
world, but rather an internal state of the individual—and may differ from individual to 
individual. Fortunately it turns out that if we adopt certain tenets of rational behaviour the 
set of axioms underlying subjective probability is the same as that underlying the 
frequentist view. The calculus is the same for the two viewpoints, even though the 
underlying interpretation is quite different. 

Of course, this does not imply that the conclusions drawn using the two approaches are 
necessarily the same. At the very least, subjective probability can make statements 

 
 

about areas that frequentist probability cannot address. Moreover, statistical inferences 
based on subjective probability necessarily involve a subjective component—the initial or 
prior belief that an event will happen. As noted above, this factor is likely to differ from 
person to person. 

Nonetheless, the frequentist and subjective viewpoints in many cases lead to roughly the 
same answers, particularly for simple hypotheses and large data sets. Rather than 
committing to one viewpoint or the other, many practitioners view both as useful in their 
own right, with each appropriate in different situations. The methodologies for data 
analysis that derive from the frequentist view tend to be computationally simpler, and 
thus (to date at least) have dominated in the development of data mining techniques 
where the size of the data sets do not favor the application of complex computational 
methods. However, when applied with care the Bayesian (subjective) methodology has 
the ability to tease out more subtle information from the data. Just as applied statistics 
has seen increased interest in Bayesian methods in recent years, we can expect to see 
more Bayesian ideas being applied in data mining in the future. In the rest of this book 
we will refer to both frequentist and Bayesian views where appropriate. As we will see 
later in this chapter, in a certain sense the two viewpoints can be reconciled: the 
frequentist methodology of fitting models and patterns to data can be implemented as a 
special case of a more general Bayesian methodology. For the practitioner this is quite 
useful, since it means that the same general modeling and computational apparatus can 
be used. 

4.3 Random Variables and Their Relationships 
We introduced the notion of a variable in chapter 2. In this chapter we introduce the 
concept of a random variable. A random variable is a mapping from a property of objects 
to a variable that can take one of a set of possible values, via a process that appears to 
the observer to have some element of unpredictability to it. The possible values of a 
random variable X are called the domain of X. We use uppercase letters such as X to 
refer to a random variable and lowercase letters such as x to refer to a value of a random 
variable. 

An example of a random variable is the outcome of a coin toss (the domain is the set 
{heads, tails}). Less obvious examples of random variables include the number of times 
we have to toss a coin to obtain the first head (the domain is the set of positive integers) 
and the flying time of a paper aeroplane in seconds (the domain is the set of positive real 
numbers). 
The appendix defines the basic properties of univariate (single) random variables, 
including both probability mass functions p(X) when the domain of X is finite and 
probability density functions ƒ(x) when the domain of X is the real-line or any interval 
defined on it. Basic properties of the expectation of X, E[X] = ? xƒ(x)dx, for real-valued X, 
are also reviewed, noting for example that since E is a linear operator we have that E[X 
+Y] = E[X]+E[Y]. These basic properties are extremely useful in allowing us to derive 
general principles for data analysis in a statistical context and we will refer to 
distributions, densities, expectation, etc., frequently throughout the remainder of this 
chapter. 

4.3.1 Multivariate Random Variables 
Since data mining often deals with multiple variables, we must also introduce the 
concept of a multivariat e random variable. A multivariate random variable X is a set X1, 
..., Xp of random variables. We use the m-dimensional vector x = {x1, ..., xp} to denote a 
set of values for X. The density function ƒ(X) of the multivariate random variable  X is 
called the joint density function of X. We denote this as ƒ(X) = ƒ(X1 = x1, ..., Xp = xp), or 
simply ƒ(x1, ..., xp). Similarly, we have joint probability distributions for variable staking 
values in a finite set. Note that ƒ(X) is a scalar function of p variables. 
The density function of any single variable in the set X (or, more generally, any subset of 
the complete set of variables) is called a marginal density of the joint density. 
Technically, it is derived from the joint density by summing or integrating across the 

variables not included in the subset. For example, for a tri-variate random variable X = 
(X1, X2, X3) the marginal density of ƒ(X1) is given by ƒ(x1) = ? ?  ƒ(x1, x2, x3)dx2dx3. 
The density of a single variable (or a subset of the complete set of variables) given (or 
""conditional on"") particular values of the other variables is a conditional density. Thus we 
can speak of the conditional density of variable X1 given that X2 takes the value 6, 
denoted ƒ(x1 |  x2 = 6). In general, the conditional density of X1 given some value of X2 is 
denoted by ƒ(x1 |  x2), and is defined as 

(4.1)  

For discrete-valued random variables we have equivalent definitions (p(a1 | a2), etc.). We 
can also use mixtures of the two—e.g., a conditional probability density function ƒ(x1 | a1) 
for a continuous variable conditioned on a categorical variable, and a conditional 
probability mass function p(a1 | x1) for the reverse case. 

Example 4.1  

 

Suppose we have data on purchases of products from supermarkets, with each 
observation (row) in the data matrix representing the products bought by one customer. Let 
each column represent a particular product, and associate a random variable with each 
column so that there is one variable per product. An observation in a given row and column 
has value 1 if the customer corresponding to that row bought the product from that column, 
and has value 0 otherwise. 
Denote by A the binary random variable for a particular column, corresponding to the event 
""purchase of product A."" A data-driven estimate of the probability that A takes value 1 is 
simply the fraction of customers who bought product A—i.e., nA/n, where n is the total 
number of customers and nA is the number of customers who bought product A. For 
example, if n = 100, 000 and nA = 10, 000, an estimate of the probability that a randomly 
selected customer bought product A is 0.1. 
Now consider a second product (a second column in the data matrix), with random variable 
B defined in the same way as A. Let nB be the number of customers who bought product B; 
assume nB = 5000 and therefore p(B = 1) = 0:05. Now let nAB be the number of customers 
who purchased both A and B. Following the same argument as above, an estimate of p(A = 
1, B = 1) is given by nAB/n. We can now estimate p(B = 1|A = 1) as nAB/nA. Thus, for 
example, if nAB = 10, we estimate p(B = 1|A = 1) as 10/10, 000 = 0.001. We see from this 
that, while the estimated probability of a customer buying product B is 0.05, this reduces to 
0.001 if we know that this customer bought product A as well. For the people in our 
database, the proportion of people buying B is far smaller among those who also bought A 
than among the people in the database as a whole (and thus smaller than among those 
who did not buy A). This prompts the question of whether buying A makes the purchase of 
B less likely in general, or whether this finding is simply an accident true only of the data we 
happen to have in our database. This is precisely the sort of question that we will address 
in the remainder of this chapter, particularly in section 4.6 on hypothesis testing. 

 

 

Note that particular variables in the multivariate set X may well be related to each other 
in some manner. Indeed, a generic problem in data mining is to find relationships 
between variables. Is purchasing item A likely to be related to purchasing item B? Is 
detection of pattern A in the trace of a measuring instrument likely to be followed shortly 
afterward by a particular fault? Variables are said to be independent if there is no 
relationship between the occurrence of values of the variables; otherwise they are 
dependent. More formally, variables X and Y are independent if and only if p(x, y) = 
p(x)p(y) for all values of X and Y . An equivalent formulation is that X and Y are 
independent if and only if p(x | y) = p(x) or p(y | x) = p(y) for all values of X and Y . (Note 
that these definitions hold whether each p in the expression is a probability mass 
function or a density function—in the latter case the variables are independent if and only 
if ƒ(x, y) = ƒ(x)ƒ(y)). The second form of the definition shows that when X and Y are 
independent the distribution of X is the same whether or not the value of Y is known. 

Thus, Y carries no information about X, in the sense that the value taken by Y does not 
influence the probability of X taking any value. The random variables A and B in example 
4.3.1 describing supermarket purchases are likely to be dependent, given the data as 
stated. 
We can generalize these ideas to more than two variables. For example, we say that X is 
conditionally independent of Y given Z if for all values of X, Y, and Z we have that p(x, y | 
z) = p(x | z)p(y | z), or equivalently p(x |  y, z) = p(x | z). To illustrate, suppose a person 
purchases bread (so that a random variable Z takes the value 1). Then subsequent 
purchases of butter (random variable X takes the value 1) and cheese (random variable 
Y takes the value 1) might be modeled as being conditionally independent—the 
probability of purchasing cheese is unaffected by whether or not butter was purchased, 
once we know that bread has been purchased. 
Note that conditional independence need not imply marginal (unconditional) 
independence. That is, the conditional independence relations above do not imply p(x, y) 
= p(x)p(y). For example, in our illustration we might reasonably expect purchases of 
butter and cheese to be dependent in general (since they are both dependent on bread 
purchases). The reverse also applies: X and Y may be (unconditionally) independent, but 
conditionally dependent given a third variable Z. The subtleties of these dependence and 
independence relations have important consequences for data miners. In particular, 
even though two observed variables (such as butter and cheese) may appear to be 
dependent given the data, their true relationship may be masked by a third (potentially 
unobserved) variable (such as bread in our illustration). 

Example 4.2  

 

Care is needed when studying and interpreting conditional independence statements. 
Consider the following hypothetical example. A and B represent two different treatments, 
and the fractions shown in the table are the fraction of patients who recover (thus, at the 
top left, 2 out of 10 ""old"" patients receiving treatment A recover). The data have been 
partitioned into ""old"" and ""young"" groups, according to whether the patients were older or 
younger than 30. 

                 A             B 

 

Old              2/10         30/90 

 

Young           48/90         10/10 

For each of the two age strata, treatment B appears superior to treatment A. However, now 
consider the overall results—obtained by aggregating the rows of the above table: 

                A                  B 

Total           50/100             40/100 
Overall, in this aggregate table, treatment A seems superior to treatment B. At first glance 
this result seems rather mysterious (in fact, it is known as Simpson's paradox (Simpson, 
1951)).  

The apparent contradiction between the two sets of results is explained by the fact that the 
first set is conditional on particular age strata, while the second is unconditional. When the 
two conditional statements are combined, the differences in sample sizes of the four 
groups cause the proportions based on the larger samples (Old B and Young A) to 
dominate the other two proportions. 

 

 

The assumption of conditional independence is widely used in the context of sequential 
data, for which the next value in the sequence is often independent of all of the past 
values in the sequence given only the current value in the sequence. In this context, 
conditional independence is known as the first-order Markov property. 

The notions of independence and conditional independence (which can be viewed as a 
generalization of independence) are central to many of the key concepts in data 
analysis, as we shall see in later chapters. The assumptions of independence and 
conditional independence enable us to factor the joint densities of many variables into 
much more tractable products of simpler densities, e.g., 

(4.2)  

where each variable  xj is conditionally independent of variables x1, ..., xj-2, given the 
value of xj (this is an example of a first-order Markov model). In addition to the 
computational benefits provided by such simplifications, it also provides important 
modeling gains by allowing us to construct more understandable models with fewer 
parameters. Nonetheless, independence is a very strong assumption that is frequently 
violated in practice (for example, assuming sequences of letters in text are first-order 
Markov may not be realistic). Still, keeping in mind that our models are inevitably 
approximations to the real world, the benefits of appropriate independence assumptions 
often outweigh the alternative of building more complex but less stable models. We will 
return to this theme of modeling in chapter 6. 
A special case of dependency is correlation, or linear dependency, as introduced in 
chapter 2. (Note that statistical dependence is not the same as correlation: two variables 
may be dependent but not linearly correlated). Variables are said to be positively 
correlated if high values of one variable tend to be associated with high values of the 
other, and to be negatively correlated if high values of one tend to be associated with low 
values of the other. It is important not to confuse correlation with causation. Two 
variables may be highly positively correlated without any causal relationship between 
them. For example, yellow-stained fingers and lung cancer may be correlated, but are 
causally linked only via a third variable, namely whether a person smokes or not. 
Similarly, human reaction time and earned income may be negatively correlated, but this 
does not mean that one causes the other. In this case a more convincing explanation is 
that a third variable, age, is causally related to both of these variables. 

Example 4.3  

 
A paper published in the Journal of the American Medical Association in 1987 (volume 257, 
page 785) examined the in-hospital mortality for 18,986 coronary bypass graft operations 
that were carried out at 77 hospitals in the United States. A regression analysis (see 
chapter 11) showed that hospitals that carried out more operations tended to have lower in-
hospital mortality rates (even adjusting for different types of cases at different hospitals). 
From this pattern it was concluded that average in-hospital mortality following this type of 
operation would be reduced if the low-volume surgery units were closed. 

However, determining the relationship between quality of outcome and number of treated 
cases in a hospital requires a longitudinal analysis in which the sizes are deliberately 
manipulated. The results of large-volume hospitals might degrade if their volume was 
increased. The correlation between out-come and size might have arisen not because 
larger size induces superior performance, but because superior performance attracts more 
cases, or because both the number of cases and the outcome are related to some other 
factor. 
 
 

4.4 Samples and Statistical Inference 
As we noted in chapter 2, many data mining problems involve the entire population of 
interest, while others involve just a sample from this population. In the latter case, the 

samples may arise at the start—perhaps only a sample of tax-payers is selected for 
detailed investigation; perhaps a complete census of the population is carried out only 
occasionally, with just a sample being selected in most years; or perhaps the data set 
consists of market research results. In other cases, even though the complete data set is 
available, the data mining operation is carried out on a sample. This is entirely legitimate 
if the aim is modeling (see chapter 1), which seeks to represent the prominent structures 
of the data, and not small idiosyncratic deviations. Such structures will be preserved in a 
sample, provided it is not too small. However, working with a small sample of a large 
data set may be less appropriate if the aim is pattern detection: in this case the aim may 
be to detect small deviations from the bulk of the data, and if the sample is too small 
such deviations may be excluded. Moreover, if the aim is to detect records that show 
anomalous behavior, the analysis must be based on the entire sample.  
It is when a sample is used that the power of inferential statistics comes into play. 
Statistical inference allows us to make statements about population structures, to 
estimate the size of these structures, and to state our degree of confidence in them, all 
on the basis of a sample. (See figure 4.1 for a simple illustration of the roles of probability 
and statistics). Thus, for example, we could say that our best estimate of a population 
value is 6.3, and that one is 95% confident that the true population value lies between 
5.9 and 6.7. (Definition and interpretation of intervals such as these is a delicate point, 
and depends on what philosophical basis we adopt—frequentist or Bayesian, for 
example. We shall say more about such intervals later in this chapter.) Note the use of 
the word estimate for the population value here. If we were basing our analysis on the 
entire population, we would use the word calculate: if all the constituent numbers are 
known, we can actually calculate the population value, and no notion of estimation 
arises. 

 

Figure 4.1: An Illustration of the Dual Roles of Probability and Statistics in Data Analysis. 
Probability Specifies How Observed Data Can be Generated From Models. Statistical 
Inference Allows Us to Infer Models From Observed Data.  

In order to make an inference about a population structure, we must have a model or 
pattern structure in mind: we would not be able to assess the evidence for some 
structure underlying the data if we never contemplated the existence of such a structure. 
So, for example, we might hypothesize that the value of some variable Z depends on the 
values of two other variables X and Y . Our model is that Z is related to X and Y . Then 
we can estimate the strength of these relationships in the population. (Of course, we may 
conclude that one or both of the relationships are of strength zero—that there is no 
relationship.) 
Statistical inference is based on the premise that the sample has been drawn from the 
population in a random manner—that each member of the population had a particular 
probability of appearing in the sample. The model will specify the distribution function for 
the population—the probability that a particular value for the random variable will arise in 
the sample. For example, if the model indicates that the data have arisen from a Normal 
distribution with a mean of 0 and a standard deviation of 1, it also tells us that the 
probability of observing a value as large as +20 is very small. Indeed, under the 
assumption that the model is correct, a precise probability can be put on observing a 
value greater than +20. Given the model, we can generally compute the probability that 
an observation will fall within any interval. For samples from categorical distributions, we 
can estimate the probability that values equal to each of the observed values would have 
arisen. In general, if we have a model M for the data we can state the probability that a 
random sampling process would lead to the data D = {x(1), ..., x(n)}, here x(i) is the ith p-
dimensional vector of measurements (the ith row in our n × p data matrix). This 

probability is expressed as p(D | M). Often we do not make dependence on the model M 
explicit and simply write p(D), relying on the context to make it clear. (As noted in the 
appendix the probability of observing any particular value of a variable that has a 
continuous cumulative distribution function is zero—particular values refer to intervals of 
length zero, and therefore the area under the probability density function across such an 
interval is zero. However, all real data actually refer to finite (if small) intervals (e.g., if 
someone is said to be 5 feet 11 inches tall, they are known to have a height in the 
interval between 5 feet 10.5 inches and 5 feet 11.5 inches). Thus it does make sense to 
talk of the probability of any particular data value being observed in practice.)? 
Let p(x(i)) be the probability of individual i having vector measurement x(i) (here p could 
be a probability mass function or a density function, depending on the nature of x). If we 
further assume that the probability of each member of the population being selected for 
inclusion in the sample has no effect on the probability of other members being selected 
(that is, that the separate observations are independent, or that the data are drawn ""at 
random""), the overall probability of observing the entire distribution of values in the 
sample is simply the product of the individual probabilities: 

(4.3)  

where M is the model and ? are the parameters of the model (assumed fixed at this 
point). (When regarded as a function of the parameters ? in the model M, this is called 
the likelihood function. We discuss it in detail below.) Methods have been developed to 
cope with situations in which observing one value alters the chance of observing 
another, but independence is by far the most commonly used assumption, even when it 
is only approximately true. 
Based on this probability, we can decide how realistic the assumed model is. If our 
calculations suggest it is very unlikely that the assumed model would have given rise to 
the observed data, we might feel justified in rejecting the model; this is the principle 
underlying hypothesis tests (section 4.6). In hypothesis testing we decide to reject an 
assumed model (the null hypothesis) if the probability of the observed data arising under 
that model is less than some pre-specified value (often 0.01 or 0.05—the significance 
level of the test). 
A similar principle is used in estimating population values for the parameters of the 
model. Suppose that our model indicates that the data arise from a Normal distribution 
with unit variance but unknown mean µ. We could propose various values for the mean, 
for each one calculating the probability that the observed data would have arisen if the 
population mean had that value. We could carry out hypothesis tests for each value, 
rejecting those with a low probability of having given rise to the observed data. Or we 
can short-cut this process and simply use the estimate of the mean with the highest 
probability of having generated the observed data. This value is called the maximum 
likelihood estimate of the mean, and the process we have described is maximum 
likelihood estimation (see section 4.5). The probability that a particular model would give 
rise to the observed data, when expressed as a function of the parameters, is called the 
likelihood function. This function can also be used to define an interval of likely values; 
we can say, for example, that, assuming our model is correct, 90% of intervals generated 
from a data sample in this way will contain the true value of the parameter. 

4.5 Estimation 
In chapter 3 we described several techniques for summarizing a given set of data. When 
we are concerned with inference, we want to make more general statements, statements 
about the entire population of values that might have been drawn. These are statements 
about the probability distribution or probability density function (or, equivalently, about 
the cumulative distribution function) from which the data are assumed to have arisen. 

4.5.1 Desirable Properties of Estimators 
In the following subsections we describe the two most important methods of estimating 
the parameters of a model: maximum likelihood estimation and Bayesian estimation. It is 
important to be aware of the differing properties of different methods so that we can 
adopt a method suited to our problem. Here we briefly describe some attractive 

 
 

. Thus,  is a random variable. Therefore, it has a distribution, with 

properties of estimators. Let  be an estimator of a parameter ?. Since 
derived from the data, if we were to draw a different sample of data, we would obtain a 
different value for 
different values arising as different samples are drawn. We can obtain descriptive 
summaries of that distribution. It will, for example, have a mean or expected value, 
. 
Here the expectation function E is taken with respect to the true (unknown) distribution 
from which the data are assumed to be sampled—that is, over all possible data sets of 
size n that could occur weighted by their probability of occurrence. 
The bias of  (a concept we introduced informally in chapter 2) is defined as 

is a number 

(4.4)  

the difference between the expected value of the estimator 
parameter ?. Estimators for which 
have bias 0 are said to be unbiased. Such 
estimators show no systematic departure from the true parameter value on average, 
is far away from ?. Note 
although for any particular single data set D we might have that 
that since both the sampling distribution and the true value of ? are unknown in practice, 
we cannot typically calculate the actual bias for a given data set. Nonetheless, the 
general concept of bias (and variance, below) is of fundamental importance in 
estimation. 

and the true value of the 

Just as the bias of an estimator can be used as a measure of its quality, so also can its 
variance: 

(4.5)  

The variance measures the random, data-driven component of error in our estimation 
procedure; it reflects how sensitive our estimator will be to the idiosyncrasies of 
individual data sets. Note that the variance does not depend on the true value of ?—it 
simply measures how much our estimates will vary across different observed data sets. 
Thus, although the true sampling distribution is unknown, we can in principle get a data-
driven estimate of the variance of an estimator, for a given value of  n, by repeatedly 
subsampling our original data set and calculating the variance of the estimated  s across 
these simulated samples. We can choose between estimators that have the same bias 
by choosing one with minimum variance. Unbiased estimators that have minimum 
variance are called, unsurprisingly, best unbiased estimators. 
As an extreme example, if we were to completely ignore our data  D and simply say 
arbitrarily that 
is zero since the estimate  never 
changes as D changes—however this would be a very the estimate ineffective estimator 
in practice since unless we made a very lucky guess we are almost certainly wrong in 
our estimate of ?, i.e., there will be a non-zero (and potentially very large) bias. 
The mean squared error of 
value of the estimator and the true value of the parameter. Mean squared error has a 
natural decomposition as the sum of the squared bias of  and its variance: 

the mean of the squared difference between the 

for every data set, then 

is 

(4.6)  

where in going from the first to second lines above we took advantage of the fact that 
various cross-terms in the squared expression cancel out, noting (for example) that E[?] 
= ? since ? is a constant, etc. Mean squared error is a very useful criterion since it 
incorporates both systematic (bias) and random (variance) differences between the 
estimated and true values. (Of course it too is primarily of theoretical interest, since to 
calculate it we need to know ?, which we don't in practice). Unfortunately, bias and 
variance often work in different directions: modifying an estimator to reduce its bias 
increases its variance, and vice versa. The trick is to arrive at the best compromise. 
Balancing bias and variance is a central issue in data mining and we will return to this 
point in chapter 6 in a general context and in later chapters in more specific contexts. 

There are also more subtle aspects to the use of mean squared error in estimation. For 
example, mean squared error treats equally large departures from ? as equally serious, 

regardless of whether they are above or below ?. This is appropriate for measures of 
location, but may not be appropriate for measures of dispersion (which, by definition, 
have a lower bound of zero) or for estimates of probabilities or probability densities. 
Suppose that we have sequence 
of estimators, based on increasing sample 
sizes n1, ..., nm. The sequence is said to be consistent if the probability of the difference 
between  and the true value ? being greater than any given value tends to 0 as the 
sample size increases. This is clearly an attractive property (especially in data mining 
contexts, with large samples) since the larger the sample is the closer the estimator is 
likely to be to the true value (assuming that the data are coming from a particular 
distribution—as discussed in chapters 1 and 2, for very large databases this may not be 
a reasonable assumption). 

4.5.2 Maximum Likelihood Estimation 
Maximum likelihood estimation is the most widely used method of parameter estimation. 
Consider a data set of n observations D = {x, ..., x(n)}, independently sampled from the 
same distribution ƒ(x | ?) (as statisticians say, independently and identically distributed 
or iid). The likelihood function L(? | x(1), ..., x(n)) is the probability that the data would 
have arisen, for a given value of ?, regarded as a function of ?, i.e., p(D | ?). Note that 
although we are implicitly assuming a particular model M here, as defined by ƒ(x | ?), for 
convenience we do not explicitly condition on  M in our likelihood definitions below—later, 
when we consider multiple models we will need to explicitly keep track of which model 
we are talking about. 

Since we have assumed that the observations are independent we have 

(4.7)  

which is a scalar function of ? (where ? itself may be a vector of parameters rather than a 
single parameter). The likelihood of a data set L(? | D), the probability of the actual 
observed data D for a particular model, is a fundamental concept in data analysis. 
Defining a likelihood for a given problem amounts to specifying a probabilistic model for 
how the data were generated. It turns out that once we can state such a likelihood, the 
door is opened to the application of many general and powerful ideas from statistical 
inference. Note that since likelihood is defined as a function of ? the convention is that 
we can drop or ignore any terms in p(D | ?) that do not contain ?, i.e., likelihood is only 
defined within an arbitrary scaling constant, so it is the shape as a function of ? that 
matters and not the actual values that it takes. Note also that the idd assumption above 
is not necessary to define a likelihood: for example, if our n observations had a Markov 
dependence (where each x(i) depends on x(i - 1), we would define the likelihood as a 
product of terms such as ƒ(x(i) | x(i - 1), ?). 
The value for ? for which the data has the highest probability of having arisen is the 
maximum likelihood estimator (or MLE). We will denote the maximum likelihood 
estimator for ? as 

. 

Example 4.4  

 
Customers in a supermarket either purchase or do not purchase milk. Suppose we want an 
estimate of the proportion of customers purchasing milk, based on a sample x(1), ..., 
x(1000) of 1000 randomly drawn observations from the database. Here x(i) takes the value 
1 if the ith customer in the sample does purchase milk and 0 if he or she does not. A simple 
model here would be the observations independently follow a Binomial distribution 
(described in the appendix) with unknown parameter 0 = ? = 1; that is, ? is the probability 
that milk is purchased by a random customer. Under the usual assumption of conditional 
independence given the model, the likelihood can be written as 

where r is the number among the 1000 who do purchase milk. Taking logs of this yields 

 

l(?) = log L(?) = r log ? + (1000 - r) log(1  - ?), 

which, after differentiating and setting to zero, yields 

 

. Thus, the proportion purchasing milk is from which we 

from which we obtain 
obtain in fact also the maximum-likelihood estimate of ? under this Binomial model. 
In figure 4.2 we plot the likelihood as a function of ? for three hypothetical data sets under 
this Binomial model. The data sets correspond to 7 milk purchases, 70 milk purchases, and 
700 milk purchases out of n = 10, n = 100, and n = 1000, total purchases respectively. The 
peak of the likelihood function is at the same value, ? = 0.7 in each case, but the 
uncertainty about the true value of ? (as reflected in the ""spread"" of the likelihood function) 
becomes much smaller as n increases (i.e., as we obtain a large customer database). Note 
that the absolute value of the likelihood function is not relevant; only its shape is of 
importance. 

Figure 4.2: The Likelihood Function for Three Hypothetical Data Sets Under a Binomial 
Model: r = 7, n = 10 (Top), r = 70, n = 100 (Center), and r = 700, n = 1000 (Bottom).  

 

 

 
Example 4.5  

 
Suppose we have assumed that our sample x(1), ..., x(n) of n data points has arisen 
independently from a Normal distribution with unit variance and unknown mean ?. This sort 
of situation can arise when the source of uncertainty is measurement error; we may know 
that the results have a certain variance (here rescaled to 1), but not know the mean value 
for the object that is being repeatedly measured. Then the likelihood function for ? is 

with log-likelihood defined as 
(4.8)  

 

To find the MLE we set the derivative 

to 0 and get 

Hence, the maximum likelihood estimator 

 

for ? is 

, the sample mean.  

Figure 4.3 shows both the likelihood function L(?) and the log-likelihood l(?) = log  L(?) as a 
function of ? for a sample of 20 data points from a Normal density with a true mean of 0 
and a known standard deviation of 1. Figure 4.4 shows the same type of plot but with 200 
data points. Note how the likelihood function is peaked around the value of the true mean 
at 0. Also note (as in the Binomial example) how the likelihood function narrows as more 
data becomes available, reflecting decreasing support from the data for values of ? that are 
not close to 0. 

Figure 4.3: The Likelihood as a Function of ? for a Sample of 20 Data Points From a Normal 
Density with a True Mean of 0 and a Known Standard Deviation of 1: (a) a Histogram of 20 
Data Points Generated From the True Model (top), (b) the Likelihood Function for ? (Center), 
and (c) the Log-Likelihood Function for ? (Bottom).  

 

Figure 4.4: The Likelihood Function for the Same Model as in Figure 4.3 but with 200 Data 
Points: (a) a Histogram of the 200 Data Points Generated From the True Model (Top), (b) the 
Likelihood Function for ? (Center), and (c) the Log-Likelihood Function for ? (Bottom).  

 

 

 
Example 4.6  

 
A useful general concept in statistical estimation is the notion of a sufficient statistic. 
Loosely speaking, we can define a quantity s(D) as a sufficient statistic for ? if the likelihood 
L(?) only depends on the data through s(D). Thus, in the Binomial model above, the total 
number of ""successes"" r (the number of people who purchase milk) is a sufficient statistic 

for the Binomial parameter ?. It is sufficient in the sense that the likelihood is only a function 
of r (assuming n is known already). Knowing which particular customers purchased milk 
(which particular rows in the data matrix have 1's in the milk column) is irrelevant from the 
point of view of our Binomial model, once we know the sum total r. Similarly, for the 
example above involving the estimation of the mean of a Normal distribution, the sum of 
the observations 
is a sufficient statistic for the likelihood of the mean (keeping in 
mind that the likelihood is only defined as a function of ? and all other terms can be 
dropped). 

For massive data sets this idea of sufficient statistics can be quite useful in practice—
instead of working with the full data set we can simply compute and store the sufficient 
statistics, knowing that these are sufficient for likelihood-based estimation. For example, if 
we are gathering large volumes of data on a daily basis (e.g., Web logs) we can in principle 
just update the sufficient statistics nightly and throw the raw data away. Unfortunately, 
however, sufficient statistics often do not exist for many of the more flexible model forms 
that we like to use in data mining applications, such as trees, mixture models, and so forth, 
that are discussed in detail later in this book. Nonetheless, for simpler models, sufficient 
statistics are a very useful concept. 

 

 

Maximum likelihood estimators are intuitively and mathematically attractive; for example, 
they are consistent estimators in the sense defined earlier. Moreover, if 
is the MLE of 
a parameter ?, then 
is the MLE of the function g(?), though some care needs to be 
exercised if  g is not a one-to-one function. On the other hand, nothing is perfect—
maximum likelihood estimators are often biased (depending on the parameter and the 
underlying model), although this bias may be extremely small for large data sets, often 
scaling as O(1/n). 
For simple problems (where ""simple"" refers to the mathematical structure of the problem, 
and not to the number of data points, which can be large), MLEs can be found using 
differential calculus. In practice, the log-likelihood l(?) is usually maximized (as in the 
Binomial and Normal density examples above), since this replaces the awkward product 
in the definition with a sum; this process leads to the same result as maximizing L(?) 
directly because the logarithm is a monotonic function. Of course we are often interested 
in models that have more than one parameter (models such as neural networks (chapter 
11) can have hundreds or thousands of parameters). The univariate definition of 
likelihood generalizes directly to the multivariate case, but in this situation the likelihood 
is a mulutivariate function of d parameters (that is, a scalar-valued function defined on a 
d-dimensional parameter space). Since d can be large, finding the maximum of this d-
dimensional function can be quite challenging if no closed-form solution exists. We will 
return to this topic of optimization in detail in chapter 8 where we discuss iterative search 
methods. Multiple maxima can present a difficult problem (which is why stochastic 
optimization methods are often necessary), as can situations in which optima occur at 
the boundaries of the parameter space. 

Example 4.7  

 
Simple linear regression is widely used in data mining. This was mentioned briefly in 
chapter 1 and is discussed again in detail in chapter 11. In its simplest form it relates two 
variables: X, a predictor or explanatory variable, and Y , a response variable. The 
relationship is assumed to take the form Y = a + bX + e, where a and b are parameters and 
e is a random variable assumed to come from a Normal distribution with a mean of 0 and a 
variance of s2, and we can write e = Y - (a + bX). Here the data consists of a set of pairs D 
= {(x(1), y(1)), ..., (x(n), y(n))} and the probability density function of the response data 
given the explanatory data is ƒ(y(1), ..., y(n) |  x(1), ..., x(n), a, b). We are interested not in 
modeling the distribution of the xs, but rather in modeling ƒ(y|x). 

Thus, the likelihood (or more precisely, conditional likelihood) function for this model can be 
written as 

To find the maximum likelihood estimators of a and b, we can take logs and discard terms 
that do not involve either a or b. This yields 

 

 

Thus, we can estimate  a and b by finding those values that minimize the sum of squared 
differences between the predicted values a + bx(i) and the observed values y(i). Such a 
procedure—minimizing a sum of squares—is ubiquitous in data mining, and goes under the 
name of the least squares method. The sum of squares criterion is of great historical 
importance, with roots going back to Gauss and beyond. At first it might seem arbitrary to 
choose a sum of squares (why not a sum of absolute values, for example?), but the above 
shows how the least squares choice arises naturally from the choice of a Normal 
distribution for the error term in the model. 

 

 

Up to this now we have been discussing point estimates, single number estimates of the 
parameter in question. A point estimate is ""best"" in some sense, but it conveys no idea of 
the uncertainty associated with it—perhaps there was a large number of almost equally 
good estimates, or perhaps this estimate was by far the best. Interval estimates provide 
this sort of information. In place of a single number they give an interval with a specified 
degree of confidence that this interval contains the unknown parameter. Such an interval 
is called a confidence interval, and the upper and lower limits of the interval are called 
confidence limits. Interpretation of confidence intervals is rather subtle. Here, since we 
are assuming that ? is unknown but fixed, it does not make sense to say that ? has a 
certain probability of lying within a given interval: it either does or it does not. However, it 
does make sense to say that an interval calculated by the given procedure contains ? 
with a certain probability: after all, the interval is calculated from the sample, and is thus 
a random variable. 

Example 4.8  

 
The following example is deliberately artificial to keep the explanation simple. Suppose the 
data consist of 100 independent observations from a Normal distribution with unknown 
mean µ and known variance s 2, and we want a 95% confidence interval for µ. That is, 
given the data x(1), ..., x(n), we want to find a lower limit l(x) and an upper limit u(x) such 
that P(µ ˛
The distribution of the sample mean 
estimate of the mean, 
variance of s2/100, and hence standard deviation of s/10. We also know, from the 
properties of the Normal distribution (see the appendix), that 95% of the probability lies 
within 1.96 standard deviations of the mean. Hence, 

is known to follow a Normal distribution with a mean of µ and a 

 [l(x), u(x)]) = 0:95. 

in this situation (which is also the maximum likelihood 

 

This can be rewritten as 

Thus, 

and 

 
define a suitable 95% confidence interval. 

 

 

Frequently confidence intervals are based on the assumption that the sample statistic 
has a roughly Normal distribution. This is often realistic: the central limit theorem tells us 
that the distribution of many statistics can be approximated well by a Normal distribution, 

especially if the sample size is large. Using this approximation, we find an interval in 
which the statistic has a known probability of lying, given the unknown true parameter 
value, ?, and invert it to find an interval for the unknown parameter. In order to apply this 
approach, we need an estimate of the standard deviation of the estimator 
. One way to 
derive such an estimate is the bootstrap method. 

Example 4.9  

, and act as if 

were the real 

, which we hope are similar to the sampling 

. What we do is draw a subsample, 

, from 

 
Many bootstrap methods, of gradually increasing sophistication and complexity, have been 
developed over the last two decades. The basic idea is as follows. The data originally 
arose from a distribution F (X), and we wish to make some statement about this 
distribution. However, we have only a sample of data (x(1), ..., x(n)), which we may denote 
by 
distribution. We can repeat this many times, computing a statistic for each of these 
subsamples. This process gives us information on the sampling  properties of statistics 
calculated from samples drawn from 
properties of statistics calculated from samples drawn from F (X). 
To illustrate, consider an early approach to estimating the performance of a predictive 
classification rule. As we have discussed above, evaluating performance of a classification 
rule simply by reclassifying the data used to design it is unwise—it is likely to lead to 
optimistically biased estimates. Suppose that eA is the estimate of misclassification rate 
obtained by the simple resubstitution process of estimating the classification error on the 
same data as was used to estimate the parameters of the classification model. We really 
want to estimate eC, the ""true"" misclassification rate which we expect to achieve on future 
objects. The difference between these is (eC - eA). If we could estimate this difference, we 
could adjust eA to yield a better estimate. In fact, we can estimate this difference, as 
follows. Suppose we regard 
. Now, acting as if 
the subsample 
these two situations will give us an estimate of the difference (eC - eA). To reduce any 
effects arising from the randomness of the sampling procedure, we repeat the subsampling 
many times and average the results. The final result is an estimate of the difference (eC - 
eA) that can be added to the value of eA obtained by resubstituting the data 
based on 

were the true distribution, we can build a rule based on the data in 
. The difference in performance in 

as the true distribution and draw from it a subsample—

, to yield an estimate of the true misclassification rate  eC. 

and apply it both to 

and to 

into the rule 

 

 

4.5.3 Bayesian Estimation 
In the frequentist approach to inference described so far the parameters of a population 
are fixed but unknown, and the data comprise a random sample from that population 
(since the sample was drawn in a random way). The intrinsic variability thus lies in the 
data D = {x(1), ..., x(n)}. In contrast, Bayesian statistics treats the data as known—after 
all, they have been observed and recorded—and the parameters ? as random variables. 
Thus, whereas frequentists regard a parameter ? as a fixed but unknown quantity, 
Bayesians regard ? as having a distribution of possible values and see the observed 
data as possibly shedding light on this distribution. p(?) reflects our degree of belief on 
where the true (unknown) parameters ? may be. If p(?) is very peaked about some value 
of ? then we are very sure about our convictions (although of course we may be entirely 
wrong!). If p(?) is very broad and flat (and this is the more typical case) then we are 
expressing a prior belief that is less certain on the location of ?. 
Note that while the term Bayesian has a fairly precise meaning in statistics, it has 
sometimes been used in a somewhat looser manner in the computer science and pattern 
recognition literature to refer to the use of any form of probabilistic model in data 
analysis. In this text we adopt the more standard and widespread statistical definition, 
which is described below. 
Before the data are analyzed, the distribution of the probabilities that ? will take different 
values is known as the prior distribution p(?). Analysis of the data D leads to modification 
of this distribution to take into account the information in the empirical data, yielding the 

posterior distribution, p(? | D). The modification from prior to posterior is carried out by 
means of a theorem named after Thomas Bayes: 

(4.9)  

Note that this updating procedure leads to a distribution, rather than a single value, for ?. 
However, the distribution can be used to yield a single value estimate. We could, for 
example, take the mean of the posterior distribution, or its mode (the latter technique is 
known as the maximum a posteriori method, or MAP). If we choose the prior p(?) in a 
specific manner (e.g., p(?) is uniform over some range), the MAP and maximum 
likelihood estimates of ? may well coincide (since in effect the prior is ""flat"" and prefers 
no one value of ? over any other). In this sense, maximum likelihood can be viewed as a 
special case of the MAP procedure, which in turn is a restricted (""point estimate"") form of 
Bayesian estimation. 
For a given set of data D and a particular model, the denominator in equation 4.9 is a 
constant, so we can alternatively write the expression as 

(4.10)  

Here we see that the posterior distribution of ? given  D (that is, the distribution 
conditional on having observed the data D) is proportional to the product of the prior p(?) 
and the likelihood p(D | ?). If we have only weak beliefs about the likely value of the 
parameter before collecting the data, we will want to choose a prior that spreads the 
probability widely (for example, a Normal distribution with large variance). In any case, 
the larger the set of observed data, the more the likelihood dominates the posterior 
distribution, and the lower the importance of the particular shape of the prior. 

Example 4.10  

 
Consider example 4.4 once again involving the proportion of customers who purchase milk, 
where we consider a single binary variable X and wish to estimate ? = p(X = 1). A widely 
used prior for a parameter ? that varies between 0 and 1 is the Beta distribution, defined as 
(4.11)  
where a > 0; ß > 0 are the two parameters of this model. It is straightforward to show that 

, that the mode of ? is 

, and the variance is 

. Thus, if we 

assume for example that a and 3ß are chosen to be both greater than 1, we can see that 
the relative sizes of a and ß control the location of both the mean and the mode: if a = ß 
then the mean and the mode are at 0. If a < ß then the mode is less than 0.5, and so forth. 
Similarly, the variance is inversely proportional to a + ß: the size of the sum a+ß controls 
the ""narrowness"" of the prior p(?). If a and ß are relatively large,we will have a relatively 
narrow peaked prior about the mode. In this manner, we can choose a and ß to reflect any 
prior beliefs we might have about the parameter ?. 
Recall from example 4.4 that the likelihood function for ? under the Binomial model can be 
written as 
(4.12)  
where r is the number of 1's in the n total observations. We see that the Beta and Binomial 
likelihoods are similar in form: the Beta looks like a Binomial likelihood with a - 1 prior 
successes and ß - 1 prior failures. Thus, in effect, we can think of a + ß - 2 as the 
equivalent sample size for the prior, i.e., it is as if our Beta prior is based on this many prior 
observations.  

Combining the likelihood and the prior, we get 
(4.13)  

This is conventiently in the form of another Beta distribution, i.e., the posterior on ?, p(?|D), 
is itself another Beta distribution but with parameters r + a and n - r + ß. 
Thus, for example, the mean of this posterior distribution p(?|D) is 
. Otherwise, we get a modified estimate, 
intuitive. If a = ß = 0 we get the standard MLE of 
where not all weight is placed on the data alone (on r and n). For example, in data mining 
practice, it is common to use the heuristic estimate of 
rather than the MLE, corresponding in effect to using a point estimate based on posterior 

for estimates of probabilities, 

. This is very 

mean and a Beta prior with a = ß = 1. This has the effect of ""smoothing"" the estimate away 
from the extreme values of 0 and 1. For example, consider a supermarket where we 
wanted to estimate the probability of a particular product being purchased, but in the 
available sample  D we had r = 0 (perhaps the product is purchased relatively rarely and no-
one happened to buy it in the day we drew a sample). The MLE estimate in this case would 
be 0, whereas the posterior mean would be 
, which is close to 0 for large n but allows for 
a small(but non-zero) probability in the model for that the product is purchased on an 
average day. 
In general, with high-dimensional data sets (i.e., large p) we can anticipate that certain 
events will not occur in our observed data set D. Rather than committing to the MLE 
estimate of a probability ? = 0, which is equivalent to stating that the event is impossible 
according to the model, it is often more prudent to use a Bayesian estimate of the form 
described here. For the supermarket example, the prior p(?) might come from historical 
data at the same supermarket, or from other stores in the same geographical location. This 
allows information from other related analyses (in time or space) to be leveraged, and 
leads to the more general concept of Bayesian hierarchical models (which is somewhat 
beyond the scope of this text). 

 

 

One of the primary distinguishing characteristics of the Bayesian approach is the 
avoidance of so-called point-estimates (such as a maximum likelihood estimate of a 
parameter) in favor of retaining full knowledge of all uncertainty involved in a problem 
(e.g., calculating a full posterior distribution on ?). 
As an example, consider the Bayesian approach to making a prediction about a new 
data point x(n + 1), a data point not in our training data set D.  
Here x might be the value of the Dow-Jones financial index at the daily closing of the 
stock-market and n + 1 is one day in the future. Instead of using a point estimate for 
in 
our model for prediction (as we would in a maximum likelihood or MAP framework), the 
Bayesian approach is to average over all possible values of ?, weighted by their 
posterior probability p(? | D): 

(4.14)  

since x(n + 1) is conditionally independent of the training data D, given ?, by definition. In 
fact, we can take this further and also average over different models, using a technique 
known as Bayesian model averaging. Naturally, all of this averaging can entail 
considerably more computation than the maximum likelihood approach. This is a primary 
reason why Bayesian methods have become practical only in recent years (at least for 
small-scale data sets). For large-scale problems and high-dimensional data, fully 
Bayesian analysis methods can impose significant computational burdens. 
Note that the structure of equations 4.9 and 4.10 enables the distribution to be updated 
sequentially. For example, after we build a model with data D1, we can update it with 
further data  D2: 

(4.15)  

This sequential updating property is very attractive for large sets of data, since the result 
is independent of the order of the data (provided, of course, that D1 and D2 are 
conditionally independent given the underlying model p). 
The denominator in equation 4.9, p(D) = ?? p(D | ?)p(  ? )d?, is called the predictive 
distribution of  D, and represents our predictions about the value of D. It includes our 
uncertainty about ?, via the prior p(?), and our uncertainty about D when ? is known, via 
p(D | ?). The predictive distribution changes as new data are observed, and can be 
useful for model checking: if observed data D have only a small probability according to 
the predictive distribution, that distribution is unlikely to be correct. 

Example 4.11  

 

Suppose we believe that a single data point x comes from a Normal distribution with 
unknown mean ? and known variance a—that is, x ~ N(?, a). Now suppose our prior 
distribution for ? is ? ~ N(?0, a0), with known ?0 and a0. Then 

The mathematics here looks horribly complicated (a fairly common occurrence with 
Bayesian methods), but consider the following reparameterization. Let 

 

 

and 

?1 = a1(?0/a0 + x/a). 

After some algebraic manipulations we get 

 

Since this is a probability density function for ?, it must integrate to unity. Hence the 
posterior on ? has the form 

 

This is a Normal distribution N(?1, a1). Thus the Normal prior distribution has been updated 
to yield a Normal posterior distribution and therefore the complicated mathematics can be 
avoided. Given a Normal prior for the mean and data arising from a Normal distribution as 
above, we can obtain the posterior merely by computing the updated parameters. 
Moreover, the updating of the parameters is not as messy as it might at first seem. 
Reciprocals of variances are called precisions. Here 1/a1, the precision of the updated 
distribution, is simply the sum of the precisions of the prior and the data distributions. This 
is perfectly reasonable: adding data to the prior should decrease the variance, or increase 
the precision. Likewise, the updated mean, ?1, is simply a weighted sum of the prior mean 
and the datum x, with weights that depend on the precisions of those two values. 
When there are n data points, with the situation described above, the posterior is again 
Normal, now with updated parameter values 

a1 = (1/a0 + n/a)-1  

and 

 

 

 

The choice of prior distribution can play an important role in Bayesian analysis (more for 
small samples than for large samples as mentioned earlier). The prior distribution 
represents our initial belief that the parameter takes different values. The more confident 
we are that it takes particular values, the more closely the prior will be bunched around 
those values. The less confident we are, the larger the dispersion of the prior. In the case 
of a Normal mean, if we had no idea of the true value, we would want to use a prior that 
gave equal probability to each possible value, i.e., a prior that was perfectly flat or that 
had infinite variance. This would not correspond to any proper density function (which 
must have some non-zero values and which must integrate to unity). Still, it is sometimes 
useful to adopt improper priors that are uniform throughout the space of the parameter. 
We can think of such priors as being essentially flat in all regions where the parameter 
might conceivably occur. Even so, there remains the difficulty that priors that are uniform 
for a particular parameter are not uniform for a nonlinear transformation of that 
parameter. 
Another issue, which might be seen as either a difficulty or a strength of Bayesian 
inference, is that priors show an individual's prior belief in the various possible values of 

a parameter—and individuals differ. It is entirely possible that your prior will differ from 
mine and therefore we will probably obtain different results from an analysis. In some 
circumstances this is fine, but in others it is not. One way to overcome this problem is to 
use a so-called reference prior, a prior that is agreed upon by convention. A common 
form of reference prior is Jeffrey's prior. To define this, we first need to define the Fisher 
information: 

(4.16)  

for a scalar parameter ?—that is, the negative of the expectation of the second derivative 
of the log-likelihood. Essentially this measures the curvature or flatness of the likelihood 
function. The flatter a likelihood function is, the less the information it provides about the 
parameter values. Jeffrey's prior is then defined as 

(4.17)  

. This means that a consistent prior will result no matter how 

This is a convenient reference prior since if f = f(?) is some function of ?, this has a 
prior proportional to 
the parameter is transformed. 
The distributions in the examples display began with a Beta or Normal prior and ended 
with a Beta or Normal posterior. Conjugate families of distributions satisfy this property in 
general: the prior distribution and posterior distribution belong to the same family. The 
advantage of using conjugate families is that the complicated updating process can be 
replaced by a simple updating of the parameters. 
We have already remarked that it is straightforward to obtain single point estimates from 
the posterior distribution. Interval estimates are also easy to obtain—integration of the 
posterior distribution over a region gives the estimated probability that the parameter lies 
in that region. When a single parameter is involved and the region is an interval, the 
result is a credibility interval. The shortest possible credibility interval is the interval 
containing a given probability (say 90%) such that the posterior density is highest over 
the interval. Given that one is prepared to accept the fundamental Bayesian notion that 
the parameter is a random variable, the interpretation of such intervals is much more 
straightforward than the interpretation of frequentist confidence intervals. 
Of course, it is a rare model that involves only one parameter. Typically models involve 
several or many parameters. In this case we can find joint posterior distributions for all 
parameters simultaneously or for individual (sets of) parameters alone. We can also 
study conditional distributions for some parameters given fixed values of the others. Until 
recently, Bayesian statistics provided an interesting philosophical viewpoint on inference 
and induction, but was of little practical value; carrying out the integrations required to 
obtain marginal distributions of individual parameters from complicated joint distributions 
was too difficult (only in rare cases could analytic solutions be found, and these often 
required the imposition of undesirable assumptions). However, in the last 10 years or so 
this area has experienced something of a revolution. Stochastic estimation methods, 
based on drawing random samples from the estimated distributions, enable properties of 
the distributions of the parameters to be estimated and studied. These methods, called 
Markov chain Monte Carlo (MCMC) methods are discussed again briefly in chapter 8. 

It is worth repeating that the primary characteristic of Bayesian statistics lies in its 
treatment of uncertainty. The Bayesian philosophy is to make all uncertainty explicit in 
any data analysis, including uncertainty about the estimated parameters as well as any 
uncertainty about the model. In the maximum likelihood approach, a point estimate of a 
parameter is often considered the primary goal, but a Bayesian analyst will report a full 
posterior distribution on the parameter as well as a posterior on model structures. 
Bayesian prediction consists of taking weighted averages over parameter values and 
model structures (where the weights are proportional to the likelihood of the parameter or 
model given the data, times the prior). In principle, this weighted averaging can provide 
more accurate predictions than the alternative (and widely used) approach of 
conditioning on a single model using point estimates of the parameters. However, in 
practice, the Bayesian approach requires estimation of the averaging weights, which in 
high-dimensional problems can be difficult. In addition, a weighted average over 
parameters or models is less likely to be interpretable if description is a primary goal. 

 

 

4.6 Hypothesis Testing 

Although data mining is primarily concerned with looking for unsuspected features in 
data (as opposed testing specific hypotheses that are formed before we see the data), in 
practice we often do want to test specific hypotheses (for example, if our data mining 
algorithm generates a potentially interesting hypothesis that we would like to explore 
further). 
In many situations we want to see whether the data support some idea about the value 
of a parameter. For example, we might want to know if a new treatment has an effect 
greater than that of the standard treatment, or if two variables are related in a population. 
Since we are often unable to measure these for an entire population, we must base our 
conclusions on a samples. Statistical tools for exploring such hypotheses are called 
hypothesis tests. 

4.6.1 Classical Hypothesis Testing 
The basic principle of hypothesis tests is as follows. We begin by defining two 
complementary hypotheses: the null hypothesis and the alternative hypothesis. Often the 
null hypothesis is some point value (e.g., that the effect inquestion has value zero—that 
there is no treatment difference or regression slope) and the alternative hypothesis is 
simply the complement of the null hypothesis. Suppose, for example, that we are trying 
to draw conclusions about a parameter ?. The null hypothesis, denoted by H0, might 
state that ? = ?0, and the alternative hypothesis (H1) might state that ? ??0. Using the 
observed data, we calculate a statistic (what form of statistic is best depends on the 
nature of the hypothesis being tested; examples are given below). The statistic would 
vary from sample to sample—it would be a random variable. If we assume that the null 
hypothesis is correct, then we can determine the expected distribution for the chosen 
statistic, and the observed value of the statistic would be one point from that distribution. 
If the observed value were way out in the tail of the distribution, we would have to 
conclude either that an unlikely event had occurred or that the null hypothesis was not, in 
fact, true. The more extreme the observed value, the less confidence we would have in 
the null hypothesis. 
We can put numbers on this procedure. Looking at the top tail of the distribution of the 
statistic (the distribution based on the assumption that the null hypothesis is true), we 
can find those potential values that, taken together, have a probability of 0.05 of 
occurring. These are extreme values of the statistic—values that deviate quite 
substantially from the bulk of the values, assuming the null hypothesis is true. If this 
extreme observed value did lie in this top region, we could reject the null hypothesis ""at 
the 5% level"": only 5% of the time would we expect to see a result in this region—as 
extreme as this—if the null hypothesis were correct. For obvious reasons, this region is 
called the rejection region or critical region. Of course, we might not merely be interested 
in deviations from the null hypothesis in one direction. That is, we might be interested in 
the lower tail, as well as the upper tail of the distribution. In this case we might define the 
rejection region as the union of the test statistic values in the lowest 2.5% of the 
probability distribution and the test statistic values in the uppermost 2.5% of the 
probability distribution. This would be a two -tailed test, as opposed to the previously 
described one-tailed test. The size of the rejection region, known as the significance 
level of the test, can be chosen at will. Common values are 1%, 5%, and 10%. 
We can compare different test procedures in terms of their power. The power of a test is 
the probability that it will correctly reject a false null hypothesis. To evaluate the power of 
a test, we need a specific alternative hypothesis so we can calculate the probability that 
the test statistic will fall in the rejection region if the alternative hypothesis is true. 
A fundamental question is how to find a good test statistic for a particular problem. One 
strategy is to use the likelihood ratio. The likelihood ratio statistic used to test the 
hypothesis H0 : ? = ?0 against the alternative H1 : ? ? ?0 is defined as 

(4.18)  

where D = {x(1), ..., x(n)}. That is, the ratio of the likelihood when ? = ?0 to the largest 
value of the likelihood when ? is unconstrained. Clearly, the null hypothesis should be 

rejected when ? is small. This procedure can easily be generalized to situations in which 
the null hypothesis is not a point hypothesis but includes a set of possible values for ?. 

Example 4.12  

 
Suppose that we have a sample of n points independently drawn from a Normal distribution 
with unknown mean and unit variance, and that we wish to test the hypothesis that the 
mean has a value of 0. The likelihood under this (null hypothesis) assumption is 

The maximum likelihood estimator of the mean of a Normal distribution is the sample 
mean, so the unconstrained maximum likelihood is 

 

 

The ratio of these simplifies to 

Therefore, our rejection region is thus {? | ? = c} for a suitably chosen value of c. This 
expression can be rewritten as 

 

 

where 
constant. 

is the sample mean. Thus, the test statistic  has to be compared with a 

 

 

Certain types of tests are used very frequently. These include tests of differences 
between means, tests to compare variances, and tests to compare an observed 
distribution with a hypothesized distribution (so-called goodness-of-fit tests). The 
common t-test of the difference between the means of two independent groups is 
described in the display below. Descriptions of other tests can be found in introductory 
statistics texts. 

Example 4.13  

 
Let x(1), ..., x(n) be a sample of n observations randomly drawn from a Normal distribution 
N(µx, s 2), and let y(1), ..., y(m) be an independent sample of m observations randomly 
drawn from a Normal distribution N(µy, s 2). Suppose we wish to test the hypothesis that the 
means are equal, H0 : µx = µy. The likelihood ratio statistic under these circumstances 
reduces to 

 

 

with 

where 

 

is the estimated variance for the x sample and 
quantity s is thus a simple weighted sum of the sample variances of the two samples, and 
the test statistic is merely the difference between the two sample means adjusted by the 
estimated standard deviation of that difference. Under the null hypothesis, t follows a t 
distribution (see the  appendix) with n + m - 2 degrees of freedom. 

is the same coefficient for the  ys. The 

Although the two populations being compared here are assumed to be Normal, this test is 
fairly robust to departures from Normality, especially if the sample sizes and the variances 
are roughly equal. This test is very widely used. 

 

 
Example 4.14  

 

Relationships between variables are often of central interest in data mining. At an extreme, 
we might want to know if two variables are not related at all, so that the distribution of the 
value taken by one is the same regardless of the value taken by the other. A suitable test 
for independence of two categorical variables is the chi-squared test. This is essentially a 
goodness-of-fit test in which the data are compared with a model based on the null 
hypothesis of independence. 
Suppose we have two variables, x and y, with  x taking the values xi, i = 1, …, r with 
probabilities p (xi) and  y taking the values yj, j = 1, …, s with probabilities p (yj). Suppose 
that the joint probabilities are p (xi, yj). Then, if x and y are independent, p (xi, yj) = p (xi) p 
(yj). The data permit us to estimate the distributions p (xi) and  p (yj) simply by calculating 
the proportions of the observations that fall at each level of x and the proportions that fall at 
each level of y. Let the estimate of the probability of the  x variable taking value  xi be n (xi) 
/n and n (xi) /n the estimate of the probability of the  y variable taking value  yj. Multiplying 
these together gives us estimates of the probabilities we would expect in each cell, under 
the independence hypothesis; thus, our estimate of p (xi, yj) under the independence 
assumption is n (xi) n (yj) /n2. Since there are n observations altogether, this means we 
would expect, under the null hypothesis, to find n (xi) n (yj) /n observations in the (xi, yj)th 
cell. For convenience, number the cells sequentially in some order from 1 to t (so t = r.s) 
and let Ek represent the expected number in the kth cell. We can compare this with the 
observed number in the kth cell, which we shall denote as Ok. Somehow, we need to 
aggregate this comparison over all t cells. A suitable aggregation is given by 
(4.19)  

The squaring here avoids the problem of positive and negative differences canceling out, 
and the division by Ek prevents large cells dominating the measure. If the null hypothesis of 
independence is correct, X2 follows a ?2 distribution with (r - 1) (s - 1) degrees of freedom, 
so that significance levels can either be found from tables or be computed directly. 

We illustrate using medical data in which the outcomes of surgical operations (no 
improvement, partial improvement, and complete improvement) are classified according to 
the kind of hospital in which they occur (""referral"" or ""non-referral""). The data are illustrated 
below, and the question of interest is whether the outcome is independent of hospital type 
(that is, whether the outcome distribution is the same for both types of hospital). 

§   

  

Referral 

No 
improvem
ent 

Partial 
improvem
ent 

Complete 
improvem

43 

29 

10 

Non-
refer
ral 

47 

120 

118 

  

ent 

Referral 

Non-
refer
ral 

The total number of patients from referral hospitals is (43 + 29 + 10) = 82, and the total 
number of patients who do not improve at all is (43 + 47) = 90. The overall total is 367. 
From this it follows that the expected number in the top left cell of the table, under the 
independence assumption, is 82 × 90/367 = 20:11. The observed number is 43, so this cell 
contributes a value of (20:11 - 43)2 /20:11 to X2. Performing similar calculations for each of 
the six cells, and adding the results yields X2 = 49:8. Comparing this with a ?2 distribution 
with (3 - 1) (2 - 1) = 2 degrees of freedom reveals a very high level of significance, 
suggesting that the outcome of surgical operations does depend on hospital type. 

 

 

The hypothesis testing strategy outlined above is based on the assumption that a 
random sample has been drawn from some distribution, and the aim of the testing is to 
make a probability statement about a parameter of that distribution. The ultimate 
objective is to make an inference from the sample to the underlying population of 
potential values. For obvious reasons, this is sometimes described as the sampling 
paradigm. An alternative strategy is sometimes appropriate, especially when we are not 
confident that the sample has been obtained though probability sampling (see chapter 
2), and therefore inference to the underlying population is not possible. In such cases, 
we can still sometimes make a probability statement about some effect under a null 
hypothesis. Consider, for example, a comparison of a treatment and a control group. We 
might adopt as our null hypothesis that there is no treatment effect, so the distribution of 
scores of people who received the treatment should be the same as that of those who 
did not. If we took a sample of people (possibly not randomly drawn) and randomly 
assign them to the treatment and control groups, we would expect the difference of 
mean scores between the groups to be small if the null hypothesis was true. Indeed, 
under fairly general assumptions, it is not difficult to work out the distribution of the 
difference between the sample means of the two groups we would expect if there were 
no treatment effect, and if such difference were just a consequence of an imbalance in 
the random allocation. We can then explore how unlikely it is that a difference as large or 
larger than that actually obtained would be seen. Tests based on this principle are 
termed randomization tests or permutation tests. Note that they make no statistical 
inference from the sample to the overall population, but they do enable us to make 
conditional probability statements about the treatment effects, conditional on the 
observed values. 
Many statistical tests make assumptions about the forms of the population distributions 
from which the samples are drawn. For example, in the two-sample t-test, illustrated 
above, an assumption of Normality was made. Often, however, it is inconvenient to make 
such assumptions. Perhaps we have little justification for the assumption, or perhaps we 
know that the data do not to follow the form required by a standard test. In such 
circumstances we can adopt distribution-free tests. Tests based on ranks fall into this 
class. Here the basic data are replaced by the numerical labels of the positions in which 
they occur. For example, to explore whether two samples arose from the same 
distribution, we could replace the actual numerical values by their ranks. If they did arise 
from the same distribution, we would expect the ranks of the members of the two 
samples to be well mixed. If, however, one distribution had a larger mean than the other, 
we would expect one sample to tend to have large ranks and the other to have small 
ranks. If the distributions had the same means but one sample had a larger variance 
than the other, we would expect one sample to show a surfeit of large and small ranks 
and the other to dominate the intermediate ranks. Test statistics can be constructed 
based on the average values or some other measurements of the ranks, and their 
significance levels can be evaluated using randomization arguments. Such test statistics 
include the sign test statistic, the rank sum test statistic, the Kolmogorov-Smirnov test 
statistic, and the Wilcoxon test statistic. Sometimes the term nonparametric test is used 

to describe such tests—the rationale being that these tests are not testing the value of a 
parameter of any assumed distribution. 
Comparison of hypotheses H0 and H1 from a Bayesian perspective is achieved by 
comparing their posterior probabilities: 

(4.20)  

Taking the ratio of these leads to a factorization in terms of the prior odds and the 
likelihood ratio, or Bayes factor: 

(4.21)  

There are some complications here, however. The likelihoods are marginal likelihoods 
obtained by integrating over parameters not specified in the hypotheses, and the prior 
probabilities will be zero if the  Hi refer to particular values from a continuum of possible 
values (e.g., if they refer to values of a parameter ?, where ? can take any value 
between 0 and 1). One strategy for dealing with this problem is to assign a discrete non-
zero prior probability to the given values of ?. 

4.6.2 Hypothesis Testing in Context 

This section has so far described the classical (frequentist) approach to statistical 
hypothesis testing. In data mining, however, analyses can become more complicated. 
Firstly, because data mining involves large data sets, we should expect to obtain 
statistical significance: even slight departures from the hypothesized model form will be 
identified as significant, even though they may be of no practical importance. (If they are 
of practical importance, of course, then well and good.) Worse, slight departures from the 
model arising from contamination or data distortion will show up as significant. We have 
already remarked on the inevitability of this problem. 
Secondly, sequential model fitting processes are common. Beginning in chapters 8 we 
will describe various stepwise model fitting procedures, which gradually refine a model 
by adding or deleting terms. Running separate tests on each model, as if it were de 
novo, leads to incorrect probabilities. Formal sequential testing procedures have been 
developed, but they can be quite complex. Moreover, they may be weak because of the 
multiple testing going on.  
Thirdly, the fact that data mining is essentially an exploratory process has various 
implications. One is that many models will be examined. Suppose we test m true (though 
we will not know this) null hypotheses at the 5% level, each based on its own subset of 
the data, independent of the other tests. For each hypothesis separately, there is a 
probability of 0.05 of incorrectly rejecting the hypothesis. Since the tests are 
independent, the probability of incorrectly rejecting at least one is p = 1 - (1 - 0.05)m. 
When m = 1 we have p = 0.05, which is fine. But when m = 10 we obtain p = 0.4013, and 
when m = 100 we obtain p = 0.9941. Thus, if we test as few as even 100 true null 
hypotheses, we are almost certain to incorrectly reject at least one. Alternatively, we 
could control the overall family error rate, setting the probability of incorrectly rejecting 
one of more of the m true null hypotheses to 0.05. In this case we use 0.05 = 1 - (1 - a)m 
for each given m to obtain the level a at which each of the separate null hypotheses is 
tested. With m = 10 we obtain a = 0.0051, and with m = 100 we obtain a = 0.0005. This 
means that we have a very small probability of incorrectly rejecting any of the separate 
component hypotheses. 

Of course, in practice things are much more complicated: the hypotheses are unlikely to 
be completely independent (at the other extreme, if they are completely dependent, 
accepting or rejecting one implies the acceptance or rejection of all), with an essentially 
unknowable dependence structure, and there will typically be a mixture of true (or 
approximately true) and false null hypotheses. 
Various simultaneous test procedures have been developed to ease these difficulties 
(even though the problem is not really one of inadequate methods, but is really more 
fundamental). A basic approach is based on the Bonferroni inequality. We can expand 
the probability (1 - a)m that none of the true null hypotheses are rejected to yield (1 - a)m 
= 1 - ma. It follows that 1 - (1 - a)m = ma—that is, the probability that one or more true 
null hypotheses is incorrectly rejected is less than or equal to ma. In general, the 
probability of incorrectly rejecting one or more of the true null hypotheses is smaller than 

 
 

the sum of probabilities of incorrectly rejecting each of them. This is a first-order 
Bonferroni inequality. By including other terms in the expansion, we can develop more 
accurate bounds—though they require knowledge of the dependence relationships 
between the hypotheses. 

With some test procedures difficulties can arise in which a global test of a family of 
hypotheses rejects the null hypothesis (so we believe at least one to be false), but no 
single component is rejected. Once again strategies have been developed for 
overcoming this in particular applications. For example, in multivariate analysis of 
variance, which compares several groups of objects that have been measured on 
multiple variables, test procedures have been developed that overcome these problems 
by comparing each test statistic with a single threshold value. 
It is obvious from the above discussion that while attempts to put probabilities on 
statements of various kinds, via hypothesis tests, do have a place in data mining, they 
are not a universal solution. However, they can be regarded as a particular type of a 
more general procedure that maps the data and statement to a numerical value or score. 
Higher scores (or lower scores, depending upon the procedure) indicate that one 
statement or model is to be preferred to another, without attempting any absolute 
probabilistic interpretation. The penalized goodness-of-fit score functions described in 
chapter 7 can be thought of in this context. 

4.7 Sampling Methods 
As mentioned earlier, data mining can be characterized as secondary analysis, and data 
miners are not typically involved directly with the data collection process. Still, if we have 
information about that process that might be useful for our analysis, we should take 
advantage of it. Traditional statistical data collection is usually carried out with a view to 
answering some particular question or questions in an efficient and effective manner. 
However, since data mining is a process seeking the unexpected or the unforeseen, it 
does not try to answer questions that were specified before the data were collected. For 
this reason we will not be discussing the sub-discipline of statistics known as 
experimental design, which is concerned with optimal ways to collect data. The fact that 
data miners typically have no control over the data collection process may sometimes 
explain poor data quality: the data may be ideally suited to the purposes for which it was 
collected, but not adequate for its data mining uses. 

We have already noted that when the database comprises the entire population, notions 
of statistical inference are irrelevant: if we want to know the  value of some population 
parameter (the mean transaction value, say, or the largest transaction value), we can 
simply calculate it. Of course, this assumes that the data describe the population 
perfectly, with no measurement error, missing data, data corruption, and so on. Since, as 
we have seen, this is an unlikely situation, we may still be interested in making an 
inference from the data as recorded to the ""true"" underlying population values.  
Furthermore, the notions of populations and samples can be dec eptive. For example, 
even when values for the entire population have been captured in the database, often 
the aim is not to describe that population, but rather to make some statement about likely 
future values. For example, we may have available the entire population of transactions 
made in a chain of supermarkets on a given day. We may well wish to make some kind 
of inferential statement—statement about the mean transaction value for the next day or 
some other future day. This also involves uncertainty, but it is of a different kind from that 
discussed above. Essentially, here, we are concerned with forecasting. In market basket 
analysis we do not really wish to describe the purchasing patterns of last month's 
shoppers, but rather to forecast how next month's shoppers are likely to behave. 
We have distinguished two ways in which samples arise in data mining. First, sometimes 
the database itself is merely a sample from some larger population. In chapter 2 we 
discussed the implications of this situation and the dangers associated with it. Second 
the database contains records for every object in the population, but the analysis of the 
data is based on only a sample from it. This second technique is appropriate only in 
modeling situations and certain pattern detection situations. It is not appropriate when we 
are seeking individual unusual records. 

Our aim is to draw a sample from the database that allows us to construct a model that 
reflects the structure of the data in the database. The reason for using just a sample, 
rather than the entire data set, is one of efficiency. At an extreme, it may be infeasible, in 
terms of time or computational requirements, to use the entirety of a large database. By 
basing our computations solely on a sample, we make the computations quicker and 
easier. It is important, however, that the sample be drawn in such a way that it reflects 
the structure of the complete set—i.e., that it is representative of the entire database. 
There are various strategies for drawing samples to try to ensure representativeness. If 
we wanted to take just 1 in 2 of the records (a sampling fraction of 0.5), we could simply 
take every other record. Such a direct approach is termed systematic sampling. Often it 
is perfectly adequate. However, it can also lead to unsuspected problems. For instance, 
if the database contained records of married couples, with husbands and wives 
alternating, systematic sampling could be disastrous—the conclusions drawn would 
probably be entirely mistaken. In general, in any sampling scheme in which cases are 
selected following some regular pattern there is a risk of interaction with an unsuspected 
regularity in the database. Clearly what we need is a selection pattern that avoids 
regularities—a random selection pattern. 

The word random is used here in the sense of avoiding regularities. This is slightly 
different from the usage employed previously in this chapter, where the term referred to 
the mechanism by which the sample was chosen. There it described the probability that 
a record would be chosen for the sample. As we have seen, samples that are random in 
this second sense can be used as the basis for statistical inference: we can, for example, 
make a statement about how likely it is that the sample mean will differ substantially from 
the population mean. 
If we draw a sample using a random process, the sample will satisfy the second meaning 
and is likely to satisfy the first as well. (Indeed, if we specify clearly what we mean by 
""regularities"" we can give a precise probability that a randomly selected sample will not 
match such regularities.) To avoid biasing our conclusions, we should design our sample 
selection mechanism in such a way that that each record in the database has an equal 
chance of being chosen. A sample with equal probability of selecting each member of 
the population is known as an epsem sample. The most basic form of epsem sampling is 
simple random sampling, in which the  n records comprising the sample are selected 
from the N records in the database in such a way that each set of n records has an equal 
probability of being chosen. The estimate of the population mean from a simple random 
sample is just the sample mean. 

At this point we should note the distinction between sampling with replacement and 
sampling without replacement. In the former, a record selected for inclusion in the 
sample has a chance of being drawn again, but in the latter, once a record is drawn it 
cannot be drawn a second time. In data mining since the sample size is often small 
relative to the population size, the differences between the results of these two 
procedures are usually negligible. 
Figure 4.5 illustrates the results of a simple random sampling process used in calculating 
the mean value of a variable for some population. It is based on drawing samples from a 
population with a true mean of 0.5. A sample of a specified size is randomly drawn and 
its mean value is calculated; we have repeated this procedure 200 times and plotted 
histograms of the results. Figure 4.5 shows the distribution of sample mean values (a) for 
samples of size 10, (b) size 100, and (c) size 1000. It is apparent from this figure that the 
larger the sample, the more closely the values of the sample mean are distributed 
around about the true mean. In general, if the variance of a population of size N is s 2, the 
variance of the mean of a simple random sample of size n from that population, drawn 
without replacement, is 

(4.22)  

Since we normally deal with situations in which N is large relative to n (i.e., situations that 
involve a small sampling fraction), we can usually ignore the second factor, so that, a 
good approximation of the variance is s2/n. From this it follows that the larger the sample 
is the less likely it is that the sample mean will deviate significantly from the population 
mean—which explains why the dispersion of the histograms in figure 4.5 decreases with 

increasing sample size. Note also that this result is independent of the population size. 
What matters here is the size of the sample, not the size of the sampling fraction, and 
not the proportion of the population that is included in the sample. We can also see that, 
when the sample size is doubled, the standard deviation is reduced not by a factor of 2, 
but only by a factor of  —there are diminishing returns to increasing the sample size. 
We can estimate s 2 from the sample using the standard estimator 

(4.23)  

where x(i) is the value of the ith sample unit and ? is the mean of the n values in the 
sample. 

Figure 4.5: Means of Samples of Size 10(a), 100(b), and 1000(c) Drawn From a Population 
with a Mean of 0.5.  

 

The simple random sample is the most basic form of sample design, but others have 
been developed that have desirable properties under different circumstances. Details 
can be found in books on survey sampling, such as those cited at the end of this chapter. 
Here we will briefly describe two important schemes. 
In stratified random sampling, the entire population is split into nonover-lapping 
subpopulations or strata, and a sample (often, but not necessarily, a simple random 
sample) is drawn separately from within each stratum. There are several potential 
advantages to using such a procedure. An obvious one is that it enables us to make 
statements about each of the subpopulations separately, without relying on chance to 
ensure that a reasonable number of observations come from each subpopulation. A 
more subtle, but often more important, advantage is that if the strata are relatively 
homogeneous in terms of the variable of interest (so that much of the variability between 
values of the variable is accounted for by differences between strata), the variance of the 
overall estimate may be smaller than that arising from a simple random sample. To 
illustrate, one of the credit card companies we work with categorizes transactions into 26 
categories: supermarket, travel agent, gas station, and so on. Suppose we wanted to 
estimate the average value of a transaction. We could take a simple random sample of 
transaction values from the database of records, and compute its mean, using this as our 
estimate. However, with such a procedure some of the transaction types might end up 
being underrepresented in our sample, and some might be overrepresented. We could 
control for this by forcing our sample to include a certain number of each transaction 
type. This would be a stratified sample, in which the transaction types were the strata. 
This example illustrates why the strata must be relatively homogeneous internally, with 
the heterogeneity occurring between strata. If all the strata had the same dispersion as 
the overall population, no advantage would be gained by stratification. 

In general, suppose that we want to estimate the population mean for some variable, and 
that we are using a stratified sample, with simple random sampling within each stratum. 
Suppose that the kth stratum has Nk elements in it, and that nk of these are chosen for 
the sample from this stratum. Denoting the sample mean within the kth stratum by 
estimate of the overall population mean is given by 

, the 

(4.24)  

where N is the total size of the population. The variance of this estimator is 

(4.25)  

where 
computed as above. 

is the variance of the simple random sample of size nk for the kth stratum, 

Data often have a hierarchical structure. For example, letters occur in words, which lie in 
sentences, which are grouped into paragraphs, which occur in chapters, which form 
books, which sit in libraries. Producing a complete sampling frame and drawing a simple 
random sample may be difficult. Files will reside on different computers at a site within 
an organization, and the organization may have many sites; if we are studying the 
properties of those files, we may find it impossible to produce a complete list from which 
we can draw a simple random sample. In cluster sampling, rather than drawing a sample 
of the individual elements that are of interest, we draw a sample of units that contain 
several elements. In the computer file example, we might draw a sample of computers. 
We can the examine all of the files on each of the chosen computers, or move on to a 
further stage of sampling. 
Clusters are often of unequal sizes. In the above example we can view a computer as 
providing a cluster of files, and it is very unlikely that all computers in an organization 
would have the same number of files. But situations with equal-sized clusters do arise. 
Manufacturing industries provide many examples: six-packs of beer or packets of 
condoms, for instance. If all of the units in each selected cluster are chosen (if the 
subsampling fraction is 1) each unit has the probability a/K of being selected, where a is 
the number of clusters chosen from the entire set of K clusters. If not all the units are 
chosen, but the sampling fraction in each cluster is the same, each unit will have the 
same probability of being included in the sample (it will be an epsem sample). This is a 
common design. Estimating the variance of a statistic based on such a design is less 
straightforward than the cases described above since the sample size is also a random 
variable (it is dependent upon which clusters happen to be included in the sample). The 
estimate of the mean of a variable is a ratio of two random variables: the total sum for 
the units included in the sample and the total number of units included in the sample. 
Denoting the size of the simple random sample chosen from the kth cluster by nk, and 
the total sum for the units chosen from the kth cluster by sk, the sample mean r is 

(4.26)  

If we denote the overall sampling fraction by ƒ (often this is small and can be ignored) 
the variance of r is 

(4.27)  

 
 

4.8 Conclusion 

Nothing is certain. In the data mining context, our objective is to make discoveries from 
data. We want to be as confident as we can that our conclusions are correct, but we 
often must be satisfied with a conclusion that could be wrong—though it will be better if 
we can also state our level of confidence in our conclusions. When we are analyzing 
entire populations, the uncertainty will creep in via less than perfect data quality: some 
values may be incorrectly recorded, some values may be missing, some members of the 
population be omitted from the database entirely, and so on. When we are working with 
samples, our aim is often to draw a conclusion that applies to the broader population 
from which the sample was drawn. The fundamental tool in tackling all of these issues is 
probability. This is a universal language for handling uncertainty, a language that has 

 
 

 
 

been refined throughout this century and has been applied across a vast array of 
situations. Application of the ideas of probability enables us to obtain ""best"" estimates of 
values, even in the face of data inadequacies, and even when only a sample has been 
measured. Moreover, application of these ideas also allows us to quantify our confidence 
in the results. 

Later chapters of this book make heavy use of probabilistic arguments. They underlie 
many—perhaps even most—data mining tools, from global modeling to pattern 
identification. 

4.9 Further Reading 
Books containing discussions of different schools of probability, along with the 
consequences for inference, include those by DeFinetti (1974, 1975), Barnett (1982), 
and Bernardo and Smith (1994). References to other work on statistics and particular 
statistical models are given at the ends of chapters 6, 9, 10 and 11. 
There are many excellent basic books on the calculus of probability, including those by 
Grimmett and Stirzaker (1992) and Feller (1968, 1971). The text by Hamming (1991) is 
oriented towards engineers and computer scientists (and contains many interesting 
examples), and Applebaum (1996) is geared toward undergraduate mathematics 
students. Probability calculus is a dynamic area of applied mathematics, and has 
benefited substantially from the different areas in which it has been applied. For 
example, Alon and Spencer (1992) give a fascinating tour of the applications of 
probability in modern computer science. 
The idea of randomness as departure from the regular or predictable is discussed in 
work on Kolmogorov complexity (e.g., Li and Vitanyi, 1993). Whittaker (1990) provides 
an excellent treatment of the general principles of conditional dependence and 
independence in graphical models. Pearl (1988) is a seminal work in this area from the 
the artificial intelligence perspective. 
There are numerous introductory texts on inference, such as those by Daly et al. (1995), 
as well as more advanced texts that contain a deeper discussion of inferential conscepts, 
such as Cox and Hinkley (1974), Schervish (1995), Lindsey (1996), and Lehmann and 
Casella (1998), and Knight (2000). A broad discussion of likelihood and its applications is 
provided by Edwards (1972). Bayesian methods are now the subjects of entire books. 
Gelman et al. (1995) provides an excellent general text on Bayesian approach. A 
comprehensive reference is given by Bernardo and Smith (1994) and a lighter 
introduction is give by Lee (1989). Nonparametric methods are described by Randles 
and Wolfe (1979) and Maritz (1981). Bootstrap methods are described by Efron and 
Tibshirani (1993). 
Miller (1980) describes simultaneous test procedures. The methods we have outlined 
above are not the only approaches to the problem of inference about multiple 
parameters; Lindsey (1999) describes another. 
Books on survey sampling discuss efficient strategies for drawing samples—see, for 
example, Cochran (1977) and Kish (1965). 

Chapter 5: A Systematic Overview of Data 
Mining Algorithms 
5.1 Introduction 
This chapter will examine what we mean in a general sense by a data mining algorithm 
as well as what components make up such algorithms. A working definition is as follows: 

A data mining algorithm is a well-defined procedure that takes data as input and 
produces output in the form of models or patterns. 
We use the term well-defined indicate that the procedure can be precisely encoded as a 
finite set of rules. To be considered an algorithm, the procedure must always terminate 
after some finite number of steps and produce an output. 

In contrast, a computational method has all the properties of an algorithm except a 
method for guaranteeing that the procedure will terminate in a finite number of steps. 
While specification of an algorithm typically involves defining many practical 
implementation details, a computational method is usually described more abstractly. For 
example, the search technique steepest descent is a computational method but is not in 
itself an algorithm (this search method repeatedly moves in parameter space in the 
direction that has the steepest decrease in the score function relative to the current 
parameter values). To specify an algorithm using the steepest descent method, we 
would have to give precise methods for determining where to begin descending, how to 
identify the direction of steepest descent (calculated exactly or approximated?), how far 
to move in the chosen direction, and when to terminate the search (e.g., detection of 
convergence to a local minimum).  
As discussed briefly in chapter 1, the specification of a data mining algorithm to solve a 
particular task involves defining specific algorithm components: 

1. 

2. 

3. 

4. 

5. 

the data mining task the algorithm is used to address (e.g., visualization, 
classification, clustering, regression, and so forth). Naturally, different 
types of algorithms are required for different tasks. 
the structure (functional form) of the model or pattern we are fitting to the 
data (e.g., a linear regression model, a hierarchical clustering model, and 
so forth). The structure defines the boundaries of what we can 
approximate or learn. Within these boundaries, the data guide us to a 
particular model or pattern. In chapter 6 we will discuss in more detail 
forms of model and pattern structures most widely used in data mining 
algorithms. 
the score function we are using to judge the quality of our fitted models 
or patterns based on observed data (e.g., misclassification error or 
squared error). As we will discuss in chapter 7, the score function is what 
we try to maximize (or minimize) when we fit parameters to our models 
and patterns. Therefore, it is important that the score function reflects the 
relative practical utility of different parameterizations of our model or 
pattern structures. Furthermore, the score function is critical for learning 
and generalization. It can be based on goodness-of-fit alone (i.e., how 
well the model can describe the observed data) or can try to capture 
generalization performance (i.e., how well will the model describe data 
we have not yet seen). As we will see in later chapters, this is a subtle 
issue. 
the search or optimization method we use to search over parameters and 
structures, i.e., computational procedures and algorithms used to find the 
maximum (or minimum) of the score function for particular models or 
patterns. Issues here include computational methods used to optimize 
the score function (e.g., steepest descent) and search-related 
parameters (e.g., the maximum number of iterations or convergence 
specification for an iterative algorithm). If the model (or pattern) structure 
is a single fixed structure (such as a kth-order polynomial function of the 
inputs), the search is conducted in parameter space to optimize the 
score function relative to this fixed structural form. If the model (or 
pattern) structure consists of a set (or family) of different structures, there 
is a search over both structures and their associated parameter spaces. 
Optimization and search are traditionally at the heart of any data mining 
algorithm, and will be discussed in much more detail in chapter 8. 
the data management technique to be used for storing, indexing, and 
retrieving data. Many statistical and machine learning algorithms do not 
specify any data management technique, essentially assuming that the 
data set is small enough to reside in main memory so that random 
access of any data point is free (in terms of time) relative to actual 
computational costs. However, massive data sets may exceed the 
capacity of available main memory and reside in secondary (e.g., disk) or 
tertiary (e.g., tape) memory. Accessing such data is typically orders of 
magnitude slower than accessing main memory, and thus, for massive 
data sets, the physical location of the data and the manner in which it is 

accessed can be critically important in terms of algorithm efficiency. This 
issue of data management will be discussed in more depth in chapter 12. 

Table 5.1 illustrates how three well-known data mining algorithms (CART, 
backpropagation, and the A Priori algorithm) can be described in terms of these basic 
components. Each of these algorithms will be discussed in detail later in this chapter. 
(One of the differences between statistical and data mining perspectives is evident from 
this table. Statisticians would regard CART as a model, and backpropagation as a 
parameter estimation algorithm. Data miners tend to see things more in terms of 
algorithms: processing the data using the algorithm to yield a result. The difference is 
really more one of perspective than substance.) 
Table 5.1: Three Well-Known Data Mining Algorithms Broken Down in Terms of their 
Algorithm Components.   

CART  

Backpropagation  

A Priori  

  

Task  

Structure  

Score 
Function  

Search 
Method  

Data 
Managem
ent 
Techniqu
e  

Classification 
and 
Regression 

Decision 
Tree 

Cross-
validated 
Loss 
Function 

Greedy 
Search over 
Structures 

Unspecified 

Regression 

Neural Network 
(Nonlinear 
functions) 

Squared Error 

Rule Pattern 
Discovery 

Association 
Rules 

Support/Accuracy 

Gradient Descent 
on Parameters 

Breath-First with 
Pruning 

Unspecified 

Linear Scans 

Specification of the model (or pattern) structures and the score function typically 
happens ""off-line"" as part of the human-centered process of setting up the data mining 
problem. Once the data, the model (or pattern) structures, and the score function have 
been decided upon, the remainder of the problem—optimizing the score function—is 
largely computational. (In practice there may be several iterations of this process as 
models and score functions are revised in light of earlier results). Thus, the algorithmic 
core of a data mining algorithm lies in the computational methods used to implement the 
search and data management components. 
The component-based description presented in this chapter provides a general high-
level framework for both analysis and synthesis of data mining algorithms. From an 
analysis viewpoint, describing existing data mining algorithms in terms of their 
components clarifies the role of each component and makes it easier to compare 
competing algorithms. For example, do two algorithms differ in terms of their model 
structures, their score functions, their search techniques, or their data management 
strategies? From a synthesis viewpoint, by combining different components in different 
combinations we can build data mining algorithms with different properties. In chapters 9 
through 14 we will discuss each of the components in much more detail in the context of 
specific algorithms. In this chapter we will focus on how the pieces fit together at a high 
level. The primary theme here is that the component-based view of data mining 
algorithms provides a parsimonious and structured ""language"" for description, analysis, 
and synthesis of data mining algorithms. 

 
 

For the most part we will limit the discussion to cases in which we have a single form of 
model or pattern structure (e.g., trees, polynomials, etc.), rather than those in which we 
are considering multiple types of model structures for the same problem. The component 
viewpoint can be generalized to handle such situations, but typically the score functions, 
the search method, and the data management techniques all become more complex. 

5.2 An Example: The CART Algorithm for Building Tree 
Classifiers 

To clarify the general idea of viewing algorithms in terms of their components, we will 
begin by looking at one well-known algorithm for classification problems. 
The CART (Classification And Regression Trees) algorithm is a widely used statistical 
procedure for producing classification and regression models with a tree-based structure. 
For the sake of simplicity we will consider only the classification aspect of CART, that is, 
mapping an input vector x to a categorical (class) output label y (see  figure 5.1). (A more 
detailed discussion of CART is provided in chapter 10.) In the context of the components 
discussed above, CART can be viewed as the ""algorithm-tuple"" consisting of the 
following: 

Figure 5.1: A Scatterplot of Data Showing Color Intensity versus Alcohol Content for a Set of 
Wines. The Data Mining Task is to Classify the Wines into One of Three Classes (Three 
Different Cultivars), Each Shown with a Different Symbol in the Plot. The Data Originate From 
a 13-Dimensional Data Set in Which Each Variable Measures of a Particular Characteristic of 
a Specific Wine.  

 

task = prediction (classification)  

1. 
2.  model structure = tree  
3.  score function = cross-validated loss function  
4.  search method = greedy local search  
5.  data management method = unspecified  

The fundamental distinguishing aspect of the CART algorithm is the model structure 
being used; the classification tree. The CART tree model consists of a hierarchy of 
univariate binary decisions. Figure 5.2 shows a simple example of such a classification 
tree for the data in  figure 5.1. Each internal node in the tree specifies a binary test on a 
single variable, using thresholds on real and integer-valued variables and subset 
membership for categorical variables. (In general we use  b branches at each node, b = 
2.) A data vector x descends a unique path from the root node to a leaf node depending 
on how the values of individual components of x match the binary tests of the internal 
nodes. Each leaf node specifies the class label of the most likely class at that leaf or, 
more generally, a probability distribution on class values conditioned on the branch 
leading to that leaf. 

Figure 5.2: A Classification Tree for the Data in Figure 5.1 in Which the Tests Consist of 
Thresholds (Shown Beside the Branches) on Variables at Each Internal Node and Leaves 
Contain Class Decisions. Note that One Leaf is Denoted ? to Illustrate that there is 
Considerable Uncertainty About the Class Labels of Data Points in this Region of the Space.  

 

The structure of the tree is derived from the data, rather than being specified a priori (this 
is where data mining comes in). CART operates by choosing the best variable for 
splitting the data into two groups at the root node. It can use any of several different 
splitting criteria; all produce the effect of partitioning the data at an internal node into two 
disjoint subsets (branches) in such a way that the class labels in each subset are as 
homogeneous as possible. This splitting procedure is then recursively applied to the data 
in each of the child nodes, and so forth. The size of the final tree is a result of a relatively 
complicated ""pruning"" process, outlined below. Too large a tree may result in overfitting, 
and too small a tree may have insufficient predictive power for accurate classification. 
The hierarchical form of the tree structure clearly separates algorithms like CART from 
classification algorithms based on non-tree structures (e.g., a model that uses a linear 
combination of all variables to define a decision boundary in the input space). A tree 
structure used for classification can readily deal with input data that contain mixed data 
types (i.e., combinations of categorical and real-valued data), since each internal node 
depends on only a simple binary test. In addition, since CART builds the tree using a 
single variable at a time, it can readily deal with large numbers of variables. On the other 
hand, the representational power of the tree structure is rather coarse: the decision 
regions for classifications are constrained to be hyper-rectangles, with boundaries 
constrained to be parallel to the input variable axes (as an example, see figure 5.3). 

Figure 5.3: The Decision Boundaries From the Classification Tree in Figure 5.2 are 
Superposed on the Original Data. Note the Axis-Parallel Nature of the Boundaries.  

 

The score function used to measure the quality of different tree structures is a general 
misclassification loss function, defined as 

(5.1)  

where C ( y(i), y(i) ) is the loss incurred (positive) when the class label for the ith data 
vector, y(i), is predicted by the tree to be y(i). In general, C is specified by an m × m 
matrix, where m is the number of classes. For the sake of simplicity we will assume here 
a loss of 1 is incurred whenever y(i) ? y(i), and the loss is 0 otherwise. (This is known as 
the ""0–1"" loss function, or the misclassification rate if we normalize the sum above by 
dividing by n.) 
CART uses a technique known as cross-validation to estimate this misclassification loss 
function. We will explain cross-validation in more detail in chapter 7. Basically, this 
method partitions the training data into a subset for building the tree and then estimates 
the misclassification rate on the remaining  validation subset. This partitioning is repeated 
multiple times on different subsets, and the misclassification rates are then averaged to 
yield a cross-validation estimate of how well a tree of a particular size will perform on 
new, unseen data. The size of tree that produces the smallest cross-validated 
misclassification estimate is selected as the appropriate size for the final tree model. 
(This description captures the essence of tree selection via cross-validation, but in 
practice the process is a little more complex.) 

Cross-validation allows CART to estimate the performance of any tree model on data not 
used in the construction of the tree—i.e., it provides an estimate of generalization 
performance. This is critical in the tree-growing procedure, since the misclassification 
rate on the training data (the data used to construct the tree) can often be reduced by 
simply making the tree more complex; thus, the training data error is not necessarily 
indicative of how the tree will perform on new data. 
Figure 5.4 illustrates this point with a hypothetical plot of typical error rates as a function 
the size of the tree. The error rate on the training data decreases monotonically (to an 
error rate of zero if the variables can produce leaves that each contain data from a only 
single class). The test error rate on new data (which is what we are typically interested in 
for prediction) also decreases at first. Very small trees (to the left) do not have sufficient 
predictive power to make accurate predictions. However, unlike the training error, the 
test error ""bottoms out"" and begins to increase again as the algorithm overfits the data 
and adds nodes that are merely predicting noise or random variation in the training data, 
and which is irrelevant to the predictive task. The goal of an algorithm like CART is to 
find a tree close to the optimal tree size (which is of course unknown ahead of time); it 
tries to find a model that is complex enough to capture any structure that exists, but not 
so complex that it overfits. For small to medium amounts of data it is preferable to do this 
without having to reserve some of our data to estimate this out-of-sample error. For very 
large data sets we can sometimes afford to simply partition the data into training and 
validation data sets and to monitor performance on the validation data. 

Figure 5.4: A Hypothetical Plot of Misclassification Error Rates for Both Training and Test 
Data as a Function of Tree Complexity (e.g., Number of Leaves in the Tree).  

 

The use of a cross-validated score function distinguishes CART from most other data 
mining algorithms based on tree models. For example, the C4.5 algorithm (a widely used 
alternative to CART for building classification trees) judges individual tree structures by 
heuristically adjusting the estimated error rate on the training data to approximate the 
test error rate (in an attempt to correct for the fact that the training error rate is generally 
an underestimate of the out-of-sample error rate). The adjusted error rate is then used in 
a pruning phase to search for the tree that maximizes this score. 

CART uses a greedy local search method to identify good candidate tree structures, 
recursively expanding the tree from a root node, and then gradually ""pruning"" back 
specific branches of this large tree. This heuristic search method is dictated by the 
combinatorially large search space (i.e., the space of all possible binary tree structures) 
and the lack of any tractable method for finding the single optimal tree (relative to a given 
score function). The folk wisdom in tree learning is that greedy local search in tree 
building works just about as well as any more sophisticated heuristic, and is much 
simpler to implement than more complex search methods. Thus, greedy local search is 
the method of choice in most practical tree learning algorithms. 

In terms of data management, CART implicitly assumes that the data are all in main 
memory. To be fair to CART, very few algorithms published out -side the database 
literature provide any explicit guidance on data management for large data sets. For 
some algorithms, adding an appropriate data management technique is straightforward 
and can be done in a relatively modular fashion. For example, if each data point needs to 
be visited only once and the order does not matter, data management is trivial (just read 
the data points sequentially in subsets into main memory). 

For tree algorithms, however, the model, the score function, and the search method are 
complex enough to make data management quite nontrivial. To understand why this is 
so, remember that a tree algorithm recursively partitions the observations (the rows of 
our data matrix) into subsets in a data-driven manner, requiring us to repeatedly find 
different subsets of observations in our database and determine various properties of 
these subsets. In a naive implementation of the algorithm for data sets too large to fit in 
main memory, this will involve many repeated scans of the secondary storage medium 
(such as a disk), leading to very poor time performance. Scalable versions of tree 
algorithms have been developed recently that use special purpose data structures to 
deal efficiently with data outside main memory. 

To summarize our reductionist view of CART, we note that the algorithm consists of (1) a 
tree model structure, (2) a cross-validated score function, and (3) a two-phase greedy 
search over tree structures (""growing"" and ""pruning""). In this sense, CART is relatively 
straightforward to understand once one grasps the key ideas involved. Clearly, we could 
develop alternative algorithms that use the same tree structure, cross-validated score 
function, and search techniques, and that are similar in spirit to CART, but that are 
application-specific in details of implementation (such as how missing data are handled 
in both training and prediction). For a given data mining application, customizing the 
algorithm in this fashion might be well worth pursuing. In short, the power of an algorithm 
such as CART is in the fundamental concepts that it embodies, rather than in the specific 
details of implementation. 

5.3 The Reductionist Viewpoint on Data Mining Algorithms 
Repeating the basic mantra of this chapter, once we have a data set and a specific data 
mining task, a data mining algorithm can be thought of as a ""tuple"" consisting of {model 
structure, score function, search method, data management technique}. While this is a 
simple observation, it has some fairly profound implications. First, the number of different 
algorithms we can generate is very large! By combining different model structures with 
different score functions, different search methods, and different data management 
techniques, we can generate a potentially infinite number of different algorithms. (This 
point has not escaped academic researchers.) 

 
 

However, the complexity of ""algorithm space"" is manageable once we realize the second 
implication: while there is a very large number of possible algorithms, there is only a 
relatively small number of fundamental ""values"" for each component in the tuple. 
Specifically, there are well-defined categories of models and patterns that we can use for 
problems such as regression, classification, or clustering; we will discuss these in detail 
in chapter 6. Similarly, as we will see in chapter 7, there are relatively few score 
functions (such as likelihood, sum-of-squared-errors, and classification rate) that have 
broad appeal. There are also just a few general classes of search and optimization 
methods that have wide applicability, and the essential principles of data management 
can be reduced to a relatively small number of different techniques (as discussed in 
chapters 8 and 12, respectively). 

Thus, many well-known data mining algorithms are composed of a combination of well-
defined components. In other words algorithms tend to be relatively tightly clustered in 
""algorithm space"" (as spanned by the ""dimensions"" of model structure, score function, 
search method, and data management technique). 
The reductionist (i.e., a component-based) view for data mining algorithms is quite useful 
in practice. It clarifies the underlying operation of a particular data mining algorithm by 
reducing it to its essential components. In turn, this makes it easier to compare different 
algorithms, since we can clearly see similarities and differences at the component level 
(e.g., we were able to distinguish between CART and C4.5 primarily in terms of what 
score functions they use). 
Even more important, this view places an emphasis on the fundamental properties of an 
algorithm avoiding the tendency to think of lists of algorithms. When faced with a data 
mining application, a data miner should think about which components fit the specifics of 
his or her problem, rather than which specific ""off-the-shelf"" algorithm to choose. In an 
ideal world, the data miners would have available a software environment within which 
they could compose components (from a library of model structures, score functions, 
search methods, etc.) to synthesize an algoithm customized for their specific 
applications. Unfortunately this remains a ideal state of affairs rather than the practical 
norm; current data analysis software packages often provide only a list of algorithms, 
rather than a component-based toolbox for algorithm synthesis. This is understandable 
given the aim of providing usable tools for data miners who do not have the background 
or the time to understand the underlying details at a component level. However these 
software tools may not be ideal for more skilled practitioners who wish to customize and 
synthesize problem-specific algorithms. The ""cookbook"" approach is also somewhat 
dangerous, since naive users of data mining tools may not fully understand the 
limitations (and underlying assumptions) of the particular black-box algorithms they are 
using. In contrast, a description based on components makes it relatively clear what is 
inside the black box. 

To illustrate the general utility of the reductionist viewpoint, in the next three sections we 
will look at three well-known algorithms in terms of their components. These and related 
algorithms will addressed in more detail in chapters 9 through 14, where we discuss a 
more complete range of solutions for different data mining tasks. 

5.3.1 Multilayer Perceptrons for Regression and Classification 
Feedforward multilayer perceptrons (MLPs) are the most widely used models in the 
general class of artificial neural network models. The MLP structure provides a nonlinear 
mapping from a real-valued input vector x to a real-valued output vector y. As a result, 
an MLP can be used as a nonlinear model for regression problems, as well as for 
classification, through appropriate interpretation of the outputs. The basic idea is that a 
vector of p input values is multiplied by a p × d1 weight matrix, and the resulting  d1 values 
are each individually transformed by a nonlinear function to produce  d1 ""hidden node"" 
outputs. The resulting d1 values are then multiplied by a d1 × d2 weight matrix (another 
""layer"" of weights), and the d2 values are each put through a non-linear function. The 
resulting d2 values can either be used as the outputs of the model or be put through 
another layer of weight multiplications and non-linear transformations, and so on (hence, 
the ""multilayer"" nature of the model; the term perceptron refers to the original model of 

and 

, are calculated via the first layer of 

, is widely used. Next h1 and  h2 are weighted and combined to 

this form proposed in the 1960s, consisting of a single layer of weights followed by a 
threshold nonlinearity). 
As an example, consider the simple network model in  figure 5.5 with a single ""hidden"" 
layer. Two inner products, 
weights (the as and the ßs), and each in turn transformed by a nonlinear function at the 
hidden nodes to produce two scalar values: h1 and  h2. The nonlinear logistic function, 
i.e., 
produce the output value 
transformation on y also). Thus, y is a nonlinear function of the input vector  x. The hs 
can be viewed as nonlinear transformations of the four-dimensional input, a new set of 
two ""basis functions,"" h1 and h2. The parameters of this model to be estimated from the 
data are the eight weights on the input layer (a1, ..., a4, ß1, ..., ß4) and the two weights on 
the output layer (w1 and  w2). In general, with p inputs, a single hidden layer with h hidden 
nodes, and a single output, there are (p + 1)h parameters (weights) in all to be estimated 
from the data. In general we can have multiple layers of such weight multiplications and 
nonlinear transformations, but a single hidden layer is used most often since multiple 
hidden layer networks can be slow to train. The weights of the MLP are the parameters 
of the model and must be determined from the data. 

(we could in principle perform a nonlinear 

 

Figure 5.5: A Diagram of a Simple Multilayer Perceptron (or Neural Network) Model with Two 
Hidden Nodes (d1 = 2) and a Single Output Node (d2 = 1).  

Note that if the output y is a scalar y (i.e., d2 = 1) and is bounded between 0 and 1 (we 
can just choose a nonlinear transformation of the weighted values coming from the 
previous layer to ensure this condition), we can use  y as an indicator of class 
membership for two-class problems and (for example) threshold at 0.5 to decide 
between class 1 and class 2. Thus, MLPs can easily be used for classification as well as 
for regression. Because of the nonlinear nature of the model, the decision boundaries 
between different classes produced by a network model can also be quite non-linear. 
Figure 5.6 provides an example of such decision boundaries. Note that they are highly 
nonlinear, in contrast to those produced by the classification tree in  figure 5.3. Unlike the 
classification tree in  figure 5.2, however, there is no simple summary form we can use to 
describe the workings of the neural network model. 

Figure 5.6: An Example of the Type of Decision Boundaries that a Neural Network Model 
Would Produce for the Two-Dimensional Wine Data of Figure 5.2(a).  

 

The reductionist view of an MLP learning algorithm yields the following ""algorithm-tuple"": 

task = prediction: classification or regression  

1. 
2.  structure = multiple layers of nonlinear transformations of weighted 

sums of the inputs  

3.  score function = sum of squared errors  
4.  search method = steepest-descent from randomly chosen initial 

parameter values  

5.  data management technique = online or batch  

The distinguishing feature of this algorithm is the multilayer, nonlinear nature of its model 
structure (note both that the output y is a nonlinear function of the inputs and that the 
parameters ? (the weights) appear nonlinearly in the score function). This clearly sets a 
neural network apart from more traditional linear and polynomial functional forms for 
regression and from tree-based models for classification. 

The sum of squared errors (SSE), the most widely used score function for MLPs, is 
defined as: 

(5.2)  

where y(i) and y(i) are the true target value and the output of the network, respectively, 
for the ith data point, and where y(i) is a function of the input vector x(i) and the MLP 
parameters (weights) ?. It is sometimes assumed that squared error is the only score 
function that can be used with a neural network model. In fact, as long as it is 
differentiable as a function of the model parameters (allowing us to determine the 
direction of steepest descent), any score function can be used as the basis for a 
steepest-descent search method such as backpropagation. For example, if we view 
squared error as just a special case of a more general log-likelihood function (as 
discussed in chapter 4), we can use a variety of other likelihood-based score functions in 
place of squared error, tailored for specific applications. 
Training a neural network consists of minimizing SSSE by treating it as a function of the 
unknown parameters ? (i.e., parameter estimation of ? given the data). Given that each 
y(i) is typically a highly nonlinear function of the parameters ?, the score function SSSE is 
also highly nonlinear as a function of ?. Thus, there is no closed-form solution for finding 
the parameters ? that minimize SSSE for an MLP. In addition, since there can be many 
local minima on the surface of SSSE as a function of ?, training a neural network (i.e., 
finding the parameters that minimize SSSE for a particular data set and model structure) is 
often a highly non-trivial multivariate optimization problem. Iterative local search 
techniques are required to find satisfactory local minima. 

The original training method proposed for MLPs, known as backpropagation, is a 
relatively simple optimization method. It essentially performs steepest-descent on the 
score function (the sum of squared errors) in parameter space, solving this nonlinear 
optimization problem by descending to a local minimum given a randomly chosen 
starting point in parameter space. (In practice we usually descend from multiple starting 
points and select the best local minimum found overall.) In a more general context, there 
is a large family of optimization methods for such nonlinear optimization problems. It is 
often assumed that steepest-descent is the only optimization method that can be used to 
train an MLP, but in fact more powerful nonlinear optimization techniques such as 
conjugate gradient techniques can be brought to bear on this problem. We discuss some 
of these techniques in chapter 8. 
In terms of data management, a neural network can be trained either online (updating 
the weights based on cycling through one data point at a time) or in batch mode 
(updating the weights after seeing all of the data points). The online updating version of 
the algorithm is a special case of a more general class of online estimation algorithms 
(see chapter 8 for further discussion of the trade-offs involved in using such algorithms). 
An important practical distinction between MLPs and classification trees is that a tree 
algorithm (such as CART) searches through models of different complexities in a 
relatively automated manner (e.g., finding the right-sized tree is a basic feature of the 
CART algorithm). In contrast, there is no widely accepted procedure for determining the 
appropriate structure for an MLP (i.e., determining how many layers and how many 
hidden nodes to include in the model). Numerous algorithms exist for constructing 
network structures automatically, including methods that start with small networks and 
add nodes and weights in an incremental ""growing"" manner, as well as methods that 
start with large networks and ""prune"" away weights and nodes that appear to be 
irrelevant. Incrementally growing a network structure can be subject to local minima 
problems (the best network with k hidden nodes may be quite different in parameter 
space from the best network with k - 1 hidden nodes). On the other hand, training an 
overly large network can be prohibitively expensive, especially when the model structure 
is large (e.g., with a large input dimensionality p). In practice, network structures are 
often determined by a trial-and-error procedure of manually adjusting the number of 
hidden nodes until satisfactory performance is reached on a validation data set (a set of 
data points not used in training). 

The component-based view of MLPs illustrates that the general approach is not very far 
removed from more traditional statistical estimation and optimization techniques. Many of 
these techniques (e.g., the incorporation of Bayesian priors into the score function to 
drive small weights to zero (to ""regularize"" the model) or the use of more sophisticated 
multivariate optimization procedures such as conjugate gradient techniques during 
weight search) can be used in training network models. In the 1980s, when neural 
network models were first introduced, the connections to the statistical literature were not 
at all obvious (although they seem quite clear in retrospect). There is no doubt that the 
primary contribution of the neural modeling approach lies in the nonlinear multilayer 
nature of the underlying model structure. 

5.3.2 The A Priori Algorithm for Association Rule Learning 
Association rules are among the most popular representations for local patterns in data 
mining. Chapter 13 provides a more in-depth description, but here we sketch the general 
idea and briefly describe a generic association rule algorithm in terms of its components. 
(This description is loosely based on the well-known A Priori algorithm, which was one of 
the earliest algorithms for finding association rules.) 

An association rule is a simple probabilistic statement about the co-occurrence of certain 
events in a database, and is particularly applicable to sparse transaction data sets. For 
the sake of simplicity we assume that all variables are binary. An association rule takes 
the following form: 

(5.3)  

where A, B, and C are binary variables and p = p(C = 1|A = 1, B = 1), i.e., the conditional 
probability that C = 1 given that A = 1 and B = 1. The conditional probability p is 
sometimes referred to as the ""accuracy"" or ""confidence"" of the rule, and p(A = 1, B = 1, 

C = 1) is referred to as the ""support."" This pattern structure or rule structure is quite 
simple and interpretable, which helps explain the general appeal of this approach. 
Typically the goal is to find all rules that satisfy the constraint that the accuracy p is 
greater than some threshold pa and the support is greater than some threshold ps (for 
example, to find all rules with support greater than 0.05 and accuracy greater than 0.8). 
Such rules comprise a relatively weak form of knowledge; they are really just summaries 
of co-occurrence patterns in the observed data, rather than strong statements that 
characterize the population as a whole. Indeed, in the sense that the term ""rule"" usually 
implies a causal interpretation (from the left to the right hand side), the term ""association 
rule"" is strictly speaking a misnomer since these patterns are inherently correlational but 
need not be causal. 
The general idea of finding association rules originated in applications involving ""market-
basket data."" These data are usually recorded in a database in which each observation 
consists of an actual basket of items (such as grocery items), and the variables indicate 
whether or not a particular item was purchased. We can think of this type of data in 
terms of a data matrix of n rows (corresponding to baskets) and  p columns 
(corresponding to grocery items). Such a matrix can be very large, with n in the millions 
and p in the tens of thousands, and is generally very sparse, since a typical basket 
contains only a few items. Association rules were invented as a way to find simple 
patterns in such data in a relatively efficient computational manner. 

In our reductionist framework, a typical data mining algorithm for association rules has 
the following components: 

task = description: associations between variables  

1. 
2.  structure = probabilistic ""association rules"" (patterns)  
3.  score function = thresholds on accuracy and support  
4.  search method = systematic search (breadth-first with pruning)  
5.  data management technique = multiple linear scans  

The score function used in association rule searching is a simple binary function. There 
are two thresholds: ps is a lower bound on the support of the rule (e.g., ps = 0.1 when we 
want only those rules that cover at least 10% of the data) and pa is a lower bound on the 
accuracy of the rule (e.g., pa = 0.9 when we want only rules that are at least 90% 
accurate). A pattern gets a score of 1 if it satisfies both of the threshold conditions, and 
gets a score of 0 otherwise. The goal is find all rules (patterns) with a score of 1. 
The search problem is formidable given the exponential number of possible association 
rules—namely, O(p2p-1) for binary variables if we limit our attention to rules with positive 
propositions (e.g., A = 1) in the left and right -hand sides. Nonetheless, by taking 
advantage of the nature of the score function, we can reduce the average run-time of the 
algorithm to much more manageable proportions. Note that if either p(A = 1) = ps or p(B 
= 1) = ps, clearly p(A = 1, B = 1) = ps. We can use this observation in our search for 
association rules by first finding all of the individual events (such as A = 1) that have a 
probability greater than the threshold ps (this takes one linear scan of the entire 
database). An event (or set of events) is called ""frequent"" if the probability of the event(s) 
is greater than the support threshold ps. We consider all possible pairs of these frequent 
events to be candidate frequent sets of size 2. 
In the more general case of going from frequent sets of size k - 1 to frequent sets of size 
k, we can prune any sets of size k that contain a subset of k - 1 items that themselves 
are not frequent at the k - 1 level. For example, if we had only frequent sets {A = 1, B = 
1} and {B = 1, C = 1}, we could combine them to get the candidate k = 3 frequent set {A 
= 1, B = 1, C = 1}. However, if the subset of items {A = 1, C = 1} was not frequent (i.e., 
this item set were not on the list of frequent sets of size k = 2), then {A = 1, B = 1, C = 1} 
could not be frequent either, and it could safely be pruned. Note that this pruning can 
take place without searching the data directly, resulting in a considerable computational 
speedup for large data sets. 
Given the pruned list of candidate frequent sets of size k, the algorithm performs another 
linear scan of the database to determine which of these sets are in fact frequent. The 
confirmed frequent sets of size k (if any) are combined to generate all possible frequent 
sets containing k + 1 events, followed by pruning, and then another scan of the 
database, and so on—until no more frequent sets can be generated. (In the worst case, 
all possible sets of events are frequent and the algorithm takes exponential time. 

However, since in practice the data are often very sparse for the types of transaction 
data sets analyzed by these algorithms, the cardinality of the largest frequent set is 
usually quite small (relative to n), at least for relatively large support values.) The 
algorithm then makes one final linear scan through the data set, using the list of all 
frequent sets that have been found. It determines which subset combinations of the 
frequent sets also satisfy the accuracy threshold when expressed as a rule, and then 
returns the corresponding association rules. 

Association rule algorithms comprise an interesting class of data mining algorithms in 
that the search and data management components are their most critical components. In 
particular, association rule algorithms use a systematic breadth-first, general-t o-specific 
search method that explicitly tries to minimize the number of linear scans through the 
database. While there exist numerous other rule-finding algorithms in the machine 
learning literature (with similar rule-based representations), association rule algorithms 
are designed specifically to operate on very large data sets in a relatively efficient 
manner. Thus, for example, research papers on association rule algorithms tend to 
emphasize computational efficiency rather than interpretation of the rules that the 
algorithms produce. 

5.3.3 Vector-Space Algorithms for Text Retrieval 
The general task of ""retrieval by content"" is loosely described as follows: we have a 
query object and a large database of objects, and we would like to find the k objects in 
the database that are most similar to the query object. We are all familiar with this 
problem in the context of searching through online collections of text. For example, our 
query could be a short set of keywords and the ""database"" could correspond to a large 
set of Web pages. Our task in this case would be to find the Web pages that are most 
relevant to our keywords. 
Chapter 14 discusses this retrieval task in greater depth. Here we look at a generic text 
retrieval algorithm in terms of its components. One of the most important aspects of this 
problem is how similarity is defined. Text documents are of different lengths and 
structure. How can we compare such diverse documents? A key idea in text retrieval is 
to reduce all documents to a uniform vector representation, as follows. Let t1, ..., tp be p 
terms (words, phrases, etc.). We can think of these as variables, or columns in our data 
matrix. A document (a row in our data matrix) is represented by a vector of length p, 
where the ith component contains the count of how often term ti appears in the 
document. As with market-basket data, in practice we can have a very large data matrix 
(n in the millions, p in the tens of thousands) that is very sparse (most documents will 
have many zeros). Again, of course, we normally would not actually store the data as a 
large n × p matrix: a more efficient representation is to store a list for each term ti of all 
the documents containing that term. 
Given this ""vector-space"" representation, we can now readily define similarity. One 
simple definition is to make the similarity distance a function of the angle between the 
two vectors in p-space. The angle measures similarity in a given direction in ""term-
space"" and factors out any differences arising from the fact that large documents tend to 
have more occurrences of a word than small documents. The vector-space 
representation and the angle similarity measure may seem relatively primitive, but in 
practice this scheme works surprisingly well, and there exists a multitude of variations on 
this basic theme in text retrieval. 
With this information, we are ready to define the components of a simple generic text-
retrieval algorithm that takes one document and finds the k most similar documents: 

1. 

task = retrieval of the k most similar documents in a database relative 
to a given query  
2. 
representation = vector of term occurrences  
3.  score function = angle between two vectors  
4.  search method = various techniques  
5.  data management technique = various fast indexing strategies  

There are many variations on the specific definitions of the components given above. For 
example, in defining the score function, we can specify similarity metrics more general 
than the angle function. In specifying the search method, various heuristic search 
techniques are possible. Note that search in this context is real-time search, since the 

 
 

algorithm has to retrieve the patterns in realtime for a user (unlike the data mining 
algorithms we looked at earlier, for which search meant off-line searching for the optimal 
parameters and model structures).  
Different applications may call for different components to be used in a retrieval 
algorithm. For example, in searching through legal documents, the absence of particular 
terms might be significant, and we might want to reflect this in our definition of a score 
function. In a different context we might want the opposite effect, i.e., to downweight the 
fact that two documents do not contain certain terms (relative to the terms they have in 
common). 
It is clear, however, that the model representation is really the key idea here. Once the 
use vector representation has been established, we can define a wide range of similarity 
metrics in vector-space, and we can use standard search and indexing techniques to find 
near neighbors in sparse p-dimensional space. Different retrieval algorithms may vary in 
the details of the score function or search methods, but most share the same underlying 
vector representation of the data. Were we to define a different representation for a 
document (say a generative model for the data based on some form of grammar), we 
would probably have to come up with fundamentally different score functions and search 
methods. 

5.4 Discussion 

For the novice and the seasoned researcher alike, wandering through the jungle of data 
mining algorithms can be somewhat bewildering. We hope that the component-based 
view presented in this chapter provides a useful practical tool for the reader in evaluating 
algorithms. The process is as follows: try to strip away the jargon and marketing spin that 
are inevitable in any research paper or product literature, and reduce the algorithm to its 
basic components. The component-based description provides a well-defined and 
""calibrated"" framework on which to base comparisons—e.g., we can compare a new 
algorithm to other well-known algorithms and see precisely how it differs in terms of its 
components, if it differs at all. 
It is interesting to note the different emphases placed on algorithmic aspects of data 
mining in different research communities. A cursory glance through most statistical 
journals will reveal plenty of equations specifying models, score functions, and 
computational methods, with relatively few detailed algorithmic specifications of how the 
models will be fit in practice. Conversely, computer science journals on machine learning 
and pattern recognition often emphasize the computational methods and algorithms, with 
little emphasis on the appropriateness of either the structure of the model or the score 
function being used to fit it. For example, it is not uncommon to see empirical 
comparisons being made among algorithms, rather than among the  underlying models or 
score functions. In the context of data mining, the different emphases in the two research 
areas have led to the development of quite different (and often complementary) 
methodologies. Statistical approaches often place significant emphasis on theoretical 
aspects of inference procedures (e.g., parameter estimation and model selection) and 
less emphasis on computational issues. Computer science approaches to data mining 
tend to do the reverse, focusing more on efficient search and data management and less 
on the appropriateness of the model (and pattern) structures, or on the relevance of the 
score function. This ""cultural"" difference is worth keeping in mind throughout this text, as 
it helps to explain the factors that motivated the development of specific models, 
inference methods, and algorithms within these two research communities. 

For both the statistical and the computer science schools of thought, it is probably fair to 
say that the typical research paper is not very clear on what the underlying components 
of a particular algorithm are. The literature is replete with fancy-sounding names and 
acronyms for different algorithms. In many papers, the descriptions of the model 
structure, the score function, and the search method are abstrusely intertwined. 
In practice, all components of a data mining algorithm are essential. The relative 
importance of the model, the score function, and the computational implementation 
varies from problem to problem. For small data sets, the interpretability and predictive 
power of the model may be (relatively speaking) a much more important factor than any 

computational concerns. However, as data sets become larger (in terms of both the 
number of measurements and the number of variables), the role of computation 
becomes increasingly important. For example, while a clustering algorithm with time 
complexity O(n2) may be tractable with n = 100, it will be completely intractable for  n = 
108 (and will likely remain intractable in our lifetime!). Furthermore, the time complexity is 
typically stated assuming that all of the data reside in main memory. If for each 
computational step in the algorithm, instead of a data point being retrieved from main 
memory, it must be retrieved from disk (for example), there will be an additional large 
multiplicative constant time factor involved in the expression for time complexity. 

For very large data sets there are clear trade-offs between the sophistication of the 
modeling we wish to carry out and the computational expense (i.e., time taken) to 
achieve a certain quality of fit. For massive data sets, the computational methodology 
directly influences what types of model structures can be fit to the data. Computational 
issues tend to play a much more prominent role in data mining than in traditional 
statistical modeling. 

Of course, the model structures and the score functions should always be carefully 
chosen and explicitly acknowledged in any data mining problem. There is little advantage 
in being able to handle vast data sets efficiently if the underlying models that are 
returned are not useful. Thus, data miners need to carefully evaluate the trade-offs 
between searching for sophisticated model/pattern structures and the computational 
resources required to find and fit such structures reliably. 

5.5 Further Reading 
There are very few papers that promote a systematic component-based view of data 
mining algorithms. An exception is Buntine, Fischer, and Pressburger (1999), who 
provide an interesting discussion (with examples) of how to achieve rapid automatic 
prototyping of data mining algorithms from high-level algorithmic specifications. Classic 
general texts on algorithms are Cormen, Leiserson and Rivest (1990) and Knuth (1997). 
The principles of CART were first described in Breiman et al. (1984), and C4.5 is 
described in detail in Quinlan (1993). Buntine (1992) and  Chipman, George, and 
McCulloch (1998) discuss Bayesian extensions to CART. Crawford (1989) describes 
methods for constructing classification trees in an incremental manner, and Gehrke et al. 
(1999) describe related ideas for scalable tree construction algorithms for massive data 
sets. Ballard (1997) is a very readable introductory text on modern neural network 
algorithms and their relation to actual brain modeling. Geman, Bienenstock, and Doursat 
(1992) provide an excellent discussion of the connections between statistical ideas and 
neural network learning algorithms. Ripley (1996) gives a thorough survey of both neural 
network algorithms (chapter 5) and tree learning algorithms (chapter 7) from a statistical 
perspective, while the text by Bishop (1995) is devoted entirely to a statistical treatment 
of neural network learning algorithms. 
Agrawal et al. (1996) provide a review of association rule algorithms, as well as an in-
depth look at the search method and its efficiency. Salton and McGill (1983) give a 
useful introduction to information retrieval, and Witten, Moffatt, and Bell (1999) include a 
detailed and thorough discussion of the various issues involved in retrieval algorithms for 
massive text and image databases. 

Chapter 6: Models and Patterns 
6.1 Introduction 
We have introduced the distinction between models and patterns in earlier chapters. 
Here we explore these ideas in more depth, and examine some of the major classes of 
models and patterns used in data mining, in preparation for a detailed examination in 
subsequent chapters. 

 
 

 
 

A model is a high-level, global description of a data set. It takes a large sample 
perspective. It may be descriptive—summarizing the data in a convenient and concise 
way—or it may be inferential, allowing one to make some statement about the population 
from which the data were drawn or about likely future data values. In this chapter we will 
discuss a variety of basic model forms such as linear regression models, mixture 
models, and Markov models. 
In contrast, a pattern is a local feature of the data, perhaps holding for only a few records 
or a few variables (or both). An example of a pattern would be a local ""structural"" feature 
in our p-dimensional variable space such as a mode (or a gap) in a density function or an 
inflexion point in a regression curve. Often patterns are of interest because they 
represent departures from the general run of the data: a pair of variables that have a 
particularly high correlation, a set of items that have exceptionally high values on some 
variables, a group of records that always score the same on some variables, and so on. 
As with models, we may want to find patterns for descriptive reasons or  for inferential 
reasons. We may want to identify members of the existing database that have unusual 
properties, or we may want to predict which future records are likely to have unusual 
properties. Examples of patterns are transient waveforms in an EEG trace, unusual 
combinations of products that are frequently purchased together by retail customers, and 
outliers in a database of semiconductor manufacturing data. 
Data compression can provide a useful way to illustrate the concept of a patterns versus 
a model. Consider transmitter T that has an image I that is to be sent to a receiver R 
(though the principle holds for data sets that are not images). There are two main 
strategies: (a) send all of the data (the pixels in the image I) exactly, or (b) transmit some 
compressed version of the image#x2014;that is, some summary of the image I. Data 
mining to a large extent corresponds to the second approach, the compression being 
achieved either by representing the original data as a model, or by identifying unusual 
features of the data through patterns. 
In modeling, some loss in fidelity is likely to be incurred when we summarize the data—
this means that the receiver R will not be able to reconstruct the data precisely. An 
example of a model for the image data might be replacing each square of 16 × 16 pixels 
in the original image by the average values of these pixels. The ""model"" in this case 
would just be a set of smaller and lower resolution (1/16th) images. A more sophisticated 
model might adaptively partition each image into local regions of different sizes and 
shapes, where the pixel values can be fairly accurately described by a constant pixel 
intensity within each such region. The ""model"" (or message) in this case would be both 
the values of the constants within each region and the description of the boundaries of 
the regions for each. For both types of models (the average-pixel model and the locally 
constant model) it is clear that the complexity of the image model (the number of pixels 
being averaged, the average size of the locally constant regions) can be traded for the 
amount of information being transmitted (or equivalently, the amount of information being 
lost in the transmission—that is, the compression rate). 
From a pattern detection viewpoint, a pattern in an image is some structure in the image 
that is purely local: for example, a partially obscured circular object in the upper-left 
corner of the image. This is clearly a different form of compression from the global 
compression models above. The receiver R can no longer reconstruct a summary of the 
whole image, but it does have a description of some local part of the image. Depending 
on the problem and objectives, local structure may be much more relevant than a global 
model. Rather than sending a summary model description of a vast noisy ""sea"" of pixel 
values, the transmitter T instead ""focuses"" the receiver  R's attention on the important 
aspects. We can think of association rules from chapter 5 in this context: they try to focus 
attention on potentially interesting associations among subsets of variables.  

The analogy between image coding and data analysis is not perfect (for example, 
compression, as we have described it, does not take into account the idea of 
generalization to unseen data), but nonetheless, it allows us to grasp the essential trade-
offs between representing local structure at a fairly high resolution and lower-resolution 
global structure. 
This chapter is organized as follows: section 6.2 discusses some of the fundamental 
properties of models and the choices we have to make in building a model. Section 6.3 
focuses on the general principles behind models in which one of the variables is singled 

out as a ""response"" to be predicted from the others. This includes regression and 
supervised classification models. Many data mining problems involve large numbers of 
variables, and these present particular challenges, which are discussed in section 6.5. 
Descriptive models are discussed in section 6.4. Many data sets contain data that have 
been collected to conform to some schema (such as time series or image data), and they 
typically require special consideration in modeling. Section 6.6 discusses issues 
associated with such structured data. Finally, section 6.7 describes patterns for both 
multivariate and sequential data. 

6.2 Fundamentals of Modeling 
A model is an abstract representation of a real-world process. For example, Y = 3X + 2 is 
a very simple model of how the variable  Y might relate to the variable X. This particular 
model can be thought of as an instance of the more general model structure Y = aX + c, 
where for this particular model we have set a = 3 and c = 2. More generally still, we could 
put Y = aX + c + e, where e is a random variable accounting for a random component of 
the mapping from X to Y (we will return to this later). We often refer to a and c as the 
parameters of the model, and  will often use the notation ? to refer to a generic parameter 
or a set (or vector) of parameters, as we did in chapter 4. In this example, ? = {a, c}. 
Given the  form or structure of a model, we choose appropriate values for its parameters 
by estimation—that is, by minimizing or maximizing an appropriate score function 
measuring the fit of the model to the data. Procedures for this were described in chapter 
4 and are described further in later chapters. 

However, before we can estimate the parameters of a model, we must first choose an 
appropriate functional form of the model itself. The aim of this section is to present a 
high-level overview of the main classes of models used in data mining.  

Model building in data mining is data-driven. It is usually not driven by the notion of any 
underlying mechanism or ""reality,"" but simply seeks to capture the relationships in the 
data. Even in those cases in which there is a postulated true generative mechanism for 
the data, we should bear in mind that, as George Box put it, ""All models are wrong but 
some are useful."" For example, while we might postulate the existence of a linear model 
to explain the data, it is likely to be a fiction, since even in the best of circumstances 
there will be small nonlinear effects that we will be unable to capture in the model. We 
are looking for a model that encapsulates the main aspects of the data generating 
process. 
Since data mining is data-driven, the discovery of a highly predictive model (for example) 
should not be taken to mean that there is a causal relationship. For example, an analysis 
of customer records may show that customers who buy high-quality wines are also more 
likely to buy designer clothes. Clearly one propensity is not causally related to the other 
propensity (in either direction). Rather, they are both more likely to be the consequence 
of a relatively high income. However, the fact that neither the wine nor the clothes 
variable causes the other does not mean that they are not useful for predictive purposes. 
Predicting the likely clothes-buying behavior from observed wine-buying behavior would 
be entirely legitimate (if the relationship were found in the data), from a marketing 
perspective. Since no causal relationship has been established, however, it would be 
false to conclude that manipulating one of the variables would lead to a change in the 
other. That is, inducing people to buy high-quality wines would be unlikely to lead them 
also to buy designer clothes, even if the relationship existed in the data. 

6.3 Model Structures for Prediction 
In a predictive model, one of the variables is expressed as a function of the others. This 
permits the value of the response variable to be predicted from given values of the 
others (the explanatory or predictor variables). The response variable in general 
predictive models is often denoted by Y , and the p predictor variables by X1, . . . , Xp. 
Thus, for example, we might want to construct a model for predicting the probability that 
an applicant for a loan will default, based on application forms and the behavior of past 

 
 

 
 

customers contained in a database. The record for the ith past customer can be 
conveniently represented as {(x(i), y(i))}. Here y(i) is the outcome class (good or bad) of 
the ith customer, and x(i) is the vector x = (x1(i), . . . , xp(i)) of application form values for 
the ith customer. The model will yield predictions, y = ƒ(x1, . . . , xp; ?) where y is the 
prediction of the model and ? represents the parameters of the model structure. When Y 
is quantitative, this task of estimating a mapping from the p-dimensional X to Y is known 
as regression. When Y is categorical, the task of learning a mapping from X to Y is called 
classification learning or supervised classification. Both of these tasks can be considered 
function approximation problems in that we are learning a mapping from a p-dimensional 
variable X to Y. For simplicity of exposition in this chapter we will focus primarily on the 
regression task, since many of the same general principles carry over directly to the 
classification task. Chapters 10 and 11 deal, respectively, with supervised classification 
and regression in detail. 

6.3.1 Regression Models with Linear Structure 

We begin our discussion of predictive models with models in which the response variable 
is a linear function of the predictor variables: 

(6.1)  

where ? = {a0, . . . , ap}. Again we note that the model is purely empirical, so that the 
existence of a well-fitting and highly predictive model does not imply any causal 
relationship. We have used Y rather than simply Y on the left of this expression because 
it is a model, which has been constructed from the data. That is, the values of Y are 
values predicted from the X, and not values actually observed. This distinction is 
discussed in more detail in chapter 11. 
Geometrically, this model describes a p-dimensional hyperplane embedded in a (p + 1)-
dimensional space with slope determined by the aj's and intercept by a0. The aim of 
parameter estimation is to choose the  a values to locate and angle this hyperplane so as 
to provide the best fit to the data {(x(i), y(i))}, i = 1, . . . , n, where the quality of fit is 
measured in terms of the differences between observed  y values and the values y 
predicted from the model. 
Models with this type of linear structure hold a special place in the history of data 
analysis, partly because estimation of parameters is straightforward with appropriate 
score functions, and partly because the structure of the model is simple and easy to 
interpret. For example, the  additive nature of the model means that the parameters tell 
us the effect of changing any one of the predictor variables ""keeping the others 
constant."" Of course, there are circumstances in which the notion of individual 
contribution makes little sense. In particular, if two variables are highly correlated, then it 
is not meaningful to talk of the contribution from changing one while ""holding the other 
constant."" Such issues are discussed in more detail in later chapters. 

We can retain the additive nature of the model, while generalizing beyond linear 
functions of the predictor variables. Thus 

(6.2)  

where the ƒj functions are smooth (but possibly nonlinear) functions of the Xjs. For 
example, the ƒjs could be log, square-root, or related transformations of the original X 
variables. This model still assumes that the dependent variable Y depends on the 
independent variables in the model (the Xs) in an additive fashion. Again, this may be a 
strong assumption in practice, but it will lead to a model in which it may be easy to 
interpret the contribution of each individual X variable. The simplicity of the model also 
means that there are relatively few parameters (p + 1) to estimate from the data, making 
the estimation problem relatively straightforward. 
We can also generalize this linear model structure to allow general polynomials in the Xs 
with cross-product terms to allow interaction among the Xjs in the model. The one-
dimensional case is again familiar—we can imagine a 2nd or 3rd or kth order polynomial 
interpolating the observed y values. The multidimensional case generalizes this so that 
we have a smooth surface defined on p variables in the (p + 1)-dimensional space. 

Note in passing that even though these predictive models are nonlinear in the variables 
X, they are still linear in the parameters. This makes estimation of these parameters 
much easier than in the case where the parameters enter in a nonlinear fashion, as we 
will see in chapter 11. 

Example 6.1  

 
In figure 6.1(a) we show a set of 50 data points that are simulated from the equation y = 
0.001x3 - 0.05x2 + x3 + e, where e is additive Gaussian noise (zero mean, standard 
deviation s = 3), over the range x ˛
 [1, 50]. A linear fit to the data is shown in  figure 6.1(b) 
and a second-order polynomial fit is shown in figure 6.1(c). Although the linear fit captures 
the general upward trend in Y as a function of X (over this particular range), the second-
order fit is clearly better. Neither fit fully captures the underlying curvature of the true 
structure, as can be seen from the structure in the errors for each model (that is, the errors 
for each model have systematic structure as a function of  x). Both fits were determined by 
minimizing a sum of squares score function. 

Figure 6.1: (a) Fifty Data Points that are Simulated From a Third-Order Polynomial Equation 

 

with Additive Gaussian (Normal) Noise, (b) The Fit of the Model aX + b (Solid Line), (c) The 
Fit of the Model aX2 + bX + c (Solid Line). The Dotted Lines in (b) and (c) Indicate the True 
Model From Which the Data Points were Generated (See Text). The Model Parameters in 
Each Case were Estimated by Minimizing the Sum of Squared Errors between Model 
Predictions and Observed Data.  

 

 

by setting a2 to zero. Thus, it is 

Note that by allowing models with higher order terms and interactions between the 
components of X we can in principle estimate a more complex surface than with a simple 
linear model (a hyperplane). However, note that as p (the dimensionality of the input 
space) increases, the number of possible interaction terms in our model (such as XjXk) 
increases as a combinatorial function of p. Since each term has a weight coefficient (a 
parameter) in the additive model, the number of parameters to be estimated for the full 
model (with all possible interaction terms of order k among p variables) increases 
dramatically as p increases. The interpretation and understanding of such a model 
makes the estimation problem more difficult, and it also becomes increasingly difficult as 
p increases. A practical alternative is to select some small subset of the overall set of 
possible interactions to participate in the model. However, if the selection is carried out in 
a data-driven fashion (as is typically the fashion in a data mining application), the number 
of all possible interaction terms (the size of the search space) scales as 2p, making the 
search problem exponentially more difficult as dimensionality p increases. We will return 
to this issue of how to handle dimensionality later in this chapter. 
The generalization to polynomials brings up an important point, namely the complexity of 
the model. The more complex models contain the simpler models as special cases (so-
called nesting). For example, the first-order a1X1 + a0 model can be viewed as a special 
case of the 2nd order polynomial model 
clear that a complex model (a high-order polynomial in the X variables) can always fit the 
observed data at least as well any simpler model can (since it includes any simpler 
model as a special case). In turn, this raises the complicated issue of how we should 
choose one model over another when the complexity (or expressive power) of each is 
different. This is a subtle question: we may want the model that is closest to some 
hypothesized unknown ""truth""; we may want to find a model that captures the main 
features of the data without being too complicated; we may want to find the model that 
has the best predictive performance on data that it has not seen; and so on. We will 
return to this in later chapters. For now, however, we return to focus on the expressive 
capabilities of the models themselves without thinking yet of how we will choose among 
such models given observed data. 
Transforming the predictor variables is one way to generalize a linear structure. Another 
way is to transform the response variable. sqrt(Y) may be perfectly related to a linear 
combination of the X variables, so that rather than fitting Y directly we may want to 
transform it by taking the square root first, and then use a linear combination of the X 
variables to predict sqrt(Y). Of course, we will not know beforehand that the square root 
is an appropriate transformation. We have to experiment, trying different transformations 
(and bearing in mind the constraints implied by the nature of the measurements involved, 
as discussed in chapter 2). This is why data mining is an exciting voyage of discovery, 
and not a mere exercise in applying standard tools in standard ways. 
As we show in chapter 11, the simple linear regression model can be thought of as 
seeking to predict the expected value of the Y distribution at each value of the X 
predictors, namely E[Y|X]. That is, the regression model provides a prediction of a 
parameter of the conditional distribution of Y , where the parameter is the mean. More 
generally, of course, we can seek to predict other parameters of the conditional Y 
distribution from a linear combination of the X variables. This leads to the ideas of 
generalized linear models and neural networks, discussed in chapter 11. 
We see that, although linear models are simple and easy to interpret (and, we will also 
see, their parameters can be easily estimated), they permit ready generalization to very 
powerful and flexible families of models. Any idea that the word linear implies a narrow 
class of models is illusory. 

6.3.2 Local Piecewise Model Structures for Regression 
Yet further generalizations of the basic linear model can be achieved if we assume that Y 
is locally linear in the X's, with a different local dependence in various regions of the X 
space—that is, a piecewise linear model. Geometrically, our model structure consists of 
a set of different p-dimensional hyperplanes, each covering a region of the input (X) 
space disjoint from the others. The parameters of this model structure include both the 
local parameters for each hyperplane as well as the locations (boundaries) of the 
hyperplanes. For a one-dimensional X the picture is quite easy to visualize: a curve is 
approximated by k different line segments (see figure 6.2 for an example). Note that, in 
this figure, the line is continuous, with the line segments joining up. We could define a 
model structure that relaxes this, not requiring continuity at the ends of the line 
segments. This can be a useful model form, but sometimes the discontinuities can be 
problematic and undesirable because they imply a sudden jump in the predicted value of 
the response variable for an infinitesimal change in a predictor variable. To take an 
example, if a split between two line segments occurs at the value $50,000 for the 
variable income, we might get widely varying y predictions of the response variable, 
probability of loan default, for two applicants who are identical except that one earns 
$50,001 and the other earns $49,999. If the discontinuities are regarded as undesirable, 
one can go further and enforce continuity of derivatives of various orders at the end of 
the segments (which would clearly no longer be straight lines). Such curve segments are 
termed splines, with the whole model being a spline function. Typically, each line 
segment is taken to be a low-degree (quadratic or cubic) polynomial. The result is a 
smooth curve, but one that may change direction many times—the model would be 
highly flexible. 

 

Figure 6.2: An Example of a Piecewise Linear Fit to the Data of Figure 6.1 with k = 5 Linear 
Segments.  

These ideas can be generalized to more than one predictor variable. Again the local 
segments (which will now be (hyper)surfaces, not merely lines) may, but need not, join at 
their edges. Tree structures (described for supervised classification problems in chapter 
10) provide an example of models of this form. 
The piecewise linear model is a good example of how we can build relatively complex 
models for nonlinear phenomena by piecing together simple components (in this case 
hyperplanes). This is a recurring theme in data mining, the idea of composing complex 
global structures from relatively simple local components—and it also provides a link 
between ideas of modeling and ideas of pattern detection. That is, the locality also 
provides a framework for decomposing a complex model into simpler local patterns. For 
example, a ""peak"" in Y as a function of X will be reflected by two appropriately sloped 
line segments that adjoin each other. 

This subsection and the preceding one together serve to show how complex models are 
built up from simpler ones, either by combining the simpler ones into more complex 
ones, or by generalizing them in various ways. No model used in data mining exists in 
splendid isolation. Rather, all such models are linked by a variety of connections, each 
being generalizations, special cases, or variants of others. The trick in effective model 

building in data mining is to choose a model form that is well suited to answer the 
question being posed. This is not simply an exercise in choosing one model form, 
applying it, and presenting the conclusion. Rather, we fit a model, modify it or extend it in 
the light of the results, and repeat the exercise. Data mining, like data analysis in 
general, is an iterative process. 

6.3.3 Nonparametric ""Memory-Based"" Local Models 

In the preceding subsection we gave some examples of how models that are based on 
local characteristics of the data are related to, indeed are on a continuum including, 
broad global models. In this subsection we develop the ideas of local modeling further. 
(We recall that patterns, while also local, are isolated structures, and are not components 
of a global summary of the data. Thus we can talk of local modeling techniques as 
distinct from patterns.) 
Roughly speaking, the spline and tree models briefly described above replace the data 
points by a function estimated from a neighborhood of data points. An alternative 
strategy is to retain the data points, and to leave the estimation of the predicted value of 
Y until the time at which a prediction is actually required. No longer are the data replaced 
by a function and its estimated parameters. For example, to estimate the value of a 
response variable Y for a new case, we could take the average of the Y values of the 
most similar k objects in the data set, where most similar is defined in terms of the 
predictor variables. 
This idea has been extended to include all of the data set objects, but to weight them 
according to how similar they are to the new object—dissimilar ones will have small 
weight, similar ones large weight. The weight determines just how much their Y value 
contributes to the final estimate. An example of such an estimator is the locally weighted 
regression or loess regression model. 
Although we have described the local smoothing ideas in a predictive modeling context, 
they can also be applied in a descriptive and density estimation context—which is the 
domain for which they were in fact first developed. Indeed, we have already seen an 
example of such methods for graphical display of a single variable in chapter 3 (where 
we used the ideas to estimate a probability density function), and we shall see more of 
them in later chapters. In this context, the so-called kernel estimators introduced in 
chapter 3 are common. 

The obvious question with such estimators is how to determine the form of the weight 
function. A weight function that decays only slowly with decreasing similarity will lead to a 
smooth estimate, while one that decays rapidly will lead to a jagged estimate. A 
compromise must be found that is best suited to the aims of the analysis. 
The weight function can be decomposed into two parts. One is its precise functional 
form, and the other is its ""bandwidth."" Thus, suppose that 
is a smoothing function, 
which determines the contribution to the estimate at a new point z from a data set point 
at x. The size of this contribution will depend on the form of K and also on the size of the 
bandwidth h. A larger bandwidth h leads to a smoother function estimate, and a smaller 
bandwidth leads to a rougher, more jagged estimate. In practice, the precise form of the 
weight function turns out to be less important than the ""band-width."" 

Example 6.2  

 
Figure 6.3 shows an example of a regression function constructed with a triangular kernel 
using three different bandwidths. Here we are estimating the proportion of Nitrous oxide 
(NOx) in emissions as a function of ethanol (E), based on measurements taken on 81 
automotive engines under different conditions. The widest bandwidth (h = 0.5) is clearly too 
broad, leading to an oversmoothed estimate that ""misses"" the central peak and the two 
tails. The narrowest bandwidth (h = 0.02) yields a very ""spiky"" estimate that appears to 
follow the noise in the observed data. The intermediate-valued bandwidth (h = 0.1) 
represents a reasonable trade-off, where the major features of the relationship between 
NOx and E are retained without overfitting. Subjective visual inspection can be useful 
technique for choosing bandwidths for simple one-dimensional problems, but does not 

generalize well to the multidimensional case. One can also use more automated methods 
such as cross-validation for choosing h in a data-driven manner. 

Figure 6.3: Nitrous Oxide (NOx) as a Function of Ethanol (E) using Kernel Regression with 
Triangular Kernels, With Bandwidths h = 0.5, 0.1, and 0.02, in Clockwise Order.  

 

 

 

Kernel methods are closely related to nearest neighbor methods. Indeed, both classes of 
methods have now been extended and developed so that in some cases they are 
identical. Whereas kernel methods define the degree of smoothing in terms of a kernel 
function and bandwidth, nearest neighbor methods let the data determine the bandwidth 
by defining it in terms of the number of nearest neighbors. For example, the basic single 
nearest neighbor classifier (where Y is a class identifier) assigns a new object to the 
same class as its most similar object in the data set, and the k-nearest neighbor 
classifier assigns a new object to the most common class amongst the most similar k 
objects in the data set. More sophisticated nearest neighbor methods weight the 
contribution of according to distance from the point to be classified, and more 
sophisticated kernel methods let the bandwidth h depend on the data—so that they can 
be seen to be almost identical in terms of model structure. 
Local model structures such as kernel models are often described as non-parametric 
because the model is largely data-driven with no parameters in the conventional sense 
(except for the bandwidth h). Such data-driven smoothing techniques (such as the kernel 
models) are useful for data interpretation, at least in one or two dimensions. 
It will be clear that local models have their attractions. However, no model provides an 
answer to all problems, and local models have weaknesses. In particular, as the number 
of variables in the predictor space increases, so the number of data points required to 

obtain accurate estimates increases exponentially (a consequence of the ""curse of 
dimensionality""—see section 6.5 below). This means that these ""local neighborhood"" 
models tend to scale poorly to high dimensions. 
Another drawback, particularly from a data mining viewpoint, is the lack of interpretability 
of the model. In low dimensions (p = 3 or so), we can plot the estimates. In high 
dimensions this is not possible, and there is no direct manner by which to summarize the 
model. Indeed, it is stretching the definition of a model to even call these representations 
models at all, since they are never explicitly defined as functions but instead are implicitly 
defined by the data. 

6.3.4 Stochastic Components of Model Structures 
Until this point, apart from a few brief references, we have ignored the fact that, with real 
data, we generally cannot find a perfect functional relationship between the predictor 
variables X and the response variable Y . That is, for any given vector of predictor 
variables x, more than one value of Y can be observed. The distribution of the values y 
at each value of X represents an aspect of variation that cannot be reduced by more 
sophisticated model building using just the variables in X. For this reason it is sometimes 
termed the unexplainable or nonsystematic or random component of the variation, with 
the variation in Y that can be explained in terms of the X variables being termed the 
explainable or systematic variation. (Of course merely because the systematic variation 
can be explained in principle by the variables in X, does not mean that we can 
necessarily build a model that will be able to do it). 
In most of our discussion we have focused on the systematic component of the models, 
but we also need to consider the random component. The random component of models 
can arise from many sources. It can arise from simple measurement error—repeated 
measurements of Y will give different results, as discussed in chapter 2. The random 
component can also arise because our set of X variables does not include all of the 
variables that are required to make a perfect prediction of Y (for example, predicting 
whether a customer will purchase a particular product or not based only on past 
purchasing behavior will ignore potentially relevant demographic information about them 
such as age, income, and so on). Indeed, we should expect this usually to be the case—
it would be a rare situation in which all of the variability in a variable was perfectly 
explained by just a handful of other variables, down to the finest detail. 

Example 6.3  

 
We can extend the regression modeling framework discussed earlier to include a 
stochastic component. We will assume that for any x we will observe a particular y but with 
some noise added; that is, there is some inherent uncertainty in the relationship between x 
and y: 
(6.3)  
where g(x; ?)) is a deterministic function of the inputs x, and e is often assumed to be a 
random variable (which is independent of x) with constant variance s2 and zero-mean. The 
random term e can reflect noise in the measurement process (that is, we don't observe the 
""true"" value for  y but instead get a measurement of  y which has random noise added). 
More generally, the random component e can reflect the fact that there are hidden 
variables (that are not being measured or are ""hidden"" from observation) that affect y in a 
manner that cannot be accounted for by the dependence of Y on the variables X alone. 
The zero-mean assumption on  e is fairly harmless, since if the noise has a constant non-
zero mean it can be absorbed into g without loss of generality. If, for example, we make the 
common assumption that e has a Normal distribution with zero mean and constant 
variance s2, then 
(6.4)  
The constant s2 assumption may require closer scrutiny in practice: for example, if Y 
represents the variable annual credit card spending, and X is income, it is plausible that the 
variability in Y will grow as a function of X. If this were the case, then to model this effect, s 
would need to be a function of x in the model above. 
Note that the functional form of  g is left free in these equations; that is, it could be chosen 
to be any of the various model structures we discussed earlier. We have already seen in 
chapter 4 that the Normal assumption on e above leads naturally to the principle of least-

squares regression—that is, finding the parameters ? that determine g such that g(x; ?) 
minimizes the sum of squares of between ƒ(x; ?) and the observed  y values. 

 

 

The random component is important when it comes to choosing suitable score functions 
for estimating parameters and choosing between models. The likelihood score function, 
for example, introduced in chapter 4 and also discussed elsewhere, is based on 
assumptions about the form of the distribution of the random component. Extensions of 
the likelihood function that include a smoothness penalty so that too complex a model is 
not fitted also require assumptions about the distribution of the random component. More 
advanced methods based on likelihood concepts (for example, so-called quasi-likelihood 
methods ) relax detailed distributional assumptions, but still base their choice of 
parameter estimates on aspects of the distribution of the random component. 

6.3.5 Predictive Models for Classification 
So far we have concentrated on predictive models in which the variable to be predicted, 
Y, was quantitative. We now briefly consider the case of a categorical variable Y, taking 
only a few possible categorical values. This is a (supervised) classification problem, with 
the aim being to assign a new object to its correct class (that is, the correct Y category) 
on the basis of its observed X values. 
In classification we are essentially interested in modeling the boundaries between 
classes. As with regression, we can could make simple parametric assumptions about 
the functional form of the boundaries. For example, a classic approach is to use a linear 
hyperplane in the p-dimensional X space to define a decision boundary between two 
classes. That is, the model partitions the X-space into disjoint decision regions (one for 
each class), where the decision regions are separated by linear boundaries (see  figure 
6.4 for an example). A more complex model might allow higher-order polynomial terms, 
yielding smooth polynomial decision boundaries. If we allow very flexible non-linear 
forms for our boundaries we arrive at models such as the neural network classifiers 
discussed in chapter 5. 

Figure 6.4: An Example of Linear Decision Boundaries for the Two-Dimensional Wine 
Classification Data Set of Chapter 5 (See Figure 5.1).  

Just as in regression modeling, another way to allow more flexibility is to combine 
multiple simple local models, e.g., combinations of piecewise linear decision boundaries, 

 

as in figure 6.5. For example, the classification tree models of chapter 5 define a 
particular class of local linear decision boundaries that are hierarchical and axis-parallel 
in structure. As mentioned earlier, the nearest-neighbor classifier is one where the class 
label of the nearest-neighbor from the training data set of a new unclassified data point is 
used for prediction. Although this technique is generally thought of as a method rather 
than a model per se, it does in fact implicitly define a piecewise linear decision boundary 
(at least when using Euclidean distance to define neighbors). 

Figure 6.5: An Example of Piecewise Linear Decision Boundaries for the Two-Dimensional 
Wine Classification Data Set of Chapter 5 (See Figure 5.1).  

 

There are a large number of different classification techniques, providing different ways 
to model decision boundaries. Something like nearest-neighbor is very flexible (allowing 
multiple local disjoint decision regions for each class, with flexible boundaries) whereas a 
single global hyperplane is a much simpler model. 
From a practical modeling standpoint, prior knowledge about the shape of classification 
boundaries may not be as readily available as knowledge we may have about how Y is 
related to X in a regression problem. Nonetheless, the functional forms used successfully 
for discrimination models are quite similar to those we discussed earlier for regression 
modeling, and the same general themes emerge. We will return to classification models 
in much more detail in chapter 10 on classification. 

6.3.6 An Aside: Selecting a Model of Appropriate Complexity 
In our discussion so far we have seen that model structures range from the relatively 
simple to the complex. For example, in regression we saw that the complexity of a 
""piecewise-local"" model structure is controlled by the number k of local regions 
(assuming that the complexity of the local function in each region is fixed). As we make k 
larger, we can obtain a curve that ""follows"" the observed data more closely. Put another 
way, the expressive power of the model structure increases in that it can represent more 
complex functions. 
As we increase the expressive power of a model it is clear that we can in general 
continue to get a better fit to the available data. However, we need to be careful. While 
our score function on the training data may be improving, our model may actually be 
getting worse in terms of generalizing to new data. (Recall our discussion of this 
""overfitting"" phenomenon in the context of classification trees in chapter 5, and figure 5.4 
in particular). On the other hand, if we go the other direction and over-simplify our model 
structure, it may end being too simple. This issue of selecting a model of the appropriate 
complexity is always a key concern in any data analysis venture where we consider 
models of different complexities. In fact we will look at this from a theoretical viewpoint in 
chapter 7, using a generalization of the bias -variance trade-off that we first introduced in 
chapter 4. 
In practice how can we choose a suitable compromise between simplicity and 
complexity? From a data-driven viewpoint (i.e., data mining) we can define a score 

 
 

function that tries to estimate how well a model will perform on new data and not just on 
the training data. A commonly used approach is to combine both the usual goodness-of-
fit term (on the training data) with an explicit second term to penalize model complexity. 
Another widely used approach is to partition the training data into two or more subsets 
(e.g., via cross-validation as described in chapter 5 for trees) and to train models on one 
subset and select models using a different validation data set. 
Since the focus of this chapter is on the representational capabilities of different model 
and pattern structures (rather than on how they are scored relative to the data), we defer 
detail discussion of score functions to chapter 7. However, for the reader who up to this 
point was wondering how we would be able to select among the many different models 
being discussed here, the answer is that there do indeed exist well-defined data-driven 
score functions that allow us to search over different model structures in a principled 
manner to find what appears to be the best model for a given task (with some caveats as 
we will see in chapter 7). 

6.4 Models for Probability Distributions and Density 
Functions 
The previous section provided an overview of predictive problems, in which one of the 
variables (we labeled it Y) was singled out as special, to be predicted from the others. 
Many modeling problems in data mining fall into this class. However, many others are 
""descriptive,"" with the aim being simply to produce a summary or description of the data. 
If the available data are the complete data (for example, all chemical compounds of a 
certain type), then no notion of inference is relevant, and the aim is merely simplifying 
description. On the other hand, if the available data are a sample, or have been 
measured with error (so that collecting them again could yield slightly different values), 
then the aim is really one of inference—inferring the ""true,"" or at least a good, model 
structure. In this latter case it is useful to think of the data as having been produced from 
an underlying probability function. 

6.4.1 General Concepts 
In this section we focus on some of the general classes of models used for density 
estimation (a more detailed discussion is given in chapter 9). While the functional form of 
the underlying models tend to be somewhat different from those we have seen earlier 
(for example, unimodal ""bump"" functions versus the linear and polynomial functions we 
saw for regression), several of the main concepts such as linear combinations of simpler 
models are once again widely applicable. 

There are two general classes of distribution and density models: 

1.  Parametric Models: where a particular functional form is assumed. 

For real-valued variables the function is often characterized by a 
location parameter (the mean) and a scale parameter (characterizing 
the variability)—for example, the Normal density function and Binomial 
distribution. Parametric models have the advantage of simplicity (easy 
to estimate and interpret) but may have relatively high bias because 
real data may not obey the assumed functional form. The appendix 
contains a brief review of some of the more well-known parametric 
density and distribution models. 

2.  Nonparametric Models: where the distribution or density estimate is 
data-driven and relatively few assumptions are made a priori about the 
functional form. For example, we can use the kernel estimates 
introduced in chapter 3 and section 6.3.3: the local density at x is 
defined as a weighted average of points near to x. 

Taking the above as the extremes, we can also define intermediate models that lie 
between these parametric and nonparametric extremes: mixture models. These are 
discussed below. 

6.4.2 Mixtures of Parametric Models 
A mixture density for x is defined as 

(6.5)  

This model decomposes the overall density (or distribution) for x into a weighted linear 
combination of K component or class densities (or distributions). Each of the component 
densities pk(x|?k) typically consists of a relatively simple parametric model (such as a 
Normal distribution) with parameters ?k. p k represents the probability that a randomly 
chosen data point was generated by component k, 
To illustrate, consider a single Normal distribution used as a model for a two-dimensional 
data set. This distribution can be thought of as a ""symmetric bump function,"" whose 
location and shape we can to try to locate in the 2-space to model the density of the data 
as well as possible (see figure 6.6 for a simple example). An intuitive interpretation of the 
mixture model is that it allows us to place k of these bumps (or components) in the two-
dimensional space to approximate the true density. The locations and shapes of the k 
bump functions can be fixed independently of each other. In addition, we are allowed to 
attach weights to the components. If the weights are positive and sum to 1 the overall 
function is still a probability density (see equation 6.5). 

. 

 

Figure 6.6: From the Top: (a) Data Points Generated From a Mixture of Three Bivariate 
Normal Distributions (Appendix 1) with Equal Weights, (b) the Underlying Component 
Densities Plotted as Contours that are Located 3s From the Means, and (c) the Resulting 
Contours of the Overall Mixture Density Function.  

As k increases, the mixture model allows for quite flexible functional forms, as local 
bumps can be placed to capture local characteristics of the density (this is reminiscent of 
the local modeling ideas in regression). Clearly k plays the role of controlling complexity: 
for larger k we get a more flexible model but also one that it is more complicated to 
interpret and more difficult to fit. The usual bias-variance trade-offs again apply. Of 
course, we are not constrained to use only Normal components (although these tend to 
be quite popular in practice). Mixtures of exponentials and other densities could equally 

well be used. The details of how the locations, shapes, and value of k are determined 
from the data are deferred until chapter 9. The important point here is that mixtures 
provide a natural generalization of the simple parametric density model (which is global) 
to a weighted sum of these models, allowing local adaptation to the density of the data in 
p-space. 
The general principles underlying a mixture model are broadly applicable, and the 
general idea occurs in many guises in probabilistic model building. For example, the idea 
of hierarchical structure can be nicely captured using mixture models. In chapter 8 we 
will discuss the mechanics of how mixtures are fitted to data, and in chapter 9 we will see 
how they can be usefully employed for detecting clusters in data. 
In terms of interpretability, either mixture models can be used simply as ""black boxes"" 
that provide a flexible model form, or the individual mixture components can be given an 
explicit interpretation. For example, components of a mixture model fitted to customer 
data could be interpreted as characterizing different types of customers. One 
interpretation of a mixture model (particularly in a clustering context) is that the 
components are generated by a hidden variable taking K values, and the location and 
shapes in p-space of the components are unknown to us a priori, but may be revealed by 
the data. Thus, mixture models share with projection pursuit and related methods the 
general idea of hypothesizing a relatively simple latent or hidden structure that may be 
generating the observed data. In chapter 8 and  10 we will discuss the use of the 
expectation-maximization (EM) algorithm for learning the parameters of mixture models 
from data. 

6.4.3 Joint Distributions for Unordered Categorical Data 
For categorical data we have a joint distribution function defined in the cross-product of 
all possible values of the p individual ariables. For example, if A is a variable taking 
values {a1, a2, a3} and B is a variable taking values {b1, b2}, then there are six possible 
values for the joint distribution of A and B. We will assume here (for simplicity) that the 
values are truly categorical and that there is (for example) no notion of scale or order. 
For small values of p, and for small numbers of variable values, it is convenient to 
display the values of the distribution in the form of a contingency table of cells, one cell 
per joint value, as shown in the example of table 6.1. This becomes impractical as the 
number of variables and values get beyond four or five. In addition, the contingency table 
does not really allow us to see any potential structure that might be in the data. For 
example, the data in table 6.1 have been constructed so that the variables are 
independent: however, this fact is not immediately apparent from looking at the table. 
Table 6.1: A Simple Contingency Table for Two-Dimensional Categorical Data for 
a Hypothetical Data Set of Medical Patients Who Have been Diagnosed for 
Dementia.  

Dementia  

  

  

Smoker  

  

  

  

None 

Mild 

Severe 

No 

Yes 

426 

284 

66 

44 

132 

88 

In contrast to the case of quantitative variables, with categorical variables in which the 
categories are unordered there is no notion of a smooth probability function. Thus, if for 
example all variables each have m possible values, one would have to specify mp - 1 
independent probability values to specify the model fully (the -1 comes from the 
constraint that they sum to 1). Clearly this quickly becomes impractical as p and m 
increase. In the next section we look at systematic techniques for structuring both 
distribution and density functions to find parsimonious ways to describe high-dimensional 
data. 

6.4.4 Factorization and Independence in High Dimensions 
Dimensionality is a fundamental challenge in density and distribution estimation. As the 
dimensions of the x, space grow it rapidly becomes more difficult to construct fully 
specified model structures since model complexity tends to grow exponentially with 
dimension (the curse of dimensionality referred to earlier in this chapter). 
Factorization of a density function into simpler component parts provides a general 
technique for constructing simple models for multivariate data. This is a simple yet 
powerful idea that recurs throughout multivariate modeling. For example, if we assume 
that the individual variables are independent, we can write the joint density function as 

(6.6)  

where x = (x1, . . . , xp) and pk is the one-dimensional density function for Xk. Typically it 
is much simpler to model the one-dimensional densities separately, than to model their 
joint density. Note that the independence model for log p(x) has an additive form, 
reminiscent of the linear and additive model structures we discussed for regression. 
This factorization certainly simplifies things, but it has come at a modeling cost. The 
assumption that the variables are independent will not be even approximately true for 
many real problems. Thus, a full independence assumption is in essence one extreme 
end of a spectrum (the low-complexity end), a spectrum that extends to the fully 
specified joint density model at the other end (the high-complexity end). Of course, we 
do not have to choose models solely from the extremes of this complexity continuum, 
and can, instead, try to find something in between. The joint probability function p(x) can 
be written in general as 

(6.7)  

The right-hand side factorizes the joint function into a sequence of conditional 
distributions. Now we can try to model each of those conditional distributions separately. 
Often considerable simplification results because each variable Xk is dependent on only 
a few of its predecessors. That is, in the conditional distribution for the kth variable, we 
can often ignore some of variables X1, . . . , Xk-1. Such factorizations permits a natural 
representation of the model as a directed graph, with the nodes corresponding to 
variables, and the edges showing dependencies between the variables. Thus the edges 
directed into the node for the kth variable will be coming from (a subset of) the variables 
x1, . . . , xk-1. These variables are, naturally enough, called the  parents of variable x1. 
Sometimes we have to experiment by fitting different models to the data to seek such 
simplifying factorizations. In other cases such simplifications will be evident from the 
structure of the data—for example, if the variables represent the same property 
measured sequentially (for instance, at different times). In this case, a Markov chain 
model is often appropriate—in which all of the previous information relevant to the kth 
variable is contained in the immediately preceding variable (so that the terms in this 
factorization simplify to p(xk|x1, . . . , xk-1) = p(xk|xk-1) ). The model structure for a first-
order Markov model is shown in  figure 6.7. 

 

Figure 6.7: A Graphical Model Structure Corresponding to a First-Order Markov Assumption.  

Graphs that are used represent probability models, such as that in figure 6.7 are often 
referred to as graphical models. In the discussion below we focus specifically on the 
widely-used subclass of acyclic directed graphs (also sometimes known in computer 
science as belief networks when used as probability models). Note that this graph 
representation emphasizes the independence structure of the model (e.g., see figure 6.7 
again) and leaves the actual functional and numeric parametrization of parent-child 
relationships unspecified. 
For another example of a graphical model, consider the variables age, education (level 
of education a person has) and baldness (whether a person is bald or not). Clearly age 
cannot depend on either of the other two variables. Conversely, both education and 
baldness are directly dependent on age. Furthermore, it is quite implausible that 
education and baldness are directly dependent on each other given age—that is, once 
we know the person's age, knowing whether or not they are bald tells us nothing about 
their education level (and vice versa). On the other hand, if we do not know a person's 

age, then baldness may provide information about education (for example, a bald person 
is more likely to be older, and hence, in turn, more likely to have a university degree). 
Thus, a plausible graphical model is the one in  figure 6.8. 

Figure 6.8: A Plausible Graphical Model Structure for Two Variables Education and Baldness 
that are Conditionally Independent Given Age.  

 

These ideas can be taken further, by the postulation of the existence of unobserved 
hidden or latent variables, which explain many of the observed relationships in the data. 
Figure 6.9 provides such an example. In this model structure a single latent variable has 
been introduced as an intermediate variable that simplifies the relationship between the 
observed data (in this case, medical symptoms) and the underlying causal factors (here, 
two independent diseases). The introduction of hidden variables in a manner such as 
this can serve to simplify the relationships in a model structure; for example, given the 
values here of the intermediate variable, the symptoms become independent. However, 
we must exercise discretion in practice in terms of how many hidden variables we 
introduce into the model structure to avoid introducing spurious structure into the fitted 
model. In addition, as we will discuss in chapters 8 and 9, parameter estimation and 
model selection with hidden variables is quite nontrivial. 

 

Figure 6.9: The Graphical Model Structure for a Problem with Two Diseases that are 
Marginally (Unconditionally) Independent, A Single Intermediate Variable Z that Directly 
Depends on Both Diseases, And Six Symptom Variables that are Conditionally Independent 
Given Z.  

In the context of classification and clustering, it is often convenient to assume that the 
variables are conditionally independent of each other given the value of the class 
variable. That is, 

(6.8)  

where y is a particular (categorical) class value. This is simply the conditional 
independence (""naive"") Bayes model introduced in the context of classification modeling 
in section 6.3.5. The graphical representation for such a model is shown in figure 6.10. 

 

Figure 6.10:  The First-Order Bayes Graphical Model Structure, With a Single Class Y and 6 
Conditionally Independent Feature Variables X1, . . . , X6.  

Equation 6.8 can also be used in the case where Y is an unobserved (hidden, latent) 
variable that is introduced to simplify the modeling of p(x), i.e., we have a finite mixture of 
the form 

(6.9)  

where Y takes K values, and each component p(x|y = k) is modeled using the conditional 
independence assumption of equation 6.8. As an example, we might model the joint 
distribution of how customers purchase p products in this fashion, where (for example) if 
a customer belongs to a specific component k then the likelihood of purchasing certain 
subsets of products, i.e., pj(xj|y = k), is increased for certain subsets of products xj. Thus, 
although the products (the xj) are modeled as being conditionally independent given y = 
k, the mixture model induces an unconditional (marginal) independence by virtue of the 
fact that certain products co-occur with higher probability in certain components k. In 
effect, the hidden Y variable acts to group the variables xj together into equivalence 
classes, where within each equivalence class the variables are modeled as being 
conditionally independent. The use of hidden variables in this manner can be a powerful 
modeling technique, and it is one we return to in more detail in chapter 9. 

 
 

6.5 The Curse of Dimensionality 
We have noted in various places that what works well in a one-dimensional setting may 
not scale up very well to multiple dimensions. In particular, the amount of data we need 
often increases exponentially with dimensionality if we are to maintain a specific level of 
accuracy in our parameter or function estimates. This is sometimes referred to as the 
""curse of dimensionality."" This can be important, since data miners are often interested 
in finding models and patterns in high-dimensional problems. Note that ""high-
dimensional"" can be as few as p = 10 variables or as many as p = 1000 variables or 
beyond—it depends on the complexity of the models concerned and on the size of the 
available data. 

Example 6.4  

 
The following example is taken from Silverman (1986) and illustrates emphatically the 
difficulties of  density estimation in high dimensions. Consider data that is simulated from a 
multivariate Normal density with unit covariance matrix and mean (0,0, . . . , 0) (see the 
appendix for a definition of the multivariate Normal density). Assume that the bandwidth h, 
in a kernel density estimate, is chosen such that it minimizes the mean square error at the 
mean. Silverman calculated the number of data points required to ensure that the relative 
mean square error at zero is less than 0.1—that is, that 

and where p(x) is the true Normal density, and 

is 

a kernel estimate using a Normal kernel with the optimal density, and bandwidth 
parameter. Thus, we are looking at the relatively ""easy"" problem of estimating (within 10% 
relative accuracy) a Normal density at the mode of this density (where the points will be 
most dense on average) by using a Normal kernel: what could be easier? Silverman 
showed (analytically) that the number of data points grows exponentially. In 1 dimension 
we need 4 points, in 2 we need 19, 3 we need 67, in 6 we need 2790, and by 10 
dimensions we need about 842,000. This is an inordinate number of data points for such a 
simple problem! The lesson to be learned is that density estimation tasks (and indeed most 
other data mining tasks) rapidly become very difficult as dimensionality increases. 

 

 

There are two basic (and fairly obvious) strategies for coping with high-dimensional 
problems. The first is simply to use a subset of relevant variables to construct the model. 
That is, to find a subset of p' variables where p' << p. The second is to transform the 
original p variables into a new set of p' variables, where again  p' << p. Examples of this 
approach include of  p principal component analysis, projection pursuit, and neural 
networks. 

6.5.1 Variable Selection for High-Dimensional Data 
Variable selection is a fairly general (and sensible) strategy when dealing with high-
dimensional problems. Consider for example the problem of predicting Y using X1, . . . , 
Xp. It is often plausible that not all of the p variables are necessary for accurate 
prediction. Some X variables may be completely unrelated to the predictor variable Y (for 
example, the month of a person's birth is unlikely to be related to their creditworthiness). 
Others may be  redundant in the sense that two or more X variables contain essentially 
the same predictive information. (For example, the variables income before tax and 
income after tax are likely to be highly correlated.) 
We can use the notion of independence (introduced in chapter 3) to gauge relevance in 
a quantitative manner. For example, if p(y|x1) = p(y) for all values of  y and x1, then the 
target variable Y is independent of input variable X1. If p(y|x1, x2) = p(y|x2), then Y is 
independent of X1 if the value of X2 is already known. In practice, of course, we are not 
necessarily able to identify from a finite sample which variables are independent and 
which are not; that is, we must estimate this effect. Furthermore, we are interested not 
only in strict independence or dependence, but also in the degree of dependence. Thus, 
we could (for example) rank individual X variables in terms of their estimated linear 
correlation coefficient with Y: that would tell us about estimated individual linear 
dependence. If Y is categorical (as in classification), we could measure the average 
mutual information between Y and X': 

(6.10)  

to provide an estimate of the dependence of X and Y, where X' here is a categorical 
variable (for example, a quantized version of a real-valued X). Other measures of the 
relationship between Y and the Xs can also be used. 
However, the interaction of individual X variables with Y does not necessarily tell us 
anything about how sets of variables may interact with Y . The classic example, for 
Boolean variables, is the parity function, where Y is defined to be 1 if the sum of the 
(binary) values of the X1, . . . , Xp variables in the set is an even integer, and Y is 0 
otherwise. Y is independent of any individual X variable, yet is a deterministic function of 
the full set. While this is something of an extreme example, it nonetheless illustrates that 
such non-linear non-additive interactions can be masked if we only look at individual 
pair-wise interactions between Xs and Y. Thus, in the general case, the set of k best 
individual X variables (as ranked by correlation for example) is not the same as the best 
set of X variables of size k. Since one can have 2p - 1 different nonempty subsets of p 
variables, exhaustive search is not feasible except for very small p. Worse still, for many 
prediction problems, there is no optimal search algorithm (in the sense of being 
guaranteed to find the best set of variables) that has worst-case time complexity any 
better than O(2p). 
This means that, in practice, subset selection methods tend to rely on heuristic search to 
find good model structures. Many algorithms are based on the simple heuristic of greedy 
selection, such as adding or deleting one variable at a time. We will return to this issue of 
search in chapter 8. 

6.5.2 Transformations for High-Dimensional Data 
The second general category of ideas is based on transforming the predictor variables. 
The intuitive idea here is to search for a set of p' variables (let us call them Z1, . . . , Zp'), 

where typically p' is much smaller than p, where the Z variables are defined as functions 
of the original X variables, and where the  Zs are chosen in some sense to be the ""best"" 
set of p' variables for our task. 
This general theme, of replacing the observed variables with a smaller set of variables 
that are somehow more fundamental to the task at hand, shows up repeatedly in 
different branches of data analysis. The Zs are variously referred to as basis functions, 
factors, latent variables, principal components, and so forth, depending on the specific 
goals and methods used to derive them. We will examine some of these models (and 
their associated fitting algorithms) in detail in later chapters, but for now we illustrate the 
general idea with just two specific examples: 

§  Projection Pursuit Regression uses a model structure of the form 

(6.11)  

§  where  x is the projection of the vector x onto the jth weight vector aj 

(both vectors being  p-dimensional, resulting in a scalar inner product), hj 
is a nonlinear function of this scalar projection, and the  wj are scalar 
weights for the resulting nonlinear functions. The procedures for 
determining the  wj, the form of the hj, and the ""projection directions"" aj 
can be rather complex and algorithm-dependent, but the underlying idea 
is quite general. 

§  For example, this is essentially the form of the model structure that 

underlies neural networks (to be discussed later in chapter 11), where for 
such networks the functional forms of the  hj are usually chosen to be 
something like hj(t) = 1/(1 + e-t). One limitation of this class of models is 
the fact that they are quite difficult to interpret unless p' = 1. Another 
limitation is that the algorithms for estimating the parameters of these 
models can be computationally quite complex and may not be practical 
for very large data sets. We will return to this model family in chapter 11. 

§  Principal Components Analysis: We introduced principal components 

analysis (PCA) in chapter 3. This is a classic technique in which the 
original p predictor variables are replaced by another set of p variables 
(Z1, . . . , Zp) that are formed from linear combinations of the original 
variables. The data vectors comprising the original data set map to new 
vectors in the  Z space and, as explained in chapter 3, the sets of weights 
defining the Zs are chosen so as to maximize the variance of the original 
data set when expressed in terms of these new variables. Principal 
components analysis is thus a special case of projection pursuit, where 
the projection index in this case is the variance along the projected 
direction. Principal components has two merits as a data reduction 
technique. Firstly, it sequentially extracts most of the variance of the data 
in the X space, so we might hope that only the first few components (far 
fewer than the full number p of original X variables) contain most of the 
information in the data. Secondly, by virtue of the way in which the 
components are extracted (see chapter 3) they are orthogonal, so that 
interpretation is eased. However, one should be aware that the principal 
component vectors in the X space may not necessarily be the ideal 
projection directions for optimizing predictive performance on a different 
variable Y (for example). For example, when we try to model differences 
among groups (or classes) in the data (for classification and clustering), 
the principal component projections need not emphasize group 
differences and indeed can even hide them. (Similar remarks can be 
made about more general projection pursuit methods.) Nonetheless, 
PCA is widely used in data analysis and can be a very useful dimension-
reduction tool. There are a wide number of other techniques (each with 
different properties) available for dimension reduction, including factor 
analysis (chapter 4), projection pursuit (chapter 11, and above), 
independent component analysis, and so forth. 

 

 

6.6 Models for Structured Data 

In many situations either the individuals, the variables, or both, possess some well-
defined relationships that are known a priori. Examples include linear chains or 
sequences (where the measurements are ordered—for example, protein sequences), 
time series (where the measurements are ordered in time, perhaps on a uniform time 
scale), and spatial or image data (where the measurements are defined on a spatial 
grid). Even more complex structure is possible. For example, in medicine one can have 
imaging data of the brain measured on a three-dimensional grid, with repeated 
measurements over time. 
Such structured data is inherently different from the types of measurements we have 
discussed in most places in this chapter. Up to this point we have implicitly assumed that 
the n individual objects (the patients, the customers) in our data set are a random 
sample from an underlying population. Specifically, we have assumed that the 
measurement vectors x(i), 1 = i = n, are conditionally independent of each other given a 
particular fitted model (that is, that the likelihood of the data can be expressed as the 
product of individual p(x(i)). For example, if we have a Normal density model for the 
variable weight, then we are assuming that knowing the weight of one person tells us 
nothing about the weight of any other person in the data set. (We are, of course, here 
ignoring subtle dependencies that may exist such as having members of the same family 
appear sequentially in our data set, where such family members might be predisposed to 
having similar overweight or underweight tendencies.) Thus, although it may be an 
approximation, we have been working with this assumption on the basis that it is a useful 
assumption for many practical situations. 

However, there are problems for which the dependence is explicit and needs to be 
modeled. For example, if we take measurements of a person's blood pressure every five 
minutes over a 24-hour period, then clearly there is very likely to be some significant 
dependence between the successive values. How should we model such dependence? 
One approach is to reduce the multiple observations on each object to one or a few 
variables (that is, a fixed multivariate description x), using ideas about the expected 
relationships between them (we referred to this possibility above). This is sometimes 
called the feature extraction approach. For example, we might expect blood pressure to 
decrease over the 24-hour period as a medication begins to take effect, so we might 
replace the 5 times 12 times 24 observations for each person by just two numbers 
showing a starting value and the decreasing slope of a linear trend. Or we might use the 
same principle and fit a curve in which the rate of decrease reduces over time. The 
numbers describing the curves for each subject (which are often called derived 
variables) can then be analyzed in the standard way. 

Note that this general approach (of converting sequential measurements into a non-
sequential vector representation) may be sufficient for a given data mining task, but in 
general there is a loss of information in this process, in that we lose the timing and order 
information present in the original measurements. For certain applications this sequential 
information may be critical. As an example, we may have a population of Web users, 
among whom are a group who navigate from Web page A, to page B, to page C, 
repeatedly in that order, in a cyclic fashion. If we were to reduce this information to a 
histogram of which pages were visited (yielding a histogram with three roughly equal 
bins), we would lose the ability to discover the dynamic cyclic pattern underlying the 
data. 
Let us consider an example of a sequential data model, namely a first-order Markov 
model for T data points observed sequentially, y1, . . . , yT. Note that for even moderately 
large values of T , a full joint density for p(y1, y2, . . . , yT) will be a very complex object 
(for example, if Y takes m discrete values, it will require the specification of O(m T) 
numbers). Thus, in modeling data with structure, we can take direct advantage of the 
ideas presented in the last section on factorization; that is, the structure of the data will 
suggest a natural structuring for any models we will build. Thus, we return to our first-
order Markov model, again defined as: 

(6.12)  

We can simplify this model considerably if we make the assumption of stationarity, 
namely that the probability functions in the model do not depend on the specific time t, 
that is, pt(yt|yt-1) = p(yt|yt-1). Thus, the same conditional probability function is used in 
different parts of the sequence. This drastically cuts down on the number of parameters 
we need for the model. For example, if Y is m-ary, the nonstationary model would require 
O(m2T) parameters (a matrix of m × m conditional probabilities for each time point in the 
sequence), while the stationary model only requires O(m2) probabilities (one matrix of m 
× m conditional probabilities that is used throughout the sequence). The notion of 
stationarity can be applied to much more general Markov models than the first-order 
model above, and indeed extends naturally to spatial data models as well (for which we 
would assume stationarity in space, rather than in time). If we assume stationarity, then 
we cannot account for changes in the statistical model as a function of time or space. 
However, stationarity is advantageous from a parametrization standpoint, making it a 
very useful and practical assumption in model building—we will assume it throughout our 
discussion unless specifically stated otherwise. 
The Markov model in equation 6.12 has a simple generative interpretation (see figure 
6.7, with ys instead of  xs). The first value in the sequence y1 is chosen by drawing a  y1 
value randomly according to some initial distribution  p(y1). The value at time  t = 2 is 
randomly chosen according to the conditional density function p(y2|y1), where the value 
y1 is known and fixed. Once y2 has been chosen in this manner, y3 is now generated 
according to p(y3|y2) where the value y2 is now fixed, and so on until time T. 
However, the Markov model assumption is rather strong (as we discussed in section 
6.4.4). In words, it says that the influence of the past is completely summarized by the 
value of Y at time t-1. Specifically, Yt does not have any ""long-range"" dependencies 
other than its immediate dependence on Yt-1. Clearly there are many situations in which 
this model may not be accurate. For example, consider modeling the grammatical 
structure of English text, where Y takes values such as verb, adjective, noun, and 
so on. The first-order Markov assumption is inadequate here since (for example) 
deciding whether a verb is singular or plural will depend on the subject of the verb, that in 
turn may be much further back in the sentence than just one word back. 
For real-valued Y s, the Markov model is often specified as a conditional Normal 
distribution: 

(6.13)  

(6.14)  

where g(yt-1) plays the role of the mean of the Normal (it is a deterministic function linking 
the past yt-1 to the present yt) and s is the noise in the model (assumed stationary here). 
A common choice for the function g is to make it a linear function of yt-1, g(yt-1) = a0 + a1yt-
1, leading to the well-known first-order autoregressive model, 

where e is zero-mean Gaussian noise with standard deviation s and the as are the 
parameters of the model. Note that equation 6.14 can be expressed in the form of 
equation 6.13 under these assumptions.  
The model in equation 6.14 has a simple interpretation from a generative viewpoint; the 
value yt at time t in the sequence is generated by taking the previous value yt-1, 
multiplying it by a constant a1, adding an offset a0, and adding some amount of random 
noise e. For  y to remain stable (bounded as t ?  8) it is necessary that  -1 < a1 < 1. 
Values of |a1| closer to 1 imply stronger dependence among successive y values; values 
of |a1| closer to 0 imply weaker dependence. This model structure is clearly closely 
related to the standard regression model structures of section 6.3. Instead of regressing 
on independent X values, here Y is regressed on ""lagged"" values of itself. Thus, from our 
knowledge of regression model structures, we can immediately think of a multitude of 
generalizations of the simple first-order model above. For example, yt can depend on 
earlier lags in the sequence; that is, we can replace the mean at time t by g(yt-1) in 
equation 6.13 with g(yt-1, yt-2, . . . , yt-k), known as a kth order Markov model. Again, a 
common choice for g(yt-1, yt-2, . . . , yt-k) is a simple linear model of the form a0 + S aiyi. In 
principle, however, rather than just linear regression, we could use any of the general 

functional forms discussed in section 6.3, such as additive models, polynomial models, 
local linear models, data-driven local models, and so forth. 
A further important generalization of the Markov model structures we have discussed so 
far is to explicitly model the notion of a hidden state variable. The general notion of 
hidden state for sequential and spatial models is prevalent in engineering and the 
sciences and recurs in a variety of functional model forms. Specific examples of such 
structures include hidden Markov models (HMMs) and Kalman filters. The HMM 
structure is easily explained by looking at its corresponding graphical model structure, 
shown in figure 6.11. From a generative viewpoint a first-order HMM operates as follows 
(picture the observations being generated by moving from left to right along the chain). 
The hidden state variable X is categorical (corresponding to m discrete states) and is 
first-order Markov. Thus, xt is generated by sampling a value from the conditional 
distribution function p(xt|xt-1) in the usual Markov chain fashion, where p(xt|xt-1) is an m × 
m matrix of conditional probabilities. Once the state at time t is generated (with value  xt), 
an observation yt is now generated with probability p(yt|xt). Here  yt could be univariate or 
multivariate, or real-valued or categorical, or a combination of both. Thus, in a HMM, the 
observations yt only depend on the state at time t, and the state sequence is a first-order 
Markov chain. The state sequence is unobserved or hidden, and the ys are directly 
observed: thus, there is uncertainty (given a model structure and a set of observed  y's) 
about which particular state sequence generated the data. 

 

Figure 6.11: A Graphical Model Structure Corresponding to a First-Order Hidden Markov 
Assumption.  

We can think of the HMM structure as a form of mixture model (m different density 
functions for the Y variable), where we have now added Markov dependence between 
""adjacent"" mixture components xt and  xt+1. For the record, the joint probability of an 
observed sequence and any particular hidden state sequence for a first-order HMM can 
be written as: 

(6.15)  

The factorization on the right -hand side is apparent from the graphical model structure in 
figure 6.11. When regarded as a function of the parameters of the distributions, this is 
the likelihood of the variables (Y1, . . . , YT, X1, . . . , XT). The likelihood of the observed  ys 
is useful for fitting such model structures to data (that is, learning the parameters of 
p(yt|xt) and p(xt|xt-1)). To calculate p(y1, . . . , yT) (the likelihood of the observed data) one 
has to sum the left-hand side terms over the mT possible state sequences, that appears 
at first glance to involve a sum over an exponential number of terms. Fortunately there is 
a convenient recursive way to perform this calculation in time proportional to O(m2T). 
Again, it is clear that we can generalize the first-order HMM structure in different 
directions. A kth order Markov model corresponds to having  xt depend on the previous k 
states. The dependence of the  ys can also be generalized, allowing for example yt to 
have a linear dependence on the k previous ys (as in an autoregressive model) as well 
as direct dependence on xt. This yields a natural generalization of the usual 
autoregressive model structure to a mixture of autoregressive models, which we can 
think of generatively as switching (in Markov fashion) among m different autoregressive 
models. Kalman filters are a closely related cousin of the HMM, where now the 
hiddenstates are real-valued (such as the unknown velocity or momentum of a vehicle, 
for example), but the independence structure of the model is essentially the same as we 
have described it for an HMM. 

Computer scientists will recognize in our generative description of a hidden Markov 
model that it is quite reminiscent of a finite state machine (FSM). In fact, as we have 
described it here, a first-order HMM is directly equivalent to a stochastic FSM with m 
states; that is, the choice of the next state is governed by p(xt+1|xt). This naturally 
suggests a generalization of model structures in terms of different grammars. Finite-state 
machines are simple forms of grammar known as regular grammars. The next level up 
(in the socalled Chomsky hierarchy of grammars) is the context-free grammar, which can 
be thought of as augmenting the finite-state machine with a stack, permitting the model 
structure to ""remember"" long-range dependencies such as closing parentheses at the 
ends of clauses, and so forth. As we ascend the grammar hierarchy, our model 
structures become more expressive, but also become much more difficult to fit to data. 
Thus, despite the fact that regular grammars (or HMMs) are relatively simple in structure, 
this form of model structure has dominated the application of Markov models to 
sequential data (over other more complex grammar structures), due to the difficulties of 
fitting such complex structures to real data. 
Finally, although we have only described simple data structures where the Y s exist in an 
ordered sequence, it is clear that for more general data dependencies (such as data on a 
two-dimensional grid) we can think of equivalent generalizations of the Markov model 
structures to model such dependence. For example, Markov random fields are 
essentially the multidimensional analogs of Markov chains (for example, in two 
dimensions we would have a grid structure rather than a chain for our graphical model). 
It turns out that such models are much more difficult to analyze and work with than chain 
models. For example, problems such as summing out the hidden variables in the 
likelihood (as we mentioned for equation 6.15) do not typically admit tractable solutions 
and must be approximated. Thus, spatial data can be more difficult to work with than 
sequential data, although conceptually the ideas of stationarity, Markovianity, linear 
models, and so forth, can all still be applied. One common approach with gridded data, 
which may or may not make sense depending on the application, is to ""shape"" the two-
dimensional grid data (say n × n grid points) into a single vector of length n2, perform 
PCA on these vectors, project each set of grid measurements onto a small set of PCA 
vectors, and model the data using standard multivariate models in this reduced 
dimensional space. This approach ignores much of the inherent spatial information in the 
original grid, but nonetheless can be quite practical in many situations. Similarly, for 
multivariate time series or sequences, where we have p different time series or 
sequences measured over the same time frame (corresponding for example to different 
biomedical monitors on the same patient), we can use PCA to reduce the p original time 
series to a much smaller number of ""component"" series for further analysis. 

6.7 Pattern Structures 
Throughout this book, we have characterized a model as describing the whole (or a large 
part of the) data set, and a pattern as characterizing some local aspect of the data. A 
pattern can be considered to be a predicate that returns true for those objects or parts 
of objects in the data  for which the pattern occurs, and false otherwise. To define a 
class of patterns we need to specify two things: the syntax of the patterns (the language 
specifying how they are defined) and their semantics (our interpretation of what they tell 
us about data to which they are applied). In this section we consider patterns for two 
different types of discrete-valued data: data in standard matrix form and data described 
as strings. 

6.7.1 Patterns in Data Matrices 
A generic approach for building patterns is to start from primitive patterns and combine 
them using logical connectives. (An alternative is to build a special class of patterns for a 
particular application.) Returning again to our data matrix notation, assume we have p 
variables X1, . . . , Xp. Let x = (x1, . . . , xp) be a p-dimensional vector of measurements of 
these variables. We denote the ith individual in the data set as x(i), where 1 = i = n. The 
entire data set D = {x(1), . . . , x(n). In turn, xk(i) is the value of the kth measurement on 
the ith individual.  

 
 

In general, a pattern for the variables X1, . . . , Xp identifies a subset of all possible 
observations over these variables. A general language for expressing patterns can be 
built by starting from primitive patterns. These are simply conditions on the values of the 
variables. For example, if c is a possible value of Xk, then Xk = c is a primitive pattern. If 
the values of Xk are ordered (for example, numbers on the real line), we can also include 
inequalities such as Xk = c as primitive conditions. If needed, the primitive patterns could 
also include multivariate conditions such as XkXj > 2 for numeric data or Xk = Xj for 
discrete data. 
Given a set of primitive patterns, we can form more complex patterns by using logical 
connectives such as AND (? ) and OR (V). For example, we can form a pattern 

(age = 40) ?  (income = 10) 

that describes a certain subset of the input records in a payroll database. Note, for 
example, that each branch of a classification tree (as described in chapter 5) forms a 
conjunctive pattern of this form. Another example is the pattern 
(chips = 1) ?  (beer = 1 V soft drink = 1) 

describing a subset of rows in a market basket database. 
A pattern class is a set of legal patterns. A pattern class C is defined by specifying the 
collection of primitive patterns and the legal ways of combining primitive patterns. For 
example, if the variables X1, . . . , Xp all range over {0,1}, we can define a class of 
patterns C consisting of all possible conjunctions of the form 

 

Patterns in this class that occur frequently in a data set D are called frequent sets (of 
variables), since each such pattern is uniquely determined by a sub-set of the variables: 
this pattern could be written just as 
. Conjunctive patterns such as frequent 
sets are relatively easy to discover from data, and we consider them in detail in chapter 
13. 
Given a pattern class and a dataset D, one of the important properties of a pattern is its 
frequency in the data set. The frequency fr(?) of a pattern ? can be defined as the 
relative number of observations in the dataset about which ? is true. In some cases, only 
patterns that occur reasonably often are of interest in data mining. However, having a 
frequency of a pattern close to 0 can also be quite informative in its own right. (Indeed, 
sometimes it is the rare but unusual pattern that is of particular interest.) Of course, the 
frequency of a pattern is not the only important property of the pattern. Properties such 
as semantic simplicity, understandability, and the novelty or surprise of the pattern are 
obviously also of interest. As an example, for any particular observation (x1, . . . , xp) in 
the data set we can write a conjunctive pattern (X1 = x1) ?  . . . ?  (Xp = xp) that matches 
exactly that observation. The disjunction of all of such conjunctive patterns forms a 
pattern that has frequency 1 for the data set. However, the pattern would be just a 
bloated way of writing out the entire data set and would be quite uninteresting. 
Given a class of patterns, a pattern discovery task is to find all patterns from that class 
that satisfy certain conditions with respect to the data sets. For example, we might be 
interested in finding all the frequent set patterns whose frequency is at least 0.1 and 
where the variable X7 occurs in the pattern. More generally, the definition of the pattern 
discovery task might include also conditions on the informativeness, novelty, and 
understandibility of the pattern. In defining the pattern class and the pattern discovery 
task the challenge is to find the right balance between expressivity of the patterns, their 
comprehensibility, and the computational complexity of solving the discovery task. 
Given a class of patterns C, we can easily define rules. A rule is simply an expression ? 
?  ? , where ? and ?  are patterns from a pattern class C. The semantics of a logical rule 
are that if the expression ? is true for an object, then ?  is also true. We can relax this 
definition to allow for uncertainty in the mapping from ? to ? , where ?  is true with some 
probability if ? is true. The accuracy of such a rule is defined as p(? |?), the conditional 
probability that ?  is true for an object, given that ? is true. As is described in chapter 4, 
we can easily estimate such probabilities from a data set using appropriate frequency 
counts; that is 

 

The support fr(? ?  ? ) of the rule ? ?  ?  can be defined either as fr(?) (the fraction of 
objects to which the rule applies) or fr(?? ? ) (the fraction of objects for which both the left 
and right-hand sides of the rule are true). 

For example, if our patterns are frequent sets, then a rule would have the form 

where each of the Aks and Bjs are binary variables. Written out in full, the rule would be 

{A1, . . . , Ak} ?  {B1, . . . , Bh}. 

(A1 = 1) ?  . . . ?  (Ak = 1) ?  (B1 = 1) ?  . . . ?  (Bh = 1).  

Such rules are called association rules, a widely used pattern structure in data mining 
(we will discuss in detail the algorithmic principles behind finding such rules in chapter 
13). 
Here we have described patterns that define subsets of the original data set. That is, 
each pattern was defined by a formula that referred only to the variables of a single 
observation. In certain cases, however, we need to use patterns defined by referring to 
several observations. For example, we might wish to identify all those points in a 
geographical database that form the vertices of an equilateral triangle. As a more formal 
example, consider a data set with discrete variables A1, . . . , Ap. A functional 
dependency is an expression of the form 

 

. That is, if 

where 1 = ij = p for i = 1, . . . , k+ 1. Note the syntactic similarity to the definition of 
association rules. However, the functional dependency defined by this expression is true 
in a data set if, for all pairs of observations x = (a1, . . . , ap) and y = (b1, . . . , bp) in the 
data set, we have that if x and y agree on all the variables 
for j = 1, . . . , k, then x and 
y agree also on 
. Functional 
dependencies have their roots in database design, and they are also of interest in query 
optimization. Knowing the functional dependencies that hold in a data set may be 
important for understanding the structure of the data. 

for all i = 1, . . . , k, then also 

The patterns or conditions written in these refer only to the values occurring in a single 
record in the database. Sometimes we are also interested in describing patterns that 
refer to other observations, such as those that arise in ""the employees whose income is 
the smallest in their department."" Such conditions can also be described using logical 
formalisms. For example, 

{xk | age = 40 ?  income = 10}. 

6.7.2 Patterns for Strings 
In the last section we discussed examples of patterns for data in the traditional matrix 
form. Other types of data require other types of patterns. To illustrate, we consider 
patterns for strings. Formally, a string over alphabet S is a sequence a1 . . . an of 
elements (also called letters) of S. The alphabet S can be the binary alphabet {0,1}, the 
set of all ASCII codes, the DNA alphabet {A,C,G,T}, or the set of all words consisting of 
ASCII characters. The set of all strings built from letters from S is denoted by S*.  

Note how string data differs from data in standard matrix form: for a string, there is no 
fixed set of variables. If and when we want to use the notions of probability to describe 
string data, we typically consider each of the letters of the string to be a random variable. 
The data can be one or several strings, and in most cases we are interested in finding 
out how many times a certain pattern occurs in the strings. (For example, we might want 
to compute the number of exact occurrences of a certain DNA sequence in a large 
collection of sequences.) The simplest string pattern is a substring: the pattern b1 . . . bk 
occurs in the string a1 . . . an at position i, if ai+j-1 = bj for all j = 1, . . . , k. For example, for 
DNA sequences we might be interested in finding occurrences of the substring pattern 
ATTATTAA, and for strings over the ASCII alphabet we might be interested in whether or 
not the pattern data mining occurs in a given string. 

For strings we might, however, be interested in a larger class of patterns. A regular 
expression E is an expression that defines a set L(E) of strings. The expression E is one 
of 

1.  a string s; then L(s) = {s} 
2.  a concatenation E1E2; in this case the set L(E1E2) consists of all strings 

that are a concatenation of a string in L(E1) and a string in L(E2) 

3.  a choice E1 | E2; then L(E1 | E2) = L(E1) ¨
4.  an iteration E*; then L(E*) consists of all strings that can be written as 

 L(E2) 

 
 

a concatenation of 0 or more strings from L(E) 

Thus, 10(00|11)*01 is a regular expression that describes all strings that start with 10 
and end with 01 and in between contain a sequence of pairs 00 and 11. 

Regular expressions are a form of patterns that are quite well suited to describing 
interesting classes of strings. While there are simple classes of strings that cannot be 
described by regular expressions (such as the set of strings consisting of all balanced 
sequences of parentheses), many quite complicated phenomena of strings can still be 
captured by using them. 
While regular expressions are fine for defining patterns over strings, they are not 
sufficiently expressive for expressing variations in the occurrence times of events. A 
simple class of patterns that can take the occurrence times into account is the episode. 
At a high level, an episode is a partially ordered collection of events occurring together. 
The events may be of different types, and may refer to different variables. For example, 
in biostatistical data an event might be a headache followed by a sense of disorientation 
occurring within a given time period. It is also useful for them to be insensitive to 
intervening events—as with, for example, alarms in a telecommunications network, logs 
of user interface actions, and so on. Episodes can also be incorporated into the type of 
rules discussed earlier. 

6.8 Further Reading 
There are many books on regression modeling. Draper and Smith (1981) and Cook and 
Weisberg (1999) each provide excellent overviews. McCullagh and Nelder (1989) is the 
definitive text on generalized linear models and Hastie and Tibshirani (1990) is equally 
definitive on generalized additive models. Fan and Gijbels (1996) provide a very 
extensive discussion of local polynomial methods and Wand and Jones (1995) contains 
a more theoretically oriented treatment of kernel estimation methods (both for regression 
and density estimation). Hand (1982) contains a detailed description of kernel methods 
in supervised classification problems. 
Fairly recent treatments of advances in classification modeling are provided by 
McLachlan (1992), Ripley (1996), Bishop (1996), Mitchell (1996), Hand (1997), and 
Cherkassky and Muller (1998). The McLachlan text and the Ripley text are aimed 
primarily at a statistical audience. A notable feature of Ripley's text is the illustration of 
basic concepts using a variety of different data sets. The Bishop text and the Cherkassky 
and Muller texts are more focused on neural networks and related developments, and 
each contains many ideas that have yet to make their way into the mainstream statistical 
literature. Duda and Hart (1973) remains a classic in the classification literature with a 
very clear and comprehensive treatment of essential ideas involved in building 
classification models. Reviews of Bishop (1996), Ripley (1996), Looney (1997), 
Nakhaeizadeh and Taylor (1997), and Mitchell (1997) are provided in Statistics and 
Computing, volume 8, number 1. 
The most comprehensive texts on mixture models are those of  Titterington, Makov, and 
Smith (1985) and McLachlan and Basford (1988), and McLachlan and Peel (2000). 
Other general discussions of the area include Redner and Walker (1984) and Everitt and 
Hand (1981). Silverman's 1992 text on density estimation contains a wealth of insight, 
while Scott's (1992) text on the same topic is notable for its discussion of ""average-
shifted histogram"" models that share some of the properties of both histograms and 
kernel estimators, and that might be of interest for models based on ""binning"" of massive 
data sets. 

 
 

The text by Jolliffe (1986) is completely devoted to principal component methods. Huber 
(1985) provides a detailed discussion of projection pursuit, and Hyvarinen (1999) 
contains a thorough survey of independent component analysis and related techniques 
for dimension reduction. 
Hidden Markov models are discussed in Elliott et al. (1995) and MacDonald and Zucchini 
(1997). A very readable introduction to the vast literature on autoregressive and related 
time-series models is Chatfield (1996). Harvey (1989), Box, Jenkins, and Reinsel (1994), 
and Hamilton (1994) provide more in-depth mathematical treatment of time-series 
modeling and its application to forecasting. Switching models are covered in depth in 
Kim and Nelson (1999). Cressie (1991) is a well-known text on spatial data analysis and 
the text by Dryden and Mardia (1998) provides a broad discussion on modeling of 2-
dimensional shapes. Grenander's 1996 book on generative models for sequences and 
spatial data is a fascinating read, linking many ideas from statistics and computer 
science. 
Ramsey and Silverman (1996) discuss a general approach to modeling of data that are 
functions of time and/or space, e.g., modeling of time-series from different weather 
stations. Books on modeling of repeated measures analysis include  Crowder and Hand 
(1990), Hand and Crowder (1996), Diggle, Liang, and Zeger (1994), and Lindsey (1999). 
The idea of using logical formulas to describe patterns is used widely in database 
systems. See for example  Ramakrishnan and Gehrke (1999) or  Ullman and Widom 
(1997) for introductory texts. Frequent sets were introduced by Agrawal, Imielinski, and 
Swami (1993). Regular expressions are treated in many textbooks on theory of 
computation such as Lewis and Papadimtriou (1998). Text patterns are discussed also in 
Gusfield (1997). Episodes are considered in Mannila, Toivonen, and Verkamo (1997). 

Chapter 7: Score Functions for Data Mining 
Algorithms 
7.1 Introduction 
In chapter 6 we focused on different representations and structures that are available to 
us for fitting models and patterns to data. Now we are ready to consider how we will 
match these structures to data. Recall that a model or pattern structure is the functional 
form, with the parameters left ""floating."" For example, Y = aX + b might be one such 
model structure, with a and b the parameters. Given a model or pattern structure, we 
must score different settings of the parameter values with respect to data, so that we can 
choose a good set (or even ""the best""). In the simple linear regression example in 
chapter 1, we saw how a least squares principle could be used to choose between 
different parameter values. This involved finding the values of the parameters a and b 
that minimized the sum of squared differences between the predicted values of y (from 
the model) and the observed (data) values of  y. In this case, the score function is thus 
the sum of squared errors between model predictions and actual target measurements. 
Our goal in this chapter is to broaden the reader's horizon in terms of the score functions 
that can be used for data mining. We will see that the venerable squared error score 
function is but one of many, and indeed can be viewed as a special case arising from 
more general principles. 

It is important to bear in mind why we are interested in score functions in the first place. 
Ultimately the purpose of a score function should be to rank models as a function of how 
useful the models are to the data miner. Unfortunately in practice it can be quite difficult 
to measure ""utility"" in terms of direct practical usefulness to the person building the 
model. For example, in predicting stock market returns one might use squared error 
between predictions and actual data as a score function to train one's model. However, if 
the model is then used in a real financial environment, a host of other factors such as 
trading costs, risks, diversity, and so forth, come into play to determine the true utility of 
the model. This illustrates that we often settle for simpler ""generic"" score functions (such 
as squared error) that have many desirable well-understood properties and are relatively 
easy to work with. Of course, one should not take this to an extreme: the score function 
being used should reflect the overall goals of the data mining task as far as is possible. 

 
 

One should try to avoid the situation, unfortunately all too common in practice, of using a 
convenient score function (perhaps because it is the default score function in the 
software package being used) that is completely inappropriate for the task. 

Different score functions have different properties, and are useful in different situations. 
One of the goals of this chapter is to make the reader aware of these differences and of 
the implications of using one score function rather than another. Just as there are a few 
fundamental principles underlying model and pattern structures, so there are some basic 
principles underlying the different score functions. These are outlined in this chapter. 
It is useful to make three distinctions at the outset. The first is between score functions 
for models, and score functions for patterns. The second is between score functions for 
predictive structures, and score functions for descriptive structures. And the third is 
between score functions for models of fixed complexity, and score functions for models 
of different complexity. These distinctions will be illustrated below. 
A minor comment on the terminology used below is in order. In some places we will refer 
to score functions (such as error) that we clearly wish to minimize, whereas in other 
places we will refer to score functions (such as log-likelihood) that we clearly wish to 
maximize. The general concept is the same in either case, since the negative (or 
""inverse"") of an ""error-based"" score function can always be maximized, and vice versa. 

7.2 Scoring Patterns 

Since the whole idea of searching for local patterns in data is relatively recent, there is a 
far smaller toolbox of techniques available for scoring patterns compared to the plethora 
of techniques for scoring models. Indeed, there is really no general consensus on how 
patterns should be scored. This is largely a result of the fact that the usefulness of a 
pattern lies in the eye of the beholder. One person's noisy outlier may be another 
person's Nobel Prize. Fundamentally, patterns might be evaluated in terms of how 
interesting or unexpected they are to the data analyst. But we could only hope to quantify 
this if some-how we had a precise model of what the user actually knows already. We 
are all familiar with the experience that the first time we learn something surprising is a 
lot more informative than the fifth or tenth time we hear the same information again. 
Thus, the degree to which a pattern is interesting to a person must be a function of their 
prior knowledge. 

In practice, however, we cannot hope (except in simple situations) to be able to model a 
person's prior knowledge. Faced with a data set, a scientist or a marketing expert would 
have difficulty in precisely formulating what it is that they already know about the 
problem. Even subjective Bayesians can have problems choosing priors for complex 
multiparameter models—and evade them by choosing standard forms for the priors, that 
are only very simplistic representations of prior knowledge. We have found that, once 
certain patterns begin to emerge from the data (via visualization, descriptive statistics, or 
rules found by a data mining algorithm), database owners often say ""Ah yes, but of 
course we knew that already,"" changing their minds about what they claim to have 
expected once they have seen the data. 
Having said all of this, the fact remains that most techniques currently used in data 
mining for scoring patterns essentially assume that they are measuring degree of 
informativeness relative to a completely uninformed prior model; that is, it is effectively 
assumed that the data analyst has no prior information at all about the problem, beyond 
perhaps a few simple marginal and descriptive statistics. The hope is that this will 
eliminate the very obvious patterns (by focusing attention on patterns that are different 
from the known simple ones) and that the user can effectively ""post-prune"" the remaining 
patterns found by the algorithm to retain the truly interesting ones. The danger, of 
course, is that for some data sets and some forms of pattern searches, almost all 
patterns that are found by the data mining algorithm will essentially be uninteresting to 
the data analyst. 
To illustrate these ideas we choose one simple (but widely used) pattern structure, the 
probabilistic rule (as discussed in chapter 5 under association rules) and explored in 
detail later in chapter 13. This has the form 

IF a THEN b with probability p  

where a and b are Boolean propositions (events) defined on a subset of the variables of 
interest and p = p(b|a). How can we measure how interesting or informative this rule is to 
an uninformed observer? One simple approach is to assume that the observer already 
knows the marginal (unconditional) probability for the event b, p(b). 
For example, suppose that we are studying a population of data miners. Let b represent 
the event that a randomly chosen person in this population is a data mining researcher, 
and let a be the event that such a person has read this book. Suppose we find that p(b) 
= 0.25 and that p(b|a) = 0.75; that is, 25% of this population are researchers and 75% of 
people who have read this book are researchers. This is interesting because it tells us 
that the proportion of people who undertake research is higher among those who have 
read the book than it is in this population of data miners in general (and hence, by 
implication, it is higher than among the people who have not read the book). Note, as an 
aside, that there are no causal implications to this. It could be that the book inspired a 
reader to take up research, or that a person involved in research hoped the book would 
help them. 
The types of simple score functions that are used to capture the informativeness of such 
a rule rely in general on how ""far"" the posterior probability p(b|a) is (after learning that 
event a is true), from the prior probability p(b). Thus, for example, one could simply 
measure absolute distance between the probabilities |p(b|a) - p(b)|, or perhaps measure 
distance on log-odds scale, log 
where  represents the event that a person is not a 
researcher. 
When we compare different patterns, such as p(b|a) and p(b|c), it is also useful to take 
into account the coverage of a pattern—that is, the proportion of the data to which it 
applies. To continue our example above, let c be the condition that the a randomly 
chosen data miner is one of the three authors of this book. A second pattern might be ""if 
c then b"" (""if a data miner is an author of this book then they are a researcher""), with 
p(b|c) = 1 since the three authors are all researchers. However, the condition c only 
applies to three data miners, which is a very small fraction of the universe of data miners. 
On the other hand, (we hope that) the coverage of event a will be much larger; that is, 
p(a) is significantly greater than p(c). To illustrate, suppose that p(a) = 0.2 and p(c) = 
0.003. Then, although the second pattern is very accurate (p(b|c) = 1) it is not particularly 
useful since it only applies to a very small fraction of the population (0.3%), whereas the 
first pattern is not as accurate (p(b|a) = 0.75) but it has much broader applicability (to 
20% of the population). It is easy to develop a  variety of measures that augment the 
score function to take coverage into account. For example, we could multiply the 
previously defined scores by the probability of the conditioning event; p(a)|p(b|a) - p(b)| = 
|p(b, a) - p(b)p(a)| that can be interpreted as measuring the difference in probability 
between an independence assumption and the observed joint probability for the two 
events a and b. Alternatively, the approach used in association rule mining (chapters 5 
and 13) defines a threshold pt, and only seeks patterns with coverage greater than pt. 

There are numerous other score functions for patterns that have been proposed in the 
data mining literature. None have gained widespread acceptance or general use, largely 
because judging the novelty and utility of a pattern is often quite subjective and 
application-specific. Thus, human interpretation by a domain expert remains the most 
practical way to evaluate patterns at present (e.g., having a human search through and 
interpret a set of candidate patterns produced by a data mining algorithm). 

7.3 Predictive versus Descriptive Score Functions 

We now turn to score functions for models, where there is a much greater selection of 
useful methods available compared to patterns. 

7.3.1 Score Functions for Predictive Models 
A convenient place to begin is by considering the distinction between prediction and 
description. Score functions for predictive problems are relatively straightforward. In a 
prediction task, our training data comes with a ""target"" value Y, this being a quantitative 

 
 

variable for regression or a categorical variable for classification, and our data set D = 
{(x(1), y(1)), ... ,(x(n), y(n))} consists of pairs of input vectors and target values. Let ƒ(x(i), 
?) be the prediction generated by the model for individual i, 1 = i = n, using parameter 
values ?. Let y(i) be the actual observed value (or ""target"") for the ith individual in the 
training data set. 
Clearly our score function should be a function of the difference between the predictions 
and the targets y(i). Commonly used score functions include the sum of squared 

errors, 

(7.1)  

for quantitative Y, and the misclassification rate (or error rate or ""zero-one"" score 
function) for categorical Y, namely, 

(7.2)  

where I(a, b) = 1 if a is not equal to b and 0 otherwise. These are the two most widely-
used score functions for regression and classification respectively. They are simple to 
understand and (in the case of squared error at least) often lead to straightforward 
optimization problems. 

However, note that we have made some strong assumptions in how these score 
functions are defined above. For example, by summing over the individual errors we are 
assuming that errors for all individuals may be treated equally. This is a very common 
assumption and generally useful. However, if (for example) we have a data set in which 
the measurements were taken at different times, we might want to assign higher weight 
in the score function to predictions on more recent items. Similarly, we might have 
different subsets of items in the data set where the target values are more reliable in 
some subsets than others (for example, some quantification of measurement error in a 
subset). Here we might wish to assign lower weight in the score function to predictions 
on the items with less reliable measurements. 
Furthermore, both are functions only of the difference between the predictions and 
targets—in particular, they do not depend on the values of the target y(i). This is 
something we might want to take account of. For example, if Y were a categorical 
variable indicating whether or not a person had cancer, we might wish to give more 
weight to the error of not detecting a true cancer and less weight to errors that 
correspond to false alarms. For real-valued Y, squared-error may not be appropriate—
perhaps the quality of the model is more appropriately reflected in absolute error 
(squared-error gives greater weight to extreme differences between the observed and 
predicted Y values than does absolute error). And, as a third example, in an investment 
scenario, we might want be more tolerant (from a risk-taking standpoint) of predictions of 
Y that underestimate the true value than we are to predictions that overestimate, 
suggesting that an asymmetric function might be more appropriate. 
The basic score functions above are rather simple. Thus, we may need in practice to 
adjust them to reflect the aims of our data mining project more accurately. Sometimes 
this is not easy (defining the ""real aims"" may be difficult, especially in data mining 
contexts, where problems are often open ended). In other cases, even if one cannot 
state the aims precisely, one might be able to improve on the basic score function. For 
example, for the cancer problem, instead of using the zero-one loss function it might be 
more appropriate to define a score function based on a  cost matrix. Thus, let  be the 
predicted class, k the true class, and define a matrix of ""costs"" 
, k = K that 
reflects the severity of classifying a patient with true class k into class  .  

, 

In selecting a score function for a particular predictive data mining task there is always a 
trade-off between choosing a simple score function (such as sum of squared errors) and 
a much more complex one. The simpler score function will usually be more convenient to 
work with computationally and will be easier to define. However, more complex score 
functions (such as those mentioned above) may reflect better the actual reality of the 
prediction problem. An important point is that many data mining algorithms (such as tree 
models, linear regression models, and so forth) can in principle handle fairly general 
score functions—e.g., an algorithm based on cross-validation can use any well-defined 

score function. Of course, even though this is true in theory, in practice not all software 
implementations allow the data miner to define their own application-specific score 
function. 

be the estimated probability of observing a data point at x, 

7.3.2 Score Functions for Descriptive Models 
For descriptive models, in which there is no ""target"" variable to be predicted, it is less 
clear how to define a score function. A fundamental approach is through the likelihood 
function, which we introduced in chapter 4, but which we here describe from a slightly 
different perspective. Let 
as defined by our model  with parameters ?, where X is categorical (the extension to 
continuous variables is straightforward, and  would then be a probability density 
function). If the model is a good one, then it might be expected to place a high probability 
at those values of X where a data point is observed. Thus 
measure of quality of the model—a score function—at the point x. This is the basic idea 
of maximum likelihood (chapter 4) once again: better models assign higher probability to 
observed data. (This is fine actually as long as we can assume that all the models we 
are considering have equal functional complexity, so that the comparison is ""fair""—t he 
case in which we are comparing models of different complexities will be discussed later 
in this chapter.) 

itself can be taken as a 

If we assume that the data points have arisen independently, we can define an overall 
score function for the model by combining these score functions for the individual data 
points simply by multiplying them together: 

(7.3)  

This is again the likelihood function of chapter 4, for a set of data points, that we 
maximize to find an estimate of ?. As we noted there, it is typically more convenient to 
work with the log-likelihood. Now the contribution of an individual data point to the overall 
score function is log 

, and the overall function is the sum of these: 

(7.4)  

If we work with the negative of the log 
needs to be minimized. We define 

(7.5)  

, as is often done, then this function 

is our error term (it gets larger as  gets 

Note again the intuitive interpretation: - 
smaller), and we are summing this over all of our data points. The largest possible value 
for  is 1 (for categorical data) and, hence, SL(?) is lower bounded by 0. Thus, we can 
think of SL(?) as a type of entropy term that measures how well the parameters ? can 
compress (or predict) the training data. 
A particularly useful feature of the likelihood (or, equivalently, the negative log-likelihood) 
is that it is very general. It can be defined for any problem in which the model or pattern 
being examined is expressed in terms of probability functions. For example, one might 
assume that Y in a predictive model is a perfect linear function of some predictor variable 
X, as well as extra randomly distributed errors, as discussed in the last section. If one 
can postulate a parametric form for the probability distribution of these errors, then one 
can compute the likelihood of the data for any proposed parameters in the model. In fact, 
as we saw in chapter 4, if the error terms are supposed to be Normally distributed with 
mean 0 about a deterministic function of X then the likelihood score function is equivalent 
to the sum of squared errors score function. 
Although (negative log-)likelihood is a powerful and useful score function, it too has its 
limitations. In particular, if a parameterization assigns any data point a probability near 0, 
the log-likelihood will approach -8. Thus, the overall error can be dominated by extreme 
points. If the true probability of that same point is also very small, then the model is being 
penalized for a prediction in the tails of the density function (very unlikely events), that 
may have little relation to the practical utility of the model. Conversely, there may be 
problems (such as predicting the occurrence of rare events) in which it is precisely in the 
tails of the density that we are most interested in accurate prediction. Thus, while 

likelihood is based on strong theoretical foundations and is generally useful for scoring 
probabilistic models, it is important to realize that it may not necessarily reflect the true 
utility of a model for a particular task. Other score functions for determining the quality of 
probabilistic predictions are also possible, each with its own particular characteristics. 
For example we can define the integrated squared error between our estimate 
the true probability 
depending on ?, we get a score function of the form 
term can be empirically approximated to provide an estimate of the true integrated 
squared error as a function of ?. 
For nonprobabilistic descriptive models, such as partition-based clustering, it is quite 
easy to come up with all sorts of score functions based on how well separated the 
clusters are, how compact they are, and so forth. For example, for simple prototype-
based clustering (the k-means model discussed in chapter 9), a simple and widely used 
score function is the sum of square errors within each cluster 

. By completing the square, and ignoring terms not 

and 

, where each 

(7.6)  

 
 

where ? is the parameter vector for the cluster model, ? = {µ1, ... ,µK}, and the µks are the 
cluster centers. However, it is quite difficult to formulate any score function for cluster 
models that reflect how close the clusters are to ""truth"" (if this is regarded as 
meaningful). The ultimate judgment on the utility of a given clustering depends on how 
useful the clustering is in the context of the particular application. Does it provide new 
insight into the data? Does it permit a meaningful categorization of the data? And so on. 
These are questions that typically can only be answered in the context of a particular 
problem and cannot be captured by a single score metric. To put it another way, once 
again the score functions for tasks such as clustering are not necessarily very closely 
related to the true utility function for the problem. We will return to the issue of score 
functions for clustering tasks in chapter 9. 

To summarize, there are simple ""generic"" score functions for tasks such as classification, 
regression, and density estimation, that are all useful in their own right. However, they do 
have limitations, and it is perhaps best to regard them as starting points from which to 
generalize to more application-specific score functions. 

7.4 Scoring Models with Different Complexities 

In the preceding sections we described score functions as minimizing some measure of 
discrepancy between the observed data and the proposed model. One might expect 
models that are close to the data (in the sense embodied in the score function) to be 
""good"" models. However, we need to be clear about why we are building the model. 

7.4.1 General Concepts in Comparing Models 

We can distinguish between two types of situations (as we have in earlier chapters). In 
one type of situation we are merely trying to build a summarizing descriptive model of a 
data set that captures its main features. Thus, for example, we might want to summarize 
the main chemical compounds among the members of a particular family of compounds, 
where our database contains records for all possible members of this family. In this case, 
accuracy of the model is paramount—though it will be mediated by considerations of 
comprehensibility. The best accuracy is given by a model that exactly reproduces the 
data, or describes the data in some equivalent form, but the whole point of the modeling 
exercise in this case is to reduce the complexity of the data to something that is more 
comprehensible. In situations like this, simple goodness of fit of the model to the data will 
be one part of an overall score measure, with comprehensibility being another part (and 
this part will be subjective). An example of a general technique in this context is based 
on data compression and information-theoretic arguments, where our score function is 
generally decomposed as 

§   

SI(?, M) 

  

= 

  

number 
of bits to 
describe 
the data 
given the 
model 
+ 
number 
of bits to 
describe 
the 
model 
(and 
paramet
ers) 

where the first term measures the goodness of fit to the data and the second measures 
the complexity of the model M and its parameters ?. In fact, for the first term (""number of 
bits to describe the data given the model"") we can use SL = - log p(D|?, M) (negative log-
likelihood, log base 2). For the second term (""number of bits to describe the model"") we 
can use - log  p(?, M) (this is in effect just taking negative logs of the general Bayesian 
score function discussed in chapter 4). Intuitively, we can think of - log p(?, M) (the 
second term) as the communication ""cost"" in bits to transmit the model structure and its 
parameters from some hypothetical transmitter to a hypothetical receiver, and SL (the 
first term) as the cost of transmitting the portion of the data (the errors) that the model 
and its parameters do not account for. These two parts will tend to work in opposite 
directions—a good fit to the data will be achieved by a complicated model, while 
comprehensibility will be achieved by a simple model. The overall score function trades 
off what is meant by an acceptable model. 

In the other general situation our aim is really to generalize from the available data to 
new data that could arise. For example, we might want to infer how new customers are 
likely to behave or infer the likely properties of new sky objects not yet observed. Once 
again, while goodness of fit to the observed data is clearly a part of what we will mean by 
a good model, it is not the whole story. In particular, since the data do not represent the 
whole population (there would be no need for generalization if they did) there will be 
aspects of the observed data (""noise"") that are not characteristic of the entire population 
and vice versa. A model that provided a very good fit to the observed data would also fit 
these aspects—and, hence, would not provide the best possible predictions. Once again, 
we need to modify the simple goodness of fit measure in order to define an overall score 
function. In particular, we need to modify it by a component that prevents the model from 
becoming too complex, and fitting all the idiosyncrasies of the observed data. 

In both situations, an ideal score function strikes some sort of compromise between how 
well the model fits the data and the simplicity of the model, although the theoretical basis 
for the compromise is different. This difference is likely to mean that different score 
functions are appropriate for the different situations. Since the compromise when the aim 
is simply to summarize the main features of a data set necessarily involves a subjective 
component (""what does the data miner regard as an acceptably simple model?""), we will 
concentrate here on the other situation: our aim is to determine, from the data we have 
available, which model will perform best on data we have not yet seen. 

7.4.2 Bias-Variance Again 
Before examining score functions that we might hope will provide a good fit to data as 
yet unseen, it will be useful to look in more detail at the need to avoid modeling the 
available data too closely. We discussed bias and variance in the context of estimates of 
parameters ? in chapter 4 and we discuss it again here in the more general context of 
score functions. 
As we have mentioned in earlier chapters, it is extremely unlikely that one's chosen 
model structure will be ""correct."" There are too many features of the real world for us to 

be able to model them exactly (and there are also deep questions about just what 
""correct"" means). This implies that the chosen model form will provide only an 
approximation to the ""truth."" Let us take a predictive model to illustrate. Then, at any 
given value of  X (which we take to be univariate for simplicity—exactly the same 
argument holds for multivariate X), the model is likely to provide predicted values of Y 
that are not exactly right. More formally, suppose we draw many different data sets, fit a 
model of the specified structure (for example, a piecewise local model with given number 
of components, each of given complexity; a polynomial function of X of given degree; 
and so on) to each of them, and determine the expected value of the predicted Y at any 
X. Then this expected predicted value is unlikely to coincide exactly with the true value. 
That is, the model is likely to provide a biased prediction of the true Y at any given X. 
(Recall that bias of an estimate was defined in chapter 4 as the difference between the 
expected value of the estimate and the true value.) Thus, perfect prediction is too much 
to hope for! 

However, we can make the difference between the expected value of the predictions and 
the unknown true value smaller (indeed, we can make it as small as we like for some 
classes of models and some situations) by increasing the complexity of the model 
structure. In the examples above, this means increasing the number of components in 
the piecewise linear model, or increasing the degree of the polynomial. 

At first glance, this looks great—we can obtain a model that is as accurate as we like, in 
terms of bias, simply by taking a complicated enough model structure. Unfortunately, 
there is no such thing as a free lunch, and the increased accuracy in terms of bias is only 
gained at a loss in other terms. 
By virtue of the very flexibility of the model structure, its predictions at any fixed X could 
vary dramatically between different data sets. That is, although the average of the 
predictions at any given X will be close to the true Y (this is what small bias means), 
there may be substantial variation between the predictions arising from the different data 
sets. Since, in practice, we will only ever observe one of these predictions (we really 
have only one data set to use to estimate the model's parameters) the fact that ""on 
average"" things are good will provide little comfort. For all we know we have picked a 
data set that yields predictions far from the average. There is no way of telling. 
There is another way of looking at this. Our very flexible model (with, for example, a 
large number of piecewise components or a high degree) has led to one that closely 
follows the data. Since, at any given X, the observed value of Y will be randomly 
distributed about its mean, our flexible model is also modeling this random component of 
the observed Y value. That is, the flexible model is overfitting the data. 

Finally (though, yet again, it is really just another way of looking at the same thing), 
increasing the complexity of the model structure means increasing the number of 
parameters to be estimated. Generally, if more parameters are being estimated, then the 
accuracy of each estimate will decrease (its variance, from data set to data set, will 
increase). 
The complementarity of bias and variance in the above, is termed the bias-variance 
trade-off. We want to choose a model in which neither is too large—but reducing either 
one tends to increase the other. They can be combined to yield an overall measure of 
discrepancy between the data and the model to yield the mean squared error (MSE). 
Consider the standard regression setting we have discussed before, where we are 
assuming that y is a deterministic function of x (where we now generalize to the vector 
case) with additive noise, that is, y = ƒ(x; ?) + e, where e is (for example) Normal with 
zeromean. Thus, µy = E[y|x] represents the true (and unknown) expected value for any 
data point x (where here the expectation E is with respect to the noise e), and y = ƒ(x; ?) 
is the estimate provided by our model and fitted parameters ?. The MSE at x is then 
defined as: 

(7.7)  

or MSE = Variance + Bias2. (The expectation E here is taken with respect to p(D), the 
probability distribution over all possible data sets for some fixed size n). This equation 
bears close inspection. We are treating our prediction y here as a random quantity, 

where the randomness arises from the random sampling that generated the training data 
D. Different data sets D would lead to different models and parameters, and different 
predictions y. The expectation, E, is over different data sets of the same size n, each 
randomly chosen from the population in question. The variance term E [y - E (y)]2 tell us 
how much our estimate y will vary across different potential data sets of size n. In other 
words, it measures the sensitivity of y to the particular data set being used to train our 
model. As an extreme example, if we always picked a constant y1 as our prediction, 
without regard to the data at all, then this variance would be zero. At the other extreme, if 
we have an extremely complex model with many parameters, our predictions y may vary 
greatly depending from one individual training data set to the next. 
The bias term E [E(y) - µy] reflects the systematic error in our prediction—that is how far 
away our average prediction is, E (y), from truth µy. If we use a constant y1 as our 
prediction, ignoring the data, we may have large bias (that is, this difference may be 
large). If we use a more complex model, our average prediction may get closer to the 
truth, but our variance may be quite large. The bias-variance quantifies the tension 
between simpler models (low variance, but potentially high bias) and more complex ones 
(potentially low bias but typically high variance). 
In practice, of course, we are interested in the average MSE over the entire domain of 
the function we are estimating, so we might define the expected MSE (with respect to the 
input distribution p(x)) as ? MSE(x)p(x)dx, that again has the same additive 
decomposition (since expectation is linear). 
Note that while we can in principle measure the variance of our predictions y (for 
example, by some form of resampling such as the bootstrap method), the bias will 
always be unknown since it involves µy that is itself unknown (this is after all what we are 
trying to learn). Thus, the bias-variance decomposition is primarily of theoretical interest 
since we cannot measure the bias component explicitly, and in turn it does not provide a 
practical score function combining these two aspects of estimation error. Nonetheless, 
the practical implications in general are clear: we need to choose a model that is not too 
inflexible (because its predictions will then have substantial bias) but not too flexible 
(since then its predictions will have substantial variance). That is, we need a score 
function that can handle models of different complexities and take into account this 
compromise, and one that can be implemented in practice. This is the focus of the  next 
section. 
We should note that in certain data mining applications, the issue of variance may not be 
too important, particularly when the models are relatively simple compared to the amount 
of data being used to fit them. This is because variance is a function of sample size (as 
we discussed in chapter 4). Increasing the sample size decreases the variance of an 
estimator. Unfortunately, no general statements can be made about when variance and 
overfitting will be important issues. It depends on both the sample size of the training 
data D and the complexity of the model being fit. 

7.4.3 Score Functions that Penalize Complexity 

How, then, can we choose a suitable compromise between flexibility (so that a 
reasonable fit to the available data is obtained) and overfitting (in which the model fits 
chance components in the data)? One way is to choose a score function that 
encapsulates the compromise. That is, we choose an overall score function that is 
explicitly composed of two components: a component that measures the goodness of fit 
of the model to the data, and an extra component that puts a premium on simplicity. This 
yields an overall score function of the form 

score(model) = error(model) + penalty-function(model), 

where we want want to minimize this score. We have discussed several different ways to 
define the error component of the score in the preceding sections. What might the 
additional penalty component look like? 
In general (though there are subtleties that mean that this is something of a 
simplification), the complexity of a model M will be related to the number of parameters, 
d, under consideration. We will adopt the following notation in this context. Consider that 
there are K different model structures, M1, ... ,MK, from which we wish to choose one 

(ideally the one that predicts best on future data). Model Mk has dk parameters. We will 
assume that for each model structure  Mk, 1 = k = K, the best fitting parameters 
for that 
model (those that maximize goodness-of-fit to the data) have already been chosen; that 
is, we have already determined point estimates of these parameters for each of these K 
model structures and now we wish to choose among these  fitted models. 
The widely used Akaike information criterion or AIC is defined as 

(7.8)  

where SL is the negative log-likelihood as defined in  equation 7.5 and the penalty term is 
2dk. This can be derived formally using asymptotic arguments. 
An alternative, based on Bayesian arguments, also takes into account the sample size, 
n. This Bayesian Information Criterion or BIC is defined as 

(7.9)  

where SL is again the negative log-likelihood of 7.5. Note the effect of the additive 
penalty term dk log n. For fixed n, the penalty term grows linearly in number of 
parameters dk, which is quite intuitive. For a fixed number of parameters dk, the penalty 
term increases in proportion to log n. Note that this logarithmic growth in n is offset by 
the potentially linear growth in SL as a function of n (since it is a sum of n terms). Thus, 
asymptotically as n gets very large, for relatively small values of dk, the error term SL 
(linear in n) will dominate the penalty term (logarithmic in n). Intuitively, for very large 
numbers of data points n, we can ""trust"" the error on the training data and the penalty 
function term is less relevant. Conversely, for small numbers of data points n, the penalty 
function term dk log n will play a more influential role in model selection. 
There are many other penalized score functions with similar additive forms to those 
above (namely an error-based term plus a penalty term) include the adjusted  R2 and  Cp 
scores for regression, the minimum description length (MDL) method (which is closely 
related to the MAP score of chapter 4), and Vapnik's structural risk minimization 
approach (SRM). 

Several of these penalty functions can be derived from fairly fundamental theoretical 
arguments. However, in practice these types of penalty functions are often used under 
far broader conditions than the assumptions used in the derivation of the theory justify. 
Nonetheless, since they are easy to compute they are often quite convenient in practice 
in terms of giving at least a general idea of what the appropriate complexity for a model 
is, given a particular data set and data mining task. 
A different approach is provided by the Bayesian framework of chapter 4. We can try to 
compute the posterior probability of each model given the data directly, and select the 
one with the highest posterior probability; that is, 

(7.10)  

where the integral represents calculating the expectation of the likelihood of the data 
over parameter space (also known as marginal likelihood), relative to a prior in 
parameter space p(?k|Mk), and the term p(Mk) is a prior probability for each model. This 
is clearly quite different from the ""point estimate"" methods—the Bayesian philosophy is 
to fully acknowledge uncertainty and, thus, average over our parameters (since we are 
unsure of their exact values) rather than ""picking"" point estimates such as 
. Note that 
this Bayesian approach implicitly penalizes complexity, since higher dimensional 
parameter spaces (more complex models) will mean that the probability mass in p(?k|Mk) 
is spread more thinly than in simpler models. 
Of course, in practice explicit integration is often intractable for many parameter spaces 
and models of interest and Monte Carlo sampling techniques are used. Furthermore, for 
large data sets, the p(D|?k) function may in fact be quite ""peaked"" about a single value 
(recall the maximum likelihood estimation examples in chapter 4), in which case we can 
reasonably approximate the Bayesian expression above by the value of the peak plus 
some estimate of the surrounding volume (for example, a Taylor series type of 

expansion around the posterior mode of p(D|?)p(?))—this type of argument can be 
shown to lead to approximations such as BIC above). 

7.4.4 Score Functions using External Validation 
A different strategy for choosing models is sometimes used, not based on adding a 
penalty term, but instead based on external validation of the model. The basic idea is to 
(randomly) split the data into two mutually exclusive parts, a design part Dd, and a 
validation part Dv. The design part is used to construct the models and estimate the 
parameters. Then the score function is recalculated using the validation part. These 
validation scores are used to select models (or patterns). An important point here is that 
our estimate of the score function for a particular model, say S(Mk), is itself a random 
variable, where the randomness comes from both the data set being used to train 
(design) the model and the data set being used to validate it. For example, if our score is 
some error function between targets and model predictions (such as sum of squared 
errors), then ideally we would like to have an unbiased estimate of the value of this score 
function on future data, for each model under consideration. In the validation context, 
since the two data sets are independently and randomly selected, for a given model the 
validation score provides an unbiased estimate of the score value of that model for new 
(""out-of-sample"") data points. That is, the bias in estimates, that inevitably arises with the 
design component, is absent from the independent validation estimate. It follows from 
this (and the linearity of expectation) that the difference between the scores of two 
models evaluated on a validation set will have an expected value in the direction favoring 
the better model. Thus, the difference in validation scores can be used to choose 
between models. Note that we have previously discussed unbiased estimates of 
parameters ? (chapter 4), unbiased estimates of what we are trying to predict µy (earlier 
in this chapter), and now unbiased estimates of our score function S. The same 
principles of bias and variance underly all three contexts, and indeed all three contexts 
are closely interlinked (accuracy in parameter estimates will affect accuracy of our 
predictions, for example)—it is important, however, to understand the distinction between 
them. 
This general idea of validation has been extended to the notion of cross-validation. The 
splitting into two independent sets is randomly repeated many times, each time 
estimating a new model (of the given form) from the design part of the data and obtaining 
an unbiased estimate the out-of-sample performance of each model from the validation 
component. These unbiased estimates are then averaged to yield an overall estimate. 
We described the use of cross-validation to choose between CART recursive partitioning 
models in chapter 5. Cross-validation is popular in practice, largely because it is simple 
and reasonably robust (in the sense that it relies on relatively few assumptions). 
However, if the partitioning is repeated m times it does come at a cost of (on the order 
of) m times the complexity of a method based on just using a single validation set. 
(There are exceptions in special cases. For example, there is an algorithm for the leave-
one-out special case of cross-validation applied to linear discriminant analysis that has 
the same order of computational complexity as the basic model construction algorithm.) 
For small data sets, the process of selecting validation subsets D? can lead to significant 
variation across data sets, and thus, the variance of the cross-validation score also 
needs to be monitored in practice to check whether or not the variation may be 
unreasonably high. Finally, there is a subtlety in cross-validation scoring in that we are 
averaging over models that have potentially different parameters but the same 
complexity. It is important that we are actually averaging over essentially the same basic 
model each time. If, for example, the fitting procedure we are using can get trapped at 
different local maxima in parameter space, on different subsets of training data, it is not 
clear that it is meaningful to average over the validation scores for these models. 
It is true, as stated above, that the estimate of performance obtained from such a 
process for a given model is unbiased. This is why such methods are very widely used 
and have been extensively developed for performance assessment (see Further 
Reading). However, some care needs to be exercised. If the validation measures are 
subsequently used to choose between models (for example, to choose between models 
of different complexity), then the validation score of the model that is finally selected will 
be a biased estimate of this model's performance. To see this, imagine that, purely by 
chance some model did exceptionally well on a validation set. That is, by the accidental 

way the validation set happened to have fallen, this model did well. Then this model is 
likely to be chosen as the ""best"" model. But clearly, this model will not do so well with 
new out-of-sample data sets. What this means in practice is that, if an assessment of the 
likely future performance of a (predictive) model is needed, then this must be based on 
yet a third data set, the test set, about which we shall say more in the next subsection. 

7.5 Evaluation of Models and Patterns 

 
 

Once we have selected a model or pattern, based on its score function, we will often 
want to know (in a predictive context) how well this model or pattern will perform on new 
unseen data. For example, what error rate, on future unseen data, would we expect from 
a predictive classification model we have built using a given training set? We have 
already referred to this issue when discussing the validation set method of model 
selection above. 
Again we note that if any of the same data that have been used for selecting a model or 
used for parameter estimation are then also used again for performance evaluation, then 
the evaluation will be optimistically biased. The model will have been chosen precisely 
because it does well on this particular data set. This means that the apparent or 
resubstitution performance, as the estimate based on reusing the training set is called, 
will tend to be optimistically biased. 
If we are only considering a single model structure, and not using  validation to select a 
model, then we can use subsampling techniques such as validation or cross-validation, 
splitting the data into training and test sets, to obtain an unbiased estimate of our 
model's future performance. Again this can be repeated multiple times, and the results 
averaged. At an extreme, the test set can consist of only one point, so that the process is 
repeated N times, with an average of the  N single scores yielding the final estimate. This 
principle of leaving out part of the data, so that it can provide an independent test set, 
has been refined and developed to a great degree of technical depth and sophistication, 
notably in jackknife and bootstrap methods, as well as the leaving-one-out cross-
validation method (all of these are different, though related and sometimes confused). 
The further reading section below gives pointers to publications containing more details. 
The essence of the above is that, to obtain unbiased estimates of likely future 
performance of a model we must assess its performance using a data set which is 
independent of the data set used to construct and select the model. This also applies if 
validation data sets are used. Suppose, for example, we chose between K models by 
partitioning the data into two subsets, where we fit parameters on the first subset, and 
select the single ""best"" model using the model scores on the second (validation) subset. 
Then, since we will choose that model which does best on the validation data set, the 
model will be selected so that it fits the idiosyncrasies of this validation data set. In effect, 
the validation data set is being used as part of the design process and performance as 
measured on the validation data will be optimistic. This be comes more severe, the 
larger is the set of models from which the final model is chosen. 

Example 7.1  

 
The problem of optimistic performance on validation data is illustrated by a hypothetical 
two-class classification problem where we have selected the best of K models using a 
validation data set of 100 data points. We have taken the two classes to have equal prior 
probabilities of 0.5 and, to take an extreme situation, have arranged things so that none of 
the ""predictor"" variables in our models have any predictive power at all; that is, all the input 
variables are independent of the class variable Y. This means that each model is in fact 
generating purely random predictions so that the long-run accuracy on new unseen data for 
any of the models will be 0.5 (although of course we would not be aware of this fact). 
Figure 7.1 shows the cross-validation accuracy obtained from a simple simulation of this 
scenario, where we increased the number of models K being considered from 1 to 100. 
When we chose from a small number of models (fewer than 10) the proportion of validation 
set points correctly classified by the best of them is close enough to 0.5. However, by K = 
15 the ""best"" model, selected using the validation set, correctly classifies a proportion 0.55 

of the validation set points, and by k = 30 the best model classifies a proportion 0.61 of the 
validation set correctly. 

Figure 7.1: Classification Accuracy of the Best Model Selected on a Validation Data Set From 
a Set of K Models, 1 = K = 100, Where Each Model is Making Random Predictions.  

 

 

 

 
 

The message here is that if one uses a validation set to choose between models, one 
cannot also use it to provide an estimate of likely future performance. The very fact that 
one is choosing models which do well on the validation set means that performance 
estimates on this set are biased as estimates of performance on other unseen data. As 
we said above, the validation set, being used to choose between models, has really 
become part of the design process. This means that to obtain unbiased estimates of 
likely future performance we ideally need access to yet another data set (a ""hold-out"" 
set) that has not been used in any way in the estimation or model selection so far. For 
very large data sets this is usually not a problem, in that data is readily available, but for 
small data sets it can be problematic since it effectively reduces the data available for 
training. 

7.6 Robust Methods 

We have pointed out elsewhere that the notion of a ""true"" model is nowadays regarded 
as a weak one. Rather, it is assumed that all models are approximations to whatever is 
going on in nature, and our aim is to find a model that is close enough for the purpose to 
hand. In view of this, it would be reassuring if our model did not change too dramatically 
as the data on which it was based changed. Thus, if a slight shift in value of one data 
point led to radically different parameter estimates and predictions in a model, one might 
be wary of using it. Put another way, we would like our models and patterns to be 
insensitive to small changes in the data. Likewise, the score functions and models may 
be based on certain assumptions (for example, about underlying probability 
distributions). Again it would be reassuring if, if such assumptions were relaxed slightly, 
the fitted model and its parameters and predictions did not change dramatically. 
Score functions aimed at achieving these aims have been developed. For example, in a 
trimmed mean a small proportion of the most extreme data points are dropped, and the 
mean of the remainder used. Now the values of outlying points have no effect on the 
estimate. The extreme version of this (assuming a univariate distribution with equal 
numbers being dropped from each tail), arising as a higher and higher proportion is 
dropped from the tails, is the median—which is well known to be less sensitive to 
changes in outlying points than is the arithmetic mean. As another example, the 

 
 

 
 

Winsorized mean involves reducing the most extreme points to have the same values as 
the next most extreme points, before computing the usual mean. 

Although such modifications can be thought of as robust forms of score functions, it is 
sometimes easier to describe them (and, indeed think of them) in terms of the algorithms 
used to compute them. 

7.7 Further Reading 
Piatetsky-Shapiro (1991), Silberschatz and Tuzhilin (1996), and Bayardo and Agrawal 
(1999) contain general discussions on score functions for patterns and probabilistic 
rules. 
Hand (1997) discuss score functions for classification problems in great detail. Bishop 
(1995) discusses score functions in the context of neural networks. Breiman et al. (1984) 
discuss how general misclassification costs can be used as score functions for tree 
classifiers. Domingos (1999) provides a flexible methodology for converting any 
classification algorithm that operates on the assumption of 0–1 classification loss into a 
more general algorithm that can use any classification cost-matrix. 
Devroye (1984) argues for the use of L1 distance measures as score functions for 
density estimation problems, while Silverman (1986) describes more conventional 
squared-error (L2) score functions in the same context. 
The topics of bias and variance are discussed in a general learning context in the paper 
by Geman, Bienenstock, and Doursat (1992). Friedman (1997) develops a bias-variance 
decomposition for classification problems that turns out to have fundamentally different 
properties to classical squared-error bias-variance. 
Linhart and Zucchini (1986) provide an overview of statistical model selection 
techniques. Chapter 2 of  Ripley (1996) provides a comprehensive overview of score 
functions for model selection in classification and regression. The first general treatment 
of cross-validation was provided by Stone (1974) while Hjort (1993) outlines more recent 
ideas on cross-validation and related sampling techniques for model selection. Books on 
statistical theory (for example  Lindsey, 1996) include discussions of penalized model 
selection in general, including measures such as AIC and BIC. Akaike (1973) introduced 
the AIC principle and Schwarz (1978) contains the original discussion on BIC. Burnham 
and Anderson (1998) provide a recent detailed treatment of BIC and related approaches. 
Vapnik (1995) contains a detailed account of the structural risk minimization (SRM) 
approach to model selection and Rissanen (1987) provides a detailed discussion of 
stochastic complexity, minimum description length (MDL) and related concepts. 
Lehmann (1986) discusses the more classical statistical approach of comparing two 
models at a time within a hypothesis-testing framework. 
Bernardo and Smith (1994) has a detailed theoretical account of Bayesian approaches to 
score functions and model selection in general (see also  Dawid (1984) and Kass and 
Raftery (1995)). 
Ripley (1996, chapter 2) and  Hand (1997) provide detailed discussions of evaluating the 
performance of classification and regression models. Salzberg (1997) and Dietterich 
(1998) discuss the specific problem of assessing statistical significance in differences of 
performance among multiple classification models and algorithms. 
Huber (1980) is an important book on robust methods. 

Chapter 8: Search and Optimization Methods 
8.1 Introduction 
In chapter 6 we saw that there are broad classes of model structures or  representations 
that can be used to represent knowledge in structured form. Sub-sequently, in chapter 7 
we discussed the principles of how such structures (in the form of models and patterns) 
can be scored in terms of how well they match the observed data. This chapter focuses 
on the computational methods used for model and pattern-fitting in data mining 
algorithms; that is, it focuses on the procedures for searching and optimizing over 
parameters and structures guided by the available data and our score functions. The 

importance of effective search and optimization is often underestimated in the data 
mining, statistical and machine learning algorithm literatures, but successful applications 
in practice depend critically on such methods. 
We recall that a score function is the function that numerically expresses our preference 
for one model or pattern over another. For example, if we are using the sum of squared 
errors, SSSE, we will prefer models with lower SSSE—this measures the error of our model 
(at least on the training data). If our algorithm is searching over multiple models with 
different representational power (and different complexities), we may prefer to use a 
penalized score function such as SBIC (as discussed in chapter 7) whereby more 
complex models are penalized by adding a penalty term related to the number of 
parameters in the model. 
Regardless of the specific functional form of our score function S, once it has been 
chosen, our goal is to optimize it. (We will usually assume without loss of generality in 
this chapter that we wish to minimize the score function, rather than maximize it). So, let 
S(?|D, M) = S(?1, ..., ?d|D, M) be the score function. It is a scalar function of a d-
dimensional parameter vector ? and a model structure M (or a pattern structure ?), 
conditioned on a specific set of observed data D. 
This chapter examines the fundamental principles of how to go about finding the values 
of parameter(s) that minimize a general score function S. It is useful in practical terms, 
although there is no high-level conceptual difference, to distinguish between two 
situations, one referring to parameters that can only take discrete values (discrete 
parameters) and the other to parameters that can take values from a continuum 
(continuous parameters). 
Examples of discrete parameters are those indexing different classes of models (so that 
1 might correspond to trees, 2 to neural networks, 3 to polynomial functions, and so on) 
and parameters that can take only integral values (for example, the number of variables 
to be included in a model). The second example indicates the magnitude of the problems 
that can arise. We might want to use, as our model, a regression model based on a 
subset of variables chosen from a possible p variables. There are K = 2p such subsets, 
which can be very large, even for moderate p. Similarly, in a pattern context, we might 
wish to examine patterns that are probabilistic rules involving some subset of p binary 
variables expressed as a conjunction on the left-hand side (with a fixed right-hand side). 
There are J = 3p possible conjunctive rules (each variable takes value 1, 0, or is not in 
the conjunction at all). Once again, this can easily be an astronomically large number. 
Clearly, both of these examples are problems of combinatorial optimization, involving 
searching over a set of possible solutions to find the one with minimum score. 

Examples of continuous parameters are a parameter giving the mean value of a 
distribution or a parameter vector giving the centers of a set of clusters into which the 
data set has been partitioned. With continuous parameter spaces, the powerful tools of 
differential calculus can be brought to bear. In some special but very important special 
cases, this leads to closed form solutions. In general, however, these are not possible 
and iterative methods are needed. Clearly the case in which the parameter vector ? is 
unidimensional is very important, so we shall examine this first. It will give us insights into 
the multidimensional case, though we will see that other problems also arise in this 
situation. Both unidimensional and multidimensional situations can be complicated by the 
existence of local minima: parameter vectors with values smaller than any other similar 
vectors, but are not the smallest values that can be achieved. We shall explore ways in 
which such problems can be overcome. 
Very often, the two problems of searching over a set of possible model structures and 
optimizing parameters within a given model go hand in hand; that is, since any single 
model or pattern structure typically has unknown parameters then, as well as finding the 
best model or pattern structure, we will also have to find the best parameters for each 
structure considered during the search. For example, consider the set of models in which 
y is predicted as a simple linear combination of some subset of the three predictor 
variables x1, x2, and  x3. One model would be y (i) = ax1 (i) + bx2 (i) + cx3 (i), and others 
would have the same form but merely involving pairs of the predictor variables or single 
predictor variables. Our search will have to roam over all possible subsets of the  xj 
variables, as noted above, but for each subset, it will also be necessary to find the values 

of the parameters (a, b, and c in the case with all three variables) that minimize the score 
function. 

This description suggests that one possible design choice, for algorithms that minimize 
score functions over both model structures and parameter estimates, is to nest a loop for 
the latter in a loop for the former. This is often used since it is relatively simple, though it 
is not always the most efficient approach from a computational viewpoint. 
It is worth remarking at this early stage that in some data mining algorithms the focus is 
on finding sets of models, patterns, or regions within parameter space, rather than just 
the single best model, pattern, or parameter vector, according to the chosen score 
function. This occurs, for example, in Bayesian averaging techniques and in searching 
for sets of patterns. Usually (although, as always, there are exceptions) in such 
frameworks similar general principles of search and optimization will arise as in the 
single model/pattern/parameter case and, so in the interests of simplicity of presentation 
we will focus primarily on the problem of finding the single best model, pattern, and/or 
parameter-vector. 
Section 2 focuses on general search methods for situations where there is no notion of 
continuity in the model space or parameter space being searched. This section includes 
discussion of the combinatorial problems that typically prevent exhaustive examination of 
all solutions, the general state-space representation for search problems, discussion of 
particular search strategies, as well as methods such as branch and bound that take 
advantage of properties of the parameter space or score function to reduce the number 
of parameter vectors that must be explicitly examined. Section 3 turns to optimization 
methods for continuous parameter spaces, covering univariate and multivariate cases, 
and problems complicated by constraints on the permissible parameter values. Section 4 
describes a powerful class of methods that apply to problems that involve (or can 
usefully be regarded as involving) missing values. In many data mining situations, the 
data sets are so large that multiple passes through the data have to be avoided. Section 
5 describes algorithms aimed at this. Finally, since many problems involve score 
functions that have multiple minima (and maxima), stochastic search methods have been 
developed to improve the chances of finding the global optimum (and not merely a rather 
poor local optimum). Some of these are described in section 6. 

8.2 Searching for Models and Patterns 

8.2.1 Background on Search 
This subsection discusses some general high level issues of search. In many practical 
data mining situations we will not know ahead of time what particular model structure M 
or pattern structure ? is most appropriate to solve our task, and we will search over a 
family of model structures M = {M1,..., MK} or pattern structures P = {?1,..., ?J}. We gave 
some examples of this earlier: finding the best subset of variables in a linear regression 
problem and finding the best set of conditions to include in the left -hand side of a 
conjunctive rule. Both of these problems can be considered ""best subsets"" problems, 
and have the general characteristic that a combinatorially large number of such solutions 
can be generated from a set of p ""components"" (p variables in this case). Finding ""best 
subsets"" is a common problem in data mining. For example, for predictive classification 
models in general (such as nearest neighbor, naive Bayes, or neural network classifiers) 
we might want to find the subset of variables that produces the lowest classification error 
rate on a validation data set. 
A related model search problem, that we used as an illustration earlier in chapter 5, is 
that of finding the best tree-structured classifier from a ""pool"" of p variables. This has 
even more awesome combinatorial properties. Consider the problem of searching over 
all possible binary trees (that is, each internal node in the tree has two children). Assume 
that all trees under consideration have depth p so that there are p variables on the path 
from the root node to any leaf node. In addition, let any variable be eligible to appear at 
any node in the tree, remembering that each node in a classification tree contains a test 
on a single variable, the outcomes of which define which branch is taken from that node. 
For this family of trees there are on the order of 

different tree structures—that is, 

 
 

classification trees that differ from each other in the specification of at least one internal 
node. In practice, the number of possible tree structures will in fact be larger since we 
also want to consider various subtrees of the full-depth trees. Exhaustive search over all 
possible trees is clearly infeasible! 
We note that from a purely mathematical viewpoint one need not necessarily distinguish 
between different model structures in the sense that all such model structures could be 
considered as special cases of a single ""full"" model, with appropriate parameters set to 
zero (or some other constant that is appropriate for the model form) so that they 
disappear from the model. For example, the linear regression model y = ax1 + b is a 
special case of  y = ax1+cx2+dx3+b with c = d = 0. This would reduce the model structure 
search problem to the type of parameter optimization problem we will discuss later in this 
chapter. Although mathematically correct, this viewpoint is often not the most useful way 
to think about the problem, since it can obscure important structural information about 
the models under consideration. 
In the discussion that follows we will often use the word models rather than the phrase 
models or patterns to save repetition, but it should be taken as referring to both types of 
structure: the same general principles that are outlined for searching for models are also 
true for the problem of searching for patterns. 

Some further general comments about search are worth making here: 

§  We noted in the opening section that finding the model or structure with 
the optimum score from a family M necessarily involves finding the best 
parameters ?k for each model structure  Mk within that family. This means 
that, conceptually and often in practice, a nested loop search process is 
needed, in which an optimization over parameter values is nested within 
a search over model structures. 

§  As we have already noted, there is typically no notion of the score 

function S being a ""smooth"" function in ""model space,"" and thus, many of 
the traditional optimization techniques that rely on smoothness 
information (for example, gradient descent) are not applicable. Instead 
we are in the realm of combinatorial optimization where the underlying 
structure of the problem is inherently discrete (such as an index over 
model structures) rather than a continuous function. Most of the 
combinatorial optimization problems that occur in data mining are 
inherently intractable in the sense that the only way to guarantee that 
one will find the best solution is to visit all possible solutions in an 
exhaustive fashion. 

§  For some problems, we will be fortunate in that we will not need to 

perform a full new optimization of parameter space as we move from one 
model structure to the next. For example, if the score function is 
decomposable, then the score function for a new structure will be an 
additive function of the score function for the previous structure as well 
as a term accounting for the change in the structure. For example, 
adding or deleting an internal node in a classification tree only changes 
the score for data points belonging to the subtree associated with that 
node. However, in many cases, changing the structure of the model will 
mean that the old parameter values are no longer optimal in the new 
model. For example, suppose that we want to build a model to predict y 
from x based on two data points (x, y) = (1, 1) and (x, y) = (3, 3). First let 
us try very simple models of the form y = a, that is y is a constant (so that 
all our predictions are the same). The value of  a that minimizes the sum 
of squared errors (1 - a)2+(3 - a)2 is 2. Now let us try the more elaborate 
model y = bx+a. This adds an extra term into the model. Now the values 
of a and b that minimize the sum of squared errors (this is a standard 
regression problem, although a particularly simple example) are, 
respectively, 0 and 1. We see that the estimate of a depends upon what 
else is in the model. It is possible to formalize the circumstances in which 
changing the model will leave parameter estimates unaltered, in terms of 
orthogonality of the data. In general, it is clearly useful to know when this 
applies, since much faster algorithms can then be developed (for 

example, if variables are orthogonal in a regression case, we can just 
examine them one at a time). However, such situations tend to arise 
more often in the context of designed experiments than in the secondary 
data occurring in data mining situations. For this reason, we will not dwell 
on this issue here. 

For linear regression, parameter estimation is not difficult and so it is 
straightforward (if somewhat time-consuming) to recalculate the optimal 
parameters for each model structure being considered. However, for more 
complex models such as neural networks, parameter optimization can be both 
computationally demanding as well as requiring careful ""tuning"" of the 
optimization method itself (as we will see later in this chapter). Thus, the ""inner 
loop"" of the model search algorithm can be quite taxing computationally. One 
way to ease the problem is to leave the existing parameters in the model fixed to 
their previous values and to estimate only the values of parameters added to the 
model. Although this strategy is clearly suboptimal, it permits a trade-off 
between highly accurate parameter estimation of just a few models or 
approximate parameter estimation of a much larger set of models. 

§  Clearly for the best subsets problem and the best classification tree 

problem, exhaustive search (evaluating the score function for all 
candidate models in the model family M) is intractable for any nontrivial 
values of  p since there are 2p and  models to be examined in each 
case. Unfortunately, this combinatorial explosion in the number of 
possible model and pattern structures will be the norm rather than the 
exception for many data mining problems involving search over model 
structure. Thus, without even taking into account the fact that for each 
model one may have to perform some computationally complex 
parameter optimization procedure, even simply enumerating the models 
is likely to become intractable for large p. This problem is particularly 
acute in data mining problems involving very high-dimensional data sets 
(large p). 

§  Faced with inherently intractable problems, we must rely on what are 

called heuristic search techniques. These are techniques that 
experimentally (or perhaps provably on average) provide good 
performance but that cannot be guaranteed to provide the best solution 
always. The greedy heuristic (also known as local improvement) is one of 
the better known examples. In a model search context, greedy search 
means that, given a current model Mk we look for other models that are 
""near"" Mk (where we will need to define what we mean by ""near"") and 
move to the best of these (according to our score function) if indeed any 
are better than  Mk. 

8.2.2 The State-Space Formulation for Search in Data Mining 

A general way to describe a search algorithm for discrete spaces is to specify the 
problem as follows: 

1.  State Space Representation: We view the search problem as one of 
moving through a discrete set of states. For model search, each model 
structure Mk consists of a state in our state space. It is conceptually 
useful to think of each state as a vertex in a graph (which is potentially 
very large). An abstract definition of our search problem is that we 
start at some particular node (or state), say M1, and wish to move 
through the state space to find the node corresponding to the state 
that has the highest score function. 

2.  Search Operators: Search operators correspond to legal ""moves"" in 

our search space. For example, for model selection in linear 
regression the operators could be defined as either adding a variable 
to or deleting a variable from the current model. The search operators 
can be thought of as defining directed edges in the state space graph. 
That is, there is a directed edge from state  Mi to Mj if there is an 

operator that allows one to move from one model structure Mi to 
another model structure  Mj. 

A simple example will help illustrate the concept. Consider the general problem of 
selecting the best subset from p variables for a particular classification model (for 
example, the nearest neighbor model). Let the score function be the cross-validated 
classification accuracy for any particular subset. Let Mk denote an individual model 
structure within the general family we are considering, namely all K = 2p - 1 different 
subsets containing at least one variable. Thus, the state-space has 2p - 1 states, ranging 
from models consisting of subsets of single variables M1 = {x1}, M2 = {x2},... all the way 
through to the full model with all p variables, MK = {x1,..., xp}. Next we define our 
operators. For subset selection it is common to consider simple operators such as 
adding one variable at a time and deleting one variable at a time. Thus, from any state 
with p' variables (model structure) there are two ""directions"" one can ""move"" in the 
model family: add a variable to move to a state with p' + 1 variables, or delete a variable 
to move to a state with  p' - 1 variables (figure 8.1 shows a state-space for subset 
selection for 4 variables with these two operators). We can easily generalize these 
operators to adding or deleting r variables at a time. Such ""greedy local"" heuristics are 
embedded in many data mining algorithms. Search algorithms using this idea vary in 
terms of what state they start from: forward selection algorithms work ""forward"" by 
starting with a minimally sized model and iteratively adding variables, whereas backward 
selection algorithms work in reverse from the full model. Forward selection is often the 
only tractable option in practice when  p is very large since working backwards may be 
computationally impractical. 

Figure 8.1: An Example of a Simple State-Space Involving Four Variables X1, X2, X3, X4. The 
Node on the Left is the Null Set—i.e., No Variables in the Model or Pattern.  

 

It is important to note that by representing our problem in a state-space with limited 
connectivity we have not changed the underlying intractability of the general model 
search problem. To find the optimal state it will still be necessary to visit all of the 
exponentially many states. What the state-space/operator representation does is to allow 
us to define systematic methods for local exploration of the state-space, where the term 
""local"" is defined in terms of which states are adjacent in the state-space (that is, which 
states have operators connecting them). 

8.2.3 A Simple Greedy Search Algorithm 

A general iterative greedy search algorithm can be defined as follows: 

1. 

2. 

Initialize: Choose an initial state  M(0) corresponding to a particular 
model structure  Mk. 
Iterate: Letting M(i) be the current model structure at the ith iteration, 
evaluate the score function at all possible adjacent states (as defined 
by the operators) and move to the best one. Note that this evaluation 
can consist of performing parameter estimation (or the change in the 
score function) for each neighboring model structure. The number of 
score function evaluations that must be made is the number of 
operators that can be applied to the current state. Thus, there is a 
trade-off between the number of operators available and the time 
taken to choose the next model in state-space. 

3.  Stopping Criterion: Repeat step 2 until no further improvement can 

be attained in the local score function (that is, a local minimum is 
reached in state-space). 

4.  Multiple Restarts: (optional) Repeat steps 1 through 3 from different 

initial starting points and choose the best solution found. 

This general algorithm is similar in spirit to the local search methods we will discuss later 
in this chapter for parameter optimization. Note that in step 2 that we must explicitly 
evaluate the effect of moving to a neighboring model structure in a discrete space, in 
contrast to parameter optimization in a continuous space where we will often be able to 
use explicit gradient information to determine what direction to move. Step 3 helps avoid 
ending at a local minimum, rather than the global minimum (though it does not guarantee 
it, a point to which we return later). For many structure search problems, greedy search 
is provably suboptimal. However, in general it is a useful heuristic (in the sense that for 
many problems it will find quite good solutions on average) and when repeated with 
multiple restarts from randomly chosen initial states, the simplicity of the method makes 
it quite useful for many practical data mining applications. 

8.2.4 Systematic Search and Search Heuristics 
The generic algorithm described above is often described as a ""hill-climbing"" algorithm 
because (when the aim is to maximize a function) it only follows a single ""path"" in state-
space to a local maximum of the score function. A more general (but more complex) 
approach is to keep track of multiple models at once rather than just a single current 
model. A useful way to think about this approach is to think of a search tree, a data 
structure that is dynamically constructed as we search the state-space to keep track of 
the states that we have visited and evaluated. (This has nothing to do with classification 
trees, of course.) The search tree is not equivalent to the state-space; rather, it is a 
representation of how a particular search algorithm moves through a state-space. 
An example will help to clarify the notion of a search tree. Consider again the problem of 
finding the best subset of variables to use in a particular classification model. We start 
with the ""model"" that contains no variables at all and predicts the value of the most likely 
class in the training data as its prediction for all data points. This is the root node in the 
search tree. Assume that we have a forward-selection algorithm that is only allowed to 
add variables one at a time. From the root node, there are p variables we can add to the 
model with no variables, and we can represent these  p new models as p children of the 
original root node. In turn, from each of these p nodes we can add p variables, creating  p 
children for each, or p2 in total (clearly, 
implement a duplicate-state detection scheme to eliminate the redundant nodes from the 
tree). 
Figure 8.2 shows a simple example of a search tree for the state space of figure 8.1. 
Here the root node contains the empty set (no variables) and only the two best states so 
far are considered at any stage of the search. The search algorithm (at this point of the 
search) has found the two best states (as determined by the score function) to be X2 and 
X1, X3, X4. 

are redundant, and in practice we need to 

 

Figure 8.2: An Example of a Simple Search Tree for the State-Space of Figure 8.1.  

Search trees evolve dynamically as we search the state-space, and we can imagine 
(hypothetically) keeping track of all of the leaf nodes (model structures) as candidate 
models for selection. This quickly becomes infeasible since at depth k in the tree there 
will be pk leaf nodes to keep track of (where the root node is at depth zero and we have 
branching factor p). We will quickly run out of memory using this brute-force method 
(which is essentially breadth-first search of the search tree). A memory-efficient 
alternative is depth-first search, which (as its name implies) explores branches in the 
search tree to some maximum depth before backing up and repeating the depth-first 
search in a recursive fashion on the next available branch. 
Both of these techniques are examples of blind search, in that they simply order the 
nodes to be explored lexicographically rather than by the score function. Typically, 
improved performance (in the sense of finding higher quality models more quickly) can 
be gained by exploring the more promising nodes first. In the search tree this means that 
the leaf node with the highest score is the one whose children are next considered; after 
the children are added as leaves, the new leaf with the highest score is examined. Again, 
this strategy can quickly lead to many more model structures (nodes in the tree) being 
generated than we will be feasibly able to keep in memory. Thus, for example, one can 
implement a beam search strategy that uses a beam width of size b to ""track"" only the b 
best models at any point in the search (equivalently to only keep track of the b best 
leaves on the tree). (In  figure 8.2 we had b = 2.) Naturally, this might be suboptimal if the 
only way to find the optimal model is to first consider models that are quite suboptimal 
(and thus, might be outside the ""beam""). However, in general, beam search can be quite 
effective. It is certainly often much more effective than simple hill-climbing, which is 
similar to depth-first search in the manner in which it explores the search tree: at any 
iteration there is only a single model being considered, and the next model is chosen as 
the child of the current model with the highest score. 

8.2.5 Branch-and-Bound 
A related and useful idea in a practical context is the notion of branch-and-bound. The 
general idea is quite simple. When exploring a search tree, and keeping track of the best 
model structure evaluated so far, it may be feasible to calculate analytically a lower 
bound on the best possible score function from a particular (as yet unexplored) branch of 
the search tree. If this bound is greater than the score of the best model so far, then we 
need not search this subtree and it can be pruned from further consideration. Consider, 
for example, the problem of finding the best subset of k variables for classification from a 
set of p variables where we use the training set error rate as our score function. Define a 
tree in which the root node is the set of all p variables, the immediate child nodes are the 

unique such nodes, 

leaves that each contain subsets of k 

p nodes each of which have a single variable dropped (so they each have  p - 1 
variables), the next layer has two variables dropped (so there are 
each with p - 2 variables), and so on down to the 
variables (these are the candidate solutions). Note that the training set error rate cannot 
decrease as we work down any branch of the tree, since lower nodes are based on 
fewer variables.  
Now let us begin to explore the tree in a depth-first fashion. After our depth-first algorithm 
has descended to visit one or more leaf nodes, we will have calculated scores for the 
models (leaves) corresponding to these sets of k variables. Clearly the smallest of these 
is our best candidate k-variable model so far. Now suppose that, in working down some 
other branch of the tree, we encounter a node that has a score larger than the score of 
our smallest k-variable node so far. Since the score cannot decrease as we continue to 
work down this branch, there is no point in looking further: nodes lower on this branch 
cannot have smaller training set error rate than the best k-variable solution we have 
already found. We can thus save the effort of evaluating nodes further down this branch. 
Instead, we back up to the nearest node above that contained an unexplored branch and 
begin to investigate that. This basic idea can be improved by ordering the tree so that we 
explore the most promising nodes first (where ""promising"" means they are likely to have 
low training set error rate). This can lead to even more effective pruning. This type of 
general branch and bound strategy can significantly improve the computational efficiency 
of model search. (Although, of course, it is not a guaranteed solution—many problems 
are too large even for this strategy to provide a solution in a reasonable time.) 

 
 

These ideas on searching for model structure have been presented in a very general 
form. More effective algorithms can usually be designed for specific model structures 
and score functions. Nonetheless, general principles such as iterative local improvement, 
beam search, and branch-and-bound have significant practical utility and recur 
commonly under various guises in the implementation of many data mining algorithms. 

8.3 Parameter Optimization Methods 

8.3.1 Parameter Optimization: Background 
Let S(?) = S(?|D, M) be the score function we are trying to optimize, where ? are the 
parameters of the model. We will usually suppress the explicit dependence on D and  M 
for simplicity. We will now assume that the model structure  M is fixed (that is, we are 
temporarily in the inner loop of parameter estimation where there may be an outer loop 
over multiple model structures). We will also assume, again, that we are trying to 
minimize S, rather than maximize it. Notice that S and g(S) will be minimized for the 
same value of ? if g is a monotonic function of S (such as log S). 
In general ? will be a d-dimensional vector of parameters. For example, in a regression 
model ? will be the set of coefficients and the intercept. In a tree model, ? will be the 
thresholds for the splits at the internal nodes. In an artificial neural network model, ? will 
be a specification of the weights in the network. 

In many of the more flexible models we will consider (neural networks being a good 
example), the dimensionality of our parameter vector can grow very quickly. For 
example, a neural network with 10 inputs and 10 hidden units and 1 output, could have 
10 × 10 + 10 = 110 parameters. This has direct implications for our optimization problem, 
since it means that in this case (for example) we are trying to find the minimum of a 
nonlinear function in 110 dimensions. 
Furthermore, the shape of this potentially high-dimensional function may be quite 
complicated. For example, except for problems with particularly simple structure, S will 
often be multimodal. Moreover, since S = S(?|D, M) is a function of the observed data D, 
the precise structure of S for any given problem is data-dependent. In turn this means 
that we may have a completely different function S to optimize for each different data set 
D, so that (for example) it may be difficult to make statements about how many local 
minima S has in the general case. 

As discussed in chapter 7, many commonly used score functions can be written in the 
form of a sum of local error functions (for example, when the training data points are 
assumed to be independent of each other): 

(8.1)  

where y?(i) is our model's estimate of the target value y(i) in the training data, and e is an 
error function measuring the distance between the model's prediction and the target 
(such as square error or log-likelihood). Note that the complexity in the functional form S 
(as a function of ?) can enter both through the complexity of the model structure being 
used (that is, the functional form of y) and also through the functional form of the error 
function e. For example, if y is linear in ? and e is defined as squared error, then S will be 
quadratic in ?, making the optimization problem relatively straightforward since a 
quadratic function has only a single (global) minimum or maximum. However, if y is 
generated by a more complex model or if  e is more complex as a function of ?, S will not 
necessarily be a simple smooth function of ? with a single easy-to-find extremum. In 
general, finding the parameters ? that minimize S(?) is usually equivalent to the problem 
of minimizing a complicated function in a high-dimensional space. 
Let us define the gradient function of S as 

(8.2)  

which is a d-dimensional vector of partial derivatives of S evaluated at ?. In general, 
? ?S(?) = 0 is a necessary condition for an extremum (such as a minumum) of S at ?. 
This is a set of d simultaneous equations (one for each partial derivative) in d variables. 
Thus, we can search for solutions ? (that correspond to extrema of S(?)) of this set of d 
equations. 

We can distinguish two general types of parameter optimization problems: 

1.  The first is when we can solve the minimization problem in closed 

form. For example, if S(?) is quadratic in ?, then the gradient g(?) will 
be linear in ? and the solution of ? S(?) = 0 involves the solution of a 
set of d linear equations. However, this situation is the exception 
rather than the rule in practical data mining problems. 

2.  The second general case occurs when S(?) is a smooth nonlinear 

function of ? such that the set of d equations g(?) = 0 does not have a 
direct closed form solution. Typically we use iterative improvement 
search techniques for these types of problems, using local information 
about the curvature of S to guide our local search on the surface of S. 
These are essentially hill-climbing or descending methods (for 
example, steepest descent). The backpropagation technique used to 
train neural networks is an example of such a steepest descent 
algorithm. 

Since the second case relies on local information, it may end up converging to a local 
minimum rather than the global minimum. Because of this, such methods are often 
supplemented by a stochastic component in which, to take just one example, the 
optimization procedure starts several times from different randomly chosen starting 
points. 

8.3.2 Closed Form and Linear Algebra Methods 
Consider the special case when S(?) is a quadratic function of ?. This is a very useful 
special case since now the gradient g(?) is linear in ? and the minimum of S is the 
unique solution to the set of d linear equations g(?) = 0 (assuming the matrix of second 
derivatives of S at these solutions satisfies the condition of being positive definite). This 
is illustrated in the context of multiple regression (which usually uses a sum of squared 
errors score function) in chapter 11. We showed in chapter 4 how the same result was 
obtained if likelihood was adopted as the score function, assuming Normal error 
distributions. In general, since such problems can be framed as solving for the inverse of 
an d × d matrix, the complexity of solving such linear problems tends to scale in general 

as O(nd2 + d3), where it takes order of nd2 steps to construct the original matrix of 
interest and order of  d3 steps to invert it. 

8.3.3 Gradient-Based Methods for Optimizing Smooth Functions 
In general of course, we often face the situation in which S(?) is not a simple function of 
? with a single minimum. For example, if our model is a neural network with nonlinear 
functions in the hidden units, then S will be a relatively complex nonlinear function of ? 
with multiple local minima. We have already noted that many approaches are based on 
iteratively repeating some local improvement to the model. 

The typical iterative local optimization algorithm can be broken down into four relatively 
simple components: 

1. 

Initialize: Choose an initial value for the parameter vector ? = ?0 (this 
is often chosen randomly). 
Iterate: Starting with i = 0, let 

2. 
(8.3)  

3.  where v is the direction of the next step (relative to ?i in parameter 

space) and ?i determines the distance. Typically (but not necessarily) 
vi is chosen to be in a direction of improving the score function. 

4.  Convergence: Repeat step 2 until S(?i) appears to have attained a 

local minimum. 

5.  Multiple Restarts: Repeat steps 1 through 3 from different initial 

starting points and choose the best minimum found. 

Particular methods based on this general structure differ in terms of the chosen direction 
vi in parameter space and the distance ?i moved along the chosen direction, amongst 
other things. Note that this is this algorithm has essentially the same design as the one 
we defined in section 8.2 for local search among a set of discrete states, except that 
here we are moving in continuous d-dimensional space rather than taking discrete steps 
in a graph. 
The direction and step size must be determined from local information gathered at the 
current point of the search—for example, whether first derivative or second derivative 
information is gathered to estimate the local curvature of S. Moreover, there are 
important trade-offs between the quality of the information gathered and the resources 
(time, memory) required to calculate this information. No single method is universally 
superior to all others; each has advantages and disadvantages. 
All of the methods discussed below require specification of initial starting points and a 
convergence (termination) criterion. The exact specifications of these aspects of the 
algorithm can vary from application to application. In addition, all of the methods are 
used to try to find a local extremum of S(?). One must check in practice that the found 
solution is in fact a minimum (and not a maximum or saddlepoint). In addition, for the 
general case of a nonlinear function S with multiple minima, little can be said about the 
quality of the local minima relative to the global minima without carrying out a brute-force 
search over the entire space (or using sophisticated probabilistic arguments that are 
beyond this text). Despite these reservations, the optimization techniques that follow are 
extremely useful in practice and form the core of many data mining algorithms. 

8.3.4 Univariate Parameter Optimization 
Consider first the special case in which we just have a single unknown parameter ? and 
we wish to minimize the score function S(?) (for example, figure 8.3). Although in 
practical data mining situations we will usually be optimizing a model with more than just 
a single parameter, the univariate case is nonetheless worth looking at, since it clearly 
illustrates some of the general principles that are relevant to the more general 
multivariate optimization problem. Moreover, univariate search can serve as a 
component in a multivariate search procedure, in which we first find the direction of 
search using the gradient and then decide how far to move in that direction using 
univariate search for a minimum along that direction. 

Figure 8.3: An Example of a Score Function S(?) of a Single Univariate Parameter ? with 
Both a Global Minimum and a Local Minimum.  

 

Letting 
derivative g' (?) > 0. If a closed form solution is possible, then we can find it and we are 
done. If not, then we can use one of the methods below.  

, the minimum of S occurs wherever g(?) = 0 and the second 

The Newton-Raphson Method 
Suppose that the solution occurs at some unknown point ?s; that is, g(?s) = 0. Now, for 
points ?* not too far from ?s we have, by using a Taylor series expansion 

(8.4)  

where this linear approximation ignores terms of order (?s - ?*)2 and above. Since ?s 
satisfies g(?s) = 0, the left-hand side of this expression is zero. Hence, by rearranging 
terms we get 

(8.5)  

In words, this says that given an initial value ?*, then an approximate solution of the 
equation g(?s) = 0 is given by adjusting ?* as indicated in equation 8.5. By repeatedly 
iterating this, we can in theory get as close to the solution as we like. This iterative 
process is the Newton-Raphson (NR) iterative update for univariate optimization based 
on first and second derivative information. The ith step is given by 

(8.6)  

The effectiveness of this method will depend on the quality of the linear approximation in 
equation 8.4. If the starting value is close to the true solution ?s then we can expect the 
approximation to work well; that is, we can locally approximate the surface around S(?*) 
as parabolic in form (or equivalently, the derivative g(?) is linear near ?* and ?s). In fact, 
when the current ? is close to the solution ?s, the convergence rate of the NR method is 
quadratic in the sense that the error at step i of the iteration ei = |?i - ?s| can be 
recursively written as 

(8.7)  

To use the Newton-Raphson update, we must know both the derivative function  g(?) and 
the second derivative g'(?) in closed form. In practice, for complex functions we may not 
have closed-form expressions, necessitating numerical approximation of g(?) and g' (?), 
which in turn may introduce more error into the determination of where to move in 
parameter space. Generally speaking, however, if we can evaluate the gradient and 
second derivative accurately in closed form, it is advantageous to do so and to use this 
information in the course of moving through parameter space during iterative 
optimization. 
The drawback of NR is, of course, that our initial estimate ?i may not be sufficiently close 
to the solution ?s to make the approximation work well. In this case, the NR step can 
easily overshoot the true minimum of S and the method need not converge at all. 

The Gradient Descent Method 
An alternative approach, which can be particularly useful early in the optimization 
process (when we are potentially far from ?s), is to use only the gradient information 
(which provides at least the correct direction to move in for a 1-dimensional problem) 
with a heuristically chosen step size ?: 

(8.8)  

The multivariate version of this method is known as gradient (or steepest) descent. Here 
? is usually chosen to be quite small to ensure that we do not step too far in the chosen 
direction. We can view gradient descent as a special case of the NR method, whereby 
the second derivative information 

is replaced by a constant ?. 

Momentum-Based Methods 
There is a practical trade-off in choosing ?. If it is too small, then gradient descent may 
converge very slowly indeed, taking very small steps at each iteration. On the other 
hand, if ? is too large, then the guarantee of convergence is lost, since we may 
overshoot the minimum by stepping too far. We can try to accelerate the convergence of 
gradient descent by adding a momentum term: 

(8.9)  

where ? i is defined recursively as 

(8.10)  

and where µ is a ""momentum"" parameter, 0 = µ = 1. Note that µ = 0 gives us the 
standard gradient descent method of equation 8.8, and µ > 0 adds a ""momentum"" term 
in the sense that the current direction  ? i is now also a function of the previous direction 
? i-1. The effect of µ in regions of low curvature in S is to accelerate convergence (thus, 
improving standard gradient descent, which can be very slow in such regions) and 
fortunately has little effect in regions of high curvature. The momentum heuristic and 
related ideas have been found to be quite useful in practice in training models such as 
neural networks. 

Bracketing Methods 
For functions which are not well behaved (if the derivative of S is not smooth, for 
example) there exists a different class of scalar optimization methods that do not rely on 
any gradient information at all (that is, they work directly on the function S and not its 
derivative g). Typically these methods are based on the notion of bracketing—finding a 
bracket [?1, ?2] that provably contains an extremum of the function. For example, if there 
exists a ""middle"" ? value ?m, such that ?1 > ?m > ?2 and S(?m) is less than both S(?1) and 
S(?2), then clearly a local minimum of the function S must exist between ?1 and ?2 
(assuming that S is continuous). One can use this idea to fit a parabola through the three 
points ?1, ?m, and ?2 and evaluate S(?p) where ?p is located at the minimum value of 
parabola. Either ?p is the desired local minimum, or else we can narrow the bracket by 
eliminating ?1 or ?2 and iterating with another parabola. A variety of methods exist that 
use this idea with varying degrees of sophistication (for example, a technique known as 
Brent's method is widely used). It will be apparent from this outline that bracketing 
methods are really a search strategy. We have included them here, however, partly 
because of their importance in finding optimal values of parameters, and partly because 
they rely on the parameter space having a connected structure (for example, ordinality) 
even if the function being minimized is not continuous. 

8.3.5 Multivariate Parameter Optimization 
We now move on to the much more difficult problem we are usually faced with in 
practice, namely, finding the minimum of a scalar score function S of a multivariate 
parameter vector ? in d-dimensions. Many of the methods used in the multivariate case 
are analogous to the scalar case. On the other hand, d may be quite large for our 
models, so that the multidimensional optimization problem may be significantly more 
complex to solve than its univariate cousin. It is possible, for example, that local minima 
may be much more prevalent in high-dimensional spaces than in lower-dimensional 
spaces. Moreover, a problem similar (in fact, formally equivalent) to the combinatorial 

explosion that we saw in the discussion of search also manifests itself in 
multidimensional optimization; this is the curse of dimensionality that we have already 
encountered in chapter 6. Suppose that we wish to find the d dimensional parameter 
vector that minimizes some score function, and where each parameter is defined on the 
unit interval, [0, 1]. Then the multivariate parameter vector ? is defined on the unit d-
dimensional hypercube. Now suppose we know that at the optimal solution none of the 
components of ? lie in [0, 0.5]. When d = 1, this means that half of the parameter space 
has been eliminated. When d = 10, however, only 
of the parameter space has 
been eliminated, and when  d = 20 only 
eliminated. Readers can imagine—or do the arithmetic themselves—to see what 
happens when really large numbers of parameters are involved. This shows clearly why 
there is a real danger of missing a global minimum, with the optimization ending on some 
(suboptimal) local minimum. 

of the parameter space has been 

Following the pattern of the previous subsection, we will first describe methods for 
optimizing functions continuous in the parameters (extensions of the Newton-Raphson 
method, and so on) and then describe methods that can be applied when the function is 
not continuous (analogous to the bracketing method). 
The iterative methods outlined in the preceding subsection began with some initial value, 
and iteratively improved it. So suppose that the parameter vector takes the value ?i at 
the ith step. Then, to extend the methods outlined in the preceding subsection to the 
multidimensional case we have to answer two questions: 

1.   

2.   

In which direction should we move from 
?i? 
How far should we step in that direction? 

 

 

Answers 

1.  
2.  

 
 

The local iterations can generally be described as 

(8.11)  

where ?i is the parameter estimate at iteration i and v is the d-dimensional vector 
specifying the next direction to move (specified in a manner dependent on the particular 
optimization technique being used). 
For example, the multivariate gradient descent method is specified as 

(8.12)  

where ? is the scalar learning rate and g(?) is a  d-dimensional gradient function (as 
defined in equation 8.2). This method is also known as steepest descent, since -g(?i) will 
point in the direction of steepest slope from ?i. Provided ? is chosen to be sufficiently 
small then gradient descent is guaranteed to converge to a local minimum of the function 
S. 
The backpropagation method for parameter estimation popular with neural networks is 
really merely a glorified steepest descent algorithm. It is somewhat more complicated 
than the standard approach only because of the multiple layers in the network, so that 
the derivatives required above have to be derived using the chain rule. 
Note that the gradient in the steepest descent algorithm need not necessarily point 
directly towards the minimum. Thus, as shown in figure 8.4, being limited to take steps 
only in the direction of the gradient can be an extremely inefficient way to find the 
minimum of a function. A more sophisticated class of multivariate optimization methods 
uses local second derivative information about ? to decide where in the parameter space 
to move to next. In particular, Newton's method (the multivariate equivalent of univariate 
NR) is defined as: 

 

Figure 8.4: An Example of a Situation in Which We Minimize a Score Function of Two 
Variables and the Shape of the Score Function is a Parabolic ""Bowl"" with the Minimum in the 
Center. Gradient Descent Does Not Point Directly to the Minimum but Instead Tends to Point 
""Across"" the Bowl (Solid Lines on the Left), Leading to a Series of Indirect Steps before the 
Minimum is Reached.  
(8.13)  

where H-1(?i) is the inverse of the d × d matrix of second derivatives of S (known as the 
Hessian matrix) evaluated at ?i. The Hessian matrix has entries defined as: 

(8.14)  

As in the univariate case, if S is quadratic the step taken by the Newton iteration in 
parameter space points directly toward the minimum of S. We might reasonably expect 
that for many functions the shape of the function is approximately locally quadratic in ? 
about its local minima (think of approximating the shape of the top of a ""smooth"" 
mountain by a parabola), and hence, that at least near the minima, the Newton strategy 
will be making the correct assumption about the shape of S. In fact, this assumption is 
nothing more than the multivariate version of Taylor's series expansion. Of course, since 
the peak will usually not be exactly quadratic in shape, it is necessary to apply the 
Newton iteration recursively until convergence. Again, as in the univariate case, the use 
of the Newton method may diverge rather than converge (for example, if the Hessian 
matrix H(?i) is singular; that is, the inverse  H-1 does not exist at ?i). 
The Newton scheme comes at a cost. Since H is a d × d matrix, there will be O(nd2 + d3) 
computations required per step to estimate H and invert it. For models with large 
numbers of parameters (such as neural networks) this may be completely impractical. 
Instead, we could, for example, approximate H by its diagonal (giving O(nd) complexity 
per step). Even though the diagonal approximation will clearly be incorrect (since we can 
expect that parameters will exhibit dependence on each other), the approximation may 
nonetheless be useful as a linear cost alternative to the full Hessian calculation. 
An alternative approach is to build an approximation to H-1 iteratively based on gradient 
information as we move through parameter space. These techniques are known as 
quasi-Newton methods. Initially we take steps in the direction of the gradient (assuming 
an initial estimate of H = I the identity matrix) and then take further steps in the direction 

Fletcher-Goldfarb-Shanno) method is a widely used technique based on this general 
idea. 
Of course, sometimes special methods have been developed for special classes of 
models and score functions. An example is the iteratively weighted least squares method 
for fitting generalized linear models, as described in chapter 11. 
The methods we have just described all find a ""good"" direction for the step at each 
iteration. A simple alternative would be merely to step in directions parallel to the axes. 
This has the disadvantage that the algorithm can become stuck—if, for example there is 
a long narrow valley diagonal to the axes. If the shape of the function in the vicinity of the 
minimum is approximated by a quadratic function, then the principal axes of this will 
define directions (probably not parallel to the axes). Adopting these as an alternative 
coordinate system, and then searching along these new axes, will lead to a quicker 
search. Indeed, if the function to be minimized really is quadratic, then this procedure will 
find the minimum exactly in d steps. These new axes are termed conjugate directions. 
Once we have determined the direction v in which we are to move, we can adopt a ""line 
search"" procedure to decide how far to move; that is, we simply apply one of the one-
dimensional methods discussed above, in the chosen direction. Often a fast and 
approximate method of choosing the size of the univariate steps may be sufficient in 
multivariate optimization problems, since the choice of direction itself will itself be based 
on various approximations. 
The methods described so far are all based on, or at least derived from, finding the local 
direction for the ""best"" step and then moving in that direction. The simplex search 

, where 

is the estimate of  H-1 at iteration i. The BFGS (Broyden-

method (not to be confused with the simplex algorithm of linear programming) evaluates 
d + 1 points arranged in a simplex (a ""hypertetrahedron"") in the d-dimensional parameter 
space and uses these to define the best direction in which to step. To illustrate, let us 
take the case of d = 2. The function is evaluated at three (= d + 1 when d = 2) points, 
arranged as the vertices of an equilateral triangle, which is the simplex in two 
dimensions. The triangle is then reflected in the side opposite the vertex with the largest 
function value. This gives a new vertex, and the process is repeated using the triangle 
based on the new vertex and the two that did not move in the previous reflection. This is 
repeated until oscillation occurs (the triangle just flips back and forth, reflecting about the 
same side). When this happens the sides of the triangle are halved, and the process 
continues.  

This basic simplex search method has been extended in various ways. For example, the 
Nelder and Mead variant allows the triangle to increase as well as decrease in size so as 
to accelerate movement in appropriate situations. There is evidence to suggest that, 
despite its simplicity, this method is comparable to the more sophisticated methods 
described above in high-dimensional spaces. Furthermore, the method does not require 
derivatives to be calculated (or even to exist). 
A related search method, called pattern search, also carries out a local search to 
determine the direction of step. If the step reduces the score function, then the step size 
is increased. If it does poorly, the step size is decreased (until it hits a minimum value, at 
which point the search is terminated). (The word pattern in the phrase pattern search has 
nothing to do with the patterns of data mining as discussed earlier.) 

8.3.6 Constrained Optimization 
Many optimization problems involve constraints on the parameters. Common examples 
include problems in which the parameters are probabilities (which are constrained to be 
positive and to sum to 1), and models that include the variance as a parameter (which 
must be positive). Constraints often occur in the form of inequalities, requiring that a 
parameter ? satisfy c1 = ? = c2, for example, with c1 and c2 being constants, but more 
complex constraints are expressed as functions: g (?1,...,?d) = 0 for example. 
Occasionally, constraints have the form of equalities. In general, the region of parameter 
vectors that satisfy the constraints is termed the feasible region. 
Problems that have linear constraints and convex score functions can be solved by 
methods of mathematical programming. For example, linear programming methods have 
been used in supervised classification problems, and quadratic programming is used in 
suport vector machines. Problems in which the score functions and constraints are 
nonlinear are more challenging. 
Sometimes constrained problems can be converted into unconstrained problems. For 
example, if the feasible region is restricted to positive values of the parameters (?1,...,?d), 
we could, instead, optimize over (f 1,...,f d), where 
complicated) transformations can remove constraints of the form c1 = ? = c2. 
A basic strategy for removing equality constraints is through Lagrange multipliers. A 
necessary condition for ? to be a local minimum of the score function S = S (?) subject to 
constraints hj (?) = 0, j = 1,..., m, is that it satisfies ?  S(?) + ? j ?j? hj (?) = 0, for some 
scalars, ?j. These equations and the constraints yield a system of (d + m) simultaneous 
(nonlinear) equations, that can be solved by standard methods (often by using a least 
squares routine to minimize the sum of squares of the left hand sides of the (d + m) 
equations). These ideas are extend to inequality constraints in the Kuhn-Tucker 
conditions (see Furt her Reading). 

, i = 1,..., d. Other (rather more 

Unconstrained optimization methods can be modified to yield constrained methods. For 
example, penalties can be added to the score function so that the parameter estimates 
are repelled if they should approach boundaries of the feasible region during the 
optimization process. 

 
 

8.4 Optimization with Missing Data: The EM Algorithm 

In this section we consider the special but important problem of maximizing a likelihood 
score function when some of the data are missing, that is, there are variables in our data 
set whose values are unobserved for some of the cases. It turns out that a large number 
of problems in practice can effectively be modeled as missing data problems. For 
example, measurements on medical patients where for each patient only a subset of test 
results are available, or application form data where the responses to some questions 
depends on the answers to others. 
More generally, any model involving a hidden variable (i.e., a variable that cannot be 
directly observed) can be modeled as a missing data problem, in which the values of this 
variable are unknown for all n objects or individuals. Clustering is a specific example; in 
effect we assume the existence of a discrete-valued hidden cluster variable  C taking 
values {c1,..., ck} and the goal is to estimate the values of C (that is, the cluster labels) for 
each observation x(i), 1 = i = n. 
The Expectation-Maximization (EM) algorithm is a rather remarkable algorithm for 
solving such missing data problems in a likelihood context. Specifically, let D = {x(1),..., 
x(n)} be a set of n observed data vectors. Let H = {z(1),..., z(n)} represent a set of n 
values of a hidden variable  Z, in one-to-one correspondence with the observed data 
points D; that is, z(i) is associated with data point x(i). We can assume Z to be discrete 
(this is not necessary, but is simply convenient for our description of the algorithm), in 
which case we can think of the unknown z(i) values as class (or cluster) labels for the 
data, that are hidden.  

We can write the log-likelihood of the observed data as 

(8.15)  

where the term on the right indicates that the observed likelihood can be expressed as 
the likelihood of both the observed and hidden data, summed over the hidden data 
values, assuming a probabilistic model in the form p(D, H|?) that is parametrized by a set 
of unknown parameters ?. Note that our optimization problem here is doubly complicated 
by the fact that both the parameters ? and the hidden data  H are unknown. 
Let Q(H) be any probability distribution on the missing data H. We can then write the log-
likelihood in the following fashion: 

(8.16)  

where the inequality is a result of the concavity of the log function (known as Jensen's 
inequality). 
The function F(Q, ?) is a lower bound on the function we wish to maximize (the likelihood 
l(?)). The EM algorithm alternates between maximizing F with respect to the distribution 
Q with the parameters ? fixed, and then maximizing F with respect to the parameters ? 
with the distribution Q = p(H) fixed. Specifically: 

(8.17)  

(8.18)  

 

It is straightforward to show that the maximum in the E-step is achieved when Qk+1 = 
p(H|D, ?k), a term that can often be calculated explicitly in a relatively straightforward 
fashion for many models. Furthermore, for this value of Q the bound becomes tight, i.e., 
the inequality becomes an equality above and l(?k) = F(Q, ?k).  

The maximization in the M-step reduces to maximizing the first term in F (since the 
second term does not depend on ?), and can be written as 

(8.19)  

This expression can also fortunately often be solved in closed form. 
Clearly the E and M steps as defined cannot decrease l(?) at each step: at the beginning 
of the M-step we have that l(?k) = F (Qk+1, ?k) by definition, and the M-step further adjusts 
? to maximize this F. 
The EM steps have a simple intuitive interpretation. In the E-step we estimate the 
distribution on the hidden variables Q, conditioned on a particular setting of the 
parameter vector ?k. Then, keeping the Q function fixed, in the M-step we choose a new 
set of parameters ?k+1 so as to maximize the expected log-likelihood of observed data 
(with expectation defined with respect to Q = p(H)). In turn, we can now find a new Q 
distribution given the new parameters ?k+1, then another application of the M-step to get 
?k+2, and so forth in an iterative manner. As sketched above, each such application of the 
E and M steps is guaranteed not to decrease the log-likelihood of the observed data, and 
under fairly general conditions this in turn implies that the parameters ? will converge to 
at least a local maximum of the log-likelihood function. 
To specify an actual algorithm we need to pick an initial starting point (for example, start 
with either an initial randomly chosen Q or ?) and a convergence detection method (for 
example, detect when any of Q, ?, or l(?) do not change appreciably from one iteration to 
the next). The EM algorithm is essentially similar to a form of local hill-climbing in 
multivariate parameter space (as discussed in earlier sections of this chapter) where the 
direction and distance of each step is implicitly (and automatically) specified by the E and 
M steps. Thus, just as with hill-climbing, the method will be sensitive to initial conditions, 
so that different choices of initial conditions can lead to different local maxima. Because 
of this, in practice it is usually wise to run EM from different initial conditions (and then 
choose the highest likelihood solution) to decrease the probability of finally settling on a 
relatively poor local maximum. The EM algorithm can converge relatively slowly to the 
final parameter values, and for example, it can be combined with more traditional 
optimization techniques (such as Newton-Raphson) to speed up convergence in the later 
iterations. Nonetheless, the standard EM algorithm is widely used given the broad 
generality of the framework and the relative ease with which an EM algorithm can be 
specified for many different problems. 
The computational complexity of the EM algorithm is dictated by both the number of 
iterations required for convergence and the complexity of each of the E and M steps. In 
practice it is often found that EM can converges relatively slowly as it approaches a 
solution, although the actual rate of convergence can depend on a variety of different 
factors. Nonetheless, for simple models at least, the algorithm can often converge to the 
general vicinity of the solution after only a few (say 5 or 10) iterations. The complexity of 
the E and M steps at each iteration depends on the nature of the model being fit to the 
data (that is, the likelihood function  p(D, H|?)). For many of the simpler models (such as 
the mixture models discussed below) the E and M steps need only take time linear in n, 
i.e., each data point need only be visited once during each iteration. 
Examples 8.1 and 8.2 illustrate the application of the EM algorithm in estimating the 
parameters of a normal mixture and a Poisson mixture (respectively) for one-dimensional 
measurements x. In each case, the data are assumed to have arisen from a mixture of K 
underlying component distributions (normal and Poisson, respectively). However, the 
component labels are unobserved, and we do not know which component each data 
point arose from. We will discuss the estimation of these types of mixture models in more 
detail again in chapter 9. 

Example 8.1  

 

We wish to fit a normal mixture distribution 
(8.20)  

where µk is the mean of the kth component, sk is the standard deviation of the kth 
component, and p k is the prior probability of a data point belonging to component k (? K p k 
= 1). Hence, for this problem, we have that the parameter vector ? = {p1,...,pK, 
µ1,...,µK,s 1,...,s K}. Suppose for the moment that we knew the values of ?. Then, the 
probability that an object with measurement vector  x arose from the kth class would be 
(8.21)  

This is the basic E-step. 
From this, we can then estimate the values of pk, µk, and sk as 
(8.22)  

 

(8.23)  

(8.24)  

where the summations are over the n points in the data set. These three equations are the 
M-steps. This set of equations leads to an obvious iterative procedure. We pick starting 
values for µk, s k, and p k, plug them into equation 8.21 to yield estimates 
estimates in equations 8.22, 8.23 and 8.24, and then iterate back using the updated 
estimates of µk, s k, and p k, cycling around until a convergence criterion (usually 
convergence of the likelihood or model parameters to a stable point) has been satisfied. 
Note that equations 8.23 and 8.24 are very similar to those involved in estimating the 
parameters of a single normal distribution, except that the contribution of each point are 
split across the separate components, in proportion to the estimated size of that component 
at the point. In essence, each data point is weighted by the probability that it belongs to that 
component. If we actually knew the class labels the weights for data point x(i) would be 1 
for the class to which the data point belongs and 0 for the other K - 1 components (in the 
standard manner). 

, use these 

 

 
Example 8.2  

 
The Poisson model can be used to model the rate at which individual events occur, for 
example, the rate at which a consumer uses a telephone calling card. For some cards, 
there might be multiple individuals (within a single family for example) on the same account 
(with copies of the card), and in theory each may have a different rate at which they use it 
(for example, the teenager uses the card frequently, the father much less frequently, and 
so on). Thus, with K individuals, we would observe event data generated by a mixture of K 
Poisson processes: 
(8.25)  

the equations for the iterative estimation procedure analogous to example 8.4 take the form 
(8.26)  

(8.27)  

(8.28)  

 
 

 

8.5 Online and Single-Scan Algorithms 
All of the optimization methods we have discussed so far implicitly assume that the data 
are all resident in main memory and, thus, that each data point can be easily accessed 
multiple times during the course of the search. For very large data sets we may be 
interested in optimization and search algorithms that see each data point only once at 
most. Such algorithms may be referred to as online or single-scan and clearly are much 
more desirable than ""multiple-pass"" algorithms when we are faced with a massive data 
set that resides in secondary memory (or further away). 
In general, it is usually possible to modify the search algorithms above directly to deal 
with data points one at a time. For example, consider simple gradient descent methods 
for parameter optimization. As discussed earlier, for the ""offline"" (or batch) version of the 
algorithm, one finds the gradient g(?) in parameter space, evaluates it at the current 
location ?k, and takes a step proportional to distance ? in that direction. Now moving in 
the direction of the gradient g(?) is only a heuristic, and it may not necessarily be the 
optimal direction. In practice, we may do just as well (at least, in the long run) if we move 
in a direction approximating that of the gradient. This idea is used in practice in an online 
approximation to the gradient, that uses the current best estimate based both on the 
current location and the current and (perhaps) ""recent"" data points. The online estimates 
can be viewed as stochastic (or ""noisy"") estimates of the full gradient estimate that would 
be produced by the batch algorithm looking at all of the data points. There exists a 
general theory in statistics for this type of search technique, known as stochastic 
approximation, which is beyond the scope of this text but that is relevant to online 
parameter estimation. Indeed, in using gradient descent to find weight parameters for 
neural networks (for example) stochastic online search has been found to be useful in 
practice. The stochastic (data-driven) nature of the search is even thought to sometimes 
improve the quality of the solutions found by allowing the search algorithm to escape 
from local minima in a manner somewhat reminiscent of simulated annealing (see 
below). 

More generally, the more sophisticated search methods (such as multivariate methods 
based on the Hessian matrix) can also be implemented in an online manner by 
appropriately defining online estimators for the required search directions and step-sizes. 

8.6 Stochastic Search and Optimization Techniques 

The methods we have presented thus far on model search and parameter optimization 
rely heavily on the notion of taking local greedy steps near the current state. The main 
disadvantage is the inherent myopia of this approach. The quality of the solution that is 
found is largely a function of the starting point. This means that, at least with a single 
starting position, there is the danger that the minimum (or maximum) one finds may be a 
nonglobal local optimum. Because of this, methods have been developed that adopt a 
more global view by allowing large steps away from the current state in a 
nondeterministic (stochastic) manner. Each of the methods below is applicable to either 
the parameter optimization or model search problem, but for simplicity we will just focus 
here on model search in a state-space. 

§ Genetic Search: Genetic algorithms are a general set of heuristic search 

techniques based on ideas from evolutionary biology. The essential idea is 
to represent states (models in our case) as chromosomes (often encoded 
as binary strings) and to ""evolve"" a population of such chromosomes by 
selectively pairing chromosomes to create new offspring. Chromosomes 
(states) are paired based on their ""fitness"" (their score function) to 
encourage the fitter chromosomes to survive from one generation to the 
next (only a limited number of chromosomes are allowed to survive from 
one generation to the next). There are many variations on this general 
theme, but the key ideas in genetic search are: 

o  Maintenance of a set of candidate states (chromosomes) 

rather than just a single state, allowing the search algorithm 
to explore different parts of the state space simultaneously 

 
 

o  Creating new states to explore based on combinations of 
existing states, allowing in effect the algorithm to ""jump"" to 
different parts of the state-space (in contrast to the local 
improvement search techniques we discussed earlier) 

Genetic search can be viewed as a specific type of heuristic, so it may work well 
on some problems and less well on others. It is not always clear that it provides 
better performance on specific problems than a simpler method such as local 
iterative improvement with random restarts. A practical drawback of the 
approach is the fact that there are usually many algorithm parameters (such as 
the number of chromosomes, specification of how chromosomes are combined, 
and so on) that must be specified and it may not be clear what the ideal settings 
are for these parameters for any given problem. 

§  Simulated Annealing: Just as genetic search is motivated by ideas 

from evolutionary biology, the approach in simulated annealing is 
motivated by ideas from physics. The essential idea is to not to restrict 
the search algorithm to moves in state-space that decrease the score 
function (for a score function we are trying to minimize), but to also 
allow (with some probability) moves that can increase the score 
function. In principle, this allows a search algorithm to escape from a 
local minimum. The probability of such non-decreasing moves is set to 
be quite high early in the process and gradually decreased as the 
search progresses. The decrease in this probability is analogous to the 
process of gradually decreasing the temperature in the physical 
process of annealing a metal with the goal of obtaining a low-energy 
state in the metal (hence the name of the method). 

For the search algorithm, higher temperatures correspond to a greater 
probability of large moves in the parameter space, while lower temperatures 
correspond to greater probability of only small moves that decrease the function 
being taken. Ultimately, the temperature schedule reduces the temperature to 
zero, so that the algorithm by then only moves to states that decrease the score 
function. Thus, at this stage of the search, the algorithm will inevitably converge 
to a point at which no further decrease is possible. The hope is that the earlier 
(more random) moves have led the algorithm to the deepest ""basin"" in the score 
function surface. In fact, one of the appeals of the approach is that it can be 
mathematically proved that (under fairly general conditions) this will happen if 
one is using the appropriate temperature schedule. In practice, however, there is 
usually no way to specify the optimal temperature schedule (and the precise 
details of how to select the possible nondecreasing moves) for any specific 
problem. Thus, the practical application of simulated annealing reduces to (yet 
another) heuristic search method with its own set of algorithm parameters that 
are often chosen in an ad hoc manner. 
We note in passing that the idea of stochastic search is quite general, where the 
next set of parameters or model is chosen stochastically based on a probability 
distribution on the quality of neighboring states conditioned on the current state. 
By exploring state-space in a stochastic fashion, a search algorithm can in 
principle spend more time (on average) in the higher quality states and build up 
a model on the distribution of the quality (or score) function across the state-
space. This general approach has become very popular in Bayesian statistics, 
with techniques such as Monte Carlo Markov Chain (MCMC) being widely used. 
Such methods can be viewed as generalizations of the basic simulated 
annealing idea, and again, the key ideas originated in physics. The focus in 
MCMC is to find the distribution of scores in parameter or state-space, weighted 
by the probability of those parameters or models given the data, rather than just 
finding the location of the single global minimum (or maximum). 

It is difficult to make general statements about the practical utility of methods such as 
simulated annealing and genetic algorithms when compared to a simpler approach such 
as iterative local improvement with random restarts, particularly if we want to take into 
account the amount of time taken by each method. It is important when comparing 
different search methods to compare not only the quality of the final solution but also the 
computational resources expended to find that solution. After all, if time is unlimited, we 

can always use exhaustive enumeration of all models to find the global optimum. It is fair 
to say that since stochastic search techniques typically involve considerable extra 
computation and overhead (compared to simpler alternatives) that they tend to be used 
in practice on specialized problems involving relatively small data sets, and are often not 
practical from a computational viewpoint for very large data sets. 

8.7 Further Reading 
Papadamitriou and Steiglitz (1982) is a classic (although now a little outdated) text on 
combinatorial optimization. Cook et al. (1998) is an authoritative and more recent text on 
the topic. Pearl (1984) deals specifically with the topic of search heuristics. The CN2 
rule-finding algorithm of  Clark and Niblett (1989) is an example of beam search in action. 
Press et al. (1988) is a useful place to start for a general introduction and some sound 
practical advice on numerical optimization techniques, particularly chapters 9 and 10. 
Other texts such as Gill, Murray, and Wright (1981) and Fletcher (1987) are devoted 
specifically to optimization and provide a wealth of practical advice as well as more 
details on specific methods. Luenberger (1984) and Nering and Tucker (1993) discuss 
linear programming and related constrained optimization techniques in detail. 
Mangasarian (1997) describes the application of constrained optimization techniques to 
a variety problems in data mining, including feature selection, clustering, and robust 
model selection. Bradley, Fayyad, and Mangasarian (1999) contain further discussion 
along these lines. 
Thisted (1988) is a very useful and comprehensive reference on the application of 
optimization and search methods specifically to statistical problems. Lange (1999) is 
more recent text on the same topic (numerical methods for statistical optimization) with a 
variety of useful techniques and results. Bishop (1995, chapter 7) has an extensive and 
well-written account of optimization in the context of parameter estimation for neural 
networks, with specific reference to online techniques. 
The seminal paper on the EM algorithm is Dempster, Laird, and Rubin (1977) which first 
established the general theoretical framework for the procedure. This paper had been 
preceded by almost a century of work in the general spirit of EM, including Newcomb 
(1886) and McKendrick (1926). The work of Baum and Petrie (1966) was an early 
development of a specific EM algorithm in the context of hidden Markov models. 
McLachlan and Krishnan (1998) provide a comprehensive treatment of the many recent 
advances in the theory and application of EM. Meilijson (1989) introduced a general 
technique for speeding up EM convergence, and Lange (1995) discusses the use of 
gradient methods in an EM context. A variety of computational issues concerning EM in 
a mixture modeling context are discussed in Redner and Walker (1984). Neal and Hinton 
(1998) discuss online versions of EM that can be particularly useful in the context of 
massive data sets. 
Online learning in a regression context can be viewed theoretically as a special case of 
the general technique of stochastic approximation of  Robbins and Monro (1951)—see 
Bishop (1995, chapter 2) for a discussion in the context of neural networks. 
Mitchell (1997) is a comprehensive introduction to the ideas underlying genetic 
algorithms. Simulated annealing was introduced by Kirkpatrick, Gelatt, and Vecchi 
(1983) but has its origins in much earlier work in statistical physics. Van Laarhoven and 
Aarts (1987) provide a general overview of the field. Brooks and Morgan (1995) contains 
a systematic comparison between simulated annealing and more conventional 
optimization techniques (such as Newton-based methods), as well as hybrids of the two. 
They conclude that hybrid methods appear better than either traditional methods or 
simulated annealing on their own. Gilks, Richardson, and Spiegelhalter (1996) is an 
edited volume containing a good sampling of recent work in statistics using stochastic 
search and MCMC methods in a largely Bayesian context. 

Chapter 9: Descriptive Modeling 

 
 

 
 

9.1 Introduction 
In earlier chapters we explained what is meant, in the context of data mining, by the 
terms model and pattern. A model is a high-level description, summarizing a large 
collection of data and describing its important features. Often a model is global in the 
sense that it applies to all points in the measurement space. In contrast, a pattern is a 
local description, applying to some subset of the measurement space, perhaps showing 
how just a few data points behave or characterizing some persistent but unusual 
structure within the data. Examples would be a mode (peak) in a density function or a 
small set of outliers in a scatter plot. 

Earlier chapters distinguished between models and patterns, and also between 
descriptive and predictive models. A descriptive model presents, in convenient form, the 
main features of the data. It is essentially a summary of the data, permitting us to study 
the most important aspects of the data without their being obscured by the sheer size of 
the data set. In contrast, a predictive model has the specific objective of allowing us to 
predict the value of some target characteristic of an object on the basis of observed 
values of other characteristics of the object. 
This chapter is concerned with descriptive models, presenting outlines of several 
algorithms for finding descriptive models that are important in data mining contexts. 
Chapters 10 and 11 will describe predictive models, and chapter 13 will describe 
descriptive patterns. 
We have already noted that data mining is usually concerned with building empirical 
models—models that are not based on some underlying theory about the mechanism 
through which the data arose, but that are simply a description of the observed data. The 
fundamental objective is to produce insight and understanding about the structure of the 
data, and to enable us to see its important features. Beyond this, of course, we hope to 
discover unsuspected structure as well as structure that is interesting and valuable in 
some sense. A good model can also be thought of as generative in the sense that data 
generated according to the model will have the same characteristics as the real data 
from which the model was produced. If such synthetically generated data have features 
not possessed by the original data, or do not possess features of the original data (such 
as, for example, correlations between variables), then the model is a poor one: it is 
failing to summarize the data adequately. 
This chapter focuses on specific techniques and algorithms for fitting descriptive models 
to data. It builds on many of the ideas introduced in earlier chapters: the principles of 
uncertainty (chapter 4), decomposing data mining algorithms into basic components 
(chapter 5), and the general principles underlying model structures, score functions, and 
parameter and model search (chapters 6, 7 and 8, respectively). 

There are, in fact, many different types of model, each related to the others in various 
ways (special cases, generalizations, different ways of looking at the same structure, and 
so on). We cannot hope to examine all possible models types in detail in a single 
chapter. Instead we will look at just some of the more important types, focusing on 
methods for density estimation and cluster analysis in particular. The reader is alerted to 
the fact that are other descriptive techniques in the literature (techniques such as 
structural equation modeling or factor analysis for example) that we do not discuss here. 
One point is worth making at the start. Since we are concerned here with global models, 
with structures that are representative of a mass of objects in some sense, then we do 
not need to worry about failing to detect just a handful of objects possessing some 
property; that is, in this chapter we are not concerned with patterns. This is good news 
from the point of view of scalability: as we discussed in chapter 4, we can, for example, 
take a (random) sample from the data set and still hope to obtain good results. 

 
 

9.2 Describing Data by Probability Distributions and 
Densities 

9.2.1 Introduction 
For data that are drawn from a larger population of values, or data that can be regarded 
as being drawn from such a larger population (for example, because the measurements 
have associated measurement error), describing data in terms of their underlying 
distribution or density function is a fundamental descriptive strategy. Adopting our usual 
notation of a  p-dimensional data matrix, with variables X1, ..., Xp, our goal is to model the 
joint distribution or density ƒ(X1, ..., Xp) as first encountered in chapter 4. For 
convenience, we will refer to ""densities"" in this discussion, but the ideas apply to discrete 
as well as to continuous X variables. 
The joint density in a certain sense provides us with complete information about the 
variables X1, ..., Xp. Given the joint density, we can answer any question about the 
relationships among any subset of variable; for example, are X3 and X7 independent? 
Thus, we can answer questions about the conditional density of some variables given 
others; for example, what is the probability distribution of X3 given the value of X7, ƒ(x3 | 
x7)?. 
There are many practical situations in which knowing the joint density is useful and 
desirable. For example, we may be interested in the modes of the density (for real-
valued Xs). Say we are looking at the variables income and credit-card spending for a 
data set of n customers at a particular bank. For large n, in a scatterplot we will just see 
a mass of points, many overlaid on top of each other. If instead we estimate the joint 
density ƒ(income, spending) (where we have yet to describe how this would be done), 
we get a density function of the two dimensions that could be plotted as a contour map 
or as a three-dimensional display with the density function being plotted in the third 
dimension. The estimated joint density would in principle impart useful information about 
the underlying structure and patterns present in the data. For example, the presence of 
peaks (modes) in the density function could indicate the presence of subgroups of 
customers. Conversely, gaps, holes, or valleys might indicate regions where (for one 
reason or another) this particular bank had no customers. And the overall shape of the 
density would provide an indication of how income and spending are related, for this 
population of customers. 
A quite different example is given by the problem of generating approximate answers to 
queries for large databases (also known as query selectivity estimation). The task is the 
following: given a query (that is, a condition that the observations must satisfy), estimate 
the fraction of rows that satisfy this condition (the selectivity of the query). Such 
estimates are needed in query optimization in database systems, and a single query 
optimization task might need hundreds of such estimates. If we have a good 
approximation for the joint distribution of the data in the database, we can use it to obtain 
approximate selectivities in a computationally efficient manner.  

Thus, the joint density is fundamental and we will need to find ways to estimate and 
conveniently summarize it (or its main features). 

9.2.2 Score Functions for Estimating Probability Distributions and Densities 
As we have noted in earlier chapters, the most common score function for estimating the 
parameters of probability functions is the likelihood (or, equivalently by virtue of the 
monotonicity of the log transform, the log-likelihood). As a reminder, if the probability 
function of random variables X is ƒ(x; ?); where ? are the parameters that need to be 
estimated, then the log-likelihood is log ƒ(D|?) where  D = {x(1), ..., x(n)} is the observed 
data. Making the common assumption that that the separate rows of the data matrix 
have arisen independently, this becomes 

(9.1)  

If ƒ has a simple functional form (for example, if it has the form of the single univariate 
distributions outlined in the appendix) then this score function can usually be minimized 

explicitly, producing a closed form estimator for the parameters ?. However, if ƒ is more 
complex, iterative optimization methods may be required. 
Despite its importance, the likelihood may not always be an adequate or appropriate 
measure for comparing models. In particular, when models of different complexity (for 
example, Normal densities with covariance structures parameterized in terms of different 
numbers of parameters) are compared then difficulties may arise. For example, with a 
nested series of models in which higher-level models include lower-level ones as special 
cases, the more flexible higher level models will always have a greater likelihood. This 
will come as no surprise. The likelihood score function is a measure of how well the 
model fits the data, and more flexible models necessarily fit the data no worse (and 
usually better) than a nested less flexible model. This means that likelihood will be 
appropriate in situations in which we are using it as a score function to summarize a 
complete body of data (since then our aim is simply closeness of fit between the 
simplifying description and the raw data) but not if we are using it to select a single 
model (from a set of candidate model structures) to apply it to a sample of data from a 
larger population (with the implicit aim being to generalize beyond the data actually 
observed). In the latter case, we can solve the problem by modifying the likelihood to 
take the complexity of the model into account. We discussed this in detail in chapter 7, 
where we outlined several score functions based on adding an extra term to the 
likelihood that penalizes model complexity. For example, the BIC (Bayesian Information 
Criterion) score function was defined as: 

(9.2)  

where dk is the number of parameters in model Mk and 
the negative log-likelihood (achieved at 
Alternatively, also as discussed in chapter 7, we can calculate the score using an 
independent sample of data, producing an ""out -of-sample"" evaluation. Thus the 
validation log-likelihood (or ""holdout log-likelihood"") is defined as 

). 

is the minimizing value of 

(9.3)  

where the points x are from the validation data set D?, the parameters  were estimated 
(for example, via maximum likelihood) on the disjoint training data Dt = D \ D?, and there 
are K models under consideration. 

9.2.3 Parametric Density Models 
We pointed out, in chapter 6, that there are two general classes of density function 
model structures: parametric and nonparametric. Parametric models assume a particular 
functional form (usually relatively simple) for the density function, such as a uniform 
distribution, a Normal distribution, an exponential distribution, a Poisson distribution, and 
so on (see Appendix A for more details on some of these common densities and 
distributions). These distribution functions are often motivated by underlying causal 
models of generic data-generating mechanisms. Choice of what might be an appropriate 
density function should be based on knowledge of the variable being measured (for 
example, the knowledge that a variable such as income can only be positive should be 
reflected in the choice of the distribution adopted to model it). Parametric models can 
often be characterized by a relatively small number of parameters. For example, the p-
dimensional Normal distribution is defined as 

(9.4)  

where S is the p × p covariance matrix of the X variables, |S| is the determinant of this 
matrix, and µ is the p-dimensional vector mean of the X s. The parameters of the model 
are the mean vector and the covariance matrix (thus, p + p(p + 1)/2 parameters in all). 
The multivariate Normal (or Gaussian) distribution is particularly important in data 
analysis. For example, because of the central limit theorem, under fairly broad 
assumptions the mean of N independent random variables (each from any distribution) 
tends to have a Normal distribution. Although the result is asymptotic in nature, even for 
relatively small values of N (e.g., N = 10) the sample mean will typically be quite Normal. 
Thus, if a measurement can be thought of as being made up of the sum of multiple 
relatively independent causes, the Normal model is often a reasonable model to adopt. 

The functional form of the multivariate Normal model in equation 9.4 is less formidable 
than it looks. The exponent, (x - µ)TS-1(x - µ), is a scalar value (a quadratic form) known 
as the Mahalanobis distance between the data point x and the mean µ, denoted as 

. This is a generalization of standard Euclidean distance that takes into account 
(through the covariance matrix S) correlations in p-space when distance is calculated. 
The denominator in equation 9.4 is simply a normalizing constant (call it C) to ensure 
that the function integrates to 1 (that is, to ensure it is a true probability density function). 
Thus, we can write our Normal model in significantly simplified form as 

(9.5)  

If we were to plot (say for p = 2) all of the points x that have the same fixed values of 
, (or equivalently, all of the points x that like on iso-density contours ƒ(x) = c for 

. Figure 9.1 provides a simple 

some constant c), we would find that they trace out an ellipse in 2-space (more 
generally, a hyperellipsoid in p-space), where the ellipse is centered at µ. That is, the 
contours describing the multivariate Normal distribution are ellipsoidal, with height falling 
exponentially from the center as a function of 
illustration in two dimensions. The eccentricity and orientation of the elliptical contours is 
determined by the form of S. If S is a multiple of the identity matrix (all variables have the 
same variance and are uncorrelated) then the contours are circles. If S is a diagonal 
matrix, but with different variance terms on the diagonals, then the axes of the elliptical 
contours are parallel to the variable axes and the contours are elongated along the 
variable axes with greater variance. Finally, if some of the variables are highly correlated, 
the (hyper) elliptical contours will tend to be elongated along vectors defined as linear 
combinations of these variables. In  figure 9.1, for example, the two variables X1 and X2 
are highly correlated, and the data are spread out along the line defined by the linear 
combination X1 + X2. 

 

Figure 9.1: Illustration of the Density Contours for a Two-Dimensional Normal Density 
Function, With Mean [3, 3] and Covariance Matrix 
Simulated From this Density.  

. Also Shown are 100 Data Points 

For high-dimensional data (large p) the number of parameters in the Normal model will 
be dominated by the O(p2) covariance terms in the covariance matrix. In practice we may 
not want to model all of these covariance terms explicitly, since for large p and finite n 
(the number of data points available) we may not get very reliable estimates of many of 
the covariance terms. We could, for example, instead assume that the variables are 
independent, which is equivalent in the Normal case to assuming that the covariance 
matrix has a diagonal structure (and, hence, has only p parameters). (Note that if we 
assume that S is diagonal it is easy to show that the p-dimensional multivariate Normal 
density factors into a product of  p univariate Normal distributions, a necessary and 
sufficient condition for independence of the p variables.) An even more extreme 
assumption would be to assume that S = s2 I, where I is the identity matrix—that is, that 
the data has the same variance for all p variables as well as being independent. 
Independence is a highly restrictive assumption. A less restrictive assumption would be 
that the covariance matrix had a block diagonal structure: we assume that there are 
groups of variables (the ""blocks"") that are dependent, but that variables are independent 

across the groups. In general, all sorts of assumptions may be possible, and it is 
important, in practice, to test the assumptions. In this regard, the multivariate Normal 
distribution has the attractive property that two variables are conditionally independent, 
given the other variables, if and only if the corresponding element of the inverse of the 
covariance matrix is zero. This means that the inverse covariance matrix S-1 reveals the 
pattern of relationships between the variables. (Or, at least, it does in principle: in fact, of 
course, it will be necessary to decide whether a small value in the inverse covariance 
matrix is sufficiently small to be regarded as zero.) It also means that we can 
hypothesize a graphical model in which there are no edges linking the nodes 
corresponding to variables that have a small value in this inverse matrix (we discussed 
graphical models in chapter 6). 
It is important to test the assumptions made in a model. Specific statistical goodness-of-
fit tests are often available, but even simple eyeballing can be revealing. The simple 
histogram, or one of its more sophisticated cousins outlined in chapter 3, can 
immediately reveal constraints on permissible ranges (for example, the non-negativity of 
income noted above), lack of symmetry, and so on. If the assumptions are not justified, 
then analysis of some transformation of the raw scores may be appropriate. 
Unfortunately, there are no hard-and-fast rules about whether or not an assumption is 
justified. Slight departures may well be unimportant—but it will depend on the problem. 
This is part of the art of data mining. In many situations in which the distributional 
assumptions break down we can obtain perfectly legitimate estimates of parameters, but 
statistical tests are invalid. For example, we can physically fit a regression model using 
the least squares score function, whether or not the errors are Normally distributed, but 
hypothesis tests on the estimated parameters may well not be accurate. This might 
matter during the model building process—in helping to decide whether or not to include 
a variable—but it may not matter for the final model. If the final model is good for its 
purpose (for example, predictive accuracy in regression) that is sufficient justification for 
it to be adopted. 
Fitting a p-dimensional Normal model is quite easy. Maximum likelihood (or indeed 
Bayesian) estimation of each of the means and the covariance terms can be defined in 
closed form (as discussed in chapter 4), and takes only O(n) steps for each parameter, 
so O(np2) in total. Other well-known parametric models (such as those defined in the 
appendix) also usually possess closed-form parameter solutions that can be calculated 
by a single pass through the data. 
The Normal model structure is a relatively simple and constrained model. It is unimodal 
and symmetric about the axes of the ellipse. It is parametrized completely in terms of its 
mean vector and covariance matrix. However, it follows from this that nonlinear 
relationships cannot be captured, nor can any form of multimodality or grouping. The 
mixture models of the next section provide a flexible framework for modeling such 
structures. The reader should also note that although the Normal model is probably the 
most widely-used parametric model in practice, there are many other density functions 
with different ""shapes"" that are very useful for certain applications (e.g., the exponential 
model, the log-normal, the Poisson, the Gamma: the interested reader is referred to the 
appendix). The multivariate t-distribution is similar in form to the multivariate Normal but 
allows for longer tails, and is found useful in practical problems where more data can 
often occur in the tails than a Normal model would predict. 

9.2.4 Mixture Distributions and Densities 
In chapter 6 we saw how simple parametric models could be generalized to allow 
mixtures of components—that is, linear combinations of simpler distributions. This can 
be viewed as the next natural step in complexity in our discussion of density modeling: 
namely, the generalization from parametric distributions to weighted linear combinations 
of such functions, providing a general framework for generating more complex density 
and distribution models as combinations of simpler ones. Mixture models are quite useful 
in practice for modeling data when we are not sure what specific parametric form is 
appropriate (later in this chapter we will see how such mixture models can also be used 
for the task of clustering). 
It is quite common in practice that a data set is heterogeneous in the sense that it 
represents multiple different subpopulations or groups, rather than one single 
homogeneous group. Heterogeneity is particularly prevalent in very large data sets, 

where the data may represent different underlying phenomena that have been collected 
to form one large data set. To illustrate this point, consider  figure 3.1 in chapter 3. This is 
a histogram of the number of weeks owners of a particular credit card used that card to 
make supermarket purchases in 1996. As we pointed out there, the histogram appears 
to be bimodal, with a large and obvious mode to the left and a smaller, but nevertheless 
possibly important mode to the right. An initial stab at a model for such data might be 
that it follows a Poisson distribution (despite being bounded above by 52), but this would 
not have a sufficiently heavy tail and would fail to pick up the right-hand mode. Likewise, 
a binomial model would also fail to follow the right-hand mode. Something more 
sophisticated and flexible is needed. An obvious suggestion here is that the empirical 
distribution should be modeled by a theoretical distribution that has two components. 
Perhaps there are two kinds of people: those who are unlikely to use their credit card in a 
supermarket and those who do so most weeks. The first set of people could be modeled 
by a Poisson distribution with a small probability. The second set could be modeled by a 
reversed Poisson distribution with its mode around 45 or 46 weeks (the position of the 
mode would be a parameter to be estimated in fitting the model to the data). This leads 
us to an overall distribution of the form 

(9.6)  

is the 

gives the 

where x is the value of the random variable X taking values between 0 and 52 (indicating 
how many weeks a year a person uses their card in a supermarket), and ?1 > 0, ?2 > 0 
are parameters of the two component Poisson models. Here p is the probability that a 
person belongs to the first group, and, given this, the expression 
probability that this person will use their card  x times in the year. Likewise, 1 - p is the 
probability that this person belong to the second group and 
conditional probability that such a person will use their card x times in the year. 
One way to think about this sort of model is as a two-stage generative process for a 
particular individual. In the first step there is a probability p (and 1 - p) that the individual 
comes from one group or the other. In the second step, an observation x is generated for 
that person according to the component distribution he or she was assigned to in the first 
step. 
Equation 9.6 is an example of a finite mixture distribution, where the overall model ƒ(x) is 
a weighted linear combination of a finite number of component distributions (in this case 
just two). Clearly it leads to a much more flexible model than a simple single Poisson 
distribution—at the very least, it involves three parameters instead of just one. However, 
by virtue of the argument that led to it, it may also be a more realistic description of what 
is underlying the data. These two aspects—the extra flexibility of the models consequent 
on the larger number of parameters and arguments based on suspicion of a 
heterogeneous underlying population—mean that mixture models are widely used for 
modeling distributions that are more complicated than simple standard forms. 
The general form of a mixture distribution (for multivariate x) is 

(9.7)  

where pk is the probability that an observation will come from the kth component (the so-
called kth mixing proportion or  weight), K is the number of components, ƒk(x; ?k) is the 
distribution of the kth component, and ?k is the vector of parameters describing the kth 
component (in the Poisson mixture example above, each ?k consisted of a single 
parameter ?k). In most applications the component distributions ƒk have the same form, 
but there are situations where this is not the case. The most widely used form of mixture 
distribution has Normal components. Note that the mixing proportions p k must lie 
between 0 and 1 and sum to 1. 

Some examples of the many practical situations in which mixture distributions might be 
expected on theoretical grounds are the length distribution of fish (since they hatch at a 
specific time of the year), failure data (where there may be different causes of failure, 
and each cause results in a distribution of failure times), time to death, and the 
distribution of characteristics of heterogeneous populations of people (e.g., heights of 
males and females). 

9.2.5 The EM Algorithm for Mixture Models 
Unlike the simple parametric models discussed earlier in this chapter, there is generally 
no direct closed-form technique for maximizing the likelihood score function when the 
underlying model is a mixture model, given a data set D = {x(1), ..., x(n)}. This is easy to 
see by writing out the log-likelihood for a mixture model—we get a sum of terms such as 
log(? k p kƒk(x;?k)), leading to a nonlinear optimization problem (unlike, for example, the 
closed form solutions for the multivariate Normal model). 
Over the years, many different methods have been applied in estimating the parameters 
of mixture distributions given a particular mixture form. One of the more widely used 
modern methods in this context is the EM approach. As discussed in chapter 8, this can 
be viewed as a general iterative optimization algorithm for maximizing a likelihood score 
function given a probabilistic model with missing data. In the present case, the mixture 
model can be regarded as a distribution in which the class labels are missing. If we knew 
these labels, we could get closed-form estimates for the parameters of each component 
by partitioning the data points into their respective groups. However, since we do not 
know the origin of each data point, we must simultaneously try to learn which component 
a data point originated from and the parameters of these components. This ""chicken-
and-egg"" problem is neatly solved by the EM algorithm; it starts with some guesses at 
the parameter values for each component, then calculates the probability that each data 
point came from one of the K components (this is known as the E-step), calculates new 
parameters for each component given these probabilistic memberships (this is the M-
step, and can typically be carried out in closed form), recalculates the probabilistic 
memberships, and continues on in this manner until the likelihood converges. As 
discussed in chapter 8, despite the seemingly heuristic nature of the algorithm, it can be 
shown that for each EM-step the likelihood can only increase, thus guaranteeing (under 
fairly broad conditions) convergence of the method to at least a local maximum of the 
likelihood as a function of the parameter space. 
The complexity of the EM algorithm depends on the complexity of the E and M steps at 
each iteration. For multivariate normal mixtures with K components the computation will 
be dominated by the calculation of the K covariance matrices during the M-step at each 
iteration. In p dimensions, with K clusters, there are O(Kp2) covariance parameters to be 
estimated, and each of these requires summing over n data points and membership 
weights, leading to a O(Kp2n) time-complexity per step. For univariate mixtures (such as 
the Poisson above) we get O(Kn). The space-complexity is typically O(Kn) to store the K 
membership probability vectors for each of the n data points x(i). However, for large n, 
we often need not store the n × K membership probability matrix explicitly, since we may 
be able to calculate the parameter estimates during each  M-step incrementally via a 
single pass through the n data points. 
EM often provides a large increase in likelihood over the first few iterations and then can 
slowly converge to its final value; however the likelihood function as a function of 
iterations need not be concave. For example, figure 9.2 illustrates the convegence of the 
log-likelihood as a function of the EM iteration number, for a problem involving fitting 
Gaussian mixtures to a two-dimensional medical data set (that we will later discuss in 
more detail in section 9.6). For many data sets and models we can often find a 
reasonable solution in only 5 to 20 iterations of the algorithm. Each solution provided by 
EM is of course a function of where one started the search (since it is a local search 
algorithm), and thus, multiple restarts from randomly chosen starting points are a good 
idea to try to avoid poor local maxima. Note that as either (or both) K and p increase, the 
number of local maxima of the likelihood can increase greatly as the dimensionality of 
the parameter space scales accordingly. 

Figure 9.2: The Log-Likelihood of the Red-Blood Cell Data Under a Two-Component Normal 
Mixture Model (See Figure 9.11) as a Function of Iteration Number.  

 

Sometimes caution has to be exercised with maximum likelihood estimates of mixture 
distributions. For example, in a normal mixture, if we put the mean of one component 
equal to one of the sample points and let its standard deviation tend to zero, the 
likelihood will increase without limit. The maximum likelihood solution in this case is likely 
to be of limited value. There are various ways around this. The largest finite value of the 
likelihood might be chosen to give the estimated parameter values. Alternatively, if the 
standard deviations are constrained to be equal, the problem does not arise. A more 
general solution is to set up the problem in a Bayesian context, with priors on the 
parameters, and maximize the MAP score function (for example) instead of the 
likelihood. Here the priors provide a framework for ""biasing"" the score function (the MAP 
score function) away from problematic regions in parameter space in a principled 
manner. Note that the EM algorithm generalizes easily from the case of maximizing 
likelihood to maximizing MAP (for example, we replace the M-step with an MAP-step, 
and so forth). 
Another problem that can arise is due to lack of identifiability. A family of mixture 
distributions is said to be identifiable if and only if the fact that two members of the family 
are equal, 

(9.8)  

implies that c = c', and that for all k there is some j such that 
. If a family 
is not identifiable, then two different members of it may be indistinguishable, which can 
lead to problems in estimation. 
Nonidentifiability is more of a problem with discrete distributions than continuous ones 
because, with m categories, only m - 1 independent equations can be set up. For 
example, in the case of a mixture of several Bernoulli components, there is effectively 
only a single piece of information available in the data, namely, the proportion of 1s that 
occur in the data. Thus, there is no way of estimating the proportions that are separately 
due to each component Bernoulli, or the parameters of those components. 

and 

9.2.6 Nonparametric Density Estimation 
In chapter 3 we briefly discussed the idea of estimating a density function by taking a 
local data-driven weighted average of x measurements about the point of interest (the 
so-called ""kernel density"" method). For example, a histogram is a relatively primitive 
version of this idea, in which we simply count the number of points that fall in certain 
bins. Our estimate for the density is the number of points in a given bin, appropriately 
scaled. The histogram is problematic as a model structure for densities for a number of 
reasons. It provides a nonsmooth estimate of what is often presumed to be truly a 
smooth function, and it is not obvious how the number of bins, bin locations, and widths 
should be chosen. Furthermore, these problems are exacerbated when we move beyond 
the one-dimensional histogram to a p-dimensional histogram. Nonetheless, for very large 
data sets and small p (particularly p = 1), the bin widths can be made quite small, and 
the resulting density estimate may still be relatively smooth and insensitive to the exact 

location or width of the bins. With large data sets it always a good idea to look at the 
histograms (with a large number of bins) for each variable, since the histogram can 
provide a wealth of information on outliers, multimodality, skewness, tail behavior, and so 
forth (recall the example of the Pima Indians blood pressure data in chapter 3, where the 
histogram clearly indicat ed the presence of some rather suspicious values at zero). 
A more general model structure for local densities is to define the density at any point x 
as being proportional to a weighted sum of all points in the training data set, where the 
weights are defi ned by an appropriately chosen kernel function. For the one-dimensional 
case we have (as defined in chapter 3) 

(9.9)  

where ƒ(x) is the kernel density estimate at a query point x, K(t) is the kernel function (for 
example, K(t) = 1 - |t|, t = 1; K(t) = 0 otherwise) and  h is the bandwidth of the kernel. 
Intuitively, the density at x is proportional to the sum of weights evaluated at x, which in 
turn depend on the proximity of the n points in the training data to x. As with 
nonparametric regression (discussed in chapter 6), the model is not defined explicitly, 
but is determined implicitly by the data and the kernel function. The approach is 
""memory-based"" in the sense that all of the data points are retained in the model; that is, 
no summarization occurs. For very large data sets of course this may be impractical from 
a computational and storage viewpoint. 
In one dimension, the kernel function K is usually chosen as a smooth unimodal function 
(such as a Normal or triangular distribution) that integrates to 1; the precise shape is 
typically not critical. As in regression, the bandwidth h plays the role of determining how 
smooth the model is. If h is relatively large, then the kernel is relatively wide so that many 
points receive significant weight in the sum and the estimate of the density is very 
smooth. If h is relatively small, the kernel estimate is determined by the small number of 
points that are close to x, and the estimate of the density is more sensitive locally to the 
data (more ""spiky"" in appearance). Estimating a good value of  h in practice can be 
somewhat problematic. There is no single objective methodology for finding the 
bandwidth h that has wide acceptance. Techniques based on cross-validation can be 
useful but are typically computationally complex and not always reliable. Simple 
""eyeballing"" of the resulting density along specific dimensions is always recommended to 
check whether or not the chosen values for h appear reasonable. 
Under appropriate assumptions these kernel models are flexible enough to approximate 
any smooth density function, if  h is chosen appropriately, which adds to their appeal. 
However, this approximation result holds in the limit as we get an infinite number of data 
points, making it somewhat less relevant for the finite data sets we see in practice. 
Nonetheless, kernel models can be very valuable for low-dimensional problems as a way 
to determine structure in the data (such as local peaks or gaps) in a manner that might 
not otherwise be visible. 

Example 9.1  

 
Figure 9.3 shows an example of different density estimates for measurements of ethanol 
(E) from a data set involving air pollution measurements at different geographic locations. 
The histogram (top left) is quite ""rough"" and noisy, at least for this particular choice of bin 
widths and bin locations. The Normal kernel with bandwidth h = 0.5 is probably too smooth 
(top right). Conversely, the estimate based on a bandwidth of  h = 0.1 (lower right) is 
probably too noisy, and introduces modes in the density that are likely to be spurious. The 
h = 0.25 estimate (lower left) is quite plausible and appears to have a better trade-off 
between over-and undersmoothing than the other estimates; it would suggest that the 
ethanol measurements have a definite bimodal characteristic. While visual inspection can 
be useful technique for determining bandwidths interactively, once again, it is largely limited 
to one-dimensional or two-dimensional problems. 

Figure 9.3: Density Estimates for the Variable Ethanol (E) using a Histogram (Top Left) and 
Gaussian Kernel Estimates with Three Different Bandwidths: h = 0.5 (Top Right), h = 0.25 
(Lower Left), and h = 0.1 (Lower Right).  

 

 

 

 
 

Density estimation with kernel models becomes much more difficult as p increases. To 
begin with, we now need to define a p-dimensional kernel function. A popular choice is to 
define the p-dimensional kernel as a product of one-dimensional kernels, each with its 
own bandwidth, which keeps the number of parameters (the bandwidths h1, ..., hp for 
each dimension) linear in the number of dimensions. A less obvious problem is the fact 
that in high dimensions it is natural for points to be farther away from each other than we 
might expect intuitively (the ""curse of dimensionality"" again). In fact, if we want to keep 
our approximation error constant as p increases, the number of data points we need 
grows exponentially with p. (Recall the example in chapter 6 where we would need 
842,000 data points to get a reliable estimate of the density value at the mean of a 10-
dimensional Normal distribution.) This is rather unfortunate and means in practice that 
kernel models are really practical only for relatively low-dimensional problems. 
Kernel methods are often complex to implement for large data sets. Unless the kernel 
function K(t) has compact support (that is, unless it is zero outside some finite range on 
t) then calculating the kernel estimate ƒ(x) at some point x potentially involves summing 
over contributions from all n data points in the database. In practice of course since most 
of these contributions will be negligible (that is, will be in the tails of the kernel) there are 
various ways to speed up this calculation. Nonetheless, this ""memory-based"" 
representation can be a relatively complex one to store and compute with (it can be O(n) 
to compute the density at just one query data point). 

9.2.7 Joint Distributions for Categorical Data 
In chapter 6 we discussed the problem of constructing joint distributions for multivariate 
categorical data. Say we have p variables each taking m values. The joint distribution 
requires the specification of O(mp) different probabilities. This exponential growth is 
problematic for a number of reasons. 
First there is the problem of how to estimate such a large number of probabilities. As an 
example, let {
} represent a list of all the joint probability terms in the unknown 
distribution we are trying to estimate from a data set with n p-dimensional observations. 
Hence, we can think of mp different ""cells,"" {
i = mp. The expected number of data points in celli, given a random sample from p(x) of 
size n, can be written as Ep(x)[ni] = npi. Assuming (for example) that p(x) is approximately 
uniform (that is, pi ˜ 1/ mp) we get that 

} each containing ni observations, 1 = 

(9.10)  

Thus, for example, if n < 0.5mp, the expected number of data points falling in any given 
cell is closer to 0 than to 1. Furthermore, if we use straightforward frequency counts (the 
maximum likelihood estimate—see chapter 4) as our method for estimating probabilities, 
we will estimate 
for each empty cell, whether or not pi = 0 in truth. Note that if p(x) is 
nonuniform the problem is actually worse since there will be more cells with smaller pi 
(that is, less chance of any data falling in them). The fundamental problem here is the mp 
exponential growth in the number of cells. With p = 20 binary variables (m = 2) we get mp 
˜ 106. By doubling the number of variables to p = 40 we now get mp ˜ 1012. Say that we 
had n data points for the case of p = 20 and that we wanted to add some new variables 
to the analysis while still keeping the expected number of data points per cell to be 
constant (that is, the same as it was with n data points). If we added extra 20 variables to 
the problem we would need to increase the data set from n to n' = 106n, an increase by a 
factor of a million. 
A second practical problem is that even if we can reliably estimate a full joint distribution 
from data, it is exponential in both space and time to work with directly. A full joint 
distribution will have a  O(mp) memory requirement; for example, O(1012) real-valued 
probabilities would need to be stored for a full distribution on 40 binary variables. 
Furthermore, many computations using this distribution will also scale exponentially. Let 
the variables be {X1, ..., Xp}, each taking m values. If we wanted to determine the 
marginal distribution on any single variable Xj (say), we could calculate it as 

(9.11)  

that is, by summing over all the other variables in the distribution. The sum on the right 
involves O(mp-1) summations—for example, O(1039) summations when p = 40 and m = 2. 
Clearly this sort of exercise is intractable except for relatively small values of m and p. 
The practical consequence is that we can only reliably estimate and work with full joint 
distributions for relatively low-dimensional problems. Although our examples were for 
categorical data, essentially the same problems also arise of course for ordered or real-
valued data. 
As we have seen in chapter 6, one of the key ideas for addressing this curse of 
dimensionality is to impose structure on the underlying distribution p(x)—for example, by 
assuming independence: 

(9.12)  

Instead of requiring O(mp) separate probabilities here we now only need p ""marginal"" 
distributions p1(x1), ..., pp(xp), each of which can be specified by m numbers, for a total of 
mp probabilities. Of course, as discussed earlier, the independence assumption is just 
that, an assumption, and typically it is far too strong an assumption for most real-world 
data mining problems. 
As described earlier in chapter 6, a somewhat weaker assumption is to presume that 
there exists a hidden (""latent"") variable  C, taking K values, and that the measurements x 
are conditionally independent given C. This is equivalent to the mixture distributions 
discussed earlier, with an additional assumption of conditional independence within each 
component; that is, 

(9.13)  

This model requires mp probabilities per component, times K components, in addition to 
the K component weights p 1, ..., p K. Thus, it scales linearly in K, m, and p, rather than 
exponentially. The EM algorithm can again be used to estimate the parameters for each 
component pk(x) (and the weights pk), where the conditional independence assumption 
is enforced during estimation. One way to think about this ""mixture of independence 
models"" is that we are trying to find K different groups in the data such that for each 
group the variables are at least approximately conditionally independent. In fact, given a 
fixed K value, EM will try to find K component distributions (each of conditional 
independence form) that maximize the overall likelihood of the data. This model can be 
quite useful for modeling large sparse transactional data sets or sets of text documents 

represented as binary vectors. Finding a suitable value of K depends on our goal: from a 
descriptive viewpoint we can vary K in accordance with how complex we wish our fitted 
model to be. Note also that this form of model is equivalent to the first-order ""naive"" 
Bayes model discussed in chapter 6 (and again in chapter 10 in the context of 
classification), whereas here the class variable C is unobserved and must be learned 
from the data. We will see later in this chapter that this also forms a useful basis for 
clustering the data, where we interpret each component pk(x) as a cluster. 
A somewhat different way to structure a probability distribution parsimoniously is to 
model conditional independence in a general manner. We have described one such 
general framework (known as belief networks, or equivalently, acyclic directed graphical 
models) back in chapter 6. Recall that the basic equation for such models can be written 
as 

(9.14)  

which is a factorization of the overall joint distribution function into a product of 
conditional distributions. In fact, such a factorization can always be defined by the chain 
rule, but this model gains its power when the dependencies can be assumed to be 
relatively sparse. Recall that the graphical formalism associates each variable Xj with a 
single node in a graph. A directed edge from Xi to Xj indicates that Xj depends directly on 
Xi. pa(xj) indicates values taken from the parent set pa(Xj) of variables for variable  Xj. 
The connectivity structure of a graph implies a set of conditional independence 
relationships for p(x). These independence relationships can be summarized by the fact 
that, given the values of the parents of Xj, pa(Xj), a node Xj is independent of all other 
variables in the graph that are non-descendants of Xj. 
If the sizes of the parent sets in the graph are relatively small compared to p, then we will 
have a much simpler representation for the joint distribution (compared to the full model). 
In this context, the independence model corresponds to a graph with no edges at all, and 
the complete graph corresponds to the full joint distribution with no independence 
structure being assumed. Another well-known graph structure is the Markov chain 
model, in which the variables are ordered in some manner (for example, temporally) and 
each variable Xj depends only on Xj-1. Here each variable is linked to just two others, so 
that the overall graph is a single line of connected nodes (see figure 6.7 in chapter 6). 
A primary attraction of the graphical formalism is that it provides a systematic and 
mathematically precise language for describing and communicating the structure of 
independence relationships in probability distributions. Perhaps more importantly it also 
provides a systematic framework for computational methods in handling probability 
calculations with the associated joint distribution. For example, if the underlying graph is 
singly-connected (that is, when directionality of the edges is ignored the graph has no 
loops), one can show that the time to compute any marginal or conditional probability of 
interest is upper bounded by pmd+1, where p is the number of variables, m is the number 
of values for each variable (assumed the same for all variables for simplicity), and d is 
the number of variables in the largest parent set in the graph. For example, for a Markov 
chain model we have d = 1 leading to the well-known O(pm2) complexity for such 
models. For graphs that have loops, there is an equivalent complexity bound of pmd'+1, 
where d' is the size of the largest parent set in an equivalent singly connected graph 
(obtained from the original graph in a systematic manner). 
From a data mining perspective there are two aspects to learning graphical models from 
data: learning the parameters given a fixed graphical structure, and the more difficult 
problem of learning parameters and structure together. Note that in the categorical case 
the parameters of the model are simply the conditional probability tables for each 
variable, p(xj|pa(Xj)), 1 = j = p. 
Given a fixed structure, there is no need to perform structure-search, and the simple 
maximum likelihood or MAP score functions work fine. If there are no hidden variables, 
the problem of learning reduces to estimating the conditional probability tables for each 
variable Xj given its parents pa(Xj): in either the maximum likelihood or MAP case this 
reduces to simple counting (see chapter 4). With hidden variables, and assuming that 
the connectivity of these hidden variables in the graph is known, the EM algorithm 
(chapter 8) is again directly applicable under fairly broad conditions. The estimation of 
the conditional probability tables is now iterative (rather than closed-form as in the 

nonhidden case), and as usual care must be taken with initial conditions and detection of 
convergence. The mixture models discussed earlier can be viewed as graphical models 
with a single hidden variable. Hidden Markov models (as used in speech) can be viewed 
as graphical models with a discrete hidden time-dependent variable that is assumed to 
be Markov.  

It is worth emphasizing that if we have strong prior belief that a particular graphical 
model structure is appropriate for our data mining problem, then it is usually worth taking 
advantage of this knowledge (assuming it is reliable) either as a fixed model or as a 
starting point for the structure learning methods described below. 
Learning the structure of a directed graphical model from data has been a topic of 
research interest recently, and there now exist numerous algorithms for this purpose. 
Consider, first, the problem of learning structure with no hidden variables. The score 
function is typically some form of penalized likelihood: the BIC score function (see 
section 9.2.2), for example, is fairly widely used because it is easy to compute. Given a 
score function, the problem reduces to searching in graph space for the graph structure 
(with estimated parameters) that produces the maximum score. The general problem of 
finding the maximum score has been shown to be NP-hard (as seems to be the case 
with most nontrivial structure-finding problems in data mining). Thus, iterative local 
search methods are used: starting with some ""prior"" structure such as the empty graph 
and then adding and deleting edges until no further local improvement in the score 
function is possible. One useful feature from a computational viewpoint is that because 
the distribution can be expressed in factored form (equation 9.14), the likelihood and 
penalty terms can also be factored into expressions that are local in terms of the graph 
structure—for example, terms that only involve Xj and its parents. Thus, we can calculate 
the effect of local changes to the model (such as adding or deleting an edge) with local 
computations (since the impact of the change affects only one factor in the score 
function). 

Learning structure with hidden variables is still considered to be something of a research 
problem. Clearly it is more difficult than learning structure with no hidden variables 
(which is itself NP-hard). The EM algorithm is again applicable, but the search problem is 
typically quite complex since there are so many different ways that one can introduce 
hidden variables into a multivariate model. 
The family of log-linear models is a further generalization of acyclic directed graphical 
models, which characterize dependence relations in a more general form. Discussion of 
this class of models is beyond the scope of this text (references are provided in the 
section on further reading). Markov random fields are another class of graphical models, 
where an undirected graph is used to represent dependence, e.g., to represent 
correlational effects between pixels in an image. These random field models have seen 
wide application in image analysis and spatial statistics, where they are used to define a 
joint distribution over measurements on a grid or image. 

9.3 Background on Cluster Analysis 
We now move beyond probability density and distribution models to focus on the related 
descriptive data mining task of cluster analysis—that is, decomposing or partitioning a 
(usually multivariate) data set into groups so that the points in one group are similar to 
each other and are as different as possible from the points in other groups. Although the 
same techniques may often be applied, we should distinguish between two different 
objectives. In one, which we might call segmentation or dissection, the aim is simply to 
partition the data in a way that is convenient. ""Convenient"" here might refer to 
administrative convenience, practical convenience, or any other kind. For example, a 
manufacturer of shirts might want to choose just a few sizes and shapes so as to 
maximize coverage of the male population. He or she will have to choose those sizes in 
terms of collar size, chest size, arm length, and so on, so that no man has a shape too 
different from that of a well-fitting shirt. To do this, he or she will partition the population 
of men into a few groups in terms of the variables collar, chest, and arm length. Shirts of 
one size will then be made for each group. 

 
 

In contrast to this, we might want to see whether a sample of data is composed of 
natural subclasses. For example, whiskies can be characterized in terms of color, nose, 
body, palate, and finish, and we might want to see whether they fall into distinct classes 
in terms of these variables. Here we are not partitioning the data for practical 
convenience, but rather are hoping to discover something about the nature of the sample 
or the population from which it arose—to discover whether the overall population is, in 
fact, heterogeneous. 
Technically, this second exercise is what cluster analysis seeks to do—to see whether 
the data fall into distinct groups, with members within each group being similar to other 
members in that group but different from members of other groups. However, the term 
""cluster analysis"" is often used in general to describe both segmentation and cluster 
analysis problems (and we shall also be a little lax in this regard). In each case the aim is 
to split the data into classes, so perhaps this is not too serious a misuse. It is resolved, 
as we shall see below, by the fact that there is a huge number of different algorithms for 
partitioning data in this way. The important thing is to match our method with our 
objective. This way, mistakes will not arise, whatever we call the activity. 

Example 9.2  

 

Owners of credit cards can be split into subgroups according to how they use their card—
what kind of purchases they make, how much money they spend, how often they use the 
card, where they use the card, and so on. It can be very useful for marketing purposes to 
identify the group to which a card owner belongs, since he or she can then be targeted with 
promotional material that might be of interest (this clearly benefits the owner of the card, as 
well as the card company). Market segmentation in general is, in fact, a heavy user of the 
kinds of techniques discussed in this section. The segmentation may be in terms of 
lifestyle, past purchasing behavior, demographic characteristics, or other features. 

A chain store might want to study whether outlets that are similar, in terms of social 
neighborhood, size, staff numbers, vicinity to other shops, and so on, have similar 
turnovers and yield similar profits. A starting point here would be to partition the outlets, in 
terms of these variables, and then to examine the distributions of turnover within each 
group. 

Cluster analysis has been heavily used in some areas of medicine, such as psychiatry, to 
try to identify whether there are different subtypes of diseases lumped together under a 
single diagnosis. 

Cluster analysis methods are used in biology to see whether superficially identical plants or 
creatures in fact belong to different species. Likewise, geographical locations can be split 
into subgroups on the basis of the species of plants or animals that live there. 

As an example of where the difference between dissection and clustering analysis might 
matter, consider partitioning the houses in a town. If we are organizing a delivery service, 
we might want to split them in terms of their geographical location. We would want to 
dissect the population of houses so that those within each group are as close as possible 
to each other. Delivery vans could then be packed with packages to go to just one group. 
On the other hand, a company marketing home improvement products might want to split 
the houses into naturally occurring groups of similar houses. One group might consist of 
small starter homes, another of three-and four-bedroom family homes, and anot her 
(presumably smaller) of executive mansions. 

 

 

It will be obvious from this that such methods (cluster and dissection techniques) hinge 
on the notion of distance. In order to decide whether a set of points can be split into 
subgroups, with members of a group being closer to other members of their group than 
to members of other groups, we need to say what we mean by ""closer to."" The notion of 

""distance,"" and different measures of it, has already been discussed in chapter 2. Any of 
the measures described there, or indeed any other distance measure, can be used as 
the basis for a cluster or dissection analysis. As far as these techniques are concerned, 
the concept of distance is more fundamental than the coordinates of the points. In 
principle, to carry out a cluster analysis all we need to know is the set of interpoint 
distances, and not the values on any variables. However, some methods make use of 
""central points"" of clusters, and so require that the raw coordinates be available. 
Cluster analysis has been the focus of a huge amount of research effort, going back for 
several decades, so that the literature is now vast. It is also scattered. Considerable 
portions of it exist in the statistical and machine learning literatures, but other many other 
publications on cluster analysis may be found elsewhere. One of the problems is that 
new methods are constantly being developed, sometimes without an awareness of what 
has already been developed. More seriously, a proper understanding of their properties 
and the way they behave with different kinds of data is available for very few of the 
methods. One of the reasons for this is that it is difficult to tell whether a cluster analysis 
has been successful. Contrast this with predictive modeling, in which we can take a test 
data set and see how accurately the value of the target variable is predicted in this set. 
For a clustering problem, unfortunately, there is no direct notion of generalization to a 
test data set, although, as we will see in our discussion of probabilistic clustering (later in 
this chapter), it is possible in some situations to pose the question of whether or not the 
cluster structure discovered in the training data is genuinely present in the underlying 
population. Generally speaking, however, the validity of a clustering is often in the eye of 
the beholder; for example, if a cluster produces an interesting scientific insight, we can 
judge it to be useful. Quantifying this precisely is difficult, if not impossible, since the 
interpretation of how interesting a clustering is will inevitably be application-dependent 
and subjective to some degree. 
As we shall see in the next few sections, different methods of cluster analysis are 
effective at detecting different kinds of clusters, and we should consider this when we 
choose a particular algorithm. That is, we should consider what we mean or intend to 
mean by a ""cluster."" In effect, different clustering algorithms will be biased toward finding 
different types of cluster structures (or ""shapes"") in the data, and it is not always easy to 
ascertain precisely what this bias is from the description of the clustering algorithm. 

To illustrate, we might take a ""cluster"" as being a collection of points such that the 
maximum distance between all pairs of points in the cluster is as small as possible. Then 
each point will be similar to each other point in the cluster. An algorithm will be chosen 
that seeks to partition the data so as to minimize this maximum interpoint distance (more 
on this below). We would clearly expect such a method to produce compact, roughly 
spherical, clusters. On the other hand, we might take a ""cluster"" as being a collection of 
points such that each point is as close as possible to some other member of the 
cluster—although not necessarily to all other members. Clusters discovered by this 
approach need not be compact or roughly spherical, but could have long (and not 
necessarily straight) sausage shapes. The first approach would simply fail to pick up 
such clusters. The first approach would be appropriate in a segmentation situation, while 
the second would be appropriate if the objects within each hypothesized group were 
measured at different stages of some evolutionary process. For example, in a cluster 
analysis of people suffering from some illness, to see whether there were different 
subtypes, we might want to allow for the possibility that the patients had been measured 
at different stages of the disease, so that they had different symptom patterns even 
though they belonged to the same subtype. 
The important lesson to be learned from this is that we must match the method to the 
objectives. In particular, we must adopt a cluster analytic tool that is effective at detecting 
clusters that conform to the definition of what is meant by ""cluster"" in the problem at 
hand. It is perhaps worth adding that we should not be too rigid about it. Data mining, 
after all, is about discovering the unexpected, so we must not be too determined in 
imposing our preconceptions on the analysis. Perhaps a search for a different kind of 
cluster structure will throw up things we have not previously thought of. 

Broadly speaking, we can identify three different general types of cluster analysis 
algorithms: those based on an attempt to find the optimal partition into a specified 

 
 

number of clusters, those based on a hierarchical attempt to discover cluster structure, 
and those based on a probabilistic model for the underlying clusters. We discuss each of 
these in turn in the next three sections. 

9.4 Partition-Based Clustering Algorithms 
In chapter 5 we described how data mining algorithms can often be conveniently thought 
of in five parts: the task , the model, the score function, the search method, and the data 
management technique. In partition-based clustering the task is to partition a data set 
into k disjoint sets of points such that the points within each set are as homogeneous as 
possible, that is, given the set of n data points D = {x(1), ..., x(n)}, our task is to find K 
clusters C = {C1, ..., CK} such that each data point x(i) is assigned to a unique cluster Ck. 
Homogeneity is captured by an appropriate score function (as discussed below), such as 
minimizing the distance between each point and the centroid of the cluster to which it is 
assigned. Partition-based clustering typically places more emphasis on the score 
function than on any formal notion of a model. Often the centroid or average of the points 
belonging to a cluster is considered to be a representative point for that cluster, and 
there is no explicit statement of what sort of shape of cluster is being sought. For cluster 
representations based on the notion of a single ""center"" for each cluster, however, the 
boundaries between clusters will be implicitly defined. For example, if a point x is 
assigned to a cluster according to which cluster center is closest in a Euclidean-distance 
sense, then we will get linear boundaries between the clusters in x space. 
We will see that maximizing (or minimizing) the score function is typically a 
computationally intractable search problem, and thus, iterative improvement heuristic 
search methods, such as those described in chapter 8, are often used to optimize the 
score function given a data set. 

9.4.1 Score Functions for Partition-Based Clustering 

A large number of different score functions can be used to measure the quality of 
clustering and a wide range of algorithms has been developed to search for an optimal 
(or at least a good) partition. 
In order to define the clustering score function we need to have a notion of distance 
between input points. Denote by d(x, y) the distance between points x, y ? D, and 
assume for simplicity that the function  d defines a metric on D. Most score functions for 
clustering stress two aspects: clusters should be compact, and clusters should be as far 
from each other as possible. A straightforward formulation of these intuitive notions is to 
look at within cluster variation wc (C) and between cluster variation bc(C) of a clustering 
C. The within cluster variation measures how compact or tight the clusters are, while the 
between cluster variation looks at the distances between different clusters. 
Suppose that we have selected cluster centers rk from each cluster. This can be a 
designated representative data point x(i) ? Ck that is defined to be ""central"" in some 
manner. If the input points belong to a space where taking means makes sense, we can 
use the centroid of the points in the cluster Ck as the cluster center, where rk will then be 
defined as 

(9.15)  

with nk the number of points in the kth cluster. A simple measure of  within cluster 
variation is to look at the sum of squares of distances from each point to the center of the 
cluster it belongs to: 

(9.16)  

For the case in which d(x, rk) is defined as Euclidean distance, wc(C) is referred to as 
the within-cluster sum-of-squares. 
Between-cluster variation can be measured by the distance between cluster centers: 

(9.17)  

The overall quality (or score function) of a clustering  C can then be defined as a 
monotone combination of the factors wc(C) and bc(C), such as the ratio bc(C)/wc(C). 
The within cluster measure above is in a sense global: for the cluster Ck to make a small 
contribution to it, all points of Ck have to be relatively close to the cluster center. Thus the 
use of this measure of cluster tightness leads to spherical clusters. The well-known K-
means algorithm, discussed in the next section, uses the means within each group as 
cluster centers and Euclidean distance for d to search for the clustering C that minimizes 
the within cluster variation of  equation 9.16, for measurements x in a Euclidean space 
Rp. 
If we are given a candidate clustering, how difficult is it to evaluate  wc(C) and bc(C)? 
Computing wc(C) takes O(? i|Ci|) = O(n) operations, while bc(C) can be computed in 
O(k 2) operations. Thus computing a score function for a single clustering requires (at 
least in principle) a pass through the whole data. 

A different notion of within cluster variation is to consider for each point in the cluster the 
distance to the nearest point in the same cluster, and take the maximum of these 
distances: 

(9.18)  

This minimum distance or single-link  criterion for cluster distance leads to elongated 
clusters. We will return to this score function in the context of hierarchical agglomerative 
clustering algorithms in section 9.5.  
We can use the notion of covariance to develop more general score functions for 
clusterings C in a Euclidean space. For points within a particular cluster Ck, we can 
define a  p × p matrix 

(9.19)  

that is an (unnormalized) covariance matrix for the points in cluster Ck. The within-cluster 
sum-of-squares for a particular cluster is then the trace (sum of diagonal elements) of 
this matrix, tr(Wk), and thus the total within-cluster sum-of-squares of equation 9.16 can 
be expressed as 

(9.20)  

In this context, letting W = ? k Wk, we can see that a score function that tries to make W 
""smaller"" (for example, minimize the trace or the determinant of W) will tend to 
encourage a more compact clustering of the data. 
We can define a matrix B that summarizes the squared differences between the cluster 
centers as 

(9.21)  

is the estimated global mean of all data points in D. This is a p × p matrix that 

where 
characterizes the covariance of the cluster means (weighted by nk) with respect to each 
other. For example, tr(B) is the weighted sum of squared distances of the cluster means 
relative to the estimated global mean of the data. Thus, having a score function that 
emphasizes a ""larger"" B will tend to encourage the cluster means to be more separated. 
We stress again the important, but often overlooked, point that the nature of the score 
function has a very important influence on what types of clusters will be found in the 
data. Different score functions (for example, different combinations of W and B) can 
express significantly different preferences in terms of cluster structure. 
Traditional score functions based on W and B are tr(W), the trace of W, the determinant | 
W | of W, and tr(BW-1 ). A disadvantage of tr(W) is that it depends on the scaling adopted 
for the separate variables. Alter the units of one of them and a different cluster structure 
may result. Of course, this can be overcome by standardizing the variables prior to 
analysis, but this is often just as arbitrary as any other choice. The tr(W) criterion tends 
to yield compact spherical clusters, and it also has a tendency to produce roughly equal 
groups. Both of these properties may make this score function useful in a segmentation 
context, but they are less attractive for discovering natural clusters (for example, in 
astronomy the discovery of a distinct very small cluster may represent a major advance). 
The | W | score function does not have the same scale dependence as tr(W), so it also 
detects elliptic structures as clusters, but it also favors equal-sized clusters. Adjustments 

score functions, analogous to that of the | W | 

. 

and ?  |  Wk |1/p, where  p is the number of 

), 
that take cluster size into account have been suggested (for example, dividing by 
so that the equal-sized cluster tendency is counteracted, but it might be better to go for a 
different criterion altogether than adjust an imperfect one. Note also that the original 
score function, | W |, has optimality properties if the data are thought to arise from a 
mixture of multivariate normal distributions, and this is sacrificed by the modification. (Of 
course, if our data are thought to be generated in that way, we might contemplate fitting 
a formal mixture model, as outlined in section 9.2.4.) 
The tr(BW-1) score function also has a tendency to yield equal-sized clusters, and this 
time of roughly equal shape. Note that since this score function is equivalent to summing 
the eigenvalues of  BW-1 it will place most emphasis on the largest eigenvalue and hence 
will tend to yield collinear clusters. 
The property that the clusters obtained from using these score functions tend to have 
similar shape is not attractive in all situations (indeed, it is probably rarely attractive). 
Score functions based on other ways of combining the separate within-cluster matrices 
Wk can relax this—for example, 
variables. Even these score functions, however, have a tendency to favor similarly-sized 
clusters. (A modification to the 
score function, that can help to overcome this property, is to divide each | Wk | by 
This is equivalent to letting the distance vary between different clusters.) 
A variant of these methods uses the sum of squared distances not from the cluster 
means, but from particular members of the cluster. The search (see below) then includes 
a search over cluster members to find the one that minimizes the score function. In 
general, of course, measures other than the sum of squared distances from the cluster 
""center"" can be used. In particular, the influence of the outlying points of a cluster can be 
reduced by replacing the sum of squared distances by robust estimates of distance. The 
L1 norm has also been proposed as a measure of distance. Typically this will be used 
with the vector of medians as the cluster ""center.""  
Methods based on minimizing a within cluster matrix of sums of squares can be 
regarded as minimizing deviations from the centroids of the groups. A technique known 
as maximal predictive classification (developed for use with binary variables in taxonomy 
but more widely applicable) can also be regarded as minimizing deviations from group 
""centers,"" though with a different definition of centers. Suppose that each component of 
the measurement vector is binary—that is, each object has given rise to a binary 
vector—and suppose we have a proposed grouping into clusters. For each group we can 
define a binary vector that consists of the most common value, within the group, of each 
variable. This vector of modes (instead of means) will serve as the ""center"" of the group. 
Distance of a group member from this center is then measured in terms of the number of 
variables that have values that differ from those in this central vector. The score function 
optimized is then the total number of differences between the objects and the centers of 
the groups they belong to. The ""best"" grouping is the one that minimizes the overall 
number of such differences. 
Hierarchical methods of cluster analysis, described in the next section, do not construct a 
single partition of the data, but rather construct a hierarchy of (typically) nested clusters. 
We can then decide where to cut the hierarchy so as to partition the data in such a way 
as to obtain the most convincing partition. For partition-based methods, however, it is 
necessary to decide at the start how many clusters we want. Of course, we can rerun the 
analysis several times, with different numbers of clusters, but this still requires us to be 
able to choose between competing numbers. There is no ""best"" solution to this problem. 
We can, of course, examine how the clustering score function changes as we increase 
the number of clusters, but this may not be comparable across different numbers; for 
example, perhaps the score shows apparent improvement as the number increases, 
regardless of whether there is really a better cluster structure (for example, the sum of 
within cluster squared distances is guaranteed to not increase with K). For a multivariate 
uniform distribution divided optimally into K clusters, the score function K2 | W | 
asymptotically takes the same value for all K; results such as this can be used as the 
basis for comparing partitions with different K values. 
It is apparent that cluster analysis is very much a data-driven tool, with relatively little 
formal model-building underlying it. However, some researchers have attempted to put it 
on a sounder model-based footing. For example, we can supplement the procedures by 
assuming that there is also a random process generating sparsely distributed points 

uniformly across the whole space, in addition to whatever mechanism generates clusters 
of points. This makes the methods less susceptible to outliers. A further assumption is to 
model the distribution of the data parametrically within each cluster using specific 
distributional assumptions—we will return to this in our discussion of probabilistic model-
based clustering in section 9.6.  

9.4.2 Basic Algorithms for Partition-Based Clustering 
We saw in the previous section that a large variety of score functions can be used to 
determine the quality of clustering. Now what about the algorithms to optimize those 
score functions? In principle, at least, the problem is straightforward. We simply search 
through the space of possible assignments C of points to clusters to find the one that 
minimizes the score (or maximizes it, depending on the chosen score function). 
The nature of the search problem can be thought of as a form of combinatorial 
optimization, since we are searching for the allocation of  n objects into K classes that 
maximizes (or minimizes) our chosen score function. The number of possible allocations 
(different clusterings of the data) is approximately Kn. For example, there are some 2100 ˜  
1010 possible allocations of 100 objects into two classes. Thus, as we have seen with 
other data mining problems, direct exhaustive search methods are certainly not 
applicable unless we are dealing with tiny data sets. Nonetheless, for some clustering 
score functions, methods have been developed that permit exhaustive coverage of all 
possible clusterings without actually carrying out an exhaustive search. These include 
branch and bound methods, which eliminate potential clusterings on the grounds that 
they have poorer scores than alternatives already found, without actually evaluating the 
scores for the potential clusterings. Such methods, while extending the range over which 
exhaustive evaluation can be made, still break down for even moderately-sized data 
sets. For this reason, we do not examine them further here. 
Unfortunately, neither do there exist closed-form solutions for any score function of 
interest; that is, there is usually no direct method for finding a specific clustering C that 
optimizes the score function. Thus, since closed form solutions do not exist and 
exhaustive search is infeasible, we must resort to some form of systematic search of the 
space of possible clusters (such search methods were discussed in chapter 8). It is 
important to emphasize that given a particular score function, the problem of clustering 
has been reduced to an optimization problem, and thus there are a large variety of 
choices available in the optimization literature that are potentially applicable.  
Iterative -improvement algorithms based on local search are particularly popular for 
cluster analysis. The general idea is to start with a randomly chosen clustering of the 
points, then to reassign points so as to give the greatest increase (or decrease) in the 
score function, then to recalculate the updated cluster centers, to reassign points again, 
and so forth until there is no change in the score function or in the cluster memberships. 
This greedy approach has the virtue of being simple and guaranteeing at least a local 
maximum (minimum) of the score function. Of course it suffers the usual drawback of 
greedy search algorithms in that we do not know how good the clustering C that it 
converges to is relative to the best possible clustering of the data (the global optimum for 
the score function being used). 
Here we describe one well-known example of this general approach, namely, the K-
means algorithm (which has close connection to the EM algorithm discussed in chapter 8 
and was mentioned in section 9.2.4). The number K of clusters is fixed before the 
algorithm is run (this is typical of many clustering algorithms). There are several variants 
of the K-means algorithm. The basic version begins by randomly picking  K cluster 
centers, assigning each point to the cluster whose mean is closest in a Euclidean 
distance sense, then computing the mean vectors of the points assigned to each cluster, 
and using these as new centers in an iterative approach. As an algorithm, the method is 
as follows: assuming we have  n data points D = {x1, ..., xn}, our task is to find K clusters 
{C1...,CK}: 
      for k = 1, ..., K let r(k) be a randomly chosen point from D; 
      while changes in clusters Ck happen do 
          form clusters: 
          for k = 1, ..., K do 

                Ck = {x ? D | d(rk, x) = d(rj, x) for all j = 1, ..., K,j ? k}; 
          end; 
          compute new cluster centers: 
          for k = 1, ..., K do 
               rk = the vector mean of the points in Ck 
          end; 
      end; 

Example 9.3  

 
Electromechanical control systems for large 34m and 70m antennas are an important 
component in NASA's Deep Space Network for tracking and communicating with deep-
space spacecraft. The motor-currents of the an tenna control systems are quite sensitive to 
subtle changes in operating behavior and can be used for online health monitoring and 
fault detection. Figure 9.4 shows sample data from a 34m Deep Space Network antenna. 
Each bivariate data point corresponds to a two-second window of motor-current 
measurements, that have been modeled by a simple autoregressive (linear) time-series 
model, and where the two dimensions correspond to the first two estimated coefficients of 
the autoregressive model for a particular window. The model is fit in real time to the data 
every two seconds, and changes in coefficients reflect changes in the spectral signature of 
the motor current measurements. 

Figure 9.4: Antenna Data. On Top the Data Points are Shown without Class Labels, and on 
the Bottom Different Symbols are Used for the Three Known Classes (Dots are Normal, 
Circles are Tachometer Noise, and x's are Short Circuit.)  

 

The data in the lower plot of figure 9.4 show which data points belong to which condition 
(three groups, one normal and two fault conditions). Figure 9.5 is an illustrative example of 
the results of applying the K-means algorithm to clustering this data, using K = 3, and 
having removed the class labels (that is, using the data in the upper plot of figure 9.4 as 
input to the K-means algorithm). All three initial starting points for the algorithm are located 
in the center (normal) cloud, but after only four iterations (figure 9.5) the algorithm quickly 
converges to a clustering (the trajectory of the cluster means are plotted in  figure 9.6). The 
final clustering after the fourth iteration (lower plot of figure 9.5) produces three groups that 
very closely match the known grouping shown in  figure 9.4. For this data the grouping is 
relatively obvious, of course, in that the various fault conditions can be seen to be 
separated from the normal cloud (particularly the tachometer noise condition on the left). 
Nonetheless it is reassuring to see that the K-means algorithm quickly and accurately 
converges to a clustering that is very close to the true groups. 

Figure 9.5: Example of Running the K-Means Algorithm on the Two-Dimensional Antenna 
Data. The Plots Show the Locations of the Means of the Clusters (Large Circles) at Various 
Iterations of the K-Means Algorithm, as well as the Classification of the Data Points at Each 
Iteration According to the Closest Mean (Dots, Circles, and xs for Each of the Three Clusters).  

 

Figure 9.6: A Summary of the Trajectories of the Three Cluster Means During the K-Means 
Iterations of Figure 9.5.  

 

 

 

 
 

The complexity of the K-means algorithm is O(KnI), where I is the number of iterations. 
Namely, given the current cluster centers rk, we can in one pass through the data 
compute all the Kn distances d(rk, x) and for each x select the minimal one; then 
computing the new cluster centers can also be done in time O(n). 
A variation of this algorithm is to examine each point in turn and update the cluster 
centers whenever a point is reassigned, repeatedly cycling through the points until the 
solution does not change. If the data set is very large, we can simply add in each data 
point, without the recycling. Further extensions (for example, the ISODATA algorithm) 
include splitting and/or merging clusters. Note that there are a large number of different 
partition-based clustering algorithms, many of which hinge around adding or removing 
one point at a time from a cluster. Efficient updating formula been developed in the 
context of evaluating the change incurred in a score function by moving one data point in 
or out of a cluster—in particular, for all of the score functions involving W discussed in 
the last section. 
The search in the K-means algorithm is restricted to a small part of the space of possible 
partitions. It is possible that a good cluster solution will be missed due to the algorithm 
converging to a local rather than global minimum of the score function. One way to 
alleviate (if not solve) this problem is to carry out multiple searches from different 
randomly chosen starting points for the cluster centers. We can even take this further 
and adopt a simulated annealing strategy (as discussed in chapter 8) to try to avoid 
getting trapped in local minima of the score function. 

Since cluster analysis is essentially a problem of searching over a huge space of 
potential solutions to find whatever optimizes a specified score function, it is no surprise 
that various kinds of mathematical programming methods have been applied to this 
problem. These include linear programming, dynamic programming, and linear and 
nonlinear integer programming. 

Clustering methods are often applied on large data sets. If the number of observations is 
so large that standard algorithms are not tractable, we can try to compress the data set 
by replacing groups of objects by succinct representations. For example, if 100 
observations are very close to each other in a metric space, we can replace them with a 
weighted observation located at the centroid of those observations and having an 
additional feature (the radius of the group of points that is represented). It is relatively 
straightforward to modify some of the clustering algorithms to operate on such 
""condensed"" representations. 

9.5 Hierarchical Clustering 
Whereas partition-based methods of cluster analysis begin with a specified number of 
clusters and search through possible allocations of points to clusters to find an allocation 
that optimizes some clustering score function, hierarchical methods gradually merge 
points or divide superclusters. In fact, on this basis we can identify two distinct types of 
hierarchical methods: the agglomerative (which merge) and the divisive (which divide). 
The agglomerative are the more important and widely used of the two. Note that 
hierarchical methods can be viewed as a specific (and particularly straightforward)  way 
to reduce the size of the search. They are analogous to stepwise methods used for 
model building in other parts of this book. 

A notable feature of hierarchical clustering is that it is difficult to separate the model from 
the score function and the search method used to determine the best clustering. 
Because of this, in this section we will focus on clustering algorithms directly. We can 
consider the final hierarchy to be a model, as a hierarchical mapping of data points to 
clusters; however, the nature of this model (that is, the cluster ""shape"") is implicit in the 
algorithm rather than being explicitly represented. Similarly for the score function, there 
is no notion of an explicit global score function. Instead, various local scores are 
calculated for pairs of leaves in the tree (that is, pairs of clusters for a particular 
hierarchical clustering of the data) to determine which pair of clusters are the best 
candidates for agglomeration (merging) or dividing (splitting). Note that as with the global 
score functions used for partition-based clustering, different local score functions can 
lead to very different final clusterings of the data. 
Hierarchical methods of cluster analysis permit a convenient graphical display, in which 
the entire sequence of merging (or splitting) of clusters is shown. Because of its tree-like 
nature, such a display is called a dendrogram. We illustrate in an example below. 
Cluster analysis is particularly useful when there are more than two variables: if there are 
only two, then  we can eyeball a scatterplot to look for structure. However, to illustrate the 
ideas on a data set where we can see what is going on, we will apply a hierarchical 
method to some two dimensional data. The data are extracted from a larger data set 
given in Azzalini and Bowman (1990). Figure 9.7 shows a scatterplot of the two-
dimensional data. The vertical axis is the time between eruptions and the horizontal axis 
is the length of the following eruption, both measured in minutes. The points are given 
numbers in this plot merely so that we can relate them to the dendrogram in this 
exposition, and have no other substantive significance. 

Figure 9.7: Duration of Eruptions Versus Waiting Time between Eruptions (in Minutes) for the 
Old Faithful Geyser in Yellowstone Park.  

 

As an example, figure 9.8 shows the dendrogram that results from agglomerative 
merging the two clusters that leads to the smallest increase in within-cluster sum of 
squares. The height of the crossbars in the dendrogram (where branches merge) shows 
values of this score function. Thus, initially, the smallest increase is obtained by merging 

points 18 and 27, and from figure 9.7 we can see that these are indeed very close (in 
fact, the closest). Note that closeness from a visual perspective is distorted because of 
the fact that the x-scale is in fact compressed on the page relative to the y-scale. The 
next merger comes from merging points 6 and 22. After a few more mergers of individual 
pairs of neighboring points, point 12 is merged with the cluster consisting of the two 
points 18 and 27, this being the merger that leads to least increase in the clustering 
criterion. This procedure continues until the final merger, which is of two large clusters of 
points. This structure is evident from the dendrogram. (It need not always be like this. 
Sometimes the final merger is of a large cluster with one single outlying point—as we 
shall see below.) The hierarchical structure displayed in the dendrogram also makes it 
clear that we could terminate the process at other points. This would be equivalent to 
making a horizontal cut through the dendrogram at some other level, and would yield a 
different number of clusters. 

Figure 9.8: Dendrogram Resulting From Clustering of Data in Figure 9.7 using the Criterion 
of Merging Clusters that Leads to the Smallest Increase in the Total Sum of Squared Errors.  

 

9.5.1 Agglomerative Methods 
Agglomerative methods are based on measures of distance between clusters. 
Essentially, given an initial clustering, they merge those two clusters that are nearest, to 
form a reduced number of clusters. This is repeated, each time merging the two closest 
clusters, until just one cluster, of all the data points, exists. Usually the starting point for 
the process is the initial clustering in which each cluster consists of a single data point, 
so that the procedure begins with the n points to be clustered. 
Assume we are given n data points D = {x(1), ..., x(n)}, and a function D(Ci, Cj) for 
measuring the distance between two clusters Ci and  Cj. Then an agglomerative algorithm 
for clustering can be described as follows: 
     for i = 1, ..., n let Ci = {x(i)}; 
     while there is more than one cluster left do 
         let Ci and Cj be the clusters 
              minimizing the distance D(Ck, Ch) between any two clusters; 
         Ci = Ci ?  Cj; 
         remove cluster Cj; 
     end; 
What is the time complexity of this method? In the beginning there are n clusters, and in 
the end 1; thus there are n iterations of the main loop. In iteration i we have to find the 
closest pair of clusters among n - i + 1 clusters. We will see shortly that there are a 

variety of methods for defining the intercluster distance  D(Ci, Cj). All of them, however, 
require in the first iteration that we locate the closest pair of objects. This takes O(n2) 
time, unless we have special knowledge about the distance between objects and so, in 
most cases, the algorithm requires O(n2) time, and frequently much more. Note also that 
the space complexity of the method is also O(n2), since all pairwise distances between 
objects must be available at the start of the algorithm. Thus, the method is typically not 
feasible for large values of n. Furthermore, interpreting a large dendrogram can be quite 
difficult (just as interpreting a large classification tree can be difficult). 
Note that in agglomerative clustering we need distances between individual data objects 
to begin the clustering, and during clustering we need to be able to compute distances 
between groups of data points (that is, distances between clusters). Thus, one 
advantage of this approach (over partition-based clustering, for example) is the fact that 
we do not need to have a vector representation for each object as long as we can 
compute distances between objects or between sets of objects. Thus, for example, 
agglomerative clustering provides a natural framework for clustering objects that are not 
easily summarized as vector measurements. A good example would be clustering of 
protein sequences where there exist several well-defined notions of distance such as the 
edit-distance between two sequences (that is, a measure of how many basic edit 
operations are required to transform one sequence into another). 
In terms of the general case of distances between sets of objects (that is, clusters) many 
measures of distance have been proposed. If the objects are vectors then any of the 
global score functions described in section 9.4 can be used, using the difference 
between the score before merger and that after merging two clusters. 
However, local pairwise distance measures (that is, between pairs of clusters) are 
especially suited to hierarchical methods since they can be computed directly from 
pairwise distances of the members of each cluster. One of the earliest and most 
important of these is the nearest neighbor or single link  method. This defines the 
distance between two clusters as the distance between the two closest points, one from 
each cluster; 

(9.22)  

where d(x, y) is the distance between objects x and y. The single link method is 
susceptible (which may be a good or bad thing, depending upon our objectives) to the 
phenomenon of ""chaining,"" in which long strings of points are assigned to the same 
cluster (contrast this with the production of compact spherical clusters). This means that 
the single link method is of limited value for segmentation. It also means that the method 
is sensitive to small perturbations of the data and to outlying points (which, again, may 
be good or bad, depending upon what we are trying to do). The single link method also 
has the property (for which it is unique—no other measure of distance between clusters 
possesses it) that if two pairs of clusters are equidistant it does not matter which is 
merged first. The overall result will be the same, regardless of the order of merger. 
The dendrogram from the single link method applied to the data in  figure 9.7 is shown in 
figure 9.9. Although on this particular data set the results of single link clustering and that 
of figure 9.8 are quite similar, the two methods can in general produce quite different 
results. 

Figure 9.9: Dendrogram of the Single Link Method Applied to the Data in Figure 9.7.  

At the other extreme from single link, furthest neighbor, or complete link , takes as the 
distance between two clusters the distance between the two most distant points, one 
from each cluster: 

 

(9.23)  

where d(x, y) is again the distance between objects x and y. For vector objects this 
imposes a tendency for the groups to be of equal size in terms of the volume of space 
occupied (and not in terms of numbers of points), making this measure particularly 
appropriate for segmentation problems. 

Other important measures, intermediate between single link and complete link, include 
(for vector objects) the centroid measure (the distance between two clusters is the 
distance between their centroids), the group average measure (the distance between 
two clusters is the average of all the distances between pairs of points, one from each 
cluster), and Ward's measure for vector data (the distance between two clusters is the 
difference between the total within cluster sum of squares for the two clusters separately, 
and the within cluster sum of squares resulting from merging the two clusters discussed 
above). Each such measure has slightly different properties, and other variants also 
exist; for example, the median measure for vector data ignores the size of clusters, 
taking the ""center"" of a combination of two clusters to be the midpoint of the line joining 
the centers of the two components. Since we are seeking the novel in data mining, it 
may well be worthwhile to experiment with several measures, in case we throw up 
something unusual and interesting.  

9.5.2 Divisive Methods 

Just as stepwise methods of variable selection can start with no variables and gradually 
add variables according to which lead to most improvement (analogous to agglomerative 
cluster analysis methods), so they can also start with all the variables and gradually 
remove those whose removal leads to least deterioration in the model. This second 
approach is analogous to divisive methods of cluster analysis. Divisive methods begin 
with a single cluster composed of all of the data points, and seek to split this into 
components. These further components are then split, and the process is taken as far as 
necessary. Ultimately, of course, the process will end with a partition in which each 
cluster consists of a single point. 
Monothetic divisive methods split clusters using one variable at a time (so they are 
analogous to the basic form of tree classification methods discussed in chapter 5). This 
is a convenient (though restrictive) way to limit the number of possible partitions that 
must be examined. It has the attraction that the result is easily described by the 

dendrogram—the split at each node is defined in terms of just a single variable. The term 
association analysis is sometimes uses to describe monothetic divisive procedures 
applied to multivariate binary data. (This is not the same use as the term ""association 
rules"" described in chapter 5.) 
Polythetic divisive methods make splits on the basis of all of the variables together. Any 
intercluster distance measure can be used. The difficulty comes in deciding how to 
choose potential allocations to clusters—that is, how to restrict the search through the 
space of possible partitions. In one approach, objects are examined one at a time, and 
that one is selected for transfer from a main cluster to a subcluster that leads to the 
greatest improvement in the clustering score. 

In general, divisive methods are more computationally intensive and tend to be less 
widely used than agglomerative methods. 

9.6 Probabilistic Model-Based Clustering using Mixture 
Models 
The mixture models of section 9.2.4 can also be used to provide a general framework for 
clustering in a probabilistic context. This is often referred to as probabilistic model-based 
clustering since there is an assumed probability model for each component cluster. In 
this framework it is assumed that the data come from a multivariate finite mixture model 
of the general form 

 
 

(9.24)  

where ƒk are the component distributions. Roughly speaking, the general procedure is as 
follows: given a data set D = {x(1), ..., x(n)}, determine how many clusters K we want to 
fit to the data, choose parametric models for each of these K clusters (for example, 
multivariate Normal distributionsare a common choice), and then use the EM algorithm 
of section 9.2.4 (and described in more detail in chapter 8) to determine the component 
parameters ?k and component probabilities p k from the data. (We can of course also try 
to determine a good value of K from the data, we will return to this question later in this 
section.) Typically the likelihood of the data (given the mixture model) is used as the 
score function, although other criteria (such as the so-called classification likelihood) can 
also be used. Once a mixture decomposition has been found, the data can then be 
assigned to clusters—for example, by assigning each point to the cluster from which it is 
most likely to have come. 

To illustrate the idea, we apply the method to a data set where the true class labels are 
in fact known but are removed and then ""discovered"" by the algorithm. 

Example 9.4  

 
Individuals with chronic iron deficiency anemia tend to produce red blood cells of lower 
volume and lower hemoglobin concentration than normal. A blood sample can be taken to 
determine a person's mean red blood cell volume and hemoglobin concentration. Figure 
9.10 shows a scatter plot of the bivariate mean volume and hemoglobin concentrations for 
182 individuals with labels determined by a diagnostic lab test. A normal mixture model 
with K = 2 was fit to these individuals, with the labels removed. The results are shown in 
figure 9.11, illustrating that a two-component normal mixture appears to capture the main 
features of the data and would provide an excellent clustering if the group labels were 
unknown (that is, if the lab test had not been performed). Figure 9.2 verifies that the 
likelihood (or equivalently, loglikelihood) is nondecreasing as a function of iteration number. 
Note, however, that the rate of convergence is nonmonotonic; that is, between iterations 5 
and 8 the rate of increase in log-likelihood slows down, and then increases again from 
iterations 8 to 12. 

Figure 9.10: Red Blood Cell Measurements (Mean Volume and Mean Hemoglobin 
Concentration) From 182 Individuals Showing the Separation of the Individuals into two 
Groups: Healthy (Circles) and Iron Deficient Anemia (Crosses).  

 

Figure 9.11: Example of Running the EM Algorithm on the Red Blood Cell Measurements of 
Figure 9.10. The Plots (Running Top to Bottom, Left First, then Right) Show the 3s 
Covariance Ellipses and Means of the Fitted Components at Various Stages of the EM 
Algorithm.  

 

 

 

The red blood cell example of figure 9.11 illustrates several features of the probabilistic 
approach: 

§ The probabilistic model provides a full distributional description for each 
component. Note, for example, the difference between the two fitted 
clusters in the red blood cell example. The normal component is relatively 
compact, indicating that variability across individuals under normal 
circumstances is rather low. The iron deficient anemia cluster, on the other 

hand, has a much greater spread, indicating more variability. This certainly 
agrees with our common-sense intuition, and it is the type of information 
that can be very useful to a scientist investigating fundamental mechanisms 
at work in the data-generating process. 

§ Given the model, each individual (each data point) has an associated K-

component vector of the probabilities that it arose from each group, and 
that can be calculated in a simple manner using Bayes rule. These 
probabilities are used as the basis for partitioning the data, and hence 
defining the clustering. For the red blood cell data, most individuals lie in 
one group or the other with probability near 1. However, there are certain 
individuals (close to the intersection of the two clouds) whose probability 
memberships will be closer to 0.5—that is, there is uncertainty about which 
group they belong to. Again, from the viewpoint of exploring the data, such 
data points may be valuable and worthy of detection and closer study (for 
example, individuals who may be just at the onset of iron deficient anemia). 

§ The score function and optimization procedures are quite natural in a 

probabilistic context, namely likelihood and EM respectively. Thus, there is 
a well-defined theory on how to fit parameters to such models as well as a 
large library of algorithms that can be leveraged. Extensions to MAP and 
Bayesian estimation (allowing incorporation of prior knowledge) are 
relatively straightforward. 

§ The basic finite mixture model provides a principled framework for a variety of 

extensions. One useful idea, for example, is to add a (K + 1)th noise 
component (for example, a uniform density) to pick up outliers and 
background points that do not appear to belong to any of the other K 
components; the relative weight p K+1 of this background component can be 
learned by EM directly from the data. 

§ The method can be extended to data that are not in p-dimensional vector 

form. For example, we can cluster sequences using mixtures of 
probabilistic sequence models (for example, mixtures of Markov models), 
cluster curves using mixtures of regression models, and so forth, all within 
the same general EM framework. 

These advantages come at a certain cost. The main ""cost"" is the assumption of a 
parametric model for each component; for many problems it may be difficult a priori to 
know what distributional forms to assume. Thus, model-based probabilistic clustering is 
really only useful when we have reason to believe that the distributional forms are 
appropriate. For our red blood cell data, we can see by visual inspection that the normal 
assumptions are quite reasonable. Furthermore, since the two measurements consist of 
estimated means from large samples of blood cells, basic statistical theory also suggests 
that a normal distribution is likely to be quite appropriate. 
The other main disadvantage of the probabilistic approach is the complexity of the 
associated estimation algorithm. Consider the difference between EM and K-means. We 
can think of K-means as a stepwise approximation to the EM algorithm applied to a 
mixture model with Normal mixture components (where the covariance matrices for each 
cluster are all assumed to be the identity matrix). However, rather than waiting until 
convergence is complete before assigning the points to the clusters, the K-means 
algorithm reassigns them at each step. 

Example 9.5  

 
Suppose that we have a data set where each variable Xj is 0/1 valued—for example, a 
large transaction data set where xj = 1 (or 0) represents whether a person purchased item j 
(or not). We can apply the mixture modeling framework as follows: assume that given the 
cluster k, the variables are conditionally independent (as discussed in section 9.2.7); that 
is, that we can write 

 

To specify a model for the data, we just need to specify the probability of observing value 1 
for the jth variable in the kth component. Denoting this probability by ?kj, we can write the 
component density for the kth component as 

which is a convenient way of writing the probability of observing value xj in component k of 
the mixture model. The full mixture equation for observation x(i) is the weighted sum of 
these component densities: 
(9.25)  

 

(9.26)  

 

where xj(i) indicates whether person i bought product j or not. 
The EM equations for this model are quite simple. Let p(k|i) be the probability that person i 
belongs to cluster k. By Bayes rule, and given a fixed set of parameters ? this can be 
written as: 
(9.27)  

where p(x(i)) is as defined in equation 9.26. Calculation of p(k|i) takes O(nK) steps since it 
must be carried out for each individual i and each cluster k. Calculation of these 
""membership probabilities"" is in effect the E-step for this problem. 
The M-step is simply a weighted estimate of the probability of a person buying item j given 
that they belong to cluster k: 
(9.28)  

where in this equation, observation  xj(i) is weighted by the probability p(k|i), namely the 
probability that individual i was generated by cluster k (according to the model). A particular 
product j purchased by individual i is in effect assigned fractionally (via the weights p(k|i), 1 
= k = K) to the K cluster models. This M-step requires O(nKp) operations since the 
weighted sum in the numerator must be performed over all n individuals, for each cluster k, 
and for each of the p parameters (one for each variable in the independence model). If we 
have I iterations of the EM algorithm in total we get O(IKnp) as the basic complexity, which 
can be thought of as I times K times the size of the data matrix. 
For really large data sets that reside on disk, however, doing I passes through the data set 
will not be computationally tractable. Techniques have been developed for summarizing 
cluster representations so that the data set can in effect be compressed during the 
clustering method. For example, in mixture modeling many data points ""gravitate"" to one 
component relatively early in the computation; that is, their membership probability for this 
component approaches 1. Updating the membership of such points could be omitted in 
future iterations. Similarly, if a point belongs to a group of points that always share cluster 
membership, then the points can be represented by using a short description. 

 

 

To conclude our discussion on probabilistic clustering, consider the problem of finding 
the best value for K from the data. Note that as K (the number of clusters) is increased, 
the value of the likelihood at its maximum cannot decrease as a function of K. Thus, 
likelihood alone cannot tell us directly about which of the models, as a function of K, is 
closest to the true data generating process. Moreover, the usual approach of hypothesis 
testing (for example, testing the hypothesis of one component versus two, two versus 
three, and so forth) does not work for technical reasons related to the mixture likelihood. 
However, a variety of other ingenious schemes have been developed based to a large 
extent on approximations of theoretical analyses. We can identify three general classes 
of techniques in relatively widespread use: 

§  Penalized Likelihood: Subtract a term from the maximizing value of the 

likelihood. The BIC (Bayesian Information Criterion) is widely used. Here 

(9.29)  

§  where SL(?K ; MK) is the minimizing value of the negative log-likelihood and dK 
is the number of parameters, both for a mixture model with K components. 
This is evaluated from K = 1 up to some Kmax and the minimum taken as the 
most likely value of K. The original derivation of BIC was based on 
asymptotic arguments in a different (regression) context, arguments that do 
not strictly hold for mixture modeling. Nonetheless, the technique has been 
found to work quite well in practice and has the merit of being relatively 
cheap to compute relative to the other methods listed below. In figure 9.12 
the negative of the BIC score function is plotted for the red blood cell data 
and points to K = 2 as the best model (recall that there is independent 
medical knowledge that the data belong to two groups here, so this result is 
quite satisfying). There are a variety of other proposals for penalty terms 
(see chapter 7), but BIC appears to be the most widely used in the 
clustering context. 

§ 

 

Figure 9.12: Log-Likelihood and BIC Score as a Function of the Number of 
Normal Components Fitted to the Red Blood Cell Data of Figure 9.11.  

§  Resampling Techniques: We can use either bootstrap methods or cross-

validated likelihood using resampling ideas as another approach to generate 
""honest"" estimates of which K value is best. These techniques have the 
drawback of requiring significantly more computation than BIC—for 
example, ten times more for the application of ten-fold cross-validation. 
However, they do provide a more direct assessment of the quality of the 
models, avoiding the need for the assumptions associated with methods 
such as BIC. 

§  Bayesian Approximations: The fully Bayesian solution to the problem is to 

estimate a distribution p(K|D),—that is, the probability of each K value given 
the data, where all uncertainty about the parameters is integrated out in the 
usual fashion. In practice, of course, this integration is intractable (recall that 
we are integrating in a dK-dimensional space) so various approximations are 
sought. Both analytic approximations (for example, the Laplace 
approximation about the mode of the posterior distribution) and sampling 
techniques (such as Markov chain Monte Carlo) are used. For large data 
sets with many parameters in the model, sampling techniques may be 
computationally impractical, so analytic approximation methods tend to be 
more widely used. For example, the AUTOCLASS algorithm of Cheeseman 
and Stutz (1996) for clustering with mixture models uses a specific analytic 
approximation of the posterior distribution for model selection. The BIC 
penalty-based score function can also be viewed as an approximation to the 
full Bayesian approach. 

In a sense, the formal probabilistic modeling implicit in mixture decomposition is more 
general than cluster analysis. Cluster analysis aims to produce merely a partition of the 
available data, whereas mixture decomposition produces a description of the distribution 
underlying the data (that this distribution is composed of a number of components). Once 
these component probability distributions have been identified, points in the data set can 

 
 

be assigned to clusters on the basis of the component that is most likely to have 
generated them. We can also look at this another way: the aim of cluster analysis is to 
divide the data into naturally occurring regions in which the points are closely or densely 
clustered, so that there are relatively sparse regions between the clusters. From a 
probability density perspective, this will correspond to regions of high density separated 
by valleys of low density, so that the probability density function is fundamentally 
multimodal. However, mixture distributions, even though they are composed of several 
components, can well be unimodal. 

Consider the case of a two-component univariate normal mixture. Clearly, if the means 
are equal, then this will be unimodal. In fact, a sufficient condition for the mixture to be 
unimodal (for all values of the mixing proportions) when the means are different is | µ1 - 
µ2 |= 2 min(s 1, s 2). Furthermore, for every choice of values of the means and standard 
deviations in a two-component normal mixture there exist values of the mixing 
proportions for which the mixture is unimodal. This means that if the means are close 
enough there will be just one cluster, even though there are two components. We can 
still use the mixture decomposition to induce a clustering, by assigning each data point to 
the cluster from which it is most likely to have come, but this is unlikely to be a useful 
clustering. 

9.7 Further Reading 
A general introduction to parametric probability modeling is Ross (1997) and an 
introduction to general concepts in multivariate data analysis is provided by Everitt and 
Dunn (1991). General texts on mixture distributions include Everitt and Hand (1981), 
Titterington, Smith, and Makov (1985), McLachlan and Bàsford (1988), Böhning (1998), 
and McLachlan and Peel (2000). Diebolt and Robert (1994) provide an example of the 
general Bayesian approach to mixture modeling. Statistical treatments of graphical 
models include those by Whittaker (1990), Edwards (1995), Cox and Wermuth (1996), 
and Lauritzen (1996). Pearl (1988) and Jensen (1996) emphasize representational and 
computational aspects of such models, and the edited collection by Jordan (1999) 
contains recent research articles on learning graphical models from data. Friedman and 
Goldszmidt (1996) and Chickering, Heckerman, and Meek (1997) provide details on 
specific algorithms for learning graphical models from data. Della Pietra, Della Pietra, 
and Lafferty (1997) describe the application of Markov random fields to text modeling, 
and Heckerman et al. (2000) describe use a form of Markov random fields for model-
based collaborative filtering. Bishop, Fienberg, and Holland (1975) is a standard 
reference on log-linear models. 
There are now many books on cluster analysis. Recommended ones include Anderberg 
(1973), Späth (1985), Jain and Dubes (1988), and Kaufman and Rousseeuw (1990). The 
distinction between dissection and finding natural partitions is not always appreciated, 
and yet it can be important and should not be ignored. Examples of authors who have 
made the distinction include Kendall (1980), Gordon (1981), and Späth (1985). Marriott 
(1971) showed that the criterion K2tr(W) was asymptotically constant for optimal 
partitions of a multivariate uniform distribution. Krzanowski and Marriott (1995), table 
10.6, give a list of updating formula for clustering criteria based on W. Maximal predictive 
classification was developed by Gower (1974). The use of branch and bound to extend 
the range of exhaustive evaluation of all possible clusterings is described in Koontz, 
Narendra, and Fukunaga (1975) and Hand (1981). The K-means algorithm is described 
in MacQueen (1967), and the ISODATA algorithm is described in  Hall and Ball (1965). 
Kaufman and Rousseeuw (1990) describe a variant in which the ""central point"" of each 
cluster is an element of that cluster, rather than the centroid of the elements. A review of 
early work on mathematical programming methods applied in cluster analysis is given by 
Rao (1971) with a more recent review provided by Mangasarian (1996).  
One of the earliest references to the single link method of cluster analysis was Florek et 
al. (1951), and Sibson (1973) was important in promoting the idea. Lance and Williams 
(1967) presented a general formula, useful for computational purposes, that included 
single link and complete link as special cases. The median method of cluster analysis is 
due to Gower (1967). Lambert and Williams (1966) describe the ""association analysis"" 
method of monothetic divisive partitioning. The polythetic divisive method of clustering is 

due to MacNaughton-Smith et al. (1964). The overlapping cluster methods are due to 
Shepard and Arabie (1979). 
Other formalisms for clustering also exist. For example, Karypis and Kumar (1998) 
discuss graph-based clustering algorithms. Zhang, Ramakrishnan, and Livny (1997) 
describe a framework for clustering that is scalable to very large databases. There are 
also countless applications of clustering. For a cluster analytic study of whiskies, see 
Lapointe and Legendre (1994). Eisen et al. (1998) illustrate the application of hierarchical 
agglomerative clustering to gene expression data. Zamir and Etzioni (1998) describe a 
clustering algorithm specifically for clustering Web documents. 
Probabilistic clustering is discussed in the context of mixture models in  Titterington, 
Smith, and Makov (1985) and in McLachlan and Basford (1987). Banfield and Raftery 
(1993) proposed the idea (in a mixture model context) of adding a ""cluster"" that pervades 
the whole space by superimposing a separate Poisson process that generated a low 
level of random points throughout the entire space, so easing the problem of clusters 
being distorted due to a handful of outlying points. More recent work on model-based 
probabilistic clustering is described in  Celeux and Govaert (1995), Fraley and Raftery 
(1998), and McLachlan and Peel (1998). The application of mixture models to clustering 
sequences is described in Poulsen (1990), Smyth (1997), Ridgeway (1997), and Smyth 
(1999). Mixture-based clustering of curves for parametric models was originally 
described by Quandt and Ramsey (1978) and Späth (1979) and later generalized to the 
non-parametric case by Gaffney and Smyth (1999). Jordan and Jacobs (1994) provide a 
generalization of standard mixtures to a mixture-based architecture called ""mixtures of 
experts"" that provides a general mixture-based framework for function approximation. 
Studies of tests for numbers of components of mixture models are described in Everitt 
(1981), McLachlan (1987), and Mendell, Finch, and Thode (1993). An early derivation of 
the BIC criterion is provided by Shibata (1978). Kass and Raftery (1995) provide a more 
recent overview including a justification for the application of BIC to a broad range of 
model selection tasks. The bootstrap method for determining the number of mixture 
model components was introduced by McLachlan (1987) and later refinements can be 
found in Feng and McCulloch (1996) and McLachlan and Peel (1997). Smyth (2000) 
describes a cross-validation approach to the same problem. Cheeseman and Stutz 
(1996) outline a general Bayesian framework to the problem of modelbased clustering, 
and Chickering and Heckerman (1998) discuss an empirical study comparing different 
Bayesian approximation methods for finding the number of components K. 
Different techniques for speeding up the basic EM algorithm for large data sets are 
described in Neal and Hinton (1998), Bradley, Fayyad, and Reina (1998), and Moore 
(1999). 
Cheng and Wallace (1993) describe an interesting application of hierarchical 
agglomerative clustering to the problem of clustering spatial atmospheric measurements 
from the Earth's upper atmosphere. Smyth, Ide, and Ghil (1999) provide an alternative 
analysis of the same data using Normal mixture models, and use cross-validated 
likelihood to provide a quantitative confirmation of the earlier Cheng and Wallace 
clusters. Mixture models in haematology are described in McLaren (1996). Wedel and 
Kamakura (1998) provide an extensive review of the development and application of 
mixture models in consumer modeling and marketing applications. Cadez et al. (2000) 
describe the application of mixtures of Markov models to the problem of clustering 
individuals based on sequences of page-requests from massive Web logs. 
The antenna data of  figure 9.4 are described in more detail in Smyth (1994) and the red 
blood cell data of figure 9.10 are described in  Cadez et al. (1999). 

Chapter 10: Predictive Modeling for 
Classification 
10.1 A Brief Overview of Predictive Modeling 
Descriptive models, as described in chapter 9, simply summarize data in convenient 
ways or in ways that we hope will lead to increased understanding of the way things 
work. In contrast, predictive models have the specific aim of allowing us to predict the 
unknown value of a variable of interest given known values of other variables. Examples 

 
 

include providing a diagnosis for a medical patient on the basis of a set of test results, 
estimating the probability that customers will buy product A given a list of other products 
they have purchased, or predicting the value of the Dow Jones index six months from 
now, given current and past values of the index. 
In chapter 6 we discussed many of the basic functional forms of models that can be used 
for prediction. In this chapter and the next, we examine such models in more detail, and 
look at some of the specific aspects of the criteria and algorithms that permit such 
models to be fitted to the data. 
Predictive modeling can be thought of as learning a mapping from an input set of vector 
measurements x to a scalar output y (we can learn mappings to vector outputs, but the 
scalar case is much more common in practice). In predictive modeling the training data 
Dtrain consists of pairs of measurements, each consisting of a vector x(i) with a 
corresponding ""target"" value  y(i), 1 = i = n. Thus the goal of predictive modeling is to 
estimate (from the training data) a mapping or a function  y = ƒ(x; ?) that can predict a 
value y given an input vector of measured values x and a set of estimated parameters ? 
for the model ƒ. Recall that ƒ is the functional form of the model structure (chapter 6), the 
?s are the unknown parameters within ƒ whose values we will determine by minimizing a 
suitable score function on the data (chapter 7), and the process of searching for the best 
? values is the basis for the actual data mining algorithm (chapter 8). We thus need to 
choose three things: a particular model structure (or a family of model structures), a 
score function, and an optimization strategy for finding the best parameters and model 
within the model family. 
In data mining problems, since we typically know very little about the functional form of 
ƒ(x; ?) ahead of time, there may be attractions in adopting fairly flexible functional forms 
or models for ƒ. On the other hand, as discussed in chapter 6, simpler models have the 
advantage of often being more stable and more interpretable, as well as often providing 
the functional components for more complex model structures. For predictive modeling, 
the score function is usually relatively straightforward to define, typically a function of the 

difference between the prediction of the model 
that is, 

(10.1)  

and the true value  y(i)—

where the sum is taken over the tuples (x(i), y(i)) in the training data set Dtrain and the 
function d defines a scalar distance such as squared error for real-valued y or an 
indicator function for categorical y (see chapter 7 for further discussion in this context). 
The actual heart of the data mining algorithm then involves minimizing S as a function of 
?; the details of this are determined both by the nature of the distance function and by 
the functional form of ƒ(x; ?) that jointly determine how S depends on ? (see the 
discussion in chapter 8). 
To compare predictive models we need to estimate their performance on ""out-of-sample 
data""—data that have not been used in constructing the models (or else, as discussed 
earlier, the performance estimates are likely to be biased). In this case we can redefine 
the score function S(?) so that it is estimated on a validation data set, or via cross-
validation, or using a penalized score function, rather than on the training data directly 
(as discussed in chapter 7). 
We noted in chapter 6 that there are two important distinct kinds of tasks in predictive 
modeling depending on whether Y is categorical or real-valued. For categorical Y the 
task is called classification (or supervised classification to distinguish it from problems 
concerned with defining the classes in the first instance, such as cluster analysis), and 
for real-valued y the task is called regression. Classification problems are the focus of 
this chapter, and regression problems are the focus of the next chapter. Although we can 
legitimately discuss both forms of modeling in the same general context (they share 
many of the same mathematical and statistical underpinnings), in the interests of 
organizational style we have assigned classification and regression each their own 
chapter. However, it is important for the reader to be aware that many of the model 
structures for classification that we discuss in this chapter have a ""twin"" in terms of being 

 
 

applicable to regression (chapter 11). For example, we discuss tree structures in the 
classification chapter, but they can also be used for regression. Similarly we discuss 
neural networks under regression, but they can also be used for classification. 
In these two chapters we cover many of the more commonly used approaches to 
classification and regression problems—that is, the more commonly used tuples of 
model structures, score functions, and optimization techniques. The natural taxonomy of 
these algorithms tends to be closely aligned with the model structures being used for 
prediction (for example, tree structures, linear models, polynomials, and so on), leading 
to a division of the chapters largely into subsections according to different model 
structures. Even though specific combinations of models, score functions, and 
optimization strategies have become very popular (""standard"" data mining algorithms) it 
is important to remember the general reductionist philosophy of data mining algorithms 
that we described in chapter 5; for a particular data mining problem we should always be 
aware of the option of tailoring the model, the score function, or the optimization strategy 
for the specific application at hand rather than just using an ""off-the-shelf"" technique. 

10.2 Introduction to Classification Modeling 
We introduced predictive models for classification in chapter 6. Here we briefly review 
some of the basic concepts. In classification we wish to learn a mapping from a vector of 
measurements x to a categorical variable Y. The variable to be predicted is typically 
called the class variable (for obvious reasons), and for convenience of notation we will 
use the variable  C, taking values in the set {c1, ..., cm} to denote this class variable for the 
rest of this chapter (instead of using Y). The observed or measured variables X1, ..., Xp 
are variously referred to as the features, attributes, explanatory variables, input 
variables, and so on—the generic term input variable will be used throughout this 
chapter. We will refer to x as a p-dimensional vector (that is, we take it to be comprised 
of p variables), where each component can be real-valued, ordinal, categorical, and so 
forth. xj(i) is the jth component of the ith input vector, where 1 = i = n, 1 = j = p. In our 
introductory discussion we will implicitly assume that we are using the so-called ""0–1"" 
loss function (see chapter 7), where a correct prediction incurs a loss of 0 and an 
incorrect class prediction incurs a loss of 1 irrespective of the true class and the 
predicted class. 

We will begin by discussing two different but related general views of classification: the 
decision boundary (or discriminative) viewpoint, and the probabilistic viewpoint. 

10.2.1 Discriminative Classification and Decision Boundaries 
In the discriminative framework a classification model ƒ(x; ?) takes as input the 
measurements in the vector x and produces as output a symbol from the set {c1, ..., cm}. 
Consider the nature of the mapping function ƒ for a simple problem with just two real-
valued input variables X1 and X2. The mapping in effect produces a piecewise constant 
surface over the (X1, X2) plane; that is, only in certain regions does the surface take the 
value c1. The union of all such regions where a c1 is predicted is known as the decision 
region for class c1; that is, if an input x(i) falls in this region its class will be predicted as 
c1 (and the complement of this region is the decision region for all other classes). 
Knowing where these decision regions are located in the (X1, X2) plane is equivalent to 
knowing where the decision boundaries or decision surfaces are between the regions. 
Thus we can think of the problem of learning a classification function ƒ as being 
equivalent to learning decision boundaries between the classes. In this context, we can 
begin to think of the mathematical forms we can use to describe decision boundaries, for 
example, straight lines or planes (linear boundaries), curved boundaries such as low-
order polynomials, and other more exotic functions. 
In most real classification problems the classes are not perfectly separable in the X 
space. That is, it is possible for members of more than one class to occur at some 
(perhaps all) values of X—though the probability that members of each class occur at 
any given value x will be different. (It is the fact that these probabilities differ that permits 
us to make a classification. Broadly speaking, we assign a point x to the most probable 
class at x.) The fact that the classes ""overlap"" leads to another way of looking at 

classification problems. Instead of focusing on decision surfaces, we can seek a function 
ƒ(x; ?) that maximizes some measure of separation between the classes. Such functions 
are termed discriminant functions. Indeed, the earliest formal approach to classification, 
Fisher's linear discriminant analysis method (Fisher, 1936), was based on precisely this 
idea: it sought that linear combination of the variables in x that maximally discriminated 
between the (two) classes. 

10.2.2 Probabilistic Models for Classification 
Let p(ck) be the probability that a randomly chosen object or individual i comes from 
class ck. Then ? k p(ck) = 1, assuming that the classes are mutually exclusive and 
exhaustive. This may not always be the case—for example, if a person had more than 
one disease (classes are not mutually exclusive) we might model the problem as set of 
multiple two-class classification problems (""disease 1 or not,"" ""disease 2 or not,"" and so 
on). Or there might be a disease that is not in our classification model (the set of classes 
is not exhaustive), in which case we could add an extra class ck+1 to the model to 
account for ""all other diseases."" Despite these potential practical complications, unless 
stated otherwise we will use the mutually exclusive and exhaustive assumption 
throughout this chapter since it is widely applicable in practice and provides the essential 
basis for probabilistic classification. 
Imagine that there are two classes, males and females, and that p(ck), k = 1, 2, 
represents the probability that at conception a person receives the appropriate 
chromosomes to develop as male or female. The p(ck) are thus the probabilities that 
individual i belongs to class ck if we have no other information (no measurements x(i)) at 
all. The p(ck) are sometime referred to as the class ""prior probabilities,"" since they 
represent the probabilities of class membership before observing the vector x. Note that 
estimating the p(ck) from data is often relatively easy: if a random sample of the entire 
population has been drawn, the maximum likelihood estimate of p(ck) is just the 
frequency with which ck occurs in the training data set. Of course, if other sampling 
schemes have been adopted, things may be more complicated. For example, in some 
medical situations it is common to sample equal numbers from each class deliberately, 
so that the priors have to be estimated by some other means. 
Objects or individuals belonging to class k are assumed to have measurement vectors x 
distributed according to some distribution or density function  p(x|ck, ?k) where the ?k are 
unknown parameters governing the characteristics of class ck. For example, for 
multivariate real-valued data, the assumed model structure for the x for each class might 
be multivariate Normal, and the parameters ?k would represent the mean (location) and 
variance (scale) characteristics for each class. If the means are far enough apart, and 
the variances small enough, we can hope that the classes are relatively well separated in 
the input space, permitting classification with very low misclassification (or error) rate. 
The general problem arises when neither the functional form nor the parameters of the 
distributions of the xs are known a priori. 
Once the p(x|ck, ?k) distributions have been estimated, we can apply Bayes theorem to 
yield the posterior probabilities  

(10.2)  

The posterior probabilities p(ck|x, ?k) implicitly carve up the input space x into m decision 
regions with corresponding decision boundaries. For example, with two classes (m = 2) 
the decision boundaries will be located along the contours where p(c1|x, ?1) = p(c2|x, ?2). 
Note that if we knew the true posterior class probabilities (instead of having to estimate 
them), we could make optimal predictions given a measurement vector x. For example, 
for the case in which all errors incur equal cost we should predict the class value ck that 
has the highest posterior probability p(ck|x) (is most likely given the data) for any given x 
value. Note that this scheme is optimal in the sense that no other prediction method can 
do better (with the given variables x)—it does not mean that it makes no errors. Indeed, 
in most real problems the optimal classification scheme will have a nonzero error rate, 
arising from the overlap of the distributions p(x|ck, ?k). This overlap means that the 
maximum class probability p(ck|x) < 1, so that there is a non-zero probability 1-p(ck|x) of 
data arising from the other (less likely) classes at x, even though the optimal decision at 
x is to choose ck. Extending this argument over the whole space, and averaging with 

respect to x (or summing over discrete-valued variables), the Bayes Error Rate is 
defined as 

(10.3)  

This is the minimum possible error rate. No other classifier can achieve a lower expected 
error rate on unseen new data. In practical terms, the Bayes error is a lower-bound on 
the best possible classifier for the problem. 

Example 10.1  

 
Figure 10.1 shows a simple artificial example with a single predictor variable X (the 
horizontal axis) and two classes. The upper two plots show how the data are distributed 
within class 1 and class 2 respectively. The plots show the joint probability of the class and 
the variable X, p(x, ck), k = 1, 2. Each has a uniform distribution over a different range of X; 
class c1 tends to have lower x values than class c2. There is a region along the x axis 
(between values  x1 and  x2) where both class populations overlap. 

 

Figure 10.1: A Simple Example Illustrating Posterior Class Probabilities for a Two-Class One-
Dimensional Classification Problem.  
The bottom plot shows the posterior class probability for class c1, p(c1|x) as calculated via 
Bayes rule given the class distributions shown in the upper two plots. For values of  x = x1, 
the probability is 1 (since only class c1 can produce data in that region), and for values of  x 
= x2 the probability is 0 (since only class c2 can produce data in that region). The region of 
overlap (between  x1 and  x2) has a posterior probability of about 1/3 for class c1 (by Bayes 
rule) since class c2 is roughly twice as likely as class c1 in this region. Thus, class c2 is the 
Bayes-optimal decision for any x = x1 (noting that in the regions where p(x, c1) or p(x, c2) 
are both zero, the posterior probability is undefined). However, note that between  x1 and  x2 
there is some fundamental ambiguity about which class may be present given an x value in 
this region; that is, although c2 is the more likely class there is a 1/3 chance of c1 occurring. 
In fact, since there is a 1/3 chance of making an incorrect decision in this region, and let us 
guess from visual inspection that there is a 20% chance of an x value falling in this region, 
this leads to a rough estimate of a Bayes error rate of about 20/3 ˜ 6.67% for this particular 
problem. 

 

 

Now consider a situation in which x is bivariate, and in which the members of one class 
are entirely surrounded by members of the other class. Here neither of the two X 
variables alone will lead to classification rules with zero error rate, but (provided an 
appropriate model was used) a rule based on both variables together could have zero 
error rate. Analogous situations, though seldom quite so extreme, often occur in practice: 
new variables add information, so that we can reduce the Bayes error rate by adding 

extra variables. This prompts this question: why should we not simply use many 
measurements in a classification problem, until the error rate is sufficiently low? The 
answer lies in the the bias-variance principle discussed in chapters 4 and 7. While the 
Bayes error rate can only stay the same or decrease if we add more variables to the 
model, in fact we do not know the optimal classifier or the Bayes error rate. We have to 
estimate a classification rule from a finite set of training data. If the number of variables 
for a fixed number of training points is increased, the training data are representing the 
underlying distributions less and less accurately. The Bayes error rate may be 
decreasing, but we have a poorer approximation to it. At some point, as the number of 
variables increases, the paucity of our approximation overwhelms the reduction in Bayes 
error rate, and the rules begin to deteriorate. 
The solution is to choose our variables with care; we need variables that, when taken 
together, separate the classes well. Finding appropriate variables (or a small number of 
features—combinations of variables) is the key to effective classification. This is perhaps 
especially marked for complex and potentially very high dimensional data such as 
images, where it is generally acknowledged that finding the appropriate features can 
have a much greater impact on classification accuracy than the  variability that may arise 
by choosing different classification models. One data-driven approach in this context is to 
use a score function such as cross-validated error rate to guide a search through 
combinations of features—of course, for some classifiers this may be very 
computationally intensive, since the classifier may need to be retrained for each subset 
examined and the total number of such subsets is combinatorial in p (the number of 
variables). 

10.2.3 Building Real Classifiers 
While this framework provides insight from a theoretical viewpoint, it does not provide a 
prescriptive framework for classification modeling. That is, it does not tell us specifically 
how to construct classifiers unless we happen to know precisely the functional form of 
p(x|ck) (which is rare in practice). We can list three fundamental approaches: 

1.  The discriminative approach: Here we try to model the decision 

boundaries directly—that is, a direct mapping from inputs x to one of 
m class label c1, ..., cm. No direct attempt is made to model either the 
class-conditional or posterior class probabilities. Examples of this 
approach include perceptrons (see section 10.3) and the more general 
support vector machines (see section 10.9). 

2.  The regression approach: The posterior class probabilities p(ck|x) 

are modeled explicitly, and for prediction the maximum of these 
probabilities (possibly weighted by a cost function) is chosen. The 
most widely used technique in this category is known as logistic 
regression, discussed in section 10.7. Note that decision trees (for 
example, CART from chapter 5) can be considered under either the 
discriminative approach (if the tree only provides the predicted class at 
each leaf) or the regression approach (if in addition the tree provides 
the posterior class probability distribution at each leaf). 

3.  The class-conditional approach: Here, the class-conditional 

distributions p(x|ck, ?k) are modeled explicitly, and along with 
estimates of p(ck) are inverted via Bayes rule (equation 10.2) to arrive 
at p(ck|x) for each class ck, a maximum is picked (possibly weighted by 
costs), and so forth, as in the regression approach. We can refer to 
this as a ""generative"" model in the sense that we are specifying (via 
p(x|ck, ?k)) precisely how the data are generated for each class. 
Classifiers using this approach are also sometimes referred to as 
""Bayesian"" classifiers because of the use of Bayes theorem, but they 
are not necessarily Bayesian in the formal sense of Bayesian 
parameter estimation discussed in chapter 4. In practice the parameter 
estimates used in equation 10.2, 
, are often estimated via maximum 
likelihood for each class ck, and ""plugged in"" to p(x|ck, ?k). There are 
Bayesian alternatives that average over ?k. Furthermore, the functional 
form of p(x|ck, ?k) can be quite general—any of parametric (for 
example, Normal), semi-parametric (for example, finite mixtures), or 

non-parametric (for example, kernels) can be used to estimate p(x|ck, 
?k). In addition, in principle, different model structures can be used for 
each class ck (for example, class c1 could be modeled as a Normal 
density, class c2 could be modeled as a mixture of exponentials, and 
class c3 could be modeled via a kernel density estimate). 

Example 10.2  

 
Choosing the most likely class is in general equivalent to picking the value of k for which 
the discriminant function gk(x) = p(ck|x) is largest, 1 = m. It is often convenient to redefine 
the discriminants as gk(x) = log p(x|ck)p(ck) (via Bayes rule). For multivariate real-valued 
data x, a commonly used class-conditional model is the multivariate Normal as discussed 
in chapter 9. If we take log (base e) of the Normal multivariate density function, and ignore 
terms that do not include k we get discriminant functions of the following form: 
(10.4)  

In the general case each of these gk(x) involve quadratics and pairwise products of the 
individual x variables. The decision boundary between any two classes k and l is defined by 
the solution to the equation gk(x) - gl(x) = 0 as a function of x, and this will also be quadratic 
in x in the general case. Thus, a multivariate Normal class-conditional model leads to 
quadratic decision boundaries in general. In fact, if the covariance matrices Sk for each 
class k are constrained to be the same (Sk = S) it is straightforward to show that the gk(x) 
functions reduce to linear functions of x and the resulting decision boundaries are linear 
(that is, they define hyperplanes in the  p-dimensional space). 
Figure 10.2 shows the results of fitting a multivariate Normal classification model to the red 
blood cell data described in chapter 9. Maximum likelihood estimates (chapter 4) of µk, Sk, 
p(ck) are obtained using the data from each of the two classes, k = 1; 2, and then ""plugged 
in"" to Bayes rule to determine the posterior probability function p(ck|x). In agreement with 
theory, we see that the resulting decision boundary is indeed quadratic in form (as indeed 
are the other plotted posterior probability contours). Note that the contours fall off rather 
sharply as one goes outwards from the mean of the healthy class (the crosses). Since the 
healthy class (class c1) is characterized by lower variance in general than the anemic class 
(class c2, the circles), the optimal classifier (assuming the Normal model) results in a 
boundary that completely encircles the healthy class. 

 

Figure 10.2: Posterior Probability Contours for p(c1|x) Where c1 is the Label for the Healthy 
Class for the Red Blood Cell Data Discussed in Chapter 9. The Heavy Line is the Decision 
Boundary (p(c1|x) = p(c2|x) = 0.5) and the Other Two Contour Lines Correspond to p(c1|x) = 
0.01 and p(c1|x) = 0.99. Also Plotted for Reference are the Original Data Points and the Fitted 
Covariance Ellipses for Each Class (Plotted as Dotted Lines).  
Figure 10.3 shows the results of the same classification procedure (multivariate Normal, 
maximum likelihood estimates) but applied to a different data set. In this case two particular 
variables from the two-class Pima Indians data set (originally discussed in chapter 3) were 

used as the class variables, where problematic measurements taking value 0 (thought to 
be outliers, see discussion in chapter 3) were removed a priori. In contrast to the red blood 
cell data of figure 10.2, the two classes (healthy and diabetic) are heavily overlapped in 
these two dimensions. The estimated covariance matrices S1 and S2 are unconstrained, 
leading again to quadratic decision boundary and posterior probability contours. The 
degree of overlap is reflected in the posterior probability contours which are now much 
more spread out (they fall off slowly) than they were previously in figure 10.2. 

Figure 10.3: Posterior Probability Contours for p(c1|x) Where c1 is the Label for the Diabetic 
Class for the Pima Indians Data of Chapter 3. The Heavy Line is the Decision Boundary 
(p(c1|x) = p(c2|x) = 0.5) and the Other Two Contour Lines Correspond to p(c1|x) = 0.1 and 
p(c1|x) = 0.9. The Fitted Covariance Ellipses for Each Class are Plotted as Dotted Lines.  

 

 

 

Note that both the discriminative and regression approaches focus on the differences 
between the classes (or, more formally, the focus is on the probabilities of class 
membership conditional on the values of x), whereas the class-conditional/generative 
approach focuses on the distributions of x for the classes. Methods that focus directly on 
the probabilities of class membership are sometimes referred to as diagnostic methods, 
while methods that focus on the distribution of the x values are termed sampling 
methods. Of course, all of the methods are related. The class-conditional/generative 
approach is related to the regression approach in that the former ultimately produces 
posterior class probabilities, but calculates them in a very specific manner (that is, via 
Bayes rule), whereas the regression approach is unconstrained in terms of how the 
posterior probabilities are modeled. Similarly, both the regression and class-
conditional/generative approaches implicitly contain decision boundaries; that is, in 
""decision mode"" they map inputs x to one of m classes; however, each does so within a 
probabilistic framework, while the ""true"" discriminative classifier is not constrained to do 
so. 
We will discuss examples of each of these approaches in the sections that follow. Which 
type of classifier works best in practice will depend on the nature of the problem. For 
some applications (such as in medical diagnosis) it may be quite useful for the classifier 
to generate posterior class probabilities rather than just class labels. Methods based on 
the class-conditional distributions also have the advantage of providing a full description 
for each class (which, for example, provides a natural way to detect outliers—inputs x 
that do not appear to belong to any of the known classes). However, as discussed in 
chapter 9, it may be quite difficult (if not impossible) to accurately estimate functions 
p(x|ck, ?k) in high dimensions. In such situations the discriminative classifier may work 
better. In general, methods based on the class-conditional distributions will require fitting 
the most parameters (and thus will lead to the most complex modeling), the regression 
approach will require fewer, and the discriminative model fewest of all. Intuitively this 

 
 

makes sense, since the optimal discriminative model contains only a subset of the 
information of the optimal regression model (the boundaries, rather than the full class 
probability surfaces), and the optimal regression model contains less information than 
the optimal class-conditional distribution model. 

10.3 The Perceptron 
One of the earliest examples of an automatic computer-based classification rule was the 
perceptron. The perceptron is an example of a discriminative rule, in that it focuses 
directly on learning the decision boundary surface. The perceptron model was originally 
motivated as a very simple artificial neural network model for the ""accumulate and fire"" 
threshold behavior of real neurons in our brain—in chapter 11 on regression models we 
will discuss more general and recent neural network models. 
In its simplest form, the perceptron model (for two classes) is just a linear combination of 
the measurements in x. Thus, define h (x) = ?  wjxj, where the  wj, 1 = j = p are the 
weights (parameters) of the model. One usually adds an additional input with constant 
value 1 to allow for an additional trainable offset term in the operation of the model. 
Classification is achieved by comparing h (x) with a threshold, which we shall here take 
to be zero for simplicity. If all class 1 points have  h (x) > 0 and all class 2 points have  h 
(x) < 0, we have perfect separation between the classes. We can try to achieve this by 
seeking a set of weights such that the above conditions are satisfied for all the points in 
the training set. This means that the score function is the number of misclassification 
errors on the training data for a given set of weights w1, ..., wp+1. Things are simplified if 
we transform the measurements of our class 2 points, replacing all the  xj by -xj . Now we 
simply need a set of weights for which h (x) > 0 for all the training set points. 
The weights wj are estimated by examining the training points sequentially. We start with 
an initial set of weights and classify the first training set point. If this is correctly 
classified, the weights remain unaltered. If it is incorrectly classified, so that h (x) < 0, the 
weights are updated, so that h (x) is increased. This is easily achieved by adding a 
multiple of the misclassified vector to the weights. That is, the updating rule is w = w+?xj. 
Here ? is a small constant. This is repeated for all the data points, cycling through the 
training set several times if necessary. It is possible to prove that if the two classes are 
perfectly separable by a linear decision surface, then this algorithm will eventually find a 
separating surface, provided a sufficiently small value of ? is chosen. The updating 
algorithm is reminiscent of the gradient descent techniques discussed in chapter 8, 
although it is actually not calculating a gradient here but instead is gradually reducing the 
error rate score function. 

Of course, other algorithms are possible, and others are indeed more attractive if the two 
classes are not perfectly linearly separable—as is often the case. In such cases, the 
misclassification error rate is rather difficult to deal with analytically (since it is not a 
smooth function of the weights), and the squared error score function is often used 
instead: 

(10.5)  

Since this is a quadratic error function it has a single global minimum as a function of the 
weight vector w and is relatively straightforward to minimize (either by a local gradient 
descent rule as in chapter 8, or more directly in closed-form using linear algebra).  
Numerous variations of the basic perceptron idea exist, including (for example) 
extensions to handle more than two classes. The appeal of the perceptron model is that 
it is simple to understand and analyze. However, its applicability in practice is limited by 
the fact that its decision boundaries are linear (that is, hyperplanes in the input space X) 
and real-world classification problems may require more complex decision surfaces for 
low error-rate classification. 

 
 

10.4 Linear Discriminants 
The linear discriminant approach to classification can be considered a ""cousin"" of the 
perceptron model within the general family of linear classifiers. It is based on the simple 
but useful concept of searching for the linear combination of the variables that best 
separates the classes. Again, it can be regarded an example of a discriminative 
approach, since it does not explicitly estimate either the posterior probabilities of class 
membership or the class-conditional distributions. Fisher (1936) presents one of the 
earliest treatments of linear discriminant analysis (for the two-class case). Let C be the 
pooled sample covariance matrix defined as 

(10.6)  

where ni is the number of training data points per class, and Ci are the  p × p sample 
(estimated) covariance matrices for each class, 1 = i = 2 (as defined in chapter 2). To 
capture the notion of separability along any p-dimensional vector w, Fisher defined a 
scalar score function as follows: 

(10.7)  

where  and  are the p × 1 mean vectors for x for data from class 1 and class 2 
respectively. The top term is the difference in projected means for each class, which we 
wish to maximize. The denominator is the estimated pooled variance of the projected 
data along direction w and takes into account the fact that the different variables xj can 
have both different individual variances and covariance with each other. 
Given the score function S(w), the problem is to determine the direction w that 
maximizes this expression. In fact, there is a closed form solution for the maximizing w, 
given by: 

(10.8)  

A new point is classified by projecting it onto the maximally separating direction, and 
classifying x to class 1 if 

(10.9)  

where p(c1) and p(c2) are the respective class probabilities. 
Figure 10.4 shows the application of the Fisher linear discriminant method to the two 
class anemia classification problem discussed earlier. The linear decision boundary is 
not quite as good at separating the training data as the quadratic boundaries of example 
10.2. 

Figure 10.4: Decision Boundary Produced by the Fisher Linear Discriminant Applied to the 
Red Blood Cell Data From Chapter 9, Where the Crosses are the Healthy Class and the 
Circles Correspond to Iron Deficient Anemia.  

 

In the special case in which the distributions within each class have a multivariate 
Normal distribution with a common covariance matrix, this method yields the optimal 
classification rule as in equation 10.2 (and, indeed, it is optimal whenever the two 

 
 

classes have ellipsoidal distributions with equal quadratic forms). Note, however, that 
since wlda was determined without assuming Normality, the linear discriminant 
methodology can often provide a useful classifier even when Normality does not hold. 
Note also that if we approach the linear discriminant analysis method from the 
perspective of assumed forms for the underlying distributions, the method might be more 
appropriately viewed as being based on the class-conditional distribution approach, 
rather than on the discriminative approach. 
A variety of extensions to Fisher's original linear discriminant model have been 
developed. Canonical discriminant functions generate m - 1 different decision boundaries 
(assuming m - 1 < p) to handle the case where the number of classes m > 2. Quadratic 
discriminant functions lead to quadratic decision boundaries in the input space when the 
assumption that the covariance matrices are equal is relaxed, as discussed in example 
10.2. Regularized discriminant analysis shrinks the quadratic method toward a simpler 
form. 
Determining the linear discriminant model has computational complexity O(mp2n). Here 
we are assuming that n >> {p, m} so that the main cost is in estimating the class 
covariance matrices Ci, 1 = i = m. All of these matrices can be found with at most two 
linear scans of the database (one to get the means and one to generate the O(p2) 
covariance matrix terms). Thus the method scales well to large numbers of observations, 
but is not particularly reliable for large numbers of variables, as the dependence (in 
terms of the number of parameters to be estimated) on p, the number of variables, is 
quadratic. 

10.5 Tree Models 
The basic principle of tree models is to partition (in a recursive manner) the space 
spanned by the input variables to maximize a score of class purity—meaning (roughly, 
depending on the particular score chosen) that the majority of points in each cell of the 
partition belong to one class. Thus, for example, with three input variables, x, y, and z, 
one might split x, so that the input space is divided into two cells. Each of these cells is 
then itself split into two, perhaps again at some threshold on  x or perhaps at some 
threshold on  y or z. This process is repeated as many times as necessary (see below), 
with each branch point defining a node of a tree. To predict the class value for a new 
case with known values of input variables, we work down the tree, at each node 
choosing the appropriate branch by comparing the new case with the threshold value of 
the variable for that node. 

Tree models have been around for a very long time, although formal methods of building 
them are a relatively recent innovation. Before the development of such methods they 
were constructed on the basis of prior human understanding of the underlying processes 
and phenomena generating the data. They have many attractive properties. They are 
easy to understand and explain. They can handle mixed variables (continuous and 
discrete, for example) with ease since, in their simplest form, trees partition the space 
using binary tests (thresholds on real variables and subset membership tests on 
categorical variables). They can predict the class value for a new case very quickly. They 
are also very flexible, so that they can provide a powerful predictive tool. However, their 
essentially sequential nature, which is reflected in the way they are constructed, can 
sometimes lead to suboptimal partitions of the space of input variables. 

The basic strategy for building tree models is simplicity itself: we simply recursively split 
the cells of the space of input variables. To split a given cell (equivalently, to choose the 
variable and threshold on which to split the node) we simply search over each possible 
threshold for each variable to find the threshold split that leads to the greatest 
improvement in a specified score function. The score is assessed on the basis of the 
training data set elements. If the aim is to predict to which one of two classes an object 
belongs, we choose the variable and threshold that leads to the greatest average 
improvement to the local score (averaged across the two child nodes). Splitting a node 
cannot lead to a deterioration in the score function on the training data. For classification 
it turns out that using classification error directly is not a useful score function for 
selecting variables to split on. Other more indirect measures such as entropy have been 

found to be much more useful. Note that, for ordered variables, a binary split simply 
corresponds to a single threshold on the variable values. For nominal variables, a split 
corresponds to partitioning the variable values into two subsets of values. 

Example 10.3  

 
The entropy criterion for a particular real-valued threshold test T (where T stands for a 
threshold test Xj > T on one of the variables) is defined as the average entropy after the 
test is performed: 
(10.10)  
where the conditional entropy H(C|T = 1) is defined as 

 

The average entropy is then the uncertainty from each branch (T = 1 or T = 0) averaged 
over the probability of going down each branch. Since we are trying to split the data into 
subsets where as many of the data points belong to one class or the other, this is directly 
equivalent to minimizing the entropy in each branch. In practice, we search among all 
variables (and all tests or thresholds on each variable) for the single test T that results in 
minimum average entropy after the binary split. 

 

 

In principle, this splitting procedure can be continued until each leaf node contains a 
single training data point—or, in the case when some training data points have identical 
vectors of input variables (which can happen if the input variables are categorical) 
continuing until each leaf node contains only training data points with identical input 
variable values. However, this can lead to severe overfitting. Better trees (in the sense 
that they lead to better predictions on new data drawn from the same distributions) can 
typically be obtained by not going to such an extreme (that is, by constructing smaller, 
more parsimonious trees). 
Early work sought to achieve this by stopping the growing process before the extreme 
had been reached (this is analogous to avoiding overfitting in neural networks by 
terminating the convergence procedure, as we will discuss in the next chapter). 
However, this approach suffers from a consequence of the sequential nature of the 
procedure. It is possible that the best improvement that can be made at the next step is 
only very small, so that growth stops, while the step after this could lead to substantial 
improvement in performance. The ""poor"" step might be necessary to set things up so 
that the next step can make a substantial improvement. There is nothing specific to trees 
about this, of course. It is a general disadvantage of sequential methods: precisely the 
same applies to the stepwise regression search algorithms discussed in chapter 11—
which is why more sophisticated methods involving stepping forward and backward have 
been developed. Similar algorithms have evolved for tree methods. 
Nowadays a common strategy is to build a large tree—to continue splitting until some 
termination criterion has been reached in each leaf (for example the points in a node all 
belong to one class or all have the same x vector)—and then to prune it back. That is, at 
each step the two leaf nodes are merged that lead to least reduction in predictive 
performance on the training set. Alternatively, measures such as minimum description 
length or cross-validation (for example, the CART algorithm described in chapter 5) are 
used to trade off goodness of fit to the training data against model complexity. 
Two other strategies for avoiding the problem of overfitting the training set are also fairly 
widely used. The first is to average the predictions obtained by the leaves and the nodes 
leading to the leaves. The second, which has attracted much attention recently, is to 
base predictions on the averages of several trees, each one constructed by slightly 
perturbing the data in some way. Such model averaging methods are, in fact, generally 
suitable for all predictive modeling situations. Model averaging works particularly well 
with tree models since trees have relatively high variance in the following sense: a tree 
can be relatively sensitive to small changes in the training data since a slight perturbation 
in the data could lead to a different root node being chosen and a completely different 

tree structure being fit. Averaging over multiple perturbations of the data set (e.g., 
averaging over trees built on bootstrap samples from the training data) tends to 
counteract this effect by reducing variance. 

The most common class value among the training data points at a given leaf node (the 
majority class) is typically declared as the predicted label for any data points that arrive 
at this leaf. In effect the region in the input space defined by the branch leading to this 
node is assigned the label of the most likely class in the region. Sometimes useful 
information is contained in the overall probability distribution of the classes in the training 
data at a given leaf. Note that for any particular class, the tree model produces 
probabilities that are in effect piecewise-constant in the input space, so small changes in 
the value of an input variable could send a data point down different branches (into a 
different leaf or region) with dramatically different class probabilities. 
When seeking the next best split while building a large tree prior to pruning, the algorithm 
searches through all variables and all possible splits on those variables. For real-valued 
variables the number of possible positions for splits is typically taken to be n' - 1 (that is, 
one less than the number of data points n' at each node), each possible position being 
located halfway between two data points (putting them halfway between is not 
necessarily optimal, but has the virtue of simplicity). The computational complexity of 
finding the best splits among p real-valued variables will typically scale as O(pn' log n') if 
it is carried out in a direct manner. The n' log n' term results from having to sort the 
variable values at the node in order to calculate the score  function: for any threshold we 
need to know how many points are above and below that threshold. For many score 
functions we can show that the optimal threshold for ordered variables must be located 
between two values of the variable that have different class labels. This fact can be used 
to speed up the search, particularly for large numbers of data points. In addition, various 
bookkeeping efficiencies can be taken advantage of to avoid resorting as we proceed 
from node to node. For categorical-valued variables, some form of combinatorial search 
must be conducted to find the best subset of variable values for defining a split. 

From a database viewpoint, tree growing can be an expensive procedure. If the number 
of data points at a node exceeds the capacity of main memory, then the function must 
operate with a cache of data in main memory and the rest in secondary memory. A 
brute-force implementation will result in linear scans of the database for each node in the 
tree, resulting in a potentially very slow algorithm. Thus, when we use tree algorithms 
with data that exceeds the capacity of main memory, we typically either use clever tree 
algorithms whose data management strategy is tailored to try to minimize secondary 
memory access, or we resort to working with a random sample that can fit in main 
memory. 
One disadvantage of the basic form of tree is that it is monothetic: each node is split on 
just one variable. Sometimes, in real problems, the class variable changes most rapidly 
with a combination of input variables. For example, in a classification problem involving 
two input variables, it might be that one class is characterized by having low values on 
both variables while the other has high values on both variables. The decision surface for 
such a problem would lie diagonally in the input variable space. Standard methods would 
try to achieve this by multiple splits, ending up with a staircaselike approximation to this 
diagonal decision surface. Figure 10.5 provides a simple illustration of this effect. The 
optimum, of course, would be achieved by using a threshold defined on a linear 
combination of the input variables—and some extensions to tree methods do just this, 
permitting linear combinations of the raw input variables to be included in the set of 
possible variables to be split. Of course, this complicates the search process required for 
building the tree. 

Figure 10.5: Decision Boundary for a Decision Tree for the Red Blood Cell Data from 
Chapter 9, Composed of ""Axis-Parallel"" Linear Segments (Contrast with the Simpler 
Boundaries in Figure 10.4).  
 
 

 

10.6 Nearest Neighbor Methods 
At their basic level, nearest neighbor methods are very straightforward: to classify a new 
object, with input vector  y, we simply examine the k closest training data set points to y 
and assign the object to the class that has the majority of points among these k. Close is 
defined here in terms of the  p - dimensional input space. Thus we are seeking those 
objects in the training data that are most similar to the new object, in terms of the input 
variables, and then classifying the new object into the most heavily represented class 
among these most similar objects. 
In theoretical terms, we are taking a small volume of the space of variables, centered at 
x, and with radius the distance to the kth nearest neighbor. Then the maximum likelihood 
estimators of the probability that a point in this small volume belongs to each class are 
given by the proportion of training points in this volume that belong to each class. The k-
nearest neighbor method assigns a new point to the class that has the largest estimated 
probability. Nearest neighbor methods are essentially in the class of what we have 
termed ""regression"" methods—they directly estimate the posterior probabilities of class 
membership.  
Of course, this simple outline leaves a lot unsaid. In particular, we must choose a value 
for k and a metric through which to define close. The most basic form takes k = 1, but 
this makes a rather unstable classifier (high variance, sensitive to the data), and the 
predictions can often be made more consistent by increasing k (reduces the variance, 
but may increase the bias of the method since there is more averaging). However, 
increasing k means that the training data points now being included are not necessarily 
very close to the object to be classified. This means that the ""small volume"" may not be 
small at all. Since the estimates are estimates of the average probability of belonging to 
each class in this volume, this may deviate substantially from the value at any particular 
point within the volume—and this deviation is likely to be larger as the volume is larger. 
The dimensionality p of course plays an important role here: for a fixed number of data 
points n we increase p (add variables) the data become more and more sparse. This 
means that the predicted probability may be biased from the true probability at the point 
in question. 
We are back at the ubiquitous issue of the bias/variance trade-off, where increasing k 
reduces variance but may increase bias. There is theoretical work on the best choice of 
k, but since this will depend on the particular structure of the data set, as well as other 
general issues, the best strategy for choosing k seems to be a data-adaptive one: try 
various values, plotting the performance criterion (the misclassification rate, for example) 
against k, to find the best. In following this approach, the evaluation must be carried out 

on a data set independent of the training data (or else the usual problem of 
overoptimistic results ensues). However, for smaller data sets it would be unwise to 
reduce the size of the training data set too much by splitting off too large a test set, since 
the best value of k clearly depends on the number of points in the training data set. A 
leaving-one-out cross-validated score function is often a useful strategy to follow, 
particularly for small data sets. 
Many applications of nearest neighbor methods adopt a Euclidean metric: if y is the input 
vector for the point to be classified, and x is the input vector for a training set point, then 
the Euclidean distance between them is ? j(xj - yj)2. As discussed in chapter 2, the 
problem with this is that it does not provide an explicit measure of the relative importance 
of the different input variables. We could seek to overcome this by using ? jwj(xj - yj)2, 
where the  wj are weights. This seems more complicated than the Euclidean metric, but 
the appearance that the Euclidean metric does not require a choice of weights is illusory. 
This is easily seen simply by changing the units of measurement of one of the variables 
before calculating the Euclidean metric. (An exception to this is when all variables are 
measured in the same units—as, for example, with situations where the same variable is 
measured on several different occasions—so-called repeated measures data.) 
In the two-class case, an optimal metric would be one defined in terms of the contours of 
probability of belonging to class c1—that is, P(c1|x). Training data points on the same 
contour as y have the same probability of belonging to class c1 as does a point at y, so 
no bias is introduced by including them in the k nearest neighbors. This is true no matter 
how far from y they are, provided they are on the contour. In contrast, points close to y 
but not on the contour of P(c1|x) through  y will have different probabilities of belonging to 
class c1, so including them among the k will tend to introduce bias. Of course, we do not 
know the positions of the contours. If we did, we would not need to undertake the 
exercise at all. This means that, in practice, we estimate approximate contours and base 
the metrics on these. Both global approaches (for example estimating the classes by 
multivariate Normal distributions) and local approaches (for example iterative application 
of nearest neighbor methods) have been used for finding approximate contours. 
Nearest neighbor methods are closely related to the kernel methods for density 
estimation that we discussed in chapter 6. The basic kernel method defines a cell by a 
fixed bandwidth and calculates the proportion of points within this cell that belong to each 
class. This means that the denominator in the proportion is a random variable. The basic 
nearest neighbor method fixes the proportion (at k/n) and lets the ""bandwidth"" be a 
random variable. More sophisticated extensions of both methods (for example, smoothly 
decaying kernel functions, differential weights on the nearest neighbor points according 
to their distance from x, or choice of bandwidth that varies according to x) often lead to 
methods that are barely distinguishable in practice. 
The nearest neighbor method has several attractive properties. It is easy to program and 
no optimization or training is required. Its classification accuracy can be very good on 
some problems, comparing favorably with alternative more exotic methods such as 
neural networks. It permits easy application of the reject option, in which a decision is 
deferred if we are not sufficiently confident about the predicted class. Extension to 
multiple classes is straightforward (though the best choice of metric is not so clear here). 
Handling missing values (in the vector for the object to be classified) is simplicity itself: 
we simply work in the subspace of those variables that are present. 
From a theoretical perspective, the nearest neighbor method is a valuable tool: as the 
design sample size increases, so the bias of the estimated probability will decrease, for 
fixed k. If we can contrive to increase k at a suitable rate (so that the variance of the 
estimates also decreases), the misclassification rate of a nearest neighbor rule will 
converge to a value related to the Bayes error rate. For example, the asymptotic nearest 
neighbor misclassification rate (the rate as the number of data points n goes to 8) is 
bounded above by twice the Bayes error rate. 
High-dimensional applications cause problems for all methods. Essentially such 
problems have to be overcome by adopting a classification rule that is not so flexible that 
it overfits the data, given the large opportunity for overfitting provided by the many 
variables. Parametric models of superficially restricted form (such as linear methods) 
often do well in such circumstances. Nearest neighbor methods often do not do well. 
With large numbers of variables (and not correspondingly large numbers of training data 
cases) the nearest k points are often quite far in real terms. This means that fairly gross 

smoothing is induced, smoothing that is not related to the classification objectives. The 
consequence is that nearest neighbor methods can perform poorly in problems with 
many variables. 
In addition, theoretical analyses suggest potential problems for nearest neighbor 
methods in high dimensions. Under some distributional conditions the ratio of the 
distance to the closest point and the distance to the most distant point, from any 
particular x point, approaches 1 as the number of dimensions grows. Thus the concept 
of the nearest neighbor becomes more or less meaningless. However, the distributional 
assumptions needed for this result are relatively strong, and other more realistic 
assumptions imply that the notion of nearest neighbor is indeed well defined. 
A potential drawback of nearest neighbor methods is that they do not build a model, 
relying instead on retaining all of the training data set points (for this reason, they are 
sometimes called ""lazy"" methods). If the training data set is large, searching through 
them to find the k nearest can be a time-consuming process. Specifically it can take 
O(np) per query data point if performed in brute force manner, visiting each of the n 
training data points and performing p operations to calculate the distance to each. From 
a memory viewpoint, the method requires us to store the full training data set of size np. 
Both the time and storage requirements make the direct approach impractical for 
applications involving very large values of  n and/or real-time classification (for example, 
real-time recommendation of a product to a visitor at a Web site using a nearest-
neighbor algorithm to find similar individuals from a database with millions of customers). 
A variety of methods have been developed for accelerating the search and reducing the 
memory demands of the basic approach. For example, branch and bound methods can 
be applied: if it is already known that at least k points lie within a distance d of the point 
to be classified, then a training set point is not worth considering if it lies within a distance 
d of a point already known to be further than 2d from the point to be classified. This 
involves preprocessing the training data set. Other preprocessing methods discard 
certain training data points. For example, condensed nearest neighbor and  reduced 
nearest neighbor methods selectively discard design set points so that those remaining 
still correctly classify all other training data points. The edited nearest neighbor method 
discards isolated points from one class that are in dense regions of another class, 
smoothing out the empirical decision surface in this manner. The gains in speed and 
memory from these methods depend in general on a variety of factors: the values of n 
and p, the nature of the particular data set at hand, the particular technique used, and 
trade-offs between time and memory. 
An alternative method for scaling up nearest neighbor methods for large data sets in high 
dimensions is to use clustering to obtain a grouping of the data. The data points are 
stored on disk according to their membership in clusters. When finding the nearest point 
for input point y, the clusters nearest to y are located and search confined to those 
clusters. With high probability, under fairly broad assumptions, this method can produce 
the true nearest neighbor. 

10.7 Logistic Discriminant Analysis 
For the two-class case, one of the most widely used basic methods of classification 
based on the regression perspective is logistic discriminant analysis. Given a data point 
x, the estimated probability that it belongs to class c1 is 

 
 

(10.11)  

Since the probabilities of belonging to the two classes sum to one, by subtraction, the 
probability of belonging to class 2 is 

(10.12)  

By inverting this relationship, it is easy to see that the logarithm of the odds ratio is a 
linear function of the  xj. That is, 

(10.13)  

This approach to modeling the posterior probabilities has several attractive properties. 
For example, if the distributions are multivariate normal with equal covariance matrices, it 
is the optimal solution. Furthermore, it is also optimal with discrete x variables if the 
distributions can be modeled by log-linear models (mentioned in chapter 9) with the 
same interaction terms. These two optimality properties can combine, to yield an 
attractive model for mixed variables (that is, discrete and continuous) types. 
Fisher's linear discriminant analysis method is also optimal for the case of multivariate 
normal classes with equal covariance matrices. If the data are known to be sampled from 
such distributions, then Fisher's method is more efficient. This is because it makes 
explicit use of this information, by modeling the covariance matrix, whereas the logistic 
method sidesteps this. On the other hand, the more general validity of the logistic 
method (no real data is ever exactly multivariate normally distributed) means that this is 
generally preferred to linear discriminant analysis nowadays. The word nowadays here 
arises because of the algorithms required to compute the parameters of the two models. 
The mathematical simplicity of the linear discriminant analysis model means that an 
explicit solution can be found. This is not the case for logistic discriminant analysis, and 
an iterative estimation procedure must be adopted. The most common such algorithm is 
a maximum likelihood approach, based on using the likelihood as the score function. 
This is described in chapter 11, in the more general context of generalized linear models. 

 
 

10.8 The Naive Bayes Model 
In principle, methods based on the class-conditional distributions in which the variables 
are all categorical are straightforward: we simply estimate the probabilities that an object 
from each class will fall in each cell of the discrete variables (each possible discrete 
value of the vector variable  X), and then use Bayes theorem to produce a classification. 
In practice, however, this is often very difficult to implement because of the sheer 
number of probabilities that must be estimated—O(k p) for p k-valued variables. For 
example, with p = 30 and binary variables (k = 2) we would need to estimate on the order 
of 230 ˜ 109 probabilities. Assuming (as a rule of thumb) that we should have at least 10 
data points for every parameter we estimate (where here the parameters in our model 
are the probabilities specifying the joint distribution), we would need on the order of 1010 
data points to accurately estimate the required joint distribution. For m classes (m > 2) 
we would need m times this number. As p grows the situation clearly becomes 
impractical. 
We pointed out in chapters 6 and 9 that we can always simplify any joint distribution by 
making appropriate independence assumptions, essentially approximating a full table of 
kp probabilities by products of much smaller tables. At an extreme, we can assume that 
all the variables are conditionally independent, given the classes—that is, that 

(10.14)  

This is sometimes referred to as the Naive Bayes or first-order Bayes assumption. The 
approximation allows us to approximate the full conditional distribution requiring O(k p) 
probabilities with a product of univariate distributions, requiring in total O(k p) probabilities 
per class. Thus the conditional independence model is linear in the number of variables 
p rather than being exponential. To use the model for classification we simply use the 
product form for the class-conditional distributions, yielding the Naive Bayes classifier. 
The reduction in the number of parameters by using the Naive Bayes model above 
comes at a cost: we are making a very strong independence assumption. In some cases 
the conditional independence assumption may be quite reasonable. For example, if the 
xj are medical symptoms, and the ck are different diseases, then it may (perhaps) be 
reasonable to assume that given that a person has disease ck, the probability of any one 
symptom depends only on the disease ck and not on the occurrence of any other 
symptom. In other words, we are modeling how symptoms appear, given each disease, 
as having no interactions (note that this does not mean that we are assuming marginal 
(unconditional) independence). In many practical cases this conditional independence 
assumption may not be very realistic. For example, let x1 and x2 be measures of annual 
income and savings total respectively for a group of people, and let ck represent their 
creditworthiness, this being divided into two classes: good and bad. Even within each 

class we might expect to observe a dependence between x1 and x2, because it is likely 
that people who earn more also save more. Assuming that two variables are 
independent means, in effect, that we will treat them as providing two distinct pieces of 
information, which is clearly not the case in this example. 

Although the independence assumption may not be a realistic model of the probabilities 
involved, it may still permit relatively accurate classification performance. There are 
various reasons for this, including: the fact that relatively few parameters are estimated 
implies that the variance of the estimates will be small; although the resulting probability 
estimates may be biased, since we are not interested in their absolute values but only in 
their ranked order, this may not matter; often a variable selection process has already 
been undertaken, in which one of each pair of highly correlated variables has been 
discarded; the decision surface from the naive Bayes classifier may coincide with that of 
the optimal classifier. 
Apart from the fact that its performance is often surprisingly good, there is another 
reason for the popularity of this particularly simple form of classifier. Using Bayes 
theorem, our estimate of the probability that a point with measurement vector x will 
belong to the kth class is 

(10.15)  

by conditional independence. Now let us take the log-odds ratio and assume that we 
have just two classes c1 and c2. After some straightforward manipulation we get 

(10.16)  

Thus the log odds that a case belongs to class c1 is given by a simple sum of 
contributions from the priors and separate contributions from each of the variables. This 
additive form can be quite useful for explanation purposes since each term, 
, can 
be viewed as contributing a positive or negative additive contribution to whether c1 is c2 
is more likely. 
The naive Bayes model can easily be generalized in many different directions. If our 
measurements xj are real-valued we can still make the conditional independence 
assumption, where now we have products of estimated univariate densities, instead of 
distributions. For any real-valued xj we can estimate ƒ(xj|ck) using any of our favorite 
density estimation techniques—for example, parametric models such as a Normal 
density, more flexible models such as a mixture, or a non-parametric estimate such as a 
kernel density function. Combinations of real-valued and discrete variables can be 
handled simply by products of distributions and densities in equation 10.15 above. 

Despite the simplicity of the form of equations above, the decision surfaces can be quite 
complicated and are certainly not constrained to be linear (e.g., the multivariate Normal 
naive Bayes model produces quadratic boundaries in general), in contrast to the linear 
surfaces produced by simple weighted sums of raw variables (such as those of the 
perceptron and Fisher's linear discriminant). The simplicity, parsimony, and 
interpretability of the naive Bayes model has led to its widespread popularity, particularly 
in the machine learning literature. 
We can generalize the model equally well by including some but not all dependencies 
beyond first-order. One can imagine searching for higher order dependencies to allow for 
selected ""significant"" pairwise dependencies in the model (such as p(xj, xk|ck), and then 
triples, and so forth). In doing so we are in fact building a general graphical model (or 
belief network—see chapter 6) for the conditional distribution p(x|ck). However, the 
conventional wisdom in practice is that such additions to the model often provide only 
limited improvements in classification performance on many data sets, once again 
underscoring the difference between building accurate density estimators and building 
good classifiers. 
Finally we comment on the computational complexity of the naive Bayes classifier. Since 
we are just using (in effect) additive models based on simple functions of univariate 
densities, the complexity scales roughly as pm times the complexity of the estimation for 
each of the individual univariate class-dependent densities or distributions. For discrete-

 
 

valued variables, the sufficient statistics are simple counts of the number of data points 
in each bin, so we can construct a naive Bayes classifier with just a single pass through 
the data. A single scan is also sufficient for parametric univariate density models of real-
valued variables (we just need to collect the sufficient statistics, such as the mean and 
the variance for Normal distributions). For more complex density models, such as 
mixture models, we may need multiple scans to build the model because of the iterative 
nature of fitting such density functions (as discussed in chapter 9). 

10.9 Other Methods 

A huge number of predictive classification methods have been developed in recent 
years. Many of these have been powerful and flexible methods, in response to the 
exciting possibilities offered by modern computing power. We have outlined some of 
these, showing how they are related. Many other methods also exist, but in just one 
chapter of one book it is not feasible to do justice to all of them. Furthermore, 
development and invention have not ended. Exciting work continues even as we write. 
Examples of methods that we have not had space to cover are: 

§ Mixture models and radial basis function approaches approximate each class-

conditional distribution by a mixture of simpler distributions (for example, 
multivariate Normal distributions). Even the use of just a few component 
distributions can lead to a function that is surprisingly effective in modeling 
the class-conditional distributions. 

§ Feed-forward neural networks (as discussed in chapter 5 under the back-

propagation algorithm and again to be discussed in chapter 11 for 
regression) are a generalization of perceptrons. Sometimes they are called 
multi-layer perceptrons. The first later generates h1 linear terms, each a 
weighted combination of the p inputs (in effect, h1 perceptrons). The h1 
terms are then non-linearly transformed (the logistic function is a popular 
choice) and the process repeated through multiple layers. The nonlinearity 
of the transformations permits highly flexible decision surface shapes, so 
that such models can be very effective for some classification problems. 
However, their fundamental nonlinearity means that estimation is not 
straightforward and iterative techniques (such as hill-climbing) must be 
used. The computational complexity of the estimation process means that 
such methods may not be particularly useful with large data sets. 

§ Projection pursuit methods can be viewed as a ""cousin"" of neural networks 

(we will return to them in the context of regression in chapter 11). They can 
be shown, mathematically, to be just as powerful, but they have the 
advantage that the estimation is more straightforward. They again consist 
of linear combinations of nonlinear transformations of linear combinations 
of the raw variables. However, whereas neural networks fix the 
transformations, in projection pursuit they are data-driven. 

§ Just as neural networks emerged from early work on the perceptron, so also 
did support vector machines. The early perceptron work assumed that the 
classes were perfectly separable, and then sought a suitable separating 
hyperplane. The best generalization performance was obtained when the 
hyperplane was as far as possible from all of the data points. Support 
vector machines generalize this to more complex surfaces by extending the 
measurement space, so that it includes transformations (combinations) of 
the raw variables. A linear decision surface that perfectly separates the 
data in this enhanced space is equivalent to a nonlinear decision surface 
that perfectly separates the data in the original raw measurement space. A 
distinct feature of this approach is the use of a unique score function, 
namely the ""margin,"" which attempts to optimize the location of the linear 
decision boundary between the two classes in a manner that is likely to 
lead to the best possible generalization performance. Practical experience 
with such methods is rapidly improving, but estimation can be slow since it 
involves solving a complicated optimization problem that can require  O(n2) 
storage and  O(n3) time to solve. 

 
 

Frequently in classification a very flexible model is fitted, and after that it is smoothed in 
some way to avoid overfitting (or the two processes occur simultaneously), and thus a 
suitable compromise between bias and variance is obtained. This is manifest in pruning 
of trees, in weight decay techniques for fitting neural networks, in regularization in 
discriminant analysis, in the ""flatness"" of support vector machines, and so on. A rather 
different strategy, that has proven highly effective in predictive modeling, is to estimate 
several (or many) models and to average their predictions, as with averaging multiple 
tree classifiers. This approach clearly has concept ual similarities to the Bayesian model-
averaging approach of chapter 4, which explicitly regards the parameters of a model (or 
the model itself) as being uncertain and then averages over this uncertainty when 
making a prediction. Whereas model averaging has its natural origins in statistics, the 
similar approach of majority voting among classifiers has its natural origins in machine 
learning. Yet other ways of combining classifiers are also possible; for example, we can 
regard the output of classifiers as inputs to a higher level classifier. In principle, any type 
of predictive classification model can be used at each stage. Of course, parameter 
estimation will generally not be easy. 
A question that obviously arises with the model averaging strategy is: how to weight the 
different contributions to the average—how much weight should each individual classifier 
be accorded? The simplest strategy is to use equal weights, but it seems obvious that 
there may be advantages to permitting the use of different weights (not least because 
equal weights are a special case of this more general model). Various strategies have 
been suggested for finding the weights, including letting them depend on the predictive 
performance of the individual model and on the relative complexity of the model. The 
method of boosting can also be viewed as a model averaging method. Here a 
succession of models is built, each one being trained on a data set in which points 
misclassified by the previous model are given more weight. This has obvious similarities 
to the basic error correction strategy used in early perceptron algorithms. Recent 
research has provided empirical and theoretical evidence suggesting that boosting can 
be a highly effective data-driven strategy for building flexible predictive models. 

10.10 Evaluating and Comparing Classifiers 

This chapter has discussed predictive classification models—models for predicting the 
likely class membership of a new object, based on a series of measurements on that 
object. There are many different methods available, so a perfectly reasonable question is 
""which particular method we should use for a given problem?"" Unfortunately, there is no 
general answer to this question. Choice must depend on features of the problem, the 
data, and the objectives. We can be aware of the properties of the different methods, 
and this can help us make a choice, but theoretical properties are not always an effective 
guide to practical performance (the effectiveness of the independence Bayes model 
illustrates this). Of course, differences in expected and observed performance serve as a 
stimulus for further theoretical work, leading to deeper understanding. 

If practical results sometimes confound the state of current understanding, we must often 
resort to empirical comparison of performance to guide our choice of method. There has 
been a huge amount of work on the assessment and evaluation of classification rules. 
Much of this work has provided an initial test bed for enhanced understanding in other 
areas of model building. This section provides a brief introduction to assessing the 
performance of classification models. 
We have so far referred to the error rate or misclassification rate of classification 
models—the proportion of future objects that the rule is likely to incorrectly classify. We 
defined the Bayes error rate as the optimal error rate—the error rate that would result if 
our model were based on the true distribution functions underlying the data. In practice, 
of course, these functional forms must be selected a priori (or the alternative 
discriminative or regression approaches used, and their parameters estimated), so that 
the model is likely to depart from the optimal. In this case, the model has a true or  actual 
error rate (which can be no smaller than the Bayes error rate). The true error rate is 
sometimes called the conditional error rate, because it is conditioned on the given 
training data set. 

We will need ways to estimate this true error rate. One obvious way to do this is to 
reclassify the training data and see what proportion was misclassified. This is the 
apparent or resubstitution error rate. Unfortunately, this is likely to underestimate the 
future proportion misclassified. This is because the predictive model has been built so 
that it does well, in some sense, on the training data. (It would be perverse, to say the 
least, deliberately to choose a model that did poorly on the training data!) Since the 
training data is merely a sample from the distributions in question, it will not perfectly 
reflect these distributions. This means that our model may well reflect part of the data-
specific aspects of the training data. Thus, if the training data are reclassified, a higher 
proportion will be correctly classified than would be the case for future data points. 
We have already discussed this phenomenon in different contexts. Many ways have 
been proposed to overcome it. One straightforward possibility is to estimate future error 
rate by calculating the proportion misclassified in a new sample—a test set. This is 
perfectly fine—apart from the fact that, if a test set is available, we might more fruitfully 
use it to make a larger training data set. This will permit a more accurate predictive 
classification model to be constructed. It seems wasteful to ignore part of the data 
deliberately when we construct the model, unless of course n is very large and we are 
confident that training on (say) one million data points (keeping another million for 
testing) is just about as good as training on the full two million. 
When our data size is more moderate, various cross-validation approaches have been 
suggested (see chapter 7 and elsewhere), in which some small portion (say, one tenth) 
of the data is left out when the rule is constructed, and then the rule is evaluated on the 
part that was left out. This can be repeated, with different parts of the data being omitted. 
Important methods based on this principle are: 

§ the leaving-one-out method, in which only one point is left out at each stage, 

There are many other methods of error rate estimation. The area has been the subject of 
several review papers—see the further reading section for details. 
Error rate treats the misclassification of all objects as equally serious. However, this is 
often (some argue almost always) unrealistic. Often, certain kinds of misclassification are 
more serious than other kinds. For example, misdiagnosing a patient with a curable but 
otherwise lethal disease as suffering from some minor illness is more serious than the 
reverse. In this case, we may want to attach costs to the different kinds of 
misclassification. In place of simple error rate, then, we seek a model that will minimize 
overall loss. 
These ideas generalize readily enough to the multiple-class case. Often it is useful to 
draw up a confusion matrix, a cross-classification of the predicted class against the true 
class. Each cell of such a matrix can be associated with the cost of making that particular 
kind of misclassification (or correct classification, in the case of the diagonal of the 
matrix) so that overall loss can be evaluated. 
Unfortunately, costs are often difficult to determine. When this is the case, an alternative 
strategy is to integrate over all possible values of the ratio of one cost to the other (for 
the two-class case—generalizations are possible for more than two classes). This 
approach leads to what is known as the Gini co-efficient of performance. This measure is 
equivalent to the test statistic used in the Mann-Whitney-Wilcoxon statistical test for 
comparing two independent samples, and is also equivalent to the area under a 

but each point in turn is left out, so that we end up with a test set of size 
equal to that of the entire training set, but where each single point test set is 
independent of the model it is tested on. Other methods use larger fractions 
of the data for the test sets (for example, one tenth of the entire data set) 
but these are more biased than the leaving-one-out method as estimates of 
the future performance of the model based on the entire data set. 

§ bootstrap methods, of which there are several. These model the relationship 
between the unknown true distributions and the sample by the relationship 
between the sample and a subsample of the same size drawn, with 
replacement, from the sample. In one method, this relationship is used to 
correct the bias of the resubstitution error rate. Some highly sophisticated 
variants of bootstrap methods have been developed, and they are the most 
effective methods known to date. Jackknife methods are also based on 
leaving one training set element out at a time (as in cross-validation), but 
are equivalent to an approximation to the bootstrap approach. 

 
 

Receiver Operating Characteristic or ROC curve (a plot of the estimated proportion of 
class 1 objects correctly classified as class 1 against the estimated proportion of class 2 
objects incorrectly classified as class 1). ROC curves and the areas under them are 
widely used in some areas of research. They are not without their interpretation 
problems, however. 
Simple performance of classification models is but one aspect of the choice of a method. 
Another is how well the method matches the data. For example, some methods are 
better suited to discrete x variables, and others to continuous x, while others work with 
either type with equal facility. Missing values, of course, are a potential (and, indeed, 
ubiquitous) problem with any method. Some methods can handle incomplete data more 
readily than others. The independence Bayes method, for example, handles such data 
very easily, whereas Fisher's linear discriminant analysis approach does not. Things are 
further complicated by the fact that data may be missing for various reasons, and that 
the reasons can affect the validity of the model built on the incomplete data. The Further 
Reading section gives references to material discussing such issues. 

In general, the assessment of classification models is an important area, and one that 
has been the subject of a huge amount of study. 

10.11 Feature Selection for Classification in High Dimensions 
An important issue that often confronts data miners in practice is the problem of having 
too many variables. Simply put, not all va riables that are measured are likely to be 
necessary for accurate discrimination and including them in the classification model may 
in fact lead to a worse model than if they were removed. Consider the simple example of 
building a system to discriminate between images of male and female faces (a task that 
humans perform effortlessly and relatively accurately but that is quite challenging for an 
image classification algorithm). The colors of a person's eyes, hair, or skin are hardly 
likely to be useful in this discriminative context. These are variables that are easy to 
measure (and indeed are general characteristics of a person's appearance) but carry 
little information as to the class identity in this particular case. 

In most data mining problems it is not so obvious which variables are (or are not) 
relevant. For example, relating a person's demographic characteristics to online 
purchasing behavior may be quite subtle and may not necessarily follow the traditional 
patterns (consider a hypothetical group of high-income PhD-educated consumers who 
spend a lot of money on comic books—if they exist, a comic-book retailer would like to 
know!). In data mining we are particularly interested in letting the data speak, which in 
the context of variable selection means using data-adaptive methods for variable 
selection (while noting as usual that should useful prior knowledge be available to inform 
us about which variables are clearly irrelevant to the task, then by all means we should 
use this information). 
We have discussed this problem in a general modeling context in chapter 6, where we 
outlined some general strategies that we briefly review here: 

§  Variable Selection: The idea here is to select a subset p' of the original p 
variables. Of course we don't know in advance what value of p' will work 
well or which variables should be included, so there is a combinatorially 
large search space of variable subsets that could be considered. Thus most 
approaches rely on some form of heuristic search through the space of 
variable subsets, often using a greedy approach to add or delete variables 
one at a time. There are two general approaches here: the first uses a 
classification algorithm that automatically performs variable selection as part 
of the definition of the basic model, the classification tree model being the 
best-known example. The second approach is to use the classifier as a 
""black box"" and to have an external loop (or ""wrapper"") that systematically 
adds and subtracts variables to the current subset, each subset being 
evaluated on the basis of how well the classification model performs. 
§  Variable Transformations: The idea here is to transform the original 

measurements by some linear or nonlinear function via a preprocessing 

step, typically resulting in a much smaller set of derived variables, and then 
to build the classifier on this transformed set. Examples of this approach 
include principal components analysis (in which we try to find the directions 
in the input space that have the highest variance, essentially a data 
compression technique—see chapters 3 and 6), projection pursuit (in which 
an algorithm searches for interesting linear projections—see chapters 6 and 
11), and related techniques such as factor analysis and independent 
components analysis. While these techniques can be quite powerful in their 
own right, they suffer the disadvantage of not necessarily being well 
matched to the overall goal of improving classification performance. A case 
in point is principal component analysis. Figure 10.6 shows an illustrative 
example in which the first principal component direction (the direction in 
which the data would be projected and potentially used as input to a 
classifier) is completely orthogonal to the best linear discriminant for the 
problem—that is, it is completely in the wrong direction for the classification 
task! This is not a problem with the principal component methodology per se 
but simply an illustration of matching an inappropriate technique to the 
classification task. This is of course a somewhat artificial and pathological 
example; in practice principal component projections can often be quite 
useful for classification, but nonetheless it is important to keep the 
objectives in mind. 

 
 

 

Figure 10.6: An Illustration of the Potential Pitfalls of using Principal Component 
Analysis as a Preprocessor for Classification. This is an Artificial Two-
Dimensional Classification Problem, With Data from Each Class Plotted with 
Different Symbols. The First Principal Component Direction (Which Would be the 
First Candidate Direction on Which to Project the Data If this were Actually a 
High-Dimensional Problem) is in Fact Almost Completely Orthogonal to the Best 
Linear Projection for Discrimination as Determined by Fisher's Linear 
Discriminant Technique.  

10.12 Further Reading 
Fisher's original paper on linear discriminant analysis dates from 1936. Duda, Hart, and 
Stork (2001) (the second edition of the classic pattern recognition text by Duda and Hart 
(1973)) contains a wealth of detail on a variety of classification methods, with a 
particularly detailed treatment of Normal multivariate classifiers (chapter 3) and linear 
discriminant and perceptron learning algorithms (chapter 5). Statistically oriented reviews 
of classification are given by Hand (1981, 1997), Devijver and Kittler (1982), Fukunaga 
(1990), McLachlan (1992), Ripley (1996), Devroye, Gyorfi, and Lugosi (1996), and Webb 
(1999). Bishop (1995) provides a neural network perspective, Mitchell (1997) offers a 

viewpoint from artificial intelligence, and Witten and Frank (2000) provide a data-mining 
oriented introduction to classification. 
Dasarathy (1991) contains many of the classic papers on nearest neighbor classification 
from the statistical pattern recognition literature and general descriptions of nearest 
neighbor methods, including outlines of methods for reducing the size of the retained set, 
may be found in Hand (1981) and McLachlan (1992). Choice of metric for nearest 
neighbor methods is discussed in Short and Fukunaga (1981), Fukunaga and Flick 
(1984), and Myles and Hand (1990). Hastie and Tibshirani (1996) describe an adaptive 
local technique for estimating a metric. Asymptotic properties of nearest neighbor rules 
are described in Devroye and Wagner (1982). The related kernel method is discussed in 
Hand (1982). The problem of the meaning of ""nearest"" neighbor in high dimensions is 
considered in Beyer et al. (1999) and Bennett, Fayad, and Geiger (1999), who also 
discuss the use of clustering for approximate searches. 
One of the earliest descriptions of tree-based models is in Morgan and Sonquist (1963). 
The application of decision trees to classification was popularized in machine learning by 
Quinlan (1986, 1993). In statistics, the book by Breiman et al. (1984) describing the 
CART (Classification And Regression Trees) algorithm was highly influential in the 
widespread adoption and application of tree models. Chapter 7 of Ripley (1996) contains 
an extensive overview of the different contributions to the tree-learning literature from 
statistics, computer science, and engineering. A recent survey article is Murthy (1998). 
Scalable algorithms for constructing decision trees are considered in Shafer, Agrawal, 
and Mehta (1996), Gehrke, Ramakrishnan, and Ganti (1998), and Rastogi and Shim 
(1998). The Sprint method of Shafer, Agrawal, and Mehta (1996) operates in a very 
small amount of main memory but applies only to the CART splitting criterion. The 
RainForest framework of Gehrke, Ramakrishnan, and Ganti (1998) can be used to scale 
up a variety of splitting criteria, but its memory usage depends on the sizes of domains of 
the variables. The method of Rastogi and Shim (1998) interleaves tree building and 
pruning, thus preventing unnecessary access to the data. A nice survey of scalability 
issues is Ganti, Gehrke, and Ramakrishnan (1999). 
Discussions of the independence Bayes method include Russek, Kronmal, and Fisher 
(1983), Hilden (1984), Kohavi (1996), Domingos and Pazzani (1997), and Hand and Yu 
(1999). 
Descriptions of support vector machines are given by Vapnik (1995), Burges (1998), and 
Vapnik (1998). Scholkopf, Burges, and Smola (1999) is a collection of recent papers on 
the same topic, and Platt (1999) describes a useful technique for speeding up the 
training of these classifiers. 
Techniques for combining classifiers, such as model averaging, are described in  Xu, 
Krzyzak, and Suen (1992), Wolpert (1992), Buntine (1992), Ho, Hull, and Srihari (1994), 
Schaffer (1994), and Oliver and Hand (1996). Freund and Schapire (1996) describe the 
boosting technique, more recent theoretical treatments are provided by Schapire et al. 
(1998) and Friedman, Hastie, and Tibshirani (2000). 
A detailed review of assessment and evaluation methods for classification algorithms is 
given in Hand (1997). Reviews of error rate estimation methods in particular are given in 
Toussaint (1974), Hand (1986), McLachlan (1987), and Schiavo and Hand (1999). The 
reject option is treated in detail in Devijver and Kittler (1982). MacMillan and Creelman 
(1991) provide an overview of ROC and related methods. 
A seminal discussion of missing data, their different types, and how to handle them, is 
given in Little and Rubin (1987). 

Chapter 11: Predictive Modeling for 
Regression 
11.1 Introduction 
In chapter 6 we discussed the distinction between predictive and descriptive models. In 
chapter 10 we described in detail predictive models in which the variable to be predicted 
(the response variable) was a nominal variable—that is, it could take one of only a finite 
(and typically small) number of values and these values had no numerical significance, 
so that they were simply class identifiers. In this chapter we turn to predictive models in 

 
 

which the response variable does have numerical significance. Examples are the amount 
a retail store might earn from a given customer over a ten-year period, the rate of fuel 
consumption of a given type of car under normal conditions, the number of people who 
might access a particular Web site in a given month, and so on. The variables to be used 
as input for prediction will be called predictor variables and the variable to be predicted is 
the response variable. Other authors sometimes use the terms dependent or target for 
the response variable, and independent, explanatory, or regressor for the predictor 
variables. Other names used in the classification context were mentioned in chapter 10. 
Note that the predictor variables can be numerical, but they need not be. Our aim, then, 
is to use a sample of objects, for which both the response variable and the predictor 
variables are known, to construct a model that will allow prediction of the numerical value 
of the response variable for a new case for which only the predictor variables are known. 
This is essentially the same problem as in chapter 10, the only difference being the 
numerical instead of nominal nature of the response variable. In fact, as we will see later 
in this chapter, we can also treat prediction of nominal variables (that is, classification) 
within this general framework of regression. 
Accuracy of prediction is one of the most important properties of such models, so various 
measures of accuracy have been devised. These measures may also be used for 
choosing between alternative models, and for choosing the values of parameters in 
models. In the terminology introduced earlier, these measures are score functions, by 
which different models may be compared. 

Predictive accuracy is a critical aspect of models, but it is not the only aspect. For 
example, we might use the model to shed insight into which of the predictor variables are 
most important. We might even insist that some variables be included in the model, 
because we know they should be there on substantive grounds, even though they lead 
to only small predictive improvement. Contrariwise, we might omit variables that we feel 
would enhance our predictive performance. (An example of this situation arises in credit 
scoring, in which, in many countries, it is illegal to include sex or race as a predictor 
variable.) We might be interested in whether predictor variables interact, in the sense 
that the effect that one has on the response variable depends on the values taken by 
others. For obvious reasons, we might be interested in whether good prediction can be 
achieved by a simple model. Sometimes we might even be willing to sacrifice some 
predictive accuracy in exchange for substantially reduced model complexity. Though 
predictive accuracy is perhaps the most important component of the performance of a 
predictive model, this has to be tempered by the context in which the model is to be 
applied. 

11.2 Linear Models and Least Squares Fitting 
Chapter 6 introduced the idea of linear models, so called because they are linear in the 
parameters. The simplest such model yields predicted values, y, of the response variable 
y, that are also a linear combination of the predictor variables xj: 

 
 

(11.1)  

In fact, of course, we will not normally be able to predict the response variable perfectly 
(life is seldom so simple) and a common aim is to predict the mean value that y takes at 
each vector of the predictor variables—so y, is our predicted estimate of the mean value 
at x = (x1, ..., xp). Models of this form are known as linear regression models. In the 
simplest case of a single predictor variable (simple regression), we have a regression 
line in the space spanned by the response and predictor variables. More generally 
(multiple regression) we have a  regression plane. Such models are the oldest, most 
important, and single most widely used form of predictive model. One reason for this is 
their evident simplicity; a simple weighted sum is very easy both to compute and to 
understand. Another compelling reason is that they often perform very well—even in 
circumstances in which we know enough to be confident that the true relationship 
between the predictor and response variables cannot be linear. This is not altogether 
surprising: when we expand continuous mathematical functions in a Taylor series we 

often find that the lowest order terms—the linear terms—are the most important, so that 
the best simple approximation is obtained by using a linear model. 
It is extremely rare that the chosen model is exactly right. This is especially true in data 
mining situations, where our model is generally empirical rather than being based on an 
underlying theory (see chapter 9). The model may not include all of the predictor 
variables that are needed for perfect prediction (many may not have been measured or 
even be measurable); it may not include certain functions of the predictor variables 
(maybe 
is needed as well as x1, or maybe products of the predictor variables are 
needed because they interact in their effect on y); and, in any case, no measurement is 
perfect; the y variable will have errors associated with it so that each vector (x1, ..., xp) 
will be associated with a distribution of possible y values, as we have noted above. 
All of this means that the actual y values in a sample will differ from the predicted values. 
The differences between observed and predicted values are called residuals, and we 
denote these by e: 

(11.2)  

In matrix terms, if we denote the observed  y measurements on the  n objects in the 
training sample by the vector  y and the p measurements of the predictor variables on the 
n objects by the n by p + 1 matrix X (an additional column of 1s are added to incorporate 
the intercept term a0 in the model), we can express the relationship between the 
observed response and predictor measurements, in terms of our model, as 

(11.3)  

(11.4)  

where y is an n × 1 matrix of response values, a = (a0, ..., ap) represents the (p+1) × 1 
vector of parameter values, and the n × 1 vector e = (e(1), ..., e(n)) contains the 
residuals. Clearly we want to choose the parameters in our model (the values in the p + 
1 vector a) so as to yield predictions that are as accurate as possible. Put another way, 
we must find estimates for the aj that minimize the e discrepancies in some way. To do 
this, we combine the elements of e in such a way as to yield a single numerical measure 
that we can minimize. Various ways of combining the e(i) have been proposed, but by far 
the most popular method is to sum their squares—that is, the sum of squared errors 
score function. Thus we seek the values for the parameter vector a that minimizes 

In this expression, y(i) is the observed y value for the ith training sample point and 

(x0(i), x1(i), ..., xp(i)) = (1, x1(i), ..., xp(i)) 

is the vector of predictor variables for this point. For obvious reasons, this method is 
known as the least squares method. For simplicity, we will denote the parameter vector 
that minimizes this by (a0, ..., ap). (It would be more correct, of course, if we used some 
notation to indicate that it is an estimate, such as (â0, ..., âp), but our notation has the 
merit of simplicity.) In matrix terms, the values of the parameters that minimize equation 
11.4 can be shown to be 

(11.5)  

In linear regression in general, the a parameters are called regression coefficients. Once 
the parameters have been estimated, they are used in equation 11.1 to yield predictions. 
The predicted value of y, yk , for a vector of predictor variables xk, is given by 

. 

11.2.1 Computational Issues in Fitting the Model 
Solving equation 11.5 directly requires that the matrix XTX be invertible. Problems will 
arise if the sample size n is small (rare in data mining situations) or if there are linear 
dependencies between the measured values of the predictor variables (not so rare). In 
the latter case, modern software packages normally issue warnings, and appropriate 
action can be taken, such as dropping some of the predictor variables. 
A rather more subtle problem arises when the measured values of the predictor variables 
are not exactly linearly dependent, but are almost so. Now the matrix can be inverted, 
but the solution will be unstable. This means that slight alterations to the observed X 
values would lead to substantial differences in the estimated values of a. Different 

measurement errors or a slightly different training sample would have led to different 
parameter estimates. This problem is termed multicollinearity. The instability in the 
estimated parameters is a problem if these values are the focus of interest—for example, 
if we want to know which of the variables is most important in the model. However, it will 
not normally be a problem as far as predictive accuracy is concerned: although 
substantially different a vectors may be produced by slight variations of the data, all of 
these vectors will lead to similar predictions for most xk vectors. 
Solving equation 11.5 is usually carried out by numerical linear algebra techniques for 
equation solving (such as the LU decomposition or the singular value decomposition 
(SVD)), which tend to have better numerical stability than that achieved by inverting the 
matrix XT X directly. The underlying computational complexity is typically the same no 
matter which particular technique is used, namely, O(p2n + p3). The p2n term comes from 
the n multiplications required to calculate each element in the p × p matrix C = XT X. The 
p3 term comes from then solving Ca = XT y for a. 
In chapter 6 we remarked that the additive nature of the regression model could be 
retained while permitting more flexible model forms by including transformations of the 
raw xj as well as the raw variables themselves. Figure 11.1 shows a plot of data 
collected in an experiment in which a subject performed a physical task at a gradually 
increasing level of difficulty. The vertical axis shows a measure on the gases expired 
from the lungs while the horizontal axis shows the oxygen uptake. The nonlinearity of the 
relationship between these two variables is quite clear from the plot. A straight line y = a0 
+ a1x provides a poor fit—as is shown in the figure. The predicted values from this model 
would be accurate only for x (oxygen uptake) values just above 1000 and just below 
4000. (Despite this, the model is not grossly inaccurate—the point made earlier about 
models linear in  x providing reasonable approximations is clearly true.) However, the 
model y = a0 + a1x + a2x2 gives the fitted line shown in figure 11.2. This model is still 
linear in the parameters, so that these can be easily estimated using the same standard 
matrix manipulation shown above in equation 11.5. It is clear that the predictions 
obtained from this model are about as good as they can be. The remaining inaccuracy in 
the model is the irreducible measurement error associated with the variance of y about 
its mean at each value of  x. 

Figure 11.1: Expired Ventilation Plotted Against Oxygen Uptake in a Series of Trials, with 
Fitted Straight Line.  

 

Figure 11.2:  The Data From Figure 11.1 with a Model that Includes a Term in  x2.  

 

11.2.2 A Probabilistic Interpretation of Linear Regression 
This informal data analytic route allows us to fit a regression model to any data set 
involving a response variable and a set of predictor variables, and to obtain a vector of 
estimated regression coefficients. If our aim were merely to produce a convenient 
summary of the training data (as, very occasionally, it is) we could stop there. However, 
this chapter is concerned with predictive models. Our aim is to go beyond the training 
data to predict y values for other ""out-of-sample"" objects. Goodness of fit to the given 
data is all very well, but we are really interested in fit to future data that arise from the 
same process, so that our future predictions are as accurate as possible. In order to 
explore this, we need to embed the model-building process in a more formal inferential 
context. To do this, we suppose that each observed value  y(i) is produced as a sum of 
weighted predictor variables aTx(i) and a random term ˛
distribution independent of other values. (Note that implicit in this is the assumption that 
the variances of the random terms are all the same—s 2 is the same for all possible 
values of the vector of predictor variables. We will discuss this assumption further 
below.) The n × 1 random vector  Y thus takes the form Y = Xa + ˛
. The observed n × 1 y 
vector in  equation 11.3 is a realization from this distribution. The components of the n × 1 
vector ˛
""error"" is a random realization from a given distribution, whereas a residual is a 
difference between a fitted model and an observed y value. Note also that a is different 
from a. a is represents the underlying and unknown truth, whereas a gives the values 
used in a model of the truth. 
It turns out that within this framework the least squares estimate a is also the maximum 
likelihood estimate of a. Furthermore, the covariance matrix of the estimate a obtained 
above is (XT X)-1s 2, where this covariance matrix expresses the uncertainty in our 
parameter estimates a. In the case of a single predictor variable, this gives 

 are often called  errors. Note that they are different from the residuals, e. An 

(i) that follows a N(0, s2) 

(11.6)  

for the variance of the intercept term and 

(11.7)  

is the sample mean of the single predictor variable. 

for the variance of the slope. Here 
The diagonal elements of the covariance matrix for a above give the variances of the 
regression coefficients—which can be used to test whether the individual regression 
coefficients are significantly different from zero. If  vj is the jth diagonal element of (XT X)-
1s 2, then the ratio 
can be compared with a t(n - p - 1) distribution to see whether the 
regression coefficient is zero. However, as we discuss below, this test makes sense only 
in the context of the other variables included in the model, and alternative methods, also 
discussed below, are available for more elaborate model-building exercises. If x is the 
vector of predictor variables for a new object, with predicted y value y, then the variance 

of y is xT(XT X)-1xs 2. With one predictor variable, this reduces to 
Note that this variance is greater the further x is from the mean of the training sample. 
That is, the least accurate predictions, in terms of variance, are those in the tails of the 
distribution of predictor variables. Note also that confidence intervals (see chapter 4) 
based on this variance are confidence values for the  predicted value of y. 
We may also be interested in (what are somewhat confusingly called) prediction 
intervals, telling us a range of plausible values for the observed  y at a given value of x, 
not a range of plausible values for the predicted value. Prediction intervals must include 
the uncertainty arising from our prediction and also that arising from the variability of  y 
about our predicted value. This means that the variance above is increased by an extra 
term s 2, yielding 

. 

. 

Example 11.1  

 
The most important special case of linear regression arises when there is just one predictor 
variable. Figure 11.3 shows a plot of the record time (in 1984, in minutes) against the 
distance (in miles) for 35 Scottish hill races. We can use regression to attempt to predict 
record time from distance. A simple linear regression of the data gives an estimated 
intercept value of -4.83 and an estimated regression coefficient of 8.33. Most modern data 
analytic packages will give the associated standard errors of the estimates, along with 
significance tests of the null hypotheses that the true parameters that led to the data are 
zero. In this case, the standard errors are 5.76 and 0.62, respectively, yielding significance 
probabilities of 0.41 and < 0.01. From this we would conclude that there is strong evidence 
that the positive linear relationship is real, but no evidence of a non-zero intercept. The plot 
in figure 11.3 shows marked skewness in both variables (they become more sparsely 
spread towards the top and right of the figure). It is clear that the position of the regression 
line will be much more sensitive to the precise position of points to the right of the figure 
than it will be to the position of points to the left. Points that can have a big effect on the 
conclusion are called points of high leverage—they are points at the extreme values of 
estimated relative performance in  figure 11.3. Points that actually do have a big effect are 
called influential points. For example, if the rightmost point in figure 11.3 had time of 100 
(while still having distance around 28), it would clearly have a big effect on the regression 
line. The asymmetry of the leverage of the points in the figure might be regarded as 
undesirable. We might try to overcome this by reducing the skewness—for example, by log 
transforming both the variables before fitting the regression line. 

Figure 11.3: A Plot of Record Time (in Minutes) Against Distance (in Miles) for 35 Scottish 
Hill Races From 1984.  

 

 

 

11.2.3 Interpreting the Fitted Model 
The coefficients in a multiple regression model can be interpreted as follows: if the jth 
predictor variable, xj, is increased by one unit, while all the other predictor variables are 
kept fixed, then the response variable y will increase by aj. The regression coefficients 
thus tell us the conditional effect of each predictor variable, conditional on keeping the 
other predictor variables constant. This is an important aspect of the interpretation. In 
particular, the size of the regression coefficient associated with the jth variable will 
depend on what other variables are in the model. This is clearly especially important if 
we are constructing models in a sequential manner: add another variable and the 
coefficients of those already in the model will change. (There is an exception to this. If 
the predictor variables are orthogonal, then the estimated regression coefficients are 
unaffected by the presence or absence of others in the model. However, this situation is 
most common in designed experiments, and is rare in the kinds of secondary data 
analyses encountered in data mining.) The sizes of the regression coefficients tell us the 
relative importance of the variables, in the sense that we can compare the effects of unit 
changes. Note also that the size of the effects depends on the chosen units of 
measurement for the predictor variables. If we measure x1 in kilometers instead of 
millimeters, then its associated regression coefficient will be multiplied by a million. This 
can make comparisons between variables difficult, so people often work with 
standardized variables—measuring each predictor variable relative to its standard 
deviation. 
We used the sum of squared errors between the predictions and the observed  y values 
as a criterion through which to choose the values of the parameters in the model. This is 
the residual sum of squares or the sum of squared residuals,
. 
In a sense, the worst model would be obtained if we simply predicted all of the  y values 
by the value 
the mean of the sample of y values that is constant relative to the x values 
(thus effectively ignoring the inputs to the model and always guessing the output to be 
the mean of  y). The total sum of squares is defined as the sum of squared errors for this 
worst model, 
model and the total sum of squares is the sum of squares that can be attributed to the 
regression for that model—it is the regression sum of squares. This is the sum of 
squared differences of the predicted values, y(i), from the overall mean, 
symbol R2 is often used for the ""multiple correlation coefficient,"" the ratio of the 
regression sum of squares to total sum of squares: 

. The difference between the residual sum of squares from a 

. The 

(11.8)  

A value near 1 tells us that the model explains most of the y variation in the data. The 
number of independent components contributing to each sum of squares is called the 
number of degrees of freedom for that sum of squares. The degrees of freedom for the 
total sum of squares is n - 1 (one less than the sample size, since the components are all 
calculated relative to the mean). The degrees of freedom for the residual sum of squares 
is n - 1 - p (although there are n terms in the summation, p + 1 regression coefficients 
are calculated). The degrees of freedom for the regression sum of squares is p, the 
difference between the total and residual degrees of freedom. These sums of squares 
and their associated degrees of freedom are usefully put together in an analysis of 
variance table, as in table 11.1, summarizing the decomposition of the totals into 
components. The meaning of the final column is described below. 
Table 11.1: The Analysis of Variance Decomposition Table for a Regression.  

Source of variation 

Regression 

Residual 

Total 

Sum of 
squares 

 

?  (y(i) - 
y(i))2  

Degrees 
of 
freedom 
p  

n - p - 1 

 

n - 1 

Mean 
square 

 

?  (y(i) - 
y(i))2 / (n - 
p - 1) 
  

11.2.4 Inference and Generalization 
We have already noted that our real aim in building predictive models is one of inference: 
we want to make statements (predictions) about objects for which we do not know the y 
values. This means that goodness of fit to the training data is not our real objective. In 
particular, for example, merely because we have obtained nonzero estimated regression 
coefficients, this does not necessarily mean that the variables are related: it could be 
merely that our model has captured chance idiosyncrasies of the training sample. This is 
particularly relevant in the context of data mining where many models may be explored 
and fit to the data in a relatively automated fashion. As discussed earlier, we need some 
way to test the model, to see how easily the observed data could have arisen by chance, 
even if there was no structure in the population the data were collected from. In this 
case, we need to test whether the population regression coefficients are really zero. (Of 
course, this is not the only test we might be interested in, but it is the one most often 
required.) It can be shown that if the values of aj are actually all zero (and still making the 
assumption that the  ˛

(i) are independently distributed as N(0, s 2)), 

(11.9)  

has an F(p, n - p - 1) distribution. This is just the ratio of the two mean squares given in 
table 11.1. The test is carried out by comparing the value of this ratio with the upper 
critical level of the F(p, n - p - 1) distribution. If the ratio exceeds this value the test is 
significant—and we would conclude that there is a linear relationship between the y and 
xj variables (or that a very unlikely event has occurred). If the ratio is less than the critical 
value we have no evidence to reject the null hypothesis that the population regression 
coefficients are all zero. 

11.2.5 Model Search and Model Building 
We have described an overall test to see whether the regression coefficients in a given 
model are all zero. However, we are more often involved in a situation of searching over 
model space—or model building—in which we examine a sequence of models to find 
one that is ""best"" in some sense. In particular, we often need to examine the effect of 
adding a set of predictor variables to a set we have already included. Note that this 
includes the special case of adding just one extra variable, and that the idea is applied in 
reverse, it can also handle the situation of removing variables from a model. 
In order to compare models we need a score function. Once again, the obvious one is 
the sum of squared errors between the predictions and the observed  y values. Suppose 
we are comparing two models: a model with p predictor variables (model  M) and the 
largest model we are prepared to contemplate, with q variables (these will include all the 
untransformed predictor variables we think might be relevant, along with any 
transformations of them we think might be relevant), model M*. Each of these models will 
have an associated residual sum of squares, and the difference between them will tell us 
how much better the larger model fits the data than the smaller model. (Equivalently, we 
could calculate the difference between the regression sums of squares. Since the 
residual and regression sum of squares sum to the total sum of squares, which is the 
same for both models, the two calculations will yield the same result.) The degrees of 
freedom associated with the difference between the residual sums of squares for the two 
models is q - p, the extra number of regression coefficients computed in fitting the larger 
model, M*. The ratio between the difference of the residual sums of squares and the 
difference of degrees of freedom again gives us a mean square—now a mean square for 
the difference between the two models. Comparison of this with the residual mean 
square for model M* gives us an F-test of whether the difference between the models is 
real or not. Table 11.2 illustrates this extension. From this table, the ratio 

is compared with the critical value of an F(q - p, n - q - 1) distribution. 
Table 11.2: The Analysis of Variance Decomposition Table for Model Building.  

 

Source of variation 

Regression Model 1 

Regression Full Model 

Difference 

Residual 

Total 

Sum 
of 
squar
es 
SS(M) 

SS(M*) 

SS(M*) 
- 
SS(M) 
SS(T) - 
SS(M*) 
SS(T) 

Degrees 
of 
freedom 

p  

q  

q - p  

n - p - 1 

Mean 
square 

SS(M)/p  

SS(M*)/q  

 

 

n - 1 

  

This is fine if we have just a few models we want to compare, but data mining problems 
are such that often we need to rely on automatic model building processes. Such 
automatic methods are available in most modern data mining computer packages. There 
are various strategies that may be adopted. A basic form is a forward selection method, 
mentioned in chapter 8, in which variables are added one at a time to an existing model. 
At each step that variable is chosen from the set of potential variables that leads to the 
greatest increase in predictive power (measured in terms of reduction of sum of squared 
residuals), provided the increase exceeds some specified threshold. Ideally, the addition 
would be made as long as the increase in predictive power was statistically significant, 
but in practice this is complicated to ensure: the variable selection process necessarily 
involves carrying out many tests, not all independent, so that computing correct 
significance values is a nontrivial process. The simple significance level based on table 
11.2 does not apply when multiple dependent tests are made. (The implication of this is 
that if the significance level is being used to choose variables, then it is being used as a 
score function, and should not be given a probabilistic interpretation.) 
We can, of course, in principle use any of the score functions discussed in chapter 7 for 
model selection in regression, such as BIC, minimum description length, cross-
validation, or more Bayesian methods. These provide an alternative to the hypothesis-
testing framework that measures the statistical significance of adding and deleting terms 
on a model-by-model basis. Penalized score functions such as BIC, and variations on 
cross-validation tailored specifically to regression, are commonly used in practice as 
score functions for model selection in regression. 
A strategy opposite to that of forward selection is backward elimination. We begin with 
the most complex model we might contemplate (the ""largest model,"" M*, above) and 
progressively eliminate variables, selecting them on the basis that eliminating them leads 
to the least increase in sum of squared residuals (again, subject to some threshold). 
Other variants include combinations of forward selection and backward elimination. For 
example, we might add two variables, eliminate one, add two, remove one, and so on. 
For data sets where the number of variables p is very large, it may be much more 
practical computationally to build the model in the forward direction than in the backward 
direction. Stepwise methods are attempts to restrict the search of the space of all 
possible sets of predictor variables, so that the search is manageable. But if the search 
is restricted, it is possible that some highly effective combination of variables may be 
overlooked. Very occasionally (if the set of potential predictor variables is small), we can 
examine all possible sets of variables (although, with p variables, there are (2p - 1) 
possible subsets). The size of problems for which all possible subsets can be examined 
has been expanded by the use of strategies such as branch and bound, which rely on 
the monotonicity of the residual sum of squares criterion (see chapter 8). 

A couple of cautionary comments are worth making here. First, as we have noted, the 
coefficients of variables already in the model will generally change as new variables are 
added. A variable that is important for one model may become less so when the model is 
extended. Second, as we have discussed in earlier chapters, if too elaborate a search is 
carried out there is a high chance of overfitting the training set—that is, of obtaining a 

model that provides a good fit to the training set (small residual sum of squares) but does 
not predict new data very well. 

11.2.6 Diagnostics and Model Inspection 
Although multiple regression is a very powerful and widely used technique, some of the 
assumptions might be regarded as restrictive. The assumption that the variance of the  y 
distribution is the same at each vector x is often inappropriate. (This assumption of equal 
variances is called homoscedasticity. The converse is heteroscedasticity.) For example, 
figure 11.4 shows the normal average January minimum temperature (in deg F) plotted 
against the latitude (deg N) for 56 cities in the United States. There is evidence that, for 
smaller latitudes, at least, the variance of the temperature increases with increasing 
latitude (although the mean temperature seems to decrease). We can still apply the 
standard least squares algorithm above to estimate parameters in this new situation (and 
the resulting estimates would still be unbiased if the model form were correct), but we 
could do better in the sense that it is possible to find estimators with smaller variance. 

 

Figure 11.4:  Temperature (Degrees F) Against Latitude (Degrees N) for 56 Cities in the 
United States.  

To do this we need to modify the basic method. Essentially, we need to arrange things 
so that those values of x associated with y values with larger variance are weighted less 
heavily in the model fitting process. This makes perfect sense—it means that the 
estimator is more influenced by the more accurate values. Formally, this idea leads to a 
modification of the solution equation 11.5. Suppose that the covariance matrix of the n × 
1 random vector ? is the n × n matrix s 2V (previously we took V = I). The case of unequal 
variances means that V is diagonal with terms that are not all equal. Now it is possible 
(see any standard text on linear algebra) to find a unique nonsingular matrix P such that 
PT P = V. We can use this to define a new random vector  f = P-1? , and it is easy to show 
that the covariance matrix of f is s 2I. Using this idea, we form a new model by 
premultiplying the old one by P-1: 

(11.10)  

or 

(11.11)  

(11.12)  

now of the form required to apply the standard least squares algorithm. If we do this, and 
then convert the solution back into the original variables Y, we obtain: 

a weighted least squares solution. The variance of this estimated parameter vector a is 
(XTV-1X)-1 s 2. 
Unequal variances of the y distributions for different x vectors is one way in which the 
assumptions of basic multiple regression can break down. There are others. What we 
really need are ways to explore the quality of the model and tools that will enable us to 
detect where and why the model deviates from the assumptions. That is, we require 
diagnostic tools. In simple regression, where there is only one predictor variable, we can 
see the quality of the model from a plot of y against x (see figures 11.1, 11.2 and 11.4). 
More generally, however, when there is more than one predictor variable, such a simple 
plot is not possible, and more sophisticated methods are needed. In general, the key 

features for examining the quality of a regression model are the residuals, the 
components of the vector e = y - y. If there is a pattern to these, it tells us that the model 
is failing to explain the distribution of the data. Various plots involving the residuals are 
used, including plotting the residuals against the fitted values, plotting standardized 
residuals (obtained by dividing the residuals by their standard errors) against the fitted 
values, and plotting the standardized residuals against standard normal quantiles. (The 
latter are ""normal probability plots."" If the residuals are approximately normally 
distributed, the points in this plot should lie roughly on a straight line.) Of course, 
interpreting some of the diagnostic plots requires practice and experience. 
One general cautionary comment, applies to all predictive models: such models are valid 
only within the bounds of the data. It can be very risky to extrapolate beyond the data. A 
very simple example is given in figure 11.5. This shows a plot of the tensile strength of 
paper plotted against the percentage of hardwood in the pulp from which the paper was 
made. But suppose only those samples with pulp values between 1 and 9 had been 
measured. The figure shows that a straight line would provide quite a good fit to this 
subset of the data. For new samples of paper, with pulp values lying between 1 and 9, 
quite good prediction of the strength could legitimately be expected. But the figure also 
shows, strikingly clearly, that our model would produce predictions that were seriously 
amiss if we used it to predict the strength of paper with pulp values greater than 9. Only 
within the bounds of our data is the model trustworthy. We saw another example of this 
sort of thing in chapter 3, where we showed the number of credit cards in circulation 
each year. A straight line fitted to years 1985 to 1990 provided a good fit—but if 
predictions beyond those years were based on this model, disaster would follow. 

Figure 11.5: A Plot of Tensile Strength of Paper against the Percentage of Hardwood in the 
Pulp.  

 

These examples are particularly clear—but they involve just a few data points and a 
single predictor variable. In data mining applications, with large data sets and many 
variables, things may not be so clear. Caution needs to be exercised when we make 
predictions. 

 
 

11.3 Generalized Linear Models 
Section 11.2 described the linear model, in which the response variable was 
decomposed into two parts: a weighted sum of the predictor variables and a random 
component: Y (i) = ? jajxj(i)+˛
(i). For inferential purposes we also assumed that the ˛
were independently distributed as N(0, s2). We can write this another way, which permits 
convenient generalization, splitting the description of the model into three parts: 

(i) 

i. 
ii. 
iii. 

The Y (i) are independent random variables, with distribution N(µ(i), s 2). 
The parameters enter the model in a linear way via the sum ?( i) = ? ajxj(i). 
The ?(i) and µ(i) are linked by ?(i) = µ(i). 

This permits two immediate generalizations, while retaining the advantages of the linear 
combination of the parameters. First, in (i) we can relax the requirement that the random 
variables follow a normal distribution. Second, we can generalize the link expressed in 

(iii), so that some other link  function g(µ(i)) = ?(i) relates the parameter of the distribution 
to the linear term ?(i) = ? ajxj(i). These extensions result in what are called generalized 
linear models. They are one of the most important advances in data analysis of the last 
two decades. As we shall see, such models can also be regarded as fundamental 
components of feed forward neural networks. 
One of the most important kinds of generalized linear model for data mining is logistic 
regression. We have already encountered this in chapter 10 in the form of logistic 
discrimination, but we describe it in rather more detail here, and use it as an illustration 
of the ideas underlying generalized linear models. In many situations the response 
variable is not continuous, as we have assumed above, but is a proportion: the number 
of flies from a given sample that die when exposed to an insecticide, the proportion of 
questions people get correct in a test, the proportion of oranges in a carton that are 
rotten. The extreme of this arises when the proportion is out of 1, that is, the observed 
response is binary: whether or not an individual insect dies, whether or not a person gets 
a particular one of the questions right, whether or not an individual orange is rotten. This 
is exactly the situation we discussed in chapter 10, though here we embed it in a more 
general context. We are now dealing with a binary response variable, with the random 
variable taking values 0 or 1 corresponding to the two possible outcomes. We will 
assume that the probability that the ith individual yields the value 1 is p(i), and that the 
responses of different individuals are independent. This means that the response for the 
ith individual follows a Bernoulli distribution: 

(11.13)  

 {0, 1}. For logistic regression, this is the generalization of (i) above: the 

where here y(i) ˛
Bernoulli distribution is replacing the normal distribution. 
Our aim is to formulate a model for the probability that an object with predictor vector x 
will take value 1. That is, we want a model for the mean value of the response, the 
probability p(y = 1|x). We could use a linear model—a weighted sum of the predictor 
variables. However, this would not be ideal. Most obviously, a linear model can take 
values less than 0 and greater than 1 (if the x values are extreme enough). This 
suggests that we need to modify the model to include a nonlinear aspect. We achieve 
this by transforming the probability, nonlinearly, so that it can be modeled by a linear 
combination. That is, we use a nonlinear link function in (iii). A suitable function (not the 
only possible one) is a logistic (or logit) link function, in which 

(11.14)  

where g (p(y = 1|x)) is modeled as ?  ajxj. As p varies from 0 to 1, log(p/1 - p) clearly 
varies from -8 to 8, matching the potential range of g(p) = ?  ajxj(i). One of the 
advantages of the logistic link function over alternatives is that it permits convenient 
interpretation. For example: 

§ The ratio 

in the transformation is the familiar odds that a 1 will be 

observed and log 

is the log odds. 

§ Given a new vector of predictor variables x = (x1, ..., xp), the predicted 

probability of observing a 1 is derived from 
changing the jth predictor variable by one unit is simply a j. Thus the 
coefficients tell us the difference in log odds—or, equivalently, the log odds 
ratio resulting from the two values. From this it is easy to see that 
is the 
factor by which the odds changes when the jth predictor variable changes 
by one unit (see the discussion of the effect of a unit change of one  variable 
in the multiple regression case discussed in section 11.2). 

. The effect on this of 

Example 11.2  

 

Two minutes into its flight on January 29, 1986, the space shuttle Challenger exploded, 
killing everyone on board. The two booster rockets for the shuttle are made of several 
pieces, with each of three joints sealed with a rubber ""O-ring,"" making six rings in total. It 
was known that these O-rings were sensitive to temperature. Records of the proportion of 
O-rings damaged in previous flights were available, along with the temperatures on those 

days. The lowest previous temperature was 53degF. On the day of the flight the 
temperature was 31degF, so there was much discussion about whether the flight should go 
ahead. One argument was based on an analysis of the seven previous flights that had 
resulted in damage to at least one O-ring. A logistic regression to predict the probability of 
failure from temperature led to a slope estimate of 0.0014 with a standard error of 0.0498. 
From this, the predicted logit of the probability of failure at 31degF is 1.3466, yielding a 
predicted probability 0.206. The slope in this model is positive, suggesting that, if anything, 
the probability of failure is lower at low temperatures. However, this slope is not 
significantly different from zero, so that there is little evidence for a relationship between 
failure probability and temperature. 
This analysis is far from ideal. First, 31degF is far below 53degF, so one is extrapolating 
beyond the data—a practice we warned against above. Secondly, there is valuable 
information in the 16 flights that had not resulted in O-ring damage. This is immediately 
obvious from a comparison of figure 11.6(a), which shows the numbers damaged for the 
seven flights above (vertical axis) against temperature (horizontal axis), and  figure 11.6(b), 
which shows the number for all 23 flights. These 16 flights all took place at relatively high 
temperatures. The second figure suggests that the relationship might, in fact, have a 
negative slope. A logistic model fitted to the data in  figure 11.6(b) gave a slope estimate of 
-0.1156, with a standard error of -2.46 (and an intercept estimate of 5.08 with standard 
error of 3.05). From this the predicted probability at 31degF is 0.817. This gives a rather 
different picture, one that could have been deduced before the flight if all the data had been 
studied. 

Figure 11.6: Number of O-Rings Damaged (Vertical Axis) against Temperature on Day of 
Flight, (a) Data Examined before the Flight, and (b) The Complete Data.  

 

 

 

Generalized linear models thus have three main features:  

i. 

ii. 

iii. 

The Y (i), i = 1, ..., n, are independent random variables, with the same 
exponential family distribution (see below). 
The predictor variables are combined in a form ?(i) = ?  ajxj(i), called the 
linear predictor, where the ajs are estimates of the ajs. 
The mean µ(i) of the distribution for a given predictor vector is related to 

the linear combination in (ii) through the link function 
. 

The exponential family of distributions is an important family that includes the normal, the 
Poisson, the Bernoulli, and the binomial distributions. Members of this family can be 
expressed in the general form 

(11.15)  

If  f  is known, then ? is called the  natural or canonical parameter. When, as is often the 
case, a(f) = f, f is called the  dispersion or scale parameter. A little algebra reveals that 
the mean of this distribution is given by b' (?) and variance by a(f )b? (?). Note that the 
variance is related to the mean via b?(?), and this, expressed in the form V (?), is 
sometimes called the  variance function. In the model as described in (i) to (iii) above, 
there are no restrictions on the link function. However (and this is where the exponential 
family comes in), things simplify if the link function is chosen to be the function 
expressing the canonical parameter for the distribution being used as a linear sum. For 
multiple regression this is simply the identity distribution and for logistic regression it is 
the logistic transformation presented above. For Poisson regression, in which the 
distribution in (i) is the Poisson distribution, the canonical link is the log link g(u) = log(u). 
Prediction from a generalized linear model requires the inversion of the relationship 
g(µ(i)) = ?  ajxj(i). The algorithms in least squares estimation were very straightforward, 
essentially involving only matrix inversion. For generalized linear models, however, 
things are more complicated: the non-linearity means that an iterative scheme has to be 
adopted. We will not go into details of the mathematics here, but it is not difficult to show 
that the maximum likelihood solution is given by solving the equations 

(11.16)  

where the i indices for ai(f) and µ(i) are in recognition of the fact that these vary from 
data point to data point. Standard application of the Newton-Raphson method (chapter 8) 
leads to iteration of the equations 

(11.17)  

where a(s) represents the vector of values of (a1, ..., ap) at the sth iteration, us-1 is the 
vector of first derivatives of the log likelihood, evaluated at a(s-1), and Ms-1 is the matrix of 
second derivatives of the log likelihood, again evaluated at a(s-1). 
An alternative method, the method of ""scoring"" (this is a traditional name, and is not to 
be confused with our use of the word score in ""score function,"" though the meaning is 
similar), replaces Ms-1 by the matrix of expected second derivatives. The iterative steps 
of this method can be expressed in a form similar to the weighted version, equation 
11.12, of the standard least squares matrix solution, equation 11.5: 

(11.18)  

where W(s-1) is a diagonal matrix with iith element ?µ(i)/??(i))2/var(Y(i)) evaluated at a(s-1) 
and z(s-1) is a vector with ith element ? jxj(i)aj+(y(i)-µ(i))??(i)/?µ(i) again evaluated at a(s-1). 
Given the similarity of this to equation 11.12 it will hardly be surprising to learn that this 
method is called iteratively weighted least squares. We need a measure of the goodness 
of fit of a generalized linear model, analogous to the sum of squares used for linear 
regression. Such a measure is the deviance of a model. In fact, the sum of squares is 
the special case of deviance when it is applied to linear models. Deviance is defined as 
D(M) = -2 (log L(M; Y) - log L(M*; Y)), essentially the difference between the log 
likelihood of model M and the log likelihood of the largest model we are prepared to 
contemplate, M*. Deviance can be decomposed like the sum of squares to permit 
exploration of classes of models. 

Example 11.3  

 
In a study of ear infections in swimmers, 287 swimmers were asked if they were frequent 
ocean swimmers, whether they preferred beach or nonbeach, their age, their sex, and also 
the number of self-diagnosed ear infections they had had in a given period. The last 
variable here is the response variable, and a predictive model is sought, in which the 
number of ear infections can be predicted from the other variables. Clearly standard linear 
regression would be inappropriate: the response variable is discrete and, being a count, is 
unlikely to look remotely like a normal distribution. Likewise, it is not a proportion, it is not 
bounded between 0 and 1, so it would be inappropriate to model it using logistic 
regression. Instead, it is reasonable to assume that the response variable follows a 
Poisson distribution, with a parameter depending on the value of the predictor variables. 
Fitting a generalized linear model to predict the number of infections from the other 
variables, with the response following a Poisson distribution and using a log function for the 
link, led to the analysis of deviance table 11.3. 
Table 11.3: Analysis of Deviance Table.  

  

Regression 

Residual 

Total 

d.f. 

4 

282 

286 

deviance 

1.67 

47.11 

48.78 

mean 
devianc
e 

0.4166 

0.1671 

0.1706 

deviance 
ratio 

0.42 
  

  

-4 

-1.67 

Change 
To test the null hypothesis of no predictive relationship between the response variable and 
the predictors, we compare the value of the regression deviance (1.67, from the top of the 
second column of numbers) with the chi-squared distribution with 4 degrees of freedom 
(given at the top of the first column of numbers). This gives a p-value of 0.7962. This is far 
from small, suggesting that there is little evidence that the response variable is related to 
the predictor variables. Not all data necessarily lead to a model that gives accurate 
predictions! 

0.4166 

0.42 

 

 

 

Before leaving this section, it is worth noting a property of equations 11.16. Although 
these were derived assuming that the random variables follow an exponential family 
distribution, examination reveals that these estimating equations make use of only the 
means µ(i); the variances ai(f )V(µ(i)), as well as the link function and the data values. 
There is nothing about any other aspect of the distributions. This means that even if we 
are not prepared to make tighter distributional assumptions, we can still estimate the 
parameters in the linear predictor ?(i) = ?  aixj(i). Because no full likelihood has to be 
formulated in this approach, it is termed quasilikelihood estimation. Once again, of 
course, iterative algorithms are needed. 

11.4 Artificial Neural Networks 
Artificial neural networks (ANNs) are one of a class of highly parameterized statistical 
models that have attracted considerable attention in recent years (other such models are 
outlined in later sections). In the present context, we will be concerned only with feed-
forward neural networks or multilayer perceptrons, as originally discussed in chapter 5. 
In this section, we can barely scratch the surface of this topic, and suitable further 
reading is suggested below. The fact that ANNs are highly parameterized makes them 
very flexible, so that they can accurately model relatively small irregularities in functions. 
On the other hand, as we have noted before, such flexibility means that there is a 
serious danger of overfitting. Indeed, early (by which is meant during the 1980s) work 

was characterized by inflated claims when such networks were overfitted to training sets, 
with predictions of future performance being based on the training set performance. In 
recent years strategies have been developed for overcoming this problem, resulting in a 
very powerful class of predictive models. 
To set ANNs in context, recall that the generalized linear models of the previous section 
formed a linear combination of the predictor variables, and transformed this via a 
nonlinear transformation. Feedforward ANNs adopt this as the basic element. However, 
instead of using just one such element, they use multiple layers of many such elements. 
The outputs from one layer—the transformed linear combinations from each basic 
element—serve as inputs to the next layer. In this next layer the inputs are combined in 
exactly the same way—each element forms a weighted sum that is then non-linearly 
transformed. Mathematically, for a network with just one layer of transformations 
between the input variables x and the output y (one hidden layer), we have 

(11.19)  

Here the  w are the weights in the linear combinations and the ƒks are the non-linear 
transformations. The nonlinearity of these transformations is essential, since otherwise 
the model reduces to a nested series of linear combinations of linear combinations—
which is simply a linear combination. The term network  derives from a graphical 
representation of this structure in which the predictor variables and each weighted sum 
are nodes, with edges connecting the terms in the summation to the node. 

There is no limit to the number of layers that can be used, though it can be proven that a 
single hidden layer (with enough nodes in that layer) is sufficient to model any 
continuous functions. Of course, the practicality of this will depend on the available data, 
and it might be convenient for other reasons (such as interpretability) to use more than 
one hidden layer. There are also generalizations, in which layers are skipped, with inputs 
to a node coming not only from the layer immediately preceding it but also from other 
preceding layers. 
The earliest forms of ANN used threshold logic units as the nonlinear transformations: 
the output was 0 if the weighted sum of inputs was below some threshold and 1 
otherwise. However, there are mathematical advantages to be gained by adopting 
differentiable forms for these functions. In applications, the two most common forms are 
logistic ƒ(x) = ex/(1 +ex) and hyperbolic tangent ƒ(x) = tanh(x) transformations of the 
weighted sums. 
We saw, when we moved from simple linear models to generalized linear models, that 
estimating the parameters became more complicated. A further extra level of 
complication occurs when we move from generalized linear models to ANNs. This will 
probably not come as a surprise, given the number of parameters (these now being the 
weights in the linear combinations) in the model and the fundamental nonlinearity of the 
transformations. As a consequence of this, neural network models can be slow to train. 
This can limit their applicability in data mining problems involving large data sets. (But 
slow estimation and convergence is not all bad. There are stories within the ANN folklore 
relating how severe overfitting by a flexible model has been avoided by accident, simply 
because the estimation procedure was stopped early.) Various estimation algorithms 
have been proposed. A popular approach is to minimize the score function consisting of 
the sum of squared deviations (again!) between the output and predicted values by 
steepest descent on the weight parameters. This can be expressed as a sequence of 
steps in which the weights are updated, working from the output node(s) back to the 
input nodes. For this reason, the method is called back -propagation. Other criteria have 
also been used. When Y takes only two values (so that the problem is really one of 
supervised classification, as discussed in chapter 10) the sum of squared deviations is 
rather unnatural (since, as we have seen, the sum of squared deviations arises as a 
score function naturally from the log-likelihood for normal distributions). A more natural 
score function, based on log-likelihood for Bernoulli data, is 

(11.20)  

 
 

As it happens, in practical applications with reasonably sized data sets, the precise 
choice of criterion seems to make little difference. The vast amount of work on neural 
networks in recent years, which has been carried out by a diverse range of intellectual 
communities, has led to the rediscovery of many concepts and phenomena already well 
known and understood in other areas. It has also led to the introduction of unnecessary 
new terminology. 
Nonetheless, research in this area has also led to several novel general forms of models 
that we have not discussed here. For example, radial basis function networks replace the 
typical logistic nonlinearity of feedforward net-works with a ""bump"" function (a radial 
basis function). An example would be a set of  p-dimensional Gaussian bumps in x 
space, with specified widths. The output is approximated as a linear weighted 
combination of these bump functions. Model training consists of estimating the locations, 
widths, and weights of the bumps, in a manner reminiscent of mixture models described 
in chapter 9. 

11.5 Other Highly Parameterized Models 
The characterizing feature of neural networks is that they provide a very flexible model 
with which to approximate functions. Partly because of this power and flexibility, but 
probably also partly because of the appeal of their name with its implied promise, they 
have attracted a great deal of media attention. However, they are not the only class of 
flexible models. Others, in some cases with an approximating power equivalent to that of 
neural net-works, have also been developed. Some of these have advantages as far as 
interpretation and estimation goes. In this section we briefly outline two of the more 
important classes of flexible model. Others are mentioned in section 11.5.2. 

11.5.1 Generalized Additive Models 
We have seen how the generalized linear model extends the ideas of linear models. Yet 
further extension arises in the form of generalized additive models. These replace the 
simple weighted sums of the predictor variables by weighted sums of transformed 
versions of the predictor variables. To achieve greater flexibility, the relationships 
between the response variable and the predictor variables are estimated 
nonparametrically—for example, by kernel or spline smoothing (see chapter 6), so that 
the generalized linear model form g(µ(i)) = ?  ajxj(i) becomes g(µ(i)) = ? ajƒj(xj(i)). The 
right-hand side here is sometimes termed the additive predictor. Such models take to the 
nonparametric limit the idea of extending the scope of linear models by transforming the 
predictor variables. Generalized additive models of this form retain the merits of linear 
and generalized linear models. In particular, how g changes with any particular predictor 
variable does not depend on how other predictor variables change; interpretation is 
eased. Of course, this is at the cost of assuming that such an additive form does provide 
a good approximation to the ""true"" surface. The model can be readily generalized by 
including multiple predictor variables within individual ƒ components of the sum, but this 
is at the cost of relaxing the simple additive interpretation. The additive form also means 
that we can examine each smoothed predictor variable separately, to see how well it fits 
the data. 
In the special case in which  g is the identity function, appropriate smoothing functions 
can be found by a backfitting algorithm. If the additive model  y(i) = ? ajƒj(xj(i)) + ?(i) is 
correct, then 

 

This leads to an iterative algorithm in which, at each step the ""partial residuals"" y - ? j ? 
kajƒj(xj(i)) for the kth predictor variable are smoothed, cycling through the predictor 
variables until the smoothed functions do not change. The precise details will, of course, 
depend on the choice of smoothing method: kernel, spline, or whatever. 
To extend this from additive to generalized additive models, we make the same 
extension as above, where we extended the ideas from linear to generalized linear 
models. We have already outlined the iteratively weighted least squares algorithm for 
fitting generalized linear models. We showed that this was essentially an iteration of a 

weighted least squares solution applied to an ""adjusted"" response variable, defined by 

linear regression we adopt an algorithm for fitting a weighted additive model. 

Example 11.4  

. For generalized additive models, instead of the weighted 

 
Sometimes blood pressure is deliberately lowered during surgery, using drugs. Once the 
operation is completed, and the administration of the drug discontinued, it is desirable that 
the blood pressure return to normal as soon as possible. The data in this example relate to 
how soon (in minutes) systolic blood pressure returned to 100 mm of mercury after the 
medication was discontinued. There are two predictor variables: the log of the dose of the 
particular drug used and the average systolic blood pressure of the patient during 
administration of the drug. A generalized additive model was fitted, using splines (in fact, 
cubic B-splines) to effect the smoothing. Figures 11.7 and 11.8 show, respectively, a plot of 
the transformed Log(dose) against observed Log(dose) values and a plot of the 
transformed blood pressure during administration against the observed values. (There is 
some nonlinearity evident in both these plots—although that in the Log(dose) plot seems to 
be attributable to a single point.) Predictions to new data points are made by adding to 
together the predictions from each of these components separately. 

Figure 11.7:  The Transformation Function of Log(dose) in the Model for Predicting Time for 
Blood Pressure to Revert to Normal.  

 

Figure 11.8:  The Transformation Function of Blood Pressure During Administration in the 
Model for Predicting Time for Blood Pressure to Revert to Normal.  

 

 

 

11.5.2 Projection Pursuit Regression 

Projection pursuit regression models can be proven to have the same ability to estimate 
arbitrary functions as neural networks, but they are not as widely used. This is perhaps 
unfortunate, since estimating their parameters can have advantages over the neural 
network situation. The additive models of the last section essentially focus on individual 
variables (albeit transformed versions of these). Such models can be extended so that 
each additive component involves several variables, but it is not clear how best to select 
such subsets. If the total number of available variables is large, then we may also be 
faced with a combinatorial explosion of possibilities. The basic projection pursuit 
regression model takes the form 

(11.21)  

 
 

This has obvious close similarities to the neural network model—it is a linear 
combination of (potentially nonlinear) transformations of linear combinations of the raw 
variables. Here, however, the ƒ functions are not constrained (as in neural networks) to 
take a particular form, but are usually found by smoothing, as in generalized additive 
models. This makes them a generalization of neural networks. Various forms of 
smoothing have been used, including spline methods, Friedman's ""supersmoother"" 
(which makes a local linear fit about the point where the smooth is required), and various 
polynomial functions. The term projection pursuit arises from the viewpoint that one is 
projecting X in direction ak, and then seeking directions of projection that are optimal for 
some purpose. (In this case, optimal as components in a predictive model.) Various 
algorithms have been developed to estimate the parameters. In one, components of the 
sum are added sequentially up to some maximum value, and then sequentially dropped, 
each time selecting on the basis of least squares fit of the model to the data. For a given 
number of terms, the model is fitted using standard iterative procedures to estimate the 
parameters in the ak vector. This fitting process is rather complex from a computational 
viewpoint, so that projection pursuit regression tends may not be practical for data sets 
that are massive (large n) and high-dimensional (large p). 

11.6 Further Reading 
Traditional linear regression is covered in depth in the classic book of Draper and Smith 
(1981), as well as in innumerable other texts. Furnival and Wilson (1974) describe the 
classic ""leaps and bounds"" algorithm, which efficiently searches for the best subset of 
predictors to include in a regression model. The seminal text on generalized linear 
models is that of McCullagh and Nelder (1989), and a comprehensive outline of 
generalized additive models is given in the book by Hastie and Tibshirani (1990). 
Projection pursuit regression (PPR) was introduced by Friedman and Stuetzle (1981), 
and theoretical approximation results are given in (for example) Diaconis and 
Shashahani (1984). A very flexible data-driven model for multivariate regression called 
MARS (Multivariate Adaptive Regression Splines) was introduced by Friedman (1991). 
Breiman et al. (1984) describe the application of tree-structure models to regression, and 
Weiss and Indurkhya (1995) describe related techniques for rule-based regression 
models. The technique of boosting, mentioned in chapter 10 in the context of 
classification, can also be usefully applied to regression. Regression can of course be 
cast in a Bayesian context, e.g., Gelman, Carlin, Stern, and Rubin (1995). 
Techniques for local regression, analogous to kernel models for density estimation 
(chapter 9) and nearest neighbor methods for classification (chapter 10), rely on adaptive 
local fits to achieve a nonparametric regression function (for example, Cleveland and 
Devlin (1988) and Atkeson, Schall and Moore (1997)). Such techniques, however, can 
be quite computationally intensive and also are susceptible to the same estimation 
problems that plague local kernel methods in general in high dimensions. 
Good introductions to neural networks are given by Bishop (1995), Ripley (1996), Golden 
(1996), Ballard (1997), and Fine (1999). Ripley's text is particularly noteworthy in that it 
includes an integrated and extensive discussion of many techniques from the fields of 
neural networks, statistics, machine learning, and pattern recognition (unlike most texts 
which tend to focus on one or two of these areas). Bayesian approaches to neural 
network training are described in MacKay (1992) and Neal (1996). 

 
 

 
 

The computer CPU data set, the oxygen uptake data set, the ear infections in swimmers 
data set, and the blood pressure after surgery data are given in  Hand et al. (1994). The 
temperature and latitude data are from Peixoto (1990). The space shuttle data are 
reproduced in Chatterjee, Hancock, and Simonoff (1995) and discussed in  Lavine 
(1991). 

Chapter 12: Data Organization and Databases 
12.1 Introduction 
One of the features that distinguishes data mining from other types of data analytic tasks 
is the quantity of data. In many data mining applications (such as Web log analysis for 
example) there may be millions of rows and thousands of columns in the standard form 
data matrix, so that questions of efficiency of data analysis algorithms are very important. 
An algorithm whose running time scales exponentially in the number n of rows may be 
unusable for all but the smallest data sets. Examples of operations that can be carried 
out in time O(n) or  O(n log n) are counting simple frequencies from the data, finding the 
mode of a discrete variable or attribute, or sorting the data. Generally, such 
computations are feasible even for large data sets. However, even a linear time 
algorithm can be prohibitively costly to use if multiple passes through a data set are 
required. 
If the number of rows n of a data set influences algorithm complexity, so also can the 
number of variables p. For some applications p is very small (less than 10, for example), 
but in others, like market basket analysis or analysis of text documents, we can 
encounter data sets with 105 or even 106 variables. In such situations we cannot use 
methods that involve, for example, operations as the O(p2) computation of pairwise 
measures of association for all pairs of attributes. 
In any data analysis project it is useful to distinguish between two phases. The first is 
actually getting the data to the analysis algorithm, and the second is running the analysis 
method itself. The first phase might seem trivial, but it can often become the bottleneck. 
For example, in analyzing a set of data it may be necessary to apply an algorithm to 
many different subsets of the data. This means we have to be able to search and identify 
the members of each subset rapidly, and also to load that subset into main memory. 
Tree algorithms provide an obvious illustration of this, where the data set is progressively 
split into smaller subsets, each of which has to be identified before the tree can be 
extended. The purpose of data organization is to find methods for storing the data so that 
accessing subgroups of data is as fast as possible. Even in cases when all the data fit 
into main memory, data organization is important. 

In addition to supporting efficient access to data for data mining algorithms, data 
organization plays an important role in the iterative and interactive nature of the overall 
data mining process. The aim of this chapter is to discuss briefly the memory hierarchy 
of modern computer and then present some index structures that database systems use 
to speed up the evaluation of queries. We then move to a discussion on relational 
databases and their query languages, as well as some special purpose database 
systems. 

12.2 Memory Hierarchy 

The memory of a computer is divided into several layers. These layers have different 
access times (where access time is the average time to retrieve a randomly selected 
byte of memory). Indeed, if disk storage were as fast as on-board cache, there would be 
no need to develop any sophisticated methods for data organization. 

A general categorization of different memory structures is the following: 

1.  Registers of the processor. Typically there are fewer than 100 of these, 
and the processor can access data in the registers directly; that is, there 
is no slowdown associated with accessing a register. 

2.  On-processor or on-board cache. This is fast semiconductor memory 

implemented on the same chip as the processor or residing on the 
mother-board. Typical size is 16–1,000 kilobytes and access time is 
about 20 ns. 

3.  Main memory. Normal semiconductor memory, with sizes from 16 

megabytes to several gigabytes, and access time about 70 ns. 

4.  Disk cache. Semiconductor memory implemented as an intermediate 

storage between main memory and disks. 

5.  Disk memory. Sizes vary from 1 gigabyte to hundreds or thousands of 

gigabytes for large arrays of disks. Typical access time is around 10 ms. 

6.  Magnetic tape. A magnetic tape can hold up to several gigabytes of 

data.Access time varies, but can be minutes. 

The differences between the access times are truly large: in the 10 milliseconds needed 
for accessing a disk, we could perform up to a million accesses to fast cache. Another 
way to think about this is to pretend that access time is linearly proportional to actual 
distance. Thus, if we imagine main memory to be an effective distance of 1 meter away 
(within reach of your hand), the equivalent distance for disk memory is order of 105 times 
greater, i.e., 100 km! 

Another major difference between main memory and disk is that individual bytes of main 
memory can be accessed, whereas for disk, whenever we access a byte, actually the 
whole disk page, about 4 kilobytes, containing that byte will be loaded to main memory. 
So if that page happens to contain information that can be used later, it will already be in 
fast memory. As an example, if we want to retrieve 1,000 integers, each taking 4 bytes to 
store, this can take between 1 and 1,000 disk accesses, depending on whether the 
integers are all stored in the same disk page or each on a page of their own. 

The physical properties of the memory hierarchy lead to the following rules of thumb: 

§ If possible, data should be in main memory. 
§ In main memory, data items that are used together should be logically close to 

each other (that is, we should quickly be able to find the next element of a 
subset). 

§ On disk, data items that are used together should be also physically close to 

each other (that is, on the same disk page, if possible). 

 
 

In practice, the user of a system typically has little control over the details of the way the 
data are placed in caches, or over the actual physical layout of data on disk. Normally, 
the systems try to load as much data as possible into main memory, and decide on their 
own how to deal the data objects onto disk pages. The user can influence the kinds of 
auxiliary structures that are created to access subgroups of the data. The next section 
describes in brief some of the data structures used for accessing large masses of data. 

12.3 Index Structures 

A primary goal of data organization is to find  ways of quickly locating all the data points 
that satisfy a given selection condition. Usually the selection condition is a conjunction of 
conditions on individual attributes, such as ""Age = 40"" and ""Income = 20,000."" We 
consider first data structures that are especially applicable to situations in which there is 
only one conjunct. 
An index on an attribute A is a data structure that makes it possible to locate points with 
a given value of A more efficiently than by a sequential scan of the whole data set. 
Indices are typically built either by the use of B*-trees or by the use of hash functions. 

12.3.1 B-trees 
A search tree is probably the simplest index structure. Suppose we have a set S of data 
vectors {x(1), ..., x(n)}, and that we want to find all points having a particular value of an 
ordinal attribute (variable) A as quickly as we can. A search tree is a binary tree structure 
such that each node has a value of A stored into it, and each leaf has a pointer to an 
element of S. Moreover, the tree is structured so that all elements of S pointed to by 

leaves from the left subtree of a node u containing value  a will have values of A which 
are less than or equal to a. Likewise, all elements of S pointed to by leaves in the right 
subtree of u have values for  A that are greater than a. 
Given a binary search tree for an attribute A, it is easy to find the data points from S that 
have a given value b for A. We simply start from the root of the tree, selecting the left or 
the right subtree by comparing b against the values stored in the nodes. When we get to 
a leaf, either we find a pointer to the record(s) with  A = b, or we find that no such pointer 
exists. 
It is also easy to find all the points from S that satisfy the condition b = A = c, a so-called 
""interval query."" Simply locate the leaf where b should be (as above), locate the leaf 
where c should be, and the desired records are pointed to by the leaves between these 
two positions. 
The time needed for finding the records with a given value for attribute  A is proportional 
to the height of the tree plus the number of such records. In the worst case, the height of 
the tree is n, the number of points in the set S, but there are ways of ensuring that the 
height of the tree will be O(log n) (although they are beyond the scope of this text). In 
practice, binary search trees are relatively seldom used, since B*-trees, discussed 
below, are clearly superior for accessing data on a disk.  
The basic idea for B*-trees is the same as for search trees: the pointers to the data 
objects are in the leaves of the tree, and interior nodes contain values of the attribute A 
that indicate where certain pointers are to be found. However, instead of having two 
children and one value for  A per interior node, a B*-tree typically has hundreds of 
children and values. 
In more detail, a B*-tree of degree  M for set of values is a tree where 

§  all leaves are at the same depth; 
§  each leaf contains between M/2 and M keys (possible target values); 
§  each interior node (except possibly the root) has K children C1, ..., CK, 

where M/2 = K = M and K - 1 values a1, ..., aK-1; for all i, all the key values 
stored in the leaves of subtree Ci are larger than  ai-1 and at most as large 
as ai. 

Searching from a B*-tree is carried out in the same way as from a binary search tree: for 
each interior node of the tree, the values ai are used to select the correct subtree. 
A B*-tree differs from the basic binary search tree in that the height is guaranteed to be 
O(log n), since all leaves are on the same depth. Actually, the depth of the tree is 
bounded by logM/2 n. Typically, the value of M is selected so that each node of the tree 
fits into a single disk page. If M is 100, then (M/2)5 is over 300 million, and we find that 
for most realistic values of n, the number of elements in the set, the tree will have at 
most five levels: This means that finding a data point from 300 million points on the basis 
of the value of a single attribute can be done in three disk accesses, as the root node 
and the second level of the tree can be held in main memory. Most database 
management systems use B*-tree structures as one of their index structures. 

12.3.2 Hash Indices 
Suppose again that we have a set S of data points, and that we want to find all points 
such that attribute A has value  a. If the set of possible values of A is small, we can do the 
following: for each possible value, construct a list of pointers to the data points with that 
value for A. Then, given the query ""Find the points with A = a,"" we need only to access 
the list for a. 
This method is not feasible, however, if there is a large number of potential values for A: 
we cannot maintain a list for each of the possible 232 integers which can be represented 
by 32 bits, for example. What we can do is to apply a transformation to the A-values so 
as to reduce the range of possible values. 
In more detail, let Dom(A) be the set of possible values of A. A hash function is a 
function h from Dom(A) to {1, ..., M}, where M is the size of the hash table r. For each j ? 
{1, ..., M} we store into r[j] a list of pointers to those records xi in S whose A value ai 
satisfies h(ai) = j. When we want to find all the data points with A = a, we simply compute 
h(a), go to location  r[h(a)] and traverse the list of data points, for each of them checking 
whether the value of A really was a, or whether it was another value b with the property 
that h(b) = h(a) (this is called a collision). 

A typical hash function is a mod M, when  M is chosen to be suitable prime larger than n, 
the number of data points. If the hash function is well chosen and the hash table is 
sufficiently large, collisions are rare, and searching for the points with a given A value 
can be done in time essentially proportional to the number of such points. Hash indices, 
however, do not directly support interval queries. 

12.4 Multidimensional Indexing 
Traditional index structures such as hashing and B*-trees provide fast access to rows of 
tables on the basis of values of a given attribute or collection of attributes. In some 
applications, however, it is necessary to express selection conditions on the basis of 
several attributes, and normal index structures do not help. Consider, for example, the 
case of storing geographic information about cities. Suppose, for example, we wish to 
find all the cities with latitude between 30 N and 40 N, longitude between 60 W and 70 
W, and population at least 1,000. Such a query is called a rectangular range query. 
Suppose the cities table is large, containing millions of city names. How should the query 
be evaluated? A B*-tree index on the latitude attribute makes it possible to find the cities 
that satisfy the conditions for that attribute, but for finding the rows that satisfy the 
conditions on longitude among these, we have to resort to a sequential scan. Similarly, 
an index on longitude does not help much. What is needed is an index structure that 
makes it possible to use directly the conditions on both attributes. 
Multidimensional indexing refers to techniques for finding rows of tables on the basis of 
conditions on multiple attributes. One of the widely used methods is the R*-tree. Each 
node in the tree corresponds to a region in the underlying space, and the node 
represents the points within that region. For dimensions up to about 10, the 
multidimensional index structures speed up searches on large databases. Fast 
evaluation of range queries for data sets with larger numbers of dimensions (e.g., in the 
100s) is still an open problem. 

12.5 Relational Databases 

 
 

 
 

In data mining we often need to access a particular subset of the data and compute a 
function from the values of certain attributes on that subset. We have discussed some 
data structures that can help in finding the relevant data points quickly. Relational 
databases provide a unified mechanism for fast access to selected parts of the data. 
In database terminology, a data model is a set of constructs that can be used to describe 
the structure of data, plus a set of operations for manipulating the data. (Note that this 
use of the word model is rather different from that given earlier in the book. Here it is a 
structure imposed on the data by design, rather than a structure discovered existing 
within the data. The dual use of the word model is perhaps unfortunate, and arises 
because of the different disciplines that have contributed to data mining; in this case, 
statistics and database theory. Fortunately, confusion seldom arises; which of the two 
meanings is intended will generally be clear from the context). The  relational data model 
is based on the idea of representing data in tabular form. A table header (schema) 
consists of the table name and a set of named columns; the column names are also 
called attributes. The actual table (an instance of the schema), also called a relation, is a 
named set of rows. Each table entry in the column for attribute A is a value from the 
domain Dom(A) of A. Note that when the attributes are defined, the domain of each must 
also be specified. An attribute can be of any data type: categorical, numeric, etc. The 
order of the row and columns in a table is not significant. 
We can put this more formally. A relation schema R is a set of attributes {A1, ..., Ap}, 
where each attribute Aj has an associated domain Dom(Aj). A row over the schema R is 
a mapping t : R ?  ? iDom(Aj) where t(Aj) ? Dom(Aj). A table or relation over the schema 
R is a collection of rows over R. A relational database schema  R is a collection {R1, ..., 
Rk} of relation schemas (with possibly some constraints on the relation instances), and a 
relational database r over the schema R consists of a relation over  Ri, for each i = 1, ..., 
k. 

Example 12.1  

 
Consider a retail outlet with barcode readers, or a Web site where we log each purchase by 
a customer. For each customer transaction, also called here a basket, we can collect 
information about which products the customer bought, and how many of each product. In 
principle, these data could be represented as a table, where there is an attribute for each 
product and a row for each transaction. For row t and attribute A the entry t(A) in the matrix 
indicates how many As the customer bought. That is, for each attribute A the domain 
Dom(A) is the set of nonnegative integers. See figure 12.1 for an example table, here 
called transactions. 

transactions 

basket-
id 
t1  

t2  
t3  

t4  

t5  
t6  

t7  

t8  

t9  
t10  

chips 

mustard 

sausage 

Pepsi  

Coca-
Cola 

Miller 

Bud 

1 

2 

1 

0 

0 

1 

4 

0 

1 

0 

0 

1 

0 

0 

1 

1 

0 

1 

0 

1 

0 

3 

1 

2 

1 

1 

2 

1 

0 

2 

0 

5 

0 

0 

1 

0 

4 

0 

1 

0 

0 

0 

1 

0 

0 

0 

0 

4 

0 

4 

1 

1 

0 

6 

0 

1 

1 

0 

0 

1 

0 

0 

0 

0 

2 

0 

0 

1 

1 

1 

 
Figure 12.1: Representing Market Basket Data as a Table with an Attribute for Each Product.  
As the product selection probably changes rapidly, encoding the names of products into 
attributes may not be a very good idea. An alternative representation would be to use a 
table such as the one called baskets, shown in  figure 12.2, where the product names are 
represented as entries. This table has three attributes, basket-id, product, and quantity, and 
the domain of product is the set of all strings, while the domain of quantity is the set of 
nonnegative numbers. Note that there is no unique way of representing a given set of data 
as a relational database: both the transactions and baskets tables represent the same data 
set. 

baskets 

basket-id 
t1  

t1  
t2  

t2  

t2  

t2  
t2  

... 

product 

quantity 

chips 

Miller 

chips 

mustard 

sausage 

Pepsi 

Miller 

1 

1 

2 

1 

3 

5 

1 

 
Figure 12.2: A More Realistic Representation of Market Basket Data.  
In addition to the data about the transactions, the retailer maintains information about the 
prices of individual products. This could be represented as a table such as the products 
table shown in  figure 12.3. 

products 

product 

price 

supplier 

category 

chips 

Miller 

mustard 

sausage 

Pepsi 

Coke 

... 

1.00 

0.55 

1.25 

2.00 

0.75 

0.75 

ABC 

ABC 

DEF 

DEF 

ABC 

DEF 

food 

drink 

spices 

food 

drink 

drink 

 
Figure 12.3: Representing Prices of Products.  
The product data can be too detailed for useful summaries. Therefore, the retailer could 
use a classification of various products into larger product categories. An example is shown 
in figure 12.4. 

product-hierarchy 

product 

Pepsi 

Coke 

Budweiser 

Miller 

soft drink 

beer 

... 

category 

soft drink 

soft drink 

beer 

beer 

drink 

drink 

 
Figure 12.4: Representing the Hierarchy of Products as a Table.  

The table describes a hierarchy, in saying that Pepsi and Coke are soft drinks, and that soft 
drinks and beers are drinks. 

The schemas of the tables in this example can be described succinctly by listing just the 
names of the tables and their attributes: 

§  baskets(basket-id,product,quantity) 
§  products(product,price) 
§  product-hierarchy(product,category) 

 

 

Thus the relational data model is based on the idea of tabular representation. The values 
in the cells may be arbitrary atomic values, such as real numbers, integers, or strings; 
sets or lists of values are not allowed. This means that, if, for example, we want to 
represent information about people, their ages, and phone numbers, we cannot store 

multiple phone numbers in one attribute. If restricted in this way, the model is said to 
have first normal form. 

The relational model is widely used in data management, and virtually all major database 
systems are based on it. Some systems provide additional functionality, such as the 
possibility of using object-oriented data modeling methods. 

Even in relatively small organizations, relational databases can have hundreds of tables 
and thousands of attributes. Managing the schema of the database can, therefore, be a 
complicated task. Sometimes it is claimed that for data analysis purposes it suffices to 
combine all the tables into a massive observation matrix, or ""universal table,"" and that 
therefore in data mining one does not have to care about the fact that the data are in a 
database. However, an examination of simple examples shows that this is not feasible: 
the universal table would be so large that operations on it would be prohibitively costly. 

Example 12.2  

 
Consider the example of products in a supermarket, and see what it would look like in a 
more realistic setting. Instead of having a table with attributes Product and Price only, we 
probably would have a table with at least attributes Product, Supplier, and Price, and an 
additional table about suppliers with attributes Supplier, Address, Phone Number, etc. If we 
wanted to combine the tables into one table, this table would have to include attributes 
Transaction ID, Product, Number, Supplier Address, Phone Number, Product Price, etc. 
Furthermore, if each product belongs on the average to K different product groups, 
including the information from the Product-Hierarchy table would increase the size of the 
representation by a factor of K. For even a moderately sized database, this combining 
process would lead to a table that would be far too large to be stored explicitly. 
 
 

12.6 Manipulating Tables 

Being able to describe the structure of data and to store data using this structure is not 
sufficient in itself for data management: we also must be able to retrieve data from the 
database. We briefly describe two languages for manipulating collections of tables (that 
is, relational databases): relational algebra, in this section, and the Structured Query 
Language (SQL), in the next. Relational algebra is based on set-theoretic notation and is 
quite handy for theoretical purposes, while SQL is widely used in practice. 
In the examples, we use r, s, etc. to refer to tables, and R, S, etc. to refer to the sets of 
attributes for those tables. 

Relational algebra contains a set of basic operations for manipulating data given in 
tabular form, and several derived operations (operations that can be expressed as a 
sequence of basic operations) are also used. The operations include the three set 
operations—union, intersection, and difference—and the projection operation for 
removing columns, the selection operation for selecting rows, and the join and Cartesian 
product operations for combining rows from two tables. 

Example 12.3  

 s = {t | t ˛

 
The operations of relational algebra are formally defined as follows: Assume r and s are 
tables over the set R of attributes, 
 r or t ˛
§  Union r ¨
§  Intersection r n  s = {t | t ˛
§  Difference r \ s = {t | t ˛
§  Projection Given X ?  R, then  r[X] = {t[X] | t ˛

 r}, where t[X] is the row 
obtained from row t by leaving only the values in the columns of X. 

 s}. 
 r and t ˛

 s}. 

 r and t ?  s}. 

§  Selection Given a condition F on rows of table r, 

s F (r) = {t ˛

 r | t satisfies F}. 

§  Join r ?  s = {tu | t ˛

obtained by pasting t and u together. 

 r, u ˛

 s, t[A] = u[A] for all A ˛

 R n  S}, where tu is the row 

 

 

Set Operations 

Tables are sets of rows, and all operations in the relational algebra are set-oriented: they 
take sets as arguments and produce a set as their result. This makes it possible to 
compose relational queries: the results of a query are relations, as are the arguments. 
Conventional set operations are useful for manipulating tables. We shall include union, 
intersection, and difference (denoted by r ¨
 s, r n  s, and r \ s, respectively) as the basic 
operations in relational algebra. The union operation combines two tables over the same 
set of attributes: the result r ¨
 s contains all the rows that occur in r or s. The intersection 
operation r n  s results in the table containing those rows that occur in r and in s. The 
difference operation r \ s gives the rows that occur in r but not in s. These operations all 
assume that r and s are tables over the same set of attributes. 
As an example, suppose r is a table representing the prices of all soft drinks, and s is a 
table representing the prices of all products costing at most $2.00. Then  r ¨
 s is the table 
of all soft drinks and products costing less than $2.00, r n  s is the table of all soft drinks 
costing less than $2.00, and r \ s contains one row for each soft drink that does not cost 
less than $2.00, i.e. that costs at least $2.00. The intersection operation could, of course, 
be defined using the union and difference operations: r n  s = (r ¨
 (s \ r)). 
Care must be taken to ensure that the resulting set is a table, in the sense that it has a 
schema. Therefore r ¨
 s, r n  s and r \ s are defined only if r and s are tables over the 
same schema—that is, over the same set of attributes. 
Intersection queries can be used in construction of rule sets, for example. (Algorithms for 
rule learning are discussed in  chapter 13.) Suppose, we have computed a table r 
corresponding to the observations that satisfy a condition F, and similarly another table s 
that corresponds to the observations satisfying condition G. The intersection rn s 
corresponds to those observations that satisfy both conditions; the cardinality of the 
intersection tells what the overlap between the conditions are. If r and s are computed 
from the same base table of observations, we can also achieve the same effect by using 
the conjunction F ?  G as the selection condition in the query. Intersection queries occur 
most naturally in situations in which we need to check whether the same value occurs in 
two tables. 

 s) \ ((r \ s) ¨

Projection 
The purpose of the projection operation is to trim a table so that only the data in specific 
columns of interest remain. Given a table  r with attributes R, and X ?  R, the projection of 
r on X is obtained by removing from the table all the columns outside X. A side effect of 
projecting a table is that the number of rows, as well as the number of columns, may 
decrease. If the argument table over  R is projected on a set of attributes X, and if table  r 
over R contains two rows that agree on the X attributes, but differ on some attribute in R 
\ X, the projected rows would be identical. Such identical rows are commonly called 
duplicates. Since tables are sets, they cannot contain duplicates, and only one 
representative of each duplicate is retained. Because this feature is implicit in the 
concept of a set, it does not show up in the definition of the projection operation. 
Commercial database systems often differ from the pure relational model on this point. In 
real implementations, tables are stored as files. Files, of course, can contain several 
identical records. Checking the uniqueness of records could take a lot of time. It is 
therefore customary that tables in commercial database management systems can 
contain duplicates. 

The projection operation in relational databases is related to but not identical to the 
projection encountered in vector spaces. Both operations take points (called rows in 
databases) and produce points in a lower-dimensional space (rows with fewer 
attributes). In relational databases, we can project only to subspaces defined directly by 
the attributes; for vector spaces, projection can be defined for any subspace (that is, any 
linear combination of basis vectors (here attributes)). 

Selection 
The selection operation is used to select rows from a table. Given a Boolean condition F 
on the rows of a table r, the selection operation s F applied to r yields the table s F (r) 
consisting of those rows of r that satisfy the condition. 

Selection is probably the most frequently used operation of the relational algebra: each 
time we want to focus on a particular row or subset of rows in a table, we need to use 
selection. Selection occurs often in the implementation of data mining algorithms. For 
example, in building a decision tree we want a list of the observations that belong to a 
particular node of the tree. This set of observations is exactly the answer to a selection 
query, where the selection condition is the conjunction of the conditions appearing in the 
nodes from the root of the tree to the node in question. Similarly, if we want to implement 
association rule algorithms using the relational algebra, one has to execute several 
selection queries, each one that looks at the subset of observations satisfying the 
condition that each variable in a candidate frequent set has value 1. 
In pure relational algebra, selections are based on exact equalities or inequalities. For 
data mining, we often need concepts of inexact or approximate matching. If a predicate 
match for approximate matching between attribute values is available, we can (at least 
in some database systems) use that directly in database operations to select rows that 
satisfy the approximate matching condition. (Chapter 14 discusses approximate 
matching in more detail.) 

Cartesian Product and Join 
Both projection and selection are used for removing data from a table. The join and 
Cartesian product operations are used for connecting data that are stored in two different 
tables. Given tables r and s with attributes R and S, respectively, and assuming that R 
and S are disjoint (that is, that no attribute name occurs in both) then the Cartesian 
product r × s of r and s is a table over the attributes R ¨
 S, and it contains all rows that 
can be obtained by pasting together a row from r and a row from s. Thus r × s will have 
|r||s| rows, where |r| is the number of rows in r. 
The Cartesian product is needed for combining rows from different tables. It is seldom 
used by itself, more often, we use the join operation. Given a selection condition F , the 
join r ? F s of  r and s is obtained by selecting the rows satisfying F from r × s. For 
example, we might compute the join of tables baskets and products, using the 
equality baskets.product = products.product as the join condition. The result of 
this operation is a table that has columns for the basket id, for the product name, 
quantity, and price. (To be precise, the result has two columns for the product name, one 
from each of the original tables; we might want to project one of them away.) 

A typical application of the join in data mining algorithms is to combine different sources 
of information. If for example, we have data about customer demographics and customer 
purchase behavior, such data are usually stored in different tables. To combine the 
relevant pieces of data, we need to do a join operation. 

12.7 The Structured Query Language (SQL) 

Relational algebra is a useful and compact notation. In database management systems, 
SQL is the standard adopted by most database management system vendors. SQL 
implements a superset of the relational algebra. Here we introduce only the basic 
structure of SQL programs. 

 
 

The basic statement of SQL is the ""select-from-where"" expression or query, which has 
the form 
   

§ 

select  

from  

where  

A1, A2, 
..., Ap  
r1, r2, 
..., rk  
list of 
conditi
ons 

Here each ri is a table, and each Aj is an attribute. The intuitive meaning is that for each 
possible choice of rows t1, ..., tk from the tables r1, ..., rk, we test whether the conditions 
are true. If they are, a row consisting of the values of the attributes Aj is output. 
The second line of the query, the from clause, specifies the tables to which the SQL 
statement is applied. The third line, the  where clause, specifies the conditions that the 
rows in those tables must satisfy to be accepted into the result of the statement. The first 
line, the select clause, then specifies which attributes of the participating tables should 
appear in the result. It corresponds to the projection operation of relational algebra (not 
the selection operation). The ""where"" clause is used for representing the selection 
conditions occurring in the selection and the join operations. For a selection operation, 
the selection conditions are simply listed in the list of conditions of the where clause, 
separated by the keywords and, or, and not. 

Example 12.4  

 

All products that cost more than 2.00 can be found by the query 

   

§ 
select  

from  
where  

product 

products 
price > 
2.00 

Finding all transactions that included at least one product that cost more than 2.00 is 
achieved by 

   

§ 
select  

from  

where  

 

 

basket-id, 
product, price 

baskets, 
products 
baskets.produc
t = 
products.produ
ct and price > 
2.00 

If some tables in the ""from"" clause have common attributes, the attribute names must be 
prefixed by a dot and the name of the table when they appear in the ""select"" clause or 
""where"" clause. If all attributes of participating tables should appear in the result, the list 
of attributes in the ""select"" clause can be replaced by a star. 

Aggregation in database queries refers to the combination of several values into one, by 
the sum or maximum operators, for example. Relational algebra does not have 
operations for aggregation, but SQL does. An aggregate is in general a quantity 
computed from the database whose value depends on several rows of the database. 

Example 12.5  

 
The following queries show how aggregate queries relating to supermarket purchases can 
be described in SQL. First, we find for each product how many exemplars of it have been 
sold. To do this, we use the group by construct of SQL. This operation groups the rows of 
the input relation by the values of a certain attribute; the other operations in the SQL 
statement are performed separately for each clause. 

§  select item, sum(quantity) 
§  from baskets 
§  group by item 

The execution of this statement would proceed by first grouping the rows of the baskets 
relation according to the item attribute, and then for each group outputting the item name 
and the sum of the quantities for that group. 

The next query finds the total sales for each product. 

§  select item, sum(quantity)*price 
§  from baskets, products 
§  where item=product 
§  group by item 

Next we find total sales for each product belonging to soft drinks. 

§  select item, sum(quantity)*price 
§  from baskets, products, product-hierarchy 
§  where item=product and products.product=product-hierarchy.product and 

class = ""soft drink"" 

§  group by item 

 

 

SQL was developed for traditional database applications such as generating reports and 
concurrent access and updating of transaction data by many users in real-time. Thus, it 
is not a big surprise that the language as such does not provide a very good platform for 
implementing data mining algorithms. There are two reasons for this: lack of suitable 
primitives and the need for efficiency. 

Regarding the primitives, in SQL it is quite easy to do counting and aggregation. 
Therefore, for example, the operations needed for association rule algorithms are 
straightforward to implement by accessing the data using SQL. For building decision 
trees we need to be able to count the number of observations that fulfill the conditions 
occurring in the tree nodes from the root to the node in question. This is possible to do 
by selection and count queries. Where the primitives of SQL fail is in common statistical 
operations, such as matrix inversion, singular value decomposition (SVD), and so forth. 
Such operations would be extremely cumbersome to implement using SQL. This means 
that fitting complicated models is usually carried out outside the database system. 

Even in cases when the SQL primitives are sufficient for expressing the operations in the 
data mining algorithm, there are reasons to implement the algorithm in a loosely-coupled 
manner, i.e., by downloading the relevant data to the algorithm. The reason is that the 
connection between a database management system and an application program 
typically enforces a large overhead for each query. Thus, while it is quite elegant to 
express the basic operations of association rule algorithms (for example) using SQL, 
such an implementation would typically be fairly slow. An additional cause for 

 
 

 
 

performance problems is that in association rule algorithms (for example) we must 
compute the frequency of a large number of candidate frequent sets. In a specialized 
implementation it is easy to do many of these counting operations in one pass through 
the data, whereas in an implementation based on using an SQL database management 
system, each candidate frequent set would cause a separate query to be issued. 

12.8 Query Execution and Optimization 

A query can be evaluated in various different ways. Consider, for example, the query 

§  select t.product 
§  from baskets t, baskets u 
§  where t.transaction = u.transaction and  u.product = ""beer"" 

Here the notation baskets t, baskets u means that, in the query, t and u refer to rows of 
the baskets table. The notation is needed because we want to be able to refer to two 
different rows of the same table. The query finds all the products that have been bought 
in a transaction that also included beer. 
The trivial method for evaluating such a query would be to try all possible pairs of rows 
from the baskets table, to check whether they agree on the basket-id attribute, and to 
test that the second row has ""beer"" in the product attribute. This would require n2 
operations on rows, where  n is the size of the baskets table. 
A more efficient method is to first locate the rows from the baskets table that have ""beer"" 
in the product attribute and sort the basket-ids of those rows into a list L. Then we can 
sort the baskets table using the basket-id attribute as the sort key and extract the 
products from the rows whose basket-id appears in the list L. Assuming that L is a 
relatively short list, this approach requires O(n) operations for finding the rows with beer, 
O(n log n) operations for sorting the rows, and O(n) operations for scanning the sorted 
list and selecting the correct values; i.e., altogether O(n log n) operations are needed. 
This is a clear improvement over the O(n2) operations needed for the naive method. 
Query optimization is the task of finding the best possible evaluation method for a given 
query. Typically, query optimizers translate the SQL query into an expression tree, where 
the leaves represent tables and the internal nodes represent operations on the children 
of the nodes. Next, algebraic equalities between operations can be used to transform the 
tree into an equivalent form that is faster to evaluate. In the previous example, we have 
used the equation sF(r ?  s) = s F(r) ?  s, where F is a selection condition that concerns 
only the attributes of r. After a suitable expression tree is found, evaluation methods for 
each of the operations are selected. For example, a join operation can be evaluated in 
several different ways: by nested loops (as in the trivial method above), by sorting, or by 
using indices. The efficiency of each method depends on the sizes of the tables and the 
distribution of the values in the tables. Thus, query optimizers keep information about 
such changing quantities to find a good evaluation method. Theoretically, finding the best 
evaluation strategy for a given query is an NP -hard problem, so that finding the best 
method is not feasible. However, good query optimizers can be surprisingly effective.  
Database management systems strive to provide good performance for a wide variety of 
queries. Thus, while for a single query it might be possible to write a program that 
computes the result more efficiently than a database management system would 
compute it, the strength of databases is that they provide fast execution for most of the 
queries. In data mining applications this is useful, as the queries are typically not known 
in advance (for example, in decision tree construction). 

12.9 Data Warehousing and Online Analytical Processing 
(OLAP) 
A retail database, with information about customers, transactions, products, prices, etc., 
is a typical example of an operational database: the database is used to conduct the 
daily operations of the organization, and the operations can rely quite heavily on it. Other 
examples of operational databases include airline reservation systems, bank account 

databases, etc. Strategic databases are databases that are used in decision making in 
the organization. The decision support viewpoint is quite closely aligned with the goal of 
data mining. Indeed one could say that a major goal of data mining is decision support. 

Typically, an organization has several different operational databases. For example, a 
retail outlet might have a database about market baskets, a warehouse system, a 
customer database (or several), a payroll database, a database about suppliers, etc. 
Indeed, a diversified service company might even have several customer databases. 
Altogether, large organizations can have tens or hundreds of different operational 
databases. For decision support purposes one needs to combine information from 
various operational databases to find out overall patterns of activity within the company 
and with its customers. Building decision support applications that directly access the 
operational databases can be quite difficult. 
Operational databases such as our hypothetical retail database, any customer database, 
or the reservation system of an airline, are most often used to answer well-defined and 
repetitive queries such as ""What is the total price of the products in this basket,"" ""What is 
the address of customer Smith,"" or ""What is the balance of account 123456?"" Such 
databases have to support a large number of transactions consisting of simple queries 
and updates on the contents of the data. This type of database usage is called online 
transaction processing (OLTP). 
Decision support tasks require different types of queries: aggregation is far more 
important. A typical decision support query might be ""Find the sales of all products by 
region and by month, and the difference compared to last year."" The term online 
analytical processing (OLAP) refers to the use of databases for obtaining summaries of 
the data, with aggregation as the principal mechanism. 

Example 12.6  

 

The tables of the database of the retailer could have the following form: 

§  baskets(basket-id, item, quantity) 
§  products(product, price, supplier, category) 
§  product-hierarchy(product,category) 
§  basket-stores(basket-id,store,day) 
§  stores(store's name,city,country) 

Here we have added the table basket-stores that tells in which store and on what date a 
certain basket was produced. For decision support purposes a more useful representation 
of the data might be using the table 

§  sales(product,store,date,amount) 

for representing the amount of a product sold at a given store on a given date. We can add 
rows to this table by SQL statements 

§  insert into sales(product,store,date,amount) 
§  select item, store, date, sum(quantity)*price 
§  from baskets, basket-stores, products 
§  where baskets.basket-id = basket-stores.basket-id and item = product 
§  group by item, store, date 

After this, we can find the total dollar sales of all product categories by countries by giving 
the following query: 

§  select products.product, store.country, sum(amount) 
§  from sales, stores, dates, products 
§  where dates.year = 1997 

o 
o 
o 

and sales.product=products.product 
and sales.store=stores.store 
and sales.date=dates.date 

§  group by products.category, store.country 

 

OLTP and OLAP pose different requirements on the database management system. 
OLTP requires that the data are completely up to date, allows the queries to modify the 
database, allow several transactions to execute concurrently without interfering with 
each other, requires that responses be fast, and so forth. However, the OLTP queries 
and updates themselves are relatively simple. In contrast, in OLAP the queries can be 
quite complex, but normally only one of them executes at a given time. OLAP queries do 
not modify the data, and in finding out facts about last year's sales it is not crucial to 
have today's sale information. The requirements are so different that it makes sense to 
use different types of storage organizations for handling the two applications. 
A data warehous e is a database system used to store information from various 
operational databases for decision support purposes. A data warehouse for a retailer 
might include information from a market basket database, a supplier database, customer 
databases, etc. The data in the payroll database might not be in the data warehouse if 
they are not considered to be crucial in decision support. A data warehouse is not 
created just by dumping the data from various databases to a single disk. Several 
integration tasks have to be carried out, such as resolving possible inconsistencies 
between attribute names and usages, finding out the semantics of attributes and values, 
and so on. Building data warehouses is often an expensive operation, as it requires 
much manual intervention and a detailed understanding of the operational databases. 

The difference between OLTP, OLAP, and data mining is not always clear cut. We can in 
fact see a continuum of queries: find the address of a customer; find the sales of this 
product in the last month; find the sales of all products by region and month; find the 
trends in the sales; find what products have similar sales patterns; find rules that predict 
the sale of a certain product customer segmentation/clustering. The first query is typically 
carried out by using an OLTP query, the second is a typical OLAP query, and the last 
two might be called data mining queries. But it is difficult to define exactly where data 
mining starts and OLAP ends. 

12.10 Data Structures for OLAP 
OLAP requires the comput ation of various aggregates from large base tables. Since 
many aggregates will be needed over and over again, it makes sense to store some of 
them. The data cube is a clever technique for viewing the results of various aggregations 
in a tabular way. 

The previous example showed the sales table with the schema 

§  sales(product,store,date,amount). 

A possible row from this table might be 

§  sales(red wine, store 1, August 25, 17.25), 

indicating that the sales of red wine at store number 1 on August 25 were $17.25. 
Inventing a new value all to stand for any product, we might consider rows like 

§  sales(all, store 1, August 25, 14214.70), 

with the intended meaning that the total sales of all products in store 1 on August 25 
were $14,214.70. In statistical terms, this gives us the marginal of the table, summing 
over values of the first attribute. 
The data cube for the sales table contains all rows 

§  sales(a, b, c, d), 

where a, b, and c, are either values from the domains of the corresponding attributes or 
the specific value all, and d is the corresponding sum. That is, the data cube consists of 
the raw table and all marginal tables: the one-dimensional ones, the two-dimensional 
ones, and so on up to those obtained by summing over each attribute individually. 

 

 
 

 
 

12.11 String Databases 

Interest in text and string-oriented databases has increased dramatically in recent years. 
Molecular biology is one of the reasons: modern biotechnology generates huge amounts 
of protein and DNA data sets that are often recorded as strings. Even more important 
has been the rise of the Web: search engines require efficient methods for finding 
documents that include a given set of terms. Relational databases are fine for storing 
data in a tabular form, but they are not well suited for representing and accessing large 
volumes of text. Recently, several commercial database systems have added support for 
the efficient querying of large text data fields. 
Given a large collection of text, a typical query might be ""find all occurrences of the word 
mining in the text."" More generally, the problem is to find occurrences of a pattern P in a 
text T . The pattern P might be a simple string, a string with wildcards, or even a regular 
expression. The occurrence of P in T might be defined as an exact match or an 
approximate match, where errors are allowed. 
The occurrences of the pattern P in text T can obviously be found by sequentially 
scanning the text and for each position testing whether P matches or not. Much more 
efficient solutions exist, however. For example, using the suffix tree data structure we 
can find the list of all occurrences of pattern p in time that is proportional to the length of 
p (and not dependent on the size of the text), and outputting the occurrences of p can be 
done in time O(|p| + L), where L is the number of occurrences of p in the text. The suffix 
tree can be constructed in linear time in the size of the original text, and it is fast also in 
practice. 
Schematically, a Web search engine might have two data structures: a relational table 
pages(page-address, page-text) and a suffix tree containing all the text of all the 
documents loaded into the system. When a user issues a query such as ""find all 
documents containing the words data and mining,"" the suffix tree is used to find two lists 
pages: those containing the word data and those containing mining. Assuming the lists 
are sorted, it is straightforward to find the documents containing both words. Note, 
however, that the number of documents containing both data and mining is probably 
much less than the number containing one of the terms. 

12.12 Massive Data Sets, Data Management, and Data Mining 

So far in this chapter we have focused on database technology in a general sense. An 
important question remains as to how data mining and database technology interact. Our 
discussion of this interaction will be relatively brief, since there is no consensus to date 
among researchers and practitioners as to any ""best"" approach in terms of handling the 
interaction between data mining algorithms and database technology. At issue is the 
following: many massive data sets are either already stored in relational databases or 
could be more effectively managed and accessed during a data mining project if they 
were converted into relational database form. On the other hand, most data mining 
algorithms focus on the modeling and optimization aspects of the problem and effectively 
assume the data reside in a flat file in main memory. If the data to be mined are primarily 
on disk, and/or stored in a relational format (perhaps with an SQL interface), how then 
should we approach the question of interfacing our data mining algorithm to the data? 
This is the issue of data management, which, as we briefly discussed in chapter 5, is 
typically not addressed explicitly in most descriptions of data mining algorithms. And 
perhaps this is indeed the most flexible approach, since the solutions we adopt in 
practice will be a function of various application factors, such as the amount of data, the 
amount of available main memory, how often the algorithm will need to be rerun, and so 
forth. Nonetheless, we can identify a few general approaches to this problem, which we 
discuss below. 

12.12.1 Force the Data into Main Memory 
The most obvious approach, and one that practitioners have used for years, is to see 
whether the data can in fact be stored in main memory and (subsequently) accessed 
efficiently by the data mining algorithm. As main memory technology allows random 

 
 

access memory sizes to grow into the gigabyte range, this approach can be quite 
practical for many ""medium-sized"" data analysis applications. Of course there are other 
applications, e.g., those with hundreds of millions of complex transactions, where we 
cannot hope to ever load the data into main memory in the forseeable future. In such 
cases we can hope to subselect parts of the data, perhaps by generating a random 
sample of records so that we have n' transactions instead of n to deal with (where n' is 
much smaller than n). 

We could also select subsets of features in some manner. For example, one of the 
authors worked on a predictive modeling application involving on the order of 1,000 
variables and 200,000 customers. Decision trees were built on random samples of 5,000 
customers, and the union of variables from the resulting trees was then used to build 
models (using trees, nonlinear regression, and other techniques) on the entire set of 
200,000 records. This is of course an entirely heuristic procedure, and an important 
variable might have been omitted from the trees as a result of the multiple random 
sampling during model building. Nonetheless, this is a fairly typical example of the type 
of ""data engineering"" that is often required in practice to obtain meaningful results in a 
reasonable amount of time. Note also that generating a random sample from a relational 
database can itself be a nontrivial process. There are, of course, numerous refinements 
to the basic idea of random sampling, e.g., taking an initial small sample to get a general 
idea of the ""data landscape,"" then further refining this sample in some automated 
manner, and so forth.  

Of course even if the data fit in main memory, we still must be careful. It may well be that 
we have to subsample the data even further to get our data mining algorithm to run in 
reasonable time. Furthermore, naive implementations of algorithms may create large 
internal data structures when they run (e.g., unnecessary copies of data matrices), which 
in turn may cause available memory to be exceeded. Thus, it goes without saying that 
efficient implementation from a memory and time viewpoint is still important, even when 
the data all reside in main memory. 

12.12.2 Scalable Versions of Data Mining Algorithms 
The term scalable is somewhat loosely used in the data mining literature, but we can 
think of it as referring to data mining algorithms that scale gracefully and predictably 
(e.g., linearly) as the number of records n and/or the number of variables p grow. For 
example, naive implementation of a decision tree algorithm will exhibit a dramatic 
slowdown in run-time performance once n becomes large enough that the algorithm 
needs to frequently access data on disk. In practice, research on scalability focuses 
more on the large n problem than on the large  p problem: large p is inherently more 
difficult than large n. 
One line of investigation in scalable data mining algorithms is to develop special-purpose 
scalable implementations of existing well-known algorithms that are guaranteed to return 
the same result as the original (naive) implementation, but that typically will run much 
faster on large data sets. An example of this general approach is that of Gehrke et al. 
(1999), who propose a family of algorithms called BOAT (Bootstrapped Optimistic 
Algorithm for Tree Construction). The BOAT approach uses two scans through the entire 
data set. In the first scan an ""optimistic tree"" is constructed using a small random sample 
from the full data (and that can fit in main memory). The second scan then takes care of 
any differences between the initial tree and the tree that would have been built using all 
of the data. The resulting tree is then the same tree that the naive algorithm would have 
constructed (in a potentially inefficient manner). The method involves various clever data 
structures to keep track of tree-node statistics. Gehrke et al. (1999) report fitting 
classification trees to nine-dimensional synthetically generated data sets with 10 million 
data vectors in about 200 seconds. 
A related strategy is to derive new approximate algorithms that inherently have desirable 
scaling performance by virtue of relying on various heuristics based on a relatively small 
number of linear scans of the data. These algorithms typically return ""good"" solutions but 
are not necessarily in agreement with the original ""nonscalable"" version of the algorithm. 
For example, scalable clustering algorithms of this nature are described by Bradley, 
Fayyad, and Reina (1998) and  Zhang, Ramakrishnan, and Livny (1997). 

12.12.3 Special-Purpose Algorithms for Disk Access 
Yet another approach to the problem of dealing with data on disk has been the 
development of new algorithms that are closely coupled with relational databases and 
transaction data. The best example in this context is that of association rule algorithms, 
which we have mentioned in chapter 5 and will discuss in more detail in chapter 13. The 
search component of association rule algorithms takes advantage of the typical sparsity 
of transaction data sets (i.e., most customers purchase relatively few items per 
transaction). At a high level, the algorithms typically involve breadth-first search 
strategies, where each level of the tree involves a single scan of the data that can be 
executed relatively easily. Agrawal et al. (1996) report results on synthetic data involving 
1,000 items and up to 10 million transactions. They empirically demonstrate that the run-
time of their algorithm scales up linearly on these data sets as a function of the number 
of transactions. Similar results have since been reported on a wide range of sparse 
transaction data sets and many variations of the basic algorithm have been developed 
(see chapter 13). 

12.12.4 Pseudo Data Sets and Sufficient Statistics 
Figure 12.5 illustrates another general idea that can be thought of as a generalization of 
random sampling. An approximate (and typically much smaller) data set is created that 
can then be accessed (e.g., in main memory) by the data mining algorithm instead of 
dealing with the full data (on disk). This general approach can, of course, only 
approximate the results we would have obtained had the algorithm been run on the full 
data. However, if the approximate data set is constructed in a clever enough manner, we 
can often get almost the same results on only a fraction of the data. It is often the case in 
practice that as part of the overall data mining process we will run our data mining 
algorithm many times, with different models, different variables, and so forth, in an 
exploratory manner, before finally settling on a final model. The use of an approximate 
data set for such exploratory modeling can be particularly useful (rather than having to 
deal with the full data set). 

 

Figure 12.5:  The Concept of Data Mining Algorithms Which Operate on an Approximate 
Version of the Full Data Set.  

In this general context Du Mouchel et al. (1999) propose a statistically motivated 
methodology for ""data-squashing"" which amounts to creating a set of n' weighted 
""pseudo"" data points, where n' is much smaller than the original number n, and where 
the pseudo data points are automatically chosen by the algorithm to mimic the statistical 
structure of the original larger data set. The general idea is to approximate the structure 
of the likelihood function as closely as possible, even without the functional form of the 
model being used in the data mining algorithm being specified. The method was 
empirically demonstrated to provide significant reduction in prediction error on a logistic 
regression problem compared to simple random sampling of a data set (Du Mouchel et 
al. (1999)). 
On a related theme, for some data sets it may be sufficient simply to store the original 
data via a more efficient data structure than as a flat file or multiple tables in a relational 
database. The AD-Tree data structure proposed by Moore and Lee (1998) provides an 
efficient mechanism for storing multivariate categorical data (i.e., counts). Data mining 
algorithms can then quickly access counts and related statistics from the AD-Tree much 
more quickly than if the algorithm had to access the original data. Computational speed-
ups of 50 to 5,000-fold on various classification algorithms (compared to naive 
implementation of the algorithms) have been reported (Moore (1999)). 

 
 

 
 

In conclusion, we see that many different techniques can be used to implement data 
mining algorithms that are efficient in both time and space when we deal with very large 
data sets. Indeed there are several other approaches we have not even mentioned here, 
including the use of online algorithms that see each data point only once (useful for 
applications where data are arriving rapidly in a continuous stream over time) and more 
hardware-oriented solutions such as parallel processing implementations of algorithms 
(in cases when both the algorithm and the data permit efficient parallel approaches). 
Choice of a particular technique often depends on quite practical aspects of the data 
mining application—i.e., how quickly must the data mining algorithm produce an answer? 
Does the model need to be continually updated? and so forth. Research on scalable 
data mining algorithms is likely to continue for some time, and we can expect more 
developments in this area. The reader should be cautioned to be aware that, as in 
everything else, there is no free lunch! In other words, there are typically trade-offs 
involving model accuracy, algorithm speed and memory, and so forth. Informed 
judgment on which type of algorithm and data structures best suit your problem will 
require careful consideration of both algorithmic issues and application details about how 
the algorithm and model will be used in practice. 

12.13 Further Reading 

There are several high-quality yearly database conferences, such as ACM's SIGMOD 
Conference on Management of Data (SIGMOD), and the SIGACT-SIGMOD-SIGART 
Symposium on Principles of Database  and Knowledge-base Systems (PODS), the Very 
Large Database Conference (VLDB), and the International Conference on Data 
Engineering (ICDE). 
There are several fine database textbooks, including  Ullman (1988), Abiteboul, Hull, and 
Vianu (1995), and Ramakrishnan and Gehrke (1999). A recent survey of query 
optimization is Chaudhuri (1998). The data cube is presented in Gray et al. (1996) and 
Gray et al. (1997). A good introduction to OLAP is Chaudhuri and Dayal (1997). 
Implementation of database management systems is described in detail in  Garcia-Molina 
et al. (1999). A nice discussion of OLAP and statistical databases is given by Shoshani 
(1997). Issues in using database management systems to implement mining algorithms 
are considered in Sarawagi et al. (2000) and Holsheimer et al. (1995). 
Madigan et al. (in press) discuss various extensions of the the original squashing 
approach. Provost and Kolluri (1999) provide an overview of different techniques for 
scaling up data mining algorithms to handle very large data sets. Provost, Jensen, and 
Oates (1999), and Domingos and Hulten (2000) give examples of sampling problems 
with very large databases in data mining. 

Chapter 13: Finding Patterns and Rules 
13.1 Introduction 

In this chapter we consider the problem of finding useful patterns and rules from large 
data sets. Recall that a pattern is a local concept, telling us something about a particular 
aspect of the data, while a model can be thought of as giving a full description of the 
data. 
For a data set describing customers of a supermarket, a pattern might be ""10 percent of 
the customers buy wine and cheese,"" and for a data set of telecommunication alarms a 
pattern could be ""if alarms A and B occur within 30 seconds of each other, then alarm C 
occurs within 60 seconds with probability 0.5."" For the Web log data set in chapter 1, an 
example pattern could be ""if a person visits the CNN Web site, there is a 60% chance 
the person will visit the ABC News Web site in the same month."" In each of these cases, 
the pattern is a potentially interesting piece of information about part of the data. 

How do we find such patterns from data? Given some way of representing patterns and 
the set of all possible patterns in this representation, the trivial method is to try each 

pattern in turn and see whether it occurs in data and/or whether it is significant in some 
sense. If the number of possible patterns is small, then this method might be applicable, 
but typically it is completely infeasible. For example, in the supermarket example we 
could define a pattern for each possible subset of the set of all products. For 1,000 
products this yields 21000 patterns. In the case of images or sequences of alarms, there is 
a potentially infinite number of patterns. 

If the patterns were completely unrelated to each other, we would have no other choice 
but to use the trivial method. However, the set of patterns typically has a great deal of 
structure. We have to use this structure of the patterns to guide the search. Typically, 
there is a generalization/specialization relation between patterns. A pattern a is more 
general than pattern ß, if whenever ß occurs in the data, a occurs too. For example, the 
pattern ""At least 10 percent of the customers buy wine"" is more general than the pattern 
""At least 5 percent of the customers buy wine and cheese."" Use of such generalization 
relationships between patterns leads to simple algorithms for finding all patterns of a 
certain type that occur in the data. 

In this chapter we present a number of methods for finding local patterns from large 
classes of data. We start from very simple pattern classes and relatively straightforward 
algorithms, and then discuss some generalizations. The basic theme in the chapter is the 
discovery of interesting patterns through the refinement of more general ones. 

Scalability of pattern and rule discovery algorithms is obviously an important issue. The 
algorithms that we describe in this chapter typically carry out only a limited number of 
passes through the data, and hence they scale rather nicely for large data sets. In 
addition, if we are interested in finding only patterns or rules that apply to relatively large 
fractions of the data set, we can effectively use sampling. The frequency of a pattern in 
the sample will be approximately the same as in the whole data set, so pattern discovery 
from the sample produces reasonably good results. If our interest is in patterns that 
occur only rarely in the data, for example, finding very rare and unusual stars or galaxies 
among tens of millions of objects in the night sky, then sampling will be insufficient. 

13.2 Rule Representations 
A rule consists of a left -hand side proposition (the antecedent or condition) and a right-
hand side (the consequent), e.g., ""If it rains then the ground will be wet."" Both the left 
and right-hand sides consist of Boolean (true or false) statements (or propositions) about 
the world. The rule states that if the left-hand side is true, then the right-hand side is also 
true. A probabilistic rule modifies this defi nition so that the right-hand side is true with 
probability p, given that the left-hand side is true—the probability p is simply the 
conditional probability of the right-hand side being true given the left-hand side is true. 
Rules have a long history as a knowledge representation paradigm in cognitive modeling 
and artificial intelligence. Rules can also be relatively easy for humans to interpret (at 
least relatively small sets of rules are) and, as such, have been found to be a useful 
paradigm for learning interpretable knowledge from data in machine learning research. In 
fact, classification tree learning (discussed in chapters 6 and 10) can be thought of as a 
special case of learning a set of rules: the conditions at the nodes along the path to each 
leaf can be considered a conjunction of statements that make up the left-hand side of a 
rule, and the class label assignment at the leaf node provides the right-hand side of the 
rule. 
Note that rules are inherently discrete in nature; that is, the left-and right -hand sides are 
Boolean statements. Thus, rules are particularly well matched to modeling discrete and 
categorical-valued variables, since it is straightforward to make statements about such 
variables in Boolean terms. We can, of course, extend the framework to real-valued 
variables by quantizing such variables into discrete-valued quanta, e.g., ""if X > 10.2 then 
Y < 1"" (this is precisely how classification trees handle real-valued variables, for 
example). 
Typically the left-hand sides of rules are expressed as simple Boolean functions (e.g., 
conjunctions) of variable-value statements about individual variables (e.g., A = a1 or Y > 

 
 

 
 

0). The simplicity of conjunctions (compared to arbitrary Boolean functions) makes 
conjunctive rules by far the most widely used rule representation in data mining. For real-
valued variables, a left-hand side such as X > 1 ?  Y > 2 is defining a left-hand side 
region whose boundaries are parallel to the axes of the variables in (X, Y ) space, i.e., a 
multidimensional ""box"" or hyperrectangle. Again, we could generalize to have statements 
about arbitrary functions of variables (leading to more complex left-hand side regions), 
but we would lose the interpretability of the simpler form. Thus, for handling real-valued 
variables in rule learning, simple univariate thresholds are popular in practice because of 
their simplicity and interpretability. 

13.3 Frequent Itemsets and Association Rules 

13.3.1 Introduction 
Association rules (briefly discussed in chapters 5 and 12) provide a very simple but 
useful form of rule patterns for data mining. Consider again an artificial example of 0/1 
data (an ""indicator matrix"") shown in  figure 13.1. The rows represent the transactions of 
individual customers (in, for example, a ""market basket"" of items that were purchased 
together), and the columns represent the items in the store. A 1 in location (i, j) indicates 
that customer i purchased item j, and a 0 indicates that that item was not purchased. 

[htb] 

basket-
id 

t1  

t2  
t3  

t4  

t5  

t6  
t7  

t8  

t9  

t10  

A  

B  

C  

D  

E  

1 

1 

1 

0 

0 

1 

1 

0 

1 

0 

0 

1 

0 

0 

1 

1 

0 

1 

0 

1 

0 

1 

1 

1 

1 

1 

1 

1 

0 

1 

0 

1 

0 

0 

1 

0 

1 

0 

1 

0 

0 

0 

1 

0 

0 

0 

0 

1 

0 

1 

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

  

 
Figure 13.1: An Artificial Example of Basket Data.  

We are interested in finding useful rules from such data. Given a set of 0,1 valued 
observations over variables A1, ..., Ap, an association rule has the form 

where 1 = ij = p for all j. Such an association rule can be written more briefly as 

 

. A pattern such as 

 

is called an itemset. Thus, association rules can be viewed as rules of the form ?  ?  ? , 
where ? is an itemset pattern and ?  is an itemset pattern consisting of a single conjunct. 
We could also allow conjunctions on the right -hand side of rules, but for simplicity we do 
not. 

The framework of association rules was originally developed for large sparse transaction 
data sets. The concept can be directly generalized to non-binary variables taking a finite 
number of values, although we will not do so here (for simplicity of notation). 
Given an itemset pattern ?, its frequency fr(?) is the number of cases in the data that 
satisfy ?. Note that the frequency ƒr(? ?  ? ) is sometimes referred to as the support. 
Given an association rule ? ?  ? , its accuracy c(? ?  ? ) (also sometimes referred to as 
the confidence) is the fraction of rows that satisfy ?  among those rows that satisfy ?, i.e., 

(13.1)  

In terms of conditional probability notation, the empirical accuracy of an association rule 
can be viewed as a maximum likelihood (frequency-based) estimate of the conditional 
probability that ?  is true, given that ? is true. We note in passing that instead of a simple 
frequency-based estimate, we could use a maximum a posteriori estimate (chapter 4) to 
get a more robust estimate of this conditional probability for small sample sizes. 
However, since association rules are typically used in applications with very large data 
sets and with a large threshold on the size of the itemsets (that is, ƒr(?) is usually fairly 
large), the simple maximum likelihood estimate above will be quite sufficient in such 
cases. 

The frequent itemsets are very simple patterns telling us that variables in the set occur 
reasonably often together. Knowing a single frequent itemset does not provide us with a 
great deal of information about the data: it only gives a narrow viewpoint on a certain 
aspect of the data. Similarly, a single association rule tells us only about a single 
conditional probability, and does not inform us about the rest of the joint probability 
distribution governing the variables. 
The task of finding frequent itemset patterns (or, frequent sets) is simple: given a 
frequency threshold s, find all itemset patterns that are frequent, and their frequencies. In 
the example of figure 13.1, the frequent sets for frequency threshold 0.4 are {A}, {B}, {C}, 
{D}, {AC}, and {BC}. From these we could find, for example, the rule A ?  C, which has 
accuracy 4/6 = 2/3, and the rule B ?  C, with accuracy 5/5 = 1. 

Algorithms for finding association rules find all rules satisfying the frequency and 
accuracy thresholds. If the frequency threshold is low, there might be many frequent sets 
and hence also many rules. Thus, finding association rules is just the beginning in a data 
mining effort: some of these rules will probably be trivial to the user, while others may be 
quite interesting. One of the research challenges in using association rules for data 
mining is to develop methods for selecting potentially interesting rules from among the 
mass of discovered rules. 
The rule frequency tells us how often a rule is applicable. In many cases, rules with low 
frequency are not interesting, and this assumption is indeed built into the definition of the 
association rule-finding problem. The accuracy of an association rule is not necessarily a 
very good indication of its interestingness. For example, consider a medical application 
where the rule is learned that pregnancy implies that the patient is female with accuracy 
1! A rule with accuracy close to 1 could be interesting, but the same is true for a rule with 
accuracy close to 0. We will return later to this question of measuring how interesting a 
rule is to a user. (We discussed issues of data quality in chapter 2. With a large data set 
we might well find that pregnancy implies that the patient is female with accuracy less 
than 1. This does not mean that there are pregnant men running around, but merely that 
data are not perfect.) 
The statistical significance of an association rule A ?  B can be evaluated using standard 
statistical significance testing techniques to determine whether the estimated probability 
p(B = 1|A = 1) differs from the estimated background probability of B = 1, and whether 
this difference would be likely to occur by chance. This is equivalent to testing whether 
p(B = 1|A = 1) differs from p(B = 1|A = 0) (e.g., see example 4.14). 
Although such testing is possible, the use of significance testing methods to evaluate the 
quality of association rules is problematic, due to the multiple testing problem discussed 
in chapter 4. If we extract many rules from the data and conduct significance tests on 

each, then it is very likely, by chance alone, that we will find a rule that appears to be 
statistically significant, even if the data were purely random. 

A set of association rules does not provide a single coherent model that would enable us 
to make inference in a systematic manner. For example, the rules do not provide a direct 
way of predicting what an unknown entry will be. Various rules might predict various 
values for a variable, and there is no central structure (as in decision trees) for deciding 
which rule is in force. 
To illustrate, suppose we now obtain a further row for figure 13.1 with A = 1, B = 1, D = 
1, and E = 1; then the set of rules obtained from that data could be used to suggest that 
(a) C = 1 with accuracy 2/3 (because of the rule A ?  C) or (b) C = 1 with accuracy 1 
(because of the rule B ?  C). Thus the set of rules does not form a global and consistent 
description of the data set. (However, the collection of association rules or frequent sets 
can be viewed as providing a useful condensed representation of the original data set, in 
the sense that a lot of the marginal information about the data can be retrieved from this 
collection.) 
Formulated in terms of the discussion of chapter 6, the model structure for association 
rules is the set of all possible conjunctive probabilistic rules. The score function can be 
thought of as binary: rules with sufficient accuracy and frequency get a score of 1 and all 
other rules have a score of 0 (only rules with a score of 1 are sought). In the next 
subsection we discuss search methods for finding all frequent sets and association rules, 
given pre-defined thresholds on frequency and accuracy. 

13.3.2 Finding Frequent Sets and Association Rules 

In this section we describe methods for finding association rules from large 0/1 matrices. 
For market basket and text document applications, a typical input data set might have 
105 to 108 data rows, and 102 to 106 variables. These matrices are often quite sparse, 
since the number of 1s in any given row is typically very small, e.g., with 0.1% or less 
chance of finding a 1 in any given entry in the matrix. 

The task in association rule discovery is to find all rules fulfilling given pre-specified 
frequency and accuracy criteria. This task might seem a little daunting, as there is an 
exponential number of potential frequent sets in the number of variables of the data, and 
that number tends to be quite large in, say, market basket applications. Fortunately, in 
real data sets it is the typical case that there will be relatively few frequent sets (for 
example, most customers will buy only a small subset of the overall universe of 
products). 

If the data set is large enough, it will not fit into main memory. Thus we aim at methods 
that read the data as few times as possible. Algorithms to find association rules from 
data typically divide the problem into two parts: first find the frequent itemsets and then 
form the rules from the frequent sets. 
If the frequent sets are known, then finding association rules is simple. If a rule X ?  B 
has frequency at least s, then the set X must by definition have frequency at least s. 
Thus, if all frequent sets are known, we can generate all rules of the form X ?  B, where 
X is frequent, and evaluate the accuracy of each of the rules in a single pass through the 
data. 
A trivial method for finding frequent sets would be to compute the frequency of all 
subsets, but obviously that is too slow. The key observation is that a set X of variables 
can be frequent only if all the subsets of X are frequent. This means that we do not have 
to find the frequency of any set X that has a non-frequent proper subset. Therefore, we 
can find all frequent sets by first finding all frequent sets consisting of 1 variable. 
Assuming these are known, we build candidate sets of size 2: sets {A, B} such that {A} is 
frequent and {B} is frequent. After building the candidate sets of size 2, we find by 
looking at the data which of them are really frequent. This gives the frequent sets of size 
2. From these, we can build candidate sets of size 3, whose frequency is then computed 
from the data, and so on. As an algorithm, the method is as follows. 
    i = 0; 

    Ci = {{A} | A is a variable }; 
    while Ci is not empty do 
        database pass: 
            for each set in Ci, test whether it is frequent; 
            let Li be the collection of frequent sets from Ci; 
        candidate formation: 
            let Ci+1 be those sets of size i + 1 
                  whose all subsets are frequent; 
    End. 
This method is known as the APriori algorithm. Two issues remain to be solved: how are 
the candidates formed? and how is the frequency of each candidate computed? The first 
problem is easy to solve in a satisfactory manner. Suppose we have a collection Li of 
frequent sets, and we want to find all sets Y of size i + 1 that possibly can be frequent; 
that is, all sets Y whose all proper subsets are frequent. This can be done by finding all 
pairs {U, V} of sets from Li such that the union of  U and V has size i + 1, and then testing 
whether the union really is a potential candidate. There are fewer than |Li|2 pairs of sets 
in Li, and for each one of them we have to check whether |Li| other sets are present. The 
worst-case complexity is approximately cubic in the size of Li. In practice the method 
usually runs in linear time with respect to the size of Li, since there are often only a few 
overlapping elements in Li. Note that candidate formation is independent of the number 
of records n in the actual data. 
Given a set Ci of candidates, their frequencies can be evaluated in a single pass through 
the database. Simply keep a counter for each candidate and increment the counter for 
each row that contains the candidate. The time needed for candidate Ci is O(|Ci|np), if 
the test is implemented in a trivial way. Additional data structure techniques can be used 
to speed up the method. 
The total time needed for the finding of the frequent sets is O(? i|Ci|np)—that is, 
proportional to the product of the size of the data (np) and the number of sets that are 
candidates on any level. The algorithm needs k or k + 1 passes through the data, where 
k is the number of elements in the largest frequent set.  

There exist many variants of the basic association rule algorithm. The methods typically 
strive toward one or more of the following three goals: minimizing the number of passes 
through the data, minimizing the number of candidates that have to be inspected, and 
minimizing the time needed for computing the frequency of individual candidates. 
One important way of speeding up the computation of frequencies of candidates is to 
use data structures that make it easy to find out which candidate sets in Ci occur for 
each row in the data set. One possible way to organize the candidates is to use a tree-
like structure with branching factor  p (the number of variables). For each variable A, the 
child of the root of the tree labeled with A contains those candidates whose fi rst variable 
(according to some ordering of the variables) is A. The child labeled A is constructed in a 
recursive manner. 
Another important way of speeding up the computation of frequent sets is to use 
sampling. Since we are interested in finding patterns describing large subgroups, that is 
patterns having frequency higher than a given threshold, it is clear that just using a 
sample instead of the whole data set will give a fairly good approximation for the 
collection of frequent sets and their frequencies. A sample can also be used to obtain a 
method that with high probability needs only two passes through the data. First, compute 
from the sample the collection of frequent sets F using a threshold that is slightly lower 
than the one given by the user. Then compute the frequencies in the whole data set of 
each set in F. This produces the exact answer to the problem of finding the frequent sets 
in the whole data set, unless there is a set Y of variables that was not frequent in the 
sample but all of whose subsets turned out to be frequent in the whole data set; in this 
case, we have to make an extra pass through the database. 

 
 

13.4 Generalizations 

The method for finding frequently occurring sets of variables can also be applied to other 
types of patterns and data, since the algorithms described above do not use any special 
properties of the frequent set patterns. What we used were (1) the conjunctive structure 
of the frequent sets and the monotonicity property, so that candidate patterns could be 
formed quickly, and (2) the ability to test quickly whether a pattern occurs in a row, so 
that the frequency of the pattern can be computed by a fast pass through the data. 
Next we formulate the same algorithms in terms of more abstract notions. Suppose that 
we have a class of atomic patterns A, and our interest is in finding conjunctions of these 
atomic patterns that occur frequently. That is, the pattern class P is the set of all 
conjunctions 

a1 ?  ... ?  ak, 

 A for all i. 

where ai ˛
Let the data set D consist of n objects d1, ..., dn, and assume we can test whether a 
pattern a is true about an object d. A conjunction ? = a1? ...? ak ˛
conjuncts ai are true about d. Let s be a threshold. The goal is to find those conjunctions 
of patterns that occur  frequently: 

 P is true about d if all 

{? ˛

 P | ? is true for for at least s objects d ˛

 D}. 

In the case of frequent itemsets, the atomic patterns were conditions of the form A = 1, 
where A is a variable, and the frequent sets like ABC were just shorthand notations for 
the conjunctions of form A = 1 ?  B = 1 ?  C = 1. 
Suppose we can decide how many times each atomic pattern a occurs in the data. Then 
we can apply the above algorithm for finding all the patterns from P that occur frequently 
enough. We simply first find the atomic patterns that occur frequently enough, then build 
the conjunctions of two atomic patterns that can possibly occur frequently, test to see 
which of those occur frequently enough, build conjunctions of size 3, etc. The method 
works in exactly the same way as before. If the patterns are complex, we may have to do 
some clever processing to build the new candidates and to test for occurrence of 
patterns. 

13.5 Finding Episodes From Sequences 
In this section we present another application of the general idea of finding association 
rules: we describe algorithms for finding episodes from sequences. 
Given a set of E of event types, an event sequence s is a sequence of pairs (e, t), where 
e ˛
partial order of event types, such as the ones shown in figure 13.2. Episodes can be 
viewed as graphs. 

 E and t is an integer, the occurrence time of the event of type e. An episode a is a 

 
 

Figure 13.2: Episodes a, ß, and ?.  

 

Given a window width W, the frequency of the episode a in event sequence S is the 
fraction of slices of width W taken from S such that the slice contains events of the types 
occurring in a in the order described by a. We now concentrate on the following 
discovery task: given an event sequence  s, a set e of episodes, a window width  win, and 
a frequency threshold min_fr, find the collection F E(s, win, min_fr) of all episodes from 
the set that occur at least in a fraction of min_fr of all the windows of width  win on the 
sequence s. The display below gives an algorithm for computing the collection of 
frequent episodes. 

The method is based on the same general idea as the association rule algorithms: the 
frequencies of patterns are computed by starting from the simplest possible patterns. 
New candidate patterns are built using the information from previous passes through the 
data, and a pattern is not considered if any of its subpatterns is not frequent enough. The 

main difference compared to the algorithms outlined in the previous sections is that the 
conjunctive structure of episodes is not as obvious. 
An episode ß is defined as a subepisode of an episode a if all the nodes of ß occur also 
in a and if all the relationships between the nodes in ß are also present in a. Using 
graph-theoretic terminology, we can say that ß is an induced subgraph of a. We write ß 
?  a if ß is a subepisode of a, and ß  ?  a if ß ?  a and ß ?  a. 

Example 13.1  

 
Given a set E of event types, an event sequence s over E, a set e of episodes, a window 
width win, and a frequency threshold min_fr, the following algorithm computes the 
collection FE(s, win, min_fr) of frequent episodes. 
     C1 := {a ˛
     l := 1; 
     while Cl ?   f  do 
          /* Database pass: */ 
          compute Fl := {a ˛
          l := l + 1; 
          /* Candidate generation: */ 
          compute Cl := {a ˛
                        and |ß| < l we have ß  ˛
     end; 
     for all l do output Fl; 

 Cl | fr(a, s, win) = min_fr}; 

 e ||a| = l and for all ß ˛

 e such that ß ?  a 

 e ||a| = 1}; 

 F|ß|}; 

 

 

 
 

The algorithm performs a levelwise (breadth-first) search in the class of episodes 
following the subepisode relation. The search starts from the most general episodes—
that is, episodes with only one event. On each level the algorithm first computes a 
collection of candidate episodes, and then checks their frequencies from the event 
sequence. 
The algorithm does at most k + 1 passes through the data, where k is the number of 
edges and vertices in the largest frequent episode. Each pass evaluates the frequency of 
|Cl| episodes. The computation of the frequency of an episode requires that we find the 
windows in the sequence in which the episode occurs. This can be done in time linear in 
the length of the sequence and the size of the episode. The running time of the episode 
discovery algorithm is thus 

, where n is the length of the sequence. 

A similar approach can be used for any conjunctive class of patterns, as long as there 
are not too many frequent patterns. 

13.6 Selective Discovery of Patterns and Rules 

13.6.1 Introduction 
The previous sections discussed methods that are used to find all rules of a certain type 
that fulfill simple frequency and accuracy criteria. While this task is useful in several 
applications, there are also simple and important classes of patterns for which we 
definitely do not want to see all the patterns. Consider, for example, a data set having 
variables with continuous values. Then, as in chapter 1, we can look at patterns such as 

§  0: if X > x1 then Y > y1 with probability p, and the frequency of the rule is q  

Such a rule is a fine partial description of the data. The problem is that if in the data we 
have k different values of X and h different values of Y , there are k h potential rules, and 
many of these will have sufficient frequency to be interesting from that point of view. For 
example, from a data set with variables Age and Income we might discover rules 

§  a: if Age>40 then Income>62755 (probability 0.34) 

§  ß: if Age>41 then Income>62855 (probability 0.33) 

First, the user will not be be happy seeing two rules that express more or less the same 
general pattern. Thus, even if we found both of these rules, we should avoid showing 
them to the user, The second problem is that in this example the pattern a is more 
general than ß, and there are very long sequences a1, a2, ... of patterns such that ai is 
more general than ai+1. Hence the basic algorithmic idea in the previous sections of 
starting from the most general pattern, looking at the data, and expanding the qualified 
patterns in all possible ways does not work, as there are many specializations of any 
single pattern and the pattern space is too large. 

All of this means that the search for patterns has to be pruned in addition to the use of 
frequency criteria. The pruning is typically done using two criteria: 

1. 

interestingness: whether a discovered pattern is sufficiently interesting 
to be output; 

2.  promise: whether a discovered pattern has a potentially interesting 

specialization. 

Note that a pattern can be promising even though it is not interesting. A simple example 
is any pattern that is true of all the data objects: it is not interesting, but some 
specialization of it can be. Interestingness can be quantified in various ways using the 
frequency and accuracy of the pattern as well as background knowledge. 

13.6.2 Heuristic Search for Finding Patterns 

Assume we have a way of defining the interestingness and promise of a pattern, as well 
as a way of pruning. Then a generic heuristic search algorithm for finding interesting 
patterns can be formulated as follows. 
C = { the most general pattern }; 
while C ?   f  do 
     E = all suitable selected specializations of 
         elements of  C; 
     for q ˛
         if q satisfies the interestingness criteria then output q; 
         if q is not promising then discard q else  retain q 
     End; 
     additionally prune E; 
         End;  
     C = E; 
end; 

 E do 

As instantiations of this algorithm, we get several more or less familiar methods: 

1.  Assume patterns are sets of variables, and define the interestingness 

and promise of an itemset X both by the predicate fr(X) > s. Do no 
additional pruning. Then this algorithm is in principle the same as the 
algorithm for finding association rules. 

2.  Suppose the patterns are rules of the form 

a1 ?  ... ?  ak ?  ß, 

where ai and ß are conditions of the form X = c, X < c, or X > c for a variable X 
and a constant c. Let the interestingness criterion be that the rule is 
statistically significant in some sense and let the promise criteria be trivially 
true. The additional pruning step retains only one rule from E, the one with the 
highest statistical significance. This gives us a hill-climbing search for a rule 
with the highest statistical significance. (Of course, significance cannot be 
interpreted in a properly formal sense here because of the large number of 
interrelated tests.) 

3.  Suppose the interestingness criterion is that the rule is statistically 

significant, the promise test is trivially true, and the additional pruning 
retains the K rules whose significance is the highest. Above we had 
the case for K = 1; an arbitrary K gives us beam search. 

13.6.3 Criteria for Interestingness 
In the previous sections we referred to measures of interestingness for rules. Given a 
rule ? ?  ? , its interestingness can be defined in many ways. Typically, background 
knowledge about the variables referred to in the patterns ? and ?  have great influence in 
the interestingness of the rule. For example, in a credit scoring data set we might decide 
beforehand that rules connecting the month of birth and credit score are not interesting. 
Or, in a market basket database, we might say that the interest in a rule is directly 
proportional to the frequency of the rules multiplied by the prices of the items mentioned 
in the product, that is, we would be more interested in rules of high frequency that 
connect expensive items. Generally, there is no single method for automatically taking 
background knowledge into account, and rule discovery systems need to make it easy 
for the user to use such application-dependent criteria for interestingness. 

Purely statistical criteria of interestingness are easier to use in an application-
independent way. Perhaps the simplest criteria can be obtained by constructing a 2 × 2 
contingency table by using the presence or absence of ? and ?  as the variables, and 
having as the counts the frequencies of the four different combinations. 

§   

  

? 

¬? 

?  
fr(? 
?  
? ) 
fr(¬? 
?  
? ) 

¬?  
fr(? 
?  
¬? ) 
fr(¬? 
?  
¬? ) 

From the data in this table we can compute different types of measures of association 
between ? and ? , e.g., the ?2 score. One particularly useful measure of interestingness 
of a rule ?  ?  ?  is the J-measure, defined as 

(13.2)  

Here, p(? |?) is the empirically observed confidence (accuracy) of the rule, and p(? ) and 
p(?) are the empirically observed marginal probabilities of ?  and ? respectively. This 
measure can be viewed as the cross-entropy between the binary variables defined by ?  
with and without conditioning on the event ?. The factor p(?) indicates how widely the 
rule is applicable. The other factor measures how dissimilar our knowledge about ?  is 
from only knowing about the marginal p(? ), compared and with knowing that ? holds, 
i.e., the conditional probability p(? |?). The J-measure has the advantage that it behaves 
well with respect to specializations, that is, it is possible to prove bounds on the value of 
the J-measure for specializations of a given rule. 
In practice it has been found that different score functions for interestingness will often 
return largely the same patterns, as long as the score functions obey some basic 
properties (such as the score monotonically increasing as the frequency of a pattern 
increases, with the accuracy remaining constant). General issues relating to the 
""interestingness"" of patterns were also discussed in chapter 7. 

 
 

13.7 From Local Patterns to Global Models 

Given a collection of patterns occurring in the data, is there a way of forming a global 
model using the patterns? In this section we briefly outline two ways of doing this. The 
first method forms a decision list or rule set for a classification task, and the second 
method constructs an approximation of the probability distribution using the frequencies 
of the patterns. 
Let B be, for simplicity, a binary variable and suppose that we have a discovered a 
collection of rules of the form ?i ?  B = 1 and ?i ?  B = 0. How would we form a decision 
list for finding out or predicting the value of B? (A decision list for variable B is an ordered 
list of rules of the form ?i ?  B = bi, where ?i is a pattern and bi is a possible value of B.) 
The accuracy of such a decision list can be defined as the fraction of rows for which the 
list gives the correct prediction. The optimal decision list could be constructed, in 
principle at least, by considering all possible orderings of the rules and checking for each 
one that produces the best solution. However, this would take exponential time in the 
number of rules. A relatively good approximation can be obtained by viewing the problem 
as a weighted set cover task and using the greedy algorithm. 
That is one way to use local patterns to obtain information about the whole data set. 
Here is another. If we know that pattern ?i has frequency fr(?i) for each i = 1, ..., k, how 
much information do we have about the joint distribution on all variables A1, ..., Ap? In 
principle, any distribution ƒ that satisfies the pattern frequencies could have generated 
the observations fr(?i). However, a reasonable model to adopt would be one that made 
no further assumptions about the general nature of the distribution (since nothing further 
is known). This would be the distribution that maximizes the entropy, subject to the 
pattern frequencies that have been observed. This distribution can be constructed 
reasonably efficiently using the iterative proportional fitting algorithm. Roughly speaking, 
this algorithm operates as follows. Start with a random distribution p(x) on the variables 
Aj, and then enforce the frequency constraint for each pattern ?i. This is done by 
computing the sum of p over the states for which ?i is true, and scaling these 
probabilities so that the resulting updated version of p has ?i true in a set of measure 
fr(?i). The update step is carried out in turn for each pattern, until the observed 
frequencies of patterns agree with those provided by p. The method converges under 
fairly general conditions, and it is widely employed—for example, in statistical text 
modeling. The drawback of the method (at least if it is applied straightforwardly) is that it 
requires the construction of each of the states of the joint distribution, so that both the 
space and time complexity of the method are exponential in the number of variables. 

13.8 Predictive Rule Induction 
In this chapter we have so far focused primarily on association rules and similar rule 
formalisms. We began the chapter with a general definition of a rule, and we now return 
to this framework. Recall that we can interpret each branch of a classification tree as a 
rule, where the internal nodes on the path from the root to the leaf define the terms in the 
conjunctive left-hand side of the rule and the class label assigned to the leaf is the right-
hand side. For classification problems the right -hand side of the rule will be of the form C 
= ck, with a particular value being predicted for the class variable C. 
Thus, we can consider our classification tree as consisting of a set of rules. This set has 
some rather specific properties—namely, it forms a mutually exclusive (disjoint) and 
exhaustive partition of the space of input variables. In this manner, any observation x will 
be classified by one and only one rule (namely the branch defining the region within 
which it lies). The set of rules is said to ""cover"" the input space in this manner. 
We can see that it may be worth considering rule sets which are more general than tree-
structured rule sets. The tree representation can be particularly inefficient (for example) 
at representing disjunctive Boolean functions. For example, consider the disjunctive 
mapping defined by (A = 1 ?  B = 1) V (D = 1 ?  E = 1) ?  C = 1 (and where C = 0 
otherwise). We can represent this quite efficiently via the two rules (A = 1 ?  B = 1) ?  C 
= 1 and (D = 1 ?  E = 1) ?  C = 1. A tree representation of the same mapping would 

 
 

necessarily involve a specific single root-node variable for all branches (e.g., A) even 
though this variable is relevant to only part of the mapping. 
One technique for generating a rule set is to build a classification tree (using any of the 
techniques described in chapter 10) and then to treat each of the branches as individual 
candidate rules. Visiting each such rule in turn, the rule-induction algorithm determines 
whether each condition on the left-hand side of each rule affects the accuracy of that rule 
on the data that it ""covers."" For example, we could assess whether the accuracy of the 
rule (which is equivalent to the estimated conditional probability) improves for the better 
(or indeed shows no significant change at all) when a particular condition is removed 
from the left-hand side. If it improves or shows no change, the condition can be deemed 
not necessary and can be removed to yield a simpler and potentially more accurate rule. 
The process can be repeated until all conditions in all rules are examined. In practice this 
method has often been found to eliminate a large fraction of the initial rule conditions, 
conditions that are introduced in the tree-growing process because of their average 
contribution in terms of model improvement, but that are not necessary for some 
particular branches. 
The final rule set produced in this manner is then used for classification. Since the 
original rules carved up the input space in a disjoint manner, and we have removed a 
subset of the conditions defining these disjoint regions, the boundaries have been 
broadened (the rules have been generalized), and these regions may now overlap. Thus, 
we might have two rules of the form A = 1 ?  C = 1 and B = 1 ?  C = 1. The natural 
question is: how do we use two such rules to classify a new observation vector x where 
both A = 1 and B = 1? One approach would be to view the two rules as constraints on 
the overall joint distribution of p(A, B, C) and to infer an estimate for p(C = 1|A = 1, B = 1) 
using the maximum entropy approach described earlier in this chapter in section 13.7. 
However, since the maximum entropy approach is somewhat complex computationally, 
much simpler techniques tend to be used in practice. For example, we can find all the 
rules that ""fire"" (i.e., for which the conditions are satisfied) given the observation vector x. 
If there is more than one rule, we can simply pick the one with the highest conditional 
probability. If no rules fire, we can simply pick the class value that is most likely a priori. 
Other more complex schemes are also possible, such as arranging the rules in an 
ordered decision list, or voting or averaging among multiple rules. 
We might ask: why start with a classification tree and then produce rules, rather than 
search for the rules directly? One advantage of looking at classification trees is that it 
automatically quantizes any real-valued variables in a relatively simple and 
computationally efficient fashion during the tree-building phase (although, of course, 
these quanta need not necessarily be optimal in any sense in the context of the final rule 
set). Another advantage is the ease of implementing the technique; there are many 
efficient techniques for producing trees (both for data in main memory and in secondary 
memory, as discussed in chapter 10) and, thus, it is relatively straightforward to add the 
rule selection component as a ""postprocessing"" step. 
Nonetheless, producing rules from trees imposes a particular bias on how we search for 
rules, and with this in mind, there has also been a great deal of work in machine learning 
and data mining on algorithms that search for rules directly, particularly for discrete-
valued data. It is worth noting once again of course that the number of possible 
conjunctive rules is immense, O(mp) for  p variables each taking m values. Thus, in 
searching for an optimal set of such rules (or even just the best single rule) we will 
usually resort to using some form of heuristic search through this space (as already 
pointed out in section 13.6 in the context of finding interesting sets of rules). 

Note here that, in the context of classification, optimality should be defined as the set of 
rules that are most accurate on average on new data (or have the minimum average loss 
when classification costs are involved). However, just as with classification trees, 
classification accuracy on the training data need not be the best score function to guide 
in the selection of rules. For example, we could define a specific rule for each training 
example containing all of the variable values present in that example. Such a specific 
rule may have high accuracy (indeed, even accuracy 1 if all examples with the same 
variable values have the same class label), but may generalize poorly since it is so 
specific. Thus, score functions other than simple accuracy are often used in practice, 
particularly for selecting the next rule to add to the existing set, e.g., some trade-off 

between the coverage of the rule (the probability of the left -hand side expression) and 
the rule accuracy, such as the J-measure described earlier. 
Having defined a suitable score function, the next issue is how to search for a set of 
rules to optimize this score on the training data. Many rule induction algorithms utilize a 
form of ""general-to-specific"" heuristic, of the same general form described earlier in 
searching for interesting rules, where now we replace the interestingness score function 
with a classification-related one. These algorithms start with a set containing the most 
general rule possible (that is, the left-hand side is empty) and proceed to add rules to this 
set in a greedy fashion by successively exploring more specific versions of the rules in 
the existing set. This can be viewed as a systematic search through the space of all 
subsets, starting from the null set and using an operator that can add only one condition 
to a rule at a time. A large variety of search techniques are applicable here, including any 
of the systematic heuristic search techniques (such as beam search) discussed in 
chapter 8. The opposite heuristic strategy of starting from the most specific rules and 
generalizing is also possible, although computationally this tends to be a bit more tricky, 
since it is not so obvious what set of rules to start from. For real-valued data we can 
either pre-quantize each real-valued variable into bins (for example by using a clustering 
algorithm on each variable), or quantize as one searches for rules. The latter option can 
be computationally quite demanding and tricky to implement; an interesting algorithm 
which operates in this manner is the PRIM algorithm (Friedman and Fisher, (1999)), that 
gradually ""shrinks"" the rule regions starting from the full range of the data for each 
variable. 
There is of course a trade-off between the more computationally (and memory) intensive 
search techniques which search more of the rule space, and the simpler techniques that 
can search only a smaller fraction of the space. In practice, as with classification trees, 
relatively simple greedy search techniques with simple operators often seem to perform 
almost as well empirically as the more complex methods and are quite popular as a 
result. As with classification trees, there is also the problem of deciding when to stop 
adding rules to the rule set (the familiar problem of deciding how complex the model 
should be—here we can interpret our set of rules as a ""model"" for the data). Once again, 
the technique of cross-validation can be quite useful in estimating the true predictive 
accuracy of a set of rules, but again it can be quite computationally intensive, particularly 
if it is invoked repeatedly at various stages of the rule search. 
We conclude this discussion on predictive rules by mentioning a few no-table extensions 
to the basic classification paradigm. The first extension is that, just as we can extend the 
ideas of classification trees to produce regression trees, so we can also perform rule-
based regression. The left-hand side condition of a rule defines a particular region of the 
input space. Given this region we can then estimate a local regression model on the data 
in this region (it can be as simple as the best-fitting constant for example). If the rules are 
disjoint we get a piecewise local regression surface; if the rules overlap we must again 
decide how to combine the various rule predictions in the overlapping regions. One 
particular advantage of the rule-based regression framework is the ease of 
interpretability, particularly in high-dimensional problems, since only a small fraction of 
the variables are often selected as being relevant for inclusion in the rules. 
The second notable extension to the basic rule induction paradigm is that of using 
relational logic as the basis for the rules. A discussion in any depth of this topic is beyond 
the scope of this text, but essentially the idea is to generalize beyond the notion of 
propositional logic statements (""Variable = value"") to what are known as first-order 
relational logic statements such as ""Parent(X, Y) ?  Male(X) ?  Father (X, Y)."" This type 
of learning is, in principle, extremely powerful, since it allows a much richer 
representational language to describe our data. A propositional version of a relational 
statement is typically quite awkward (and can be exponentially large), since there is no 
notion in the (simpler) propositional framework of objects and relations among objects. 
The extra representational power of relational logic comes at a cost of course, both in 
terms of reasoning with such rules and in terms of learning them from data. Algorithms 
for learning relational rules have been developed under the title ""inductive logic 
programming,"" with some promising results, although largely on logical rather than 
probabilistic representations for data. 

 

 

 
 

13.9 Further Reading 
The association rule problem was introduced in  Agrawal et al. (1993). The Apriori 
algorithm is independently due to Agrawal and Srikant (1994) and Mannila et al. (1994); 
see also Agrawal et al. (1996). There is an extensive literature on various algorithms for 
finding association rules; see, for example, Agrawal, Aggarwal, and Prasad 
(forthcoming), Brin et al. (1998), Fukuda et al. (1996), Han and Fu (1995), Holsheimer et 
al. (1995), Savasere et al. (1995), Srikant and Agrawal (1995, 1996), Toivonen (1996), 
and Webb (2000). Post-processing of association rules is considered, for example, in 
Klemettinen et al. (1994) and Silberschatz and Tuzhilin (1996). The question of 
integrating association rule discovery into database systems is discussed in Imielinski 
and Mannila (1996), Mannila (1997), Meo et al. (1996), Imielinski et al. (1999), Imielinski 
and Virmani (1999), and Sarawagi et al. (1998). Algorithms for finding episodes in 
sequences are described in Mannila et al. (1997). 
It is fair to say that there are far more papers published on algorithms to discover 
association rules than there are papers published on applications of association rules, 
i.e., at this point in time it is not yet clear what the primary applications of association 
rules are beyond exploratory data analysis. Nonetheless one interesting application of 
association rules is in cross-selling applications in a retail context, e.g., Brijs et al. (2000) 
and Lawrence et al. (2001). 
Measures of interestingness for rules are discussed in Smyth and Good-man (1992), 
where the J-measure is introduced, and also in Silberschatz and Tuzhilin (1996). 
A large number of different rule induction algorithms have been proposed in the machine 
learning literature, typically distinguished from each other in terms of the details of how 
the search is performed. The algorithm C4.5Rules (Quinlan (1987, 1993)) is the best-
known method for deriving rules from classification trees. The CN2 algorithm (Clark and 
Niblett (1989)) uses an entropy -based measure to select rules by manner of a beam 
search. Other more recent rule induction algorithms, with demonstrated ability to provide 
accurate classification on large data sets, include RL (Clearwater and Stern (1991)), 
Brute (Segal and Etzioni (1994)) and Ripper (Cohen (1995)) — rule induction designers 
seem to have a preference for rather obscure names! The RISE algorithm (Domingos 
(1996)) is an interesting example of a rule-induction algorithm using a specific-to-general 
search heuristic. Holte (1993) describes an interesting study in which very simple 
classification rule models provide classification accuracies about as good as other more 
complex and widely used classifiers. Aronis and Provost (1997) discuss some practical 
tips on efficient implementation of rule induction algorithms for massive data sets. 
An algorithmic framework for ""bump-hunting"" in high-dimensional data is described in 
Friedman and Fisher (1999) and is unusual in the rule induction literature in the following 
respects: it uses a ""patient"" search strategy rather than the more commonly used purely 
greedy strategy, it is cast in a general function approximation framework allowing both 
real-valued and categorical target variables, and it is motivated from a statistical 
perspective. Rule-based regression is described (for example) in Weiss and In-durkhya 
(1993) and in the commercial package known as Cubist from RuleQuest (2000). 
Quinlan's FOIL algorithm (1990) was one of the first relational rule induction programs. 
More recent work in relational rule learning (also known as inductive logic programming) 
is summarized in texts such as Lavrac and Dzeroski (1994) and Muggleton (1995). 

Chapter 14: Retrieval by Content 
14.1 Introduction 
In a database context, the traditional notion of a query is well defined, as an operation 
that returns a set of records (or entities) that exactly match a set of required 
specifications. An example of such a query in a personnel database would be [level = 
MANAGER] AND [age < 30], which (presumably) would return a list of young 
employees with significant responsibility. Traditional database management systems 
have been designed to provide answers to such precise queries efficiently as discussed 
in chapter 12. 

However, there are many instances, particularly in data analysis, in which we are 
interested in more general, but less precise, queries. Consider a medical context, with a 
patient for whom we have demographic information (such as age, sex, and so forth), 
results from blood tests and other routine physical tests, as well as biomedical time 
series and X-ray images. To assist in the diagnosis of this patient, a physician would like 
to know whether the hospital's database contains any similar patients, and if so, what the 
diagnoses, treatments, and outcomes were for each. The difficult part of this problem is 
determining similarity among patients based on different data types (here, multivariate, 
time series and image data). However, the notion of an exact match is not directly 
relevant here, since it is highly unlikely that any other patient will match this particular 
patient exactly in terms of measurements. 

In this chapter we will discuss problems of this nature, specifically the technical problems 
which must be addressed to allow queries to our data set of the following general form: 
Find the k objects in the database that are most similar to either a specific query or a 
specific object. 

Examples of such queries might be 

§ searching historical records of the Dow Jones index for past occurrences of a 

particular time series pattern, 

§ searching a database of satellite images of the earth for any images which 

contain evidence of recent volcano eruptions in Central America, 
§ searching the Internet for online documents that provide reviews of 

restaurants in Helsinki. 

This form of retrieval can be viewed as interactive data mining in the sense that the user 
is directly involved in exploring a data set by specifying queries and interpreting the 
results of the matching process. This is in contrast to many of the predictive and 
descriptive forms of data mining discussed in earlier chapters, where the role of human 
judgment is often not as prominent. 
If the data sets are annotated by content (for example, if the image database above had 
been manually reviewed and indexed based on visual content), the retrieval problem 
reduces to a standard (but potentially challenging) problem in database indexing, as 
discussed in chapter 12. We are interested here, however, in the more common case in 
practice in which the database is not preindexed. Instead, we have an example of what 
we are trying to find, namely, the query pattern Q. From the query pattern, we must infer 
which other objects in the data set are most similar to it. This approach to retrieval is 
known as retrieval by content. The best-known applications of such an approach are in 
text retrieval. Here the query pattern Q is usually quite short (a list of query words) and is 
matched with large sets of documents. 

We will focus primarily on text document retrieval, since this is the most well-known and 
mature application of these ideas. However we will also discuss the generalization to 
applications in retrieval of image and time series data. The general problem can be 
thought of as having three fundamental components, namely: 

§ how to define a similarity measure between objects, 
§ how to implement a computationally efficient search algorithm (given a 

similarity measure), and 

§ how to incorporate user feedback and interaction in the retrieval process. 

We will focus primarily on the first and third problems. The second problem typically 
reduces to an indexing problem (i.e., find the closest record in a database to a specific 
query) which was discussed in chapter 12.  
Retrieval by content relies heavily on the notion of similarity. Throughout the discussion 
we will use both the terms similarity and distance. From a retrieval point of view it is not 
so important whether we use one or the other, since we can either maximize similarity or 
minimize distance. Thus, we will implicitly assume that, loosely speaking, these two 
terms are inverses of each other, and either can be used in practice. 
We will see that across various applications (text, images, and so forth) it is common to 
reduce the measurements to a standard fixed-length vector format, and to then define 
distance measures using standard geometric notions of distance between vectors. 
Recall that in chapter 2 we discussed several standard distance measures such as 

Euclidean distance, weighted Euclidean distance, Manhattan distance, and so forth. It is 
worth keeping in mind that while these standard distance functions can be extremely 
useful, they are primarily mathematical constructs and, as such, may not necessarily 
match our human intuition about similarity. This will be particularly relevant when we 
discuss similarity in the context of data types such as text and images, where the 
retrieval performance of humans based on semantic content can be difficult to match 
using algorithms based on general domain-independent distance functions. 
In section 14.2 we discuss a subtle issue: how to objectively evaluate the performance of 
a specific retrieval algorithm. The evaluation is significantly complicated by the fact that 
the ultimate judgment of performance comes from the subjective opinion of the user 
issuing the query, who determines whether the retrieved data is relevant. 
For structured data (such as sequences, images, and text), solving the retrieval by 
content problem has an additional aspect, namely, determining the representation used 
for calculation of the similarity measure. For example, it is common to use color, texture, 
and similar features in representing images, and to use word counts in representing text. 
Such abstractions typically involve significant loss of information such as local context. 
Yet they are often essential, due to the difficulty of defining meaningful similarity 
measures at the pixel or ascii character level (for images and text respectively). Section 
14.3 discusses retrieval by content for text data, focusing in particular on the vector-
space representation. Algorithms for matching queries to documents, latent semantic 
indexing, and document classification are all discussed in this context. Section 14.4 
introduces the topics of relevance feedback and automated recommender systems for 
modeling human preferences for one object over another. In section 14.5 we discuss 
representation and retrieval issues in image retrieval algorithms. General image retrieval 
is a difficult problem, and we will look at both the strengths and limitations of current 
approaches, in particular the issue of invariance. Section 14.6 reviews basic concepts in 
time series and sequence matching. As the one-dimensional analog of image retrieval, 
similar representational and invariance issues arise for sequential data as for image 
data. Section 14.7 and section 14.8 contain a summary overview and discussion of 
further reading, respectively. 

14.2 Evaluation of Retrieval Systems 

14.2.1 The Difficulty of Evaluating Retrieval Performance 

In classification and regression the performance of a model can always be judged in an 
objective manner by empirically estimating the accuracy of the model (or more generally 
its loss) on unseen test data. This makes comparisons of different models and 
algorithms straightforward. 

For retrieval by content, however, the problem of evaluating the performance of a 
particular retrieval algorithm or technique is more complex and subtle. The primary 
difficulty is that the ultimate measure of a retrieval system's performance is determined 
by the usefulness of the retrieved information to the user. Thus, retrieval performance in 
a real-world situation is inherently subjective (again, in contrast to classification or 
regression). Retrieval is a human-centered, interactive process, which makes 
performance evaluation difficult. This is an important point to keep in mind. 
Although it may be very difficult to measure how useful a particular retrieval system is to 
an average user directly, there are nonetheless some relatively objective methods we 
can use if we are willing to make some simplifications. First we assume that (for test 
purposes) objects can be labeled as being relevant or not, relative to a particular query. 
In other words, for any query Q, it will be assumed that there exists a set of binary 
classification labels of all objects in the data set indicating which are relevant and which 
are not. In practice, of course, this is a simplification, since relevance is not necessarily a 
binary concept; e.g., particular articles among a set of journal articles can be individually 
more or less relevant to a particular student's research questions. Furthermore, this 
methodology also implicitly assumes that relevance is absolute (not user-centric) in the 
sense that the relevance of any object is the same for any pair of users, relative to a 
given query Q. Finally, it is assumed that somehow the objects have been labeled, 

 
 

presumably by a relatively objective and consistent human judge. In practice, with large 
data sets, getting such relevance judgments can be a formidable task.  
Note in passing that one could treat the retrieval problem as a form of a classification 
task, where the class label is dependent on the query Q , i.e., ""relevant or not to the 
query Q"" and where the objects in the database are having their class labels estimated 
relative to Q. However, the retrieval problem has some distinguishing characteristics 
which make worthwhile to treat independently from classification. Firstly, the definition of 
the class variable is in the hands of the user (since the user defines the query Q ) and 
can change every time the system is used. Secondly, the primary goal is not so much to 
classify all the objects in the database, but instead to return the set of most relevant 
objects to the user. 

14.2.2 Precision versus Recall 
Despite the caveats mentioned above, the general technique of labeling the objects in a 
large data set as being relevant or not (relative to a given set of predefined queries) is 
nonetheless quite useful in terms of providing a framework for objective empirical 
performance evaluation of various retrieval algorithms. We will discuss the issue of 
labeling in more detail in section 14.2.3. One practical approach is to use a committee of 
human experts to classify objects as relevant or nonrelevant. 
Assume that we are evaluating the performance of a specific retrieval algorithm in 
response to a specific query Q on an independent test data set. The objects in the test 
data have already been preclassified as either relevant or irrelevant to the query Q. It is 
assumed that this test data set has not been used to tune the performance of this 
retrieval algorithm (otherwise the algorithm could simply memorize the mapping from the 
given query Q to the class labels). We can think of the retrieval algorithm as simply 
classifying the objects in the data set (in terms of relevance relative to Q), where the true 
class labels are hidden from the algorithm but are known for test purposes. 
If the algorithm is using a distance measure (between each object in the data set and Q) 
to rank  the set of objects, then the algorithm is typically parametrized by a threshold T. 
Thus, KT objects will be returned by the algorithm, the ordered list of KT objects that are 
closer than threshold T to the query object Q. Changing this threshold allows us to 
change the performance of the retrieval algorithm. If the threshold is very small, then we 
are being conservative in deciding which of the objects we classify as being relevant. 
However, we may miss some potentially relevant objects this way. A large threshold will 
have the opposite effect: more objects returned, but a potentially greater chance that (in 
truth) they are not relevant.  
Suppose that in a test data set with N objects, a retrieval algorithm returns KT objects of 
potential relevance. The performance of the algorithm can be summarized by table 14.1, 
where N = TP + FP + FN + TN is the total number of labeled objects, TP + FP = KT is the 
number of objects returned by the algorithm, and TP + FN is the total number of relevant 
objects. Precision is defined as the fraction of retrieved objects that are relevant, i.e., 
TP/(TP + FP ). Recall is defined as the proportion of relevant objects that are retrieved 
relative to the total number of relevant objects in the data set, i.e., TP/(TP + FN). There 
is a natural trade-off here. As the number of returned objects KT is increased (i.e., as we 
increase the threshold and allow the algorithm to declare more objects to be relevant) we 
can expect recall to increase (in the limit we can return all objects, in which case recall is 
1), while precision can be expected to decrease (as KT is increased, it will typically be 
more difficult to return only relevant objects). If we run the retrieval algorithm for different 
values of the threshold T, we will obtain a set of pairs of (recall, precision) points. In turn 
these can be plotted providing a recall-precision characterization of this particular 
retrieval algorithm (relative to the query Q, the particular data set, and the labeling of the 
data). In practice, rather than evaluating performance relative to a single query Q, we 
estimate the average recall-precision performance over a set of queries (see figure 14.1 
for an example). Note that the recall-performance curve is essentially equivalent (except 
for a relabeling of the axes) to the well-known receiver-operating characteristic (ROC) 
used to characterize the performance of binary classifiers with variable thresholds. 

 

Figure 14.1: A Simple (Synthetic) Example of Precision-Recall Curves for Three Hypothetical 
Query Algorithms. Algorithm A Has the Highest Precision for Low Recall Values, While 
Algorithm B Has the Highest Precision for High Recall Values. Algorithm C is Universally 
Worse Than A or B, But We Cannot Make a Clear Distinction between A or B Unless (For 
Example) We were to Operate at a Specific Recall Value. The Actual Recall-Precision 
Numbers Shown are Fairly Typical of Current Text-Retrieval Algorithms; e.g., the Ballpark 
Figures of 50% Precision at 50% Recall are Not Uncommon Across Different Text-Retrieval 
Applications.  

Table 14.1: A Schematic of the Four Possible Outcomes in a Retrieval Experiment 
Where Documents are Labeled as Being ""Relevant"" or ""Not Relevant"" (Relative to 
a Particular Query Q). The Columns Correspond to Truth and Rows Correspond 
to the Algorithm's Decisions on the Documents. TP, FP, FN, TN Refer to True 
Positive, False Positive, False Negative, And True Negative Respectively, Where 
Positive/Negative Refers to the Relevant/Nonrelevant Classification Provided by 
the Algorithm. A Perfect Retrieval Algorithm Would Produce a Diagonal Matrix 
with FP = FN = 0. This Form of Reporting Classification Results is Sometimes 
Referred to as a Confusion Matrix.  

  

Algorithm: Relevant 

Algorithm: Not Relevant 

Truth: 
Relev
ant 

TP  

FN  

Truth: 
Not-
Relev
ant 
FP  

TN  

Now consider what happens if we plot the recall-precision of a set of different retrieval 
algorithms relative to the same data set and set of queries. Very often, no one curve will 
dominate the others; i.e., for different recall values, different algorithms may be best in 
terms of precision (see figure 14.1 for a simple example). Thus, precision-recall curves 
do not necessarily allow one to state that one algorithm is in some sense better than 
another. Nonetheless they can provide a useful characterization of the relative and 
absolute performance of retrieval algorithms over a range of operating conditions. There 
are a number of schemes we can use to summarize precision-recall performance with a 
single number, e.g., precision at some fixed number of documents retrieved, precision at 
the point where recall and precision are equal, or average precision over multiple recall 
levels. 

14.2.3 Precision and Recall in Practice 
Precision-recall evaluations have been particularly popular in text retrieval research, 
although in principle the methodology is applicable to retrieval of any data type. The Text 
Retrieval Conferences (TREC) are an example of a large-scale precision-recall 
evaluation experiment, held roughly annually by the U.S. National Institute of Standards 
and Technology (NIST). A number of gigabyte-sized text data sets are used, consisting 
of roughly 1 million separate documents (objects) indexed by about 500 terms on 

average. A significant practical problem in this context is the evaluation of relevance, in 
particular determining the total number of relevant documents (for a given query Q) for 
the calculation of recall. With 50 different queries being used, this would require each 
human judge to supply on the order of 50 million class labels! Because of the large 
number of participants in the TREC conference (typically 30 or more), the TREC judges 
restrict their judgments to the set consisting of the union of the top 100 documents 
returned by each participant, the assumption being that this set typically contains almost 
all of the relevant documents in the collection. Thus, only a few thousand relevance 
judgments need to be made rather than tens of millions. 

More generally, determining recall can be a significant practical problem. For example, in 
the retrieval of documents on the Internet it can be extremely difficult to accurately 
estimate the total number of potentially available relevant documents. Sampling 
techniques can in principle be used, but, combined with the fact that subjective human 
judgment is involved in determining relevance in the first place, precision-recall 
experiments on a large-scale can be extremely nontrivial to carry out. 

14.3 Text Retrieval 
Retrieval of text-based information has traditionally been termed information retrieval (IR) 
and has recently become a topic of great interest with the advent of text search engines 
on the Internet. Text is considered to be composed of two fundamental units, namely the 
document and the term. A document can be a traditional document such as a book or 
journal paper, but moregenerally is used as a name for any structured segment of text 
such as chapters, sections, paragraphs, or even e-mail messages, Web pages, 
computer source code, and so forth. A term can be a word, word-pair, or phrase within a 
document, e.g., the word data or word-pair data mining. 
Traditionally in IR, text queries are specified as sets of terms. Although documents will 
usually be much longer than queries, it is convenient to think of a single representation 
language that we can use to represent both documents and queries. By representing 
both in a unified manner, we can begin to think of directly computing distances between 
queries and documents, thus providing a framework within which to directly implement 
simple text retrieval algorithms. 

 

14.3.1 Representation of Text 
As we will see again with image retrieval later in this chapter, much research in text 
retrieval focuses on finding general representations for documents that support both 

the capability to retain as much of the semantic content of the data as 
possible, and 
the computation of distance measures between queries and documents 
in an efficient manner. 

§ 

§ 

A user who is using a text retrieval system (such as a search engine on the Web) wants 
to retrieve documents that are relevant to his or her needs in terms of semantic content. 
At a fundamental level, this requires solving a long-standing problem in artificial 
intelligence, namely, natural language processing (NLP), or the ability to program a 
computer to ""understand"" text data in the sense that it can map the ascii letters of the 
text into some well-defined semantic representation. In its general unconstrained form, 
this has been found to be an extremely challenging problem. Polysemy (the same word 
having several different meanings) and synonmy (several different ways to describe the 
same thing) are just two of the factors that make automated understanding of text rather 
difficult. Thus, perhaps not surprisingly, NLP techniques (which try to explicitly model and 
extract the semantic content of a document) are not the mainstay of most practical IR 
systems in use today; i.e., practical retrieval systems do not typically contain an explicit 
model of the meaning of a document. 
Instead, current IR systems typically rely on simple term matching and counting 
techniques, where the content of a document is implicitly and approximately captured (at 
least in theory) by a vector of term occurrence counts. Assume that a set of terms tj for 
retrieval, 1 = j = T, has been defined a priori. The size of this set can be quite large (e.g., 
T = 50,000 terms or more).  
Each individual document Di, 1 = i = N, is then represented as a term vector: 

(14.1)  

where dij represents some information about the occurrence of the jth term in the ith 
document. The individual dijs are referred to as term weights (although, to be more 
precise, they are just the component values of the term vector). 
In the Boolean representation, the term weights simply indicate whether certain terms 
appear in the document, i.e., dij = 1 if document i contains term j, and dij = 0 otherwise. In 
the vector space representation each term weight can be some real-valued number, e.g., 
a function of how often the term appears in the document, or (perhaps) the relative 
frequency of that term in the overall set of documents. We will look in more detail at term 
weights in section 14.3.2. 
Note that when a document is represented as a T-dimensional term vector, not only has 
the word order of the original document been lost, but so also has syntactic information 
such as sentence structure. Despite this loss of information, term vectors can 
nonetheless be quite useful and effective in a variety of retrieval applications. 

As an example, consider a very simple toy example with 10 documents and 6 terms, 
where the terms are 

§ 
§ 
§ 
§ 
§ 
§ 

t1 = database  
t2 = SQL  
t3 = index  
t4 = regression  
t5 = likelihood  
t6 = linear, 

and we have a 10 × 6 document-term frequency matrix M as shown in table 14.2. Entry ij 
(row i, column j) contains the number of times term j is contained in document i. We can 
clearly see that the first five documents d1 to d5 contain mainly database terms 
(combinations of query, SQL, index) while the last five documents d6 to d10 contain 
mainly regression terms (combinations of regression, likelihood, linear). We 
will return to this example later in this chapter. 
Table 14.2: A Toy Document-Term Matrix for 10 Documents and 6 Terms. Each 
ijth Entry Contains the Number of Times that Term j Appears in Document i.  

  

d1 

d2 

d3 

d4 

d5 

d6 

d7 

d8 

d9 

t1 

24 

32 

12 

6 

43 

2 

0 

3 

1 

t2 

21 

10 

16 

7 

31 

0 

0 

0 

0 

t3 

9 

5 

5 

2 

20 

0 

1 

0 

0 

t4 

t5 

t6 

0 

0 

0 

0 

0 

18 

32 

22 

34 

0 

3 

0 

0 

3 

7 

12 

4 

27 

3 

0 

0 

0 

0 

16 

0 

2 

25 

6 

0 

d10 

23 
Given a particular vector-space representation, it is straightforward to define distance 
between documents as some simple well-defined function of distance between their 
document vectors. Most of the various distance measures we described in chapter 2 can 
be (and have been) used for document comparison. One widely used distance measure 
in this context is the cosine distance, which is defined as 

0 

17 

4 

(14.2)  

This is the cosine of the angle between the two vectors (equivalently, their inner product 
after each has been normalized to have unit length) and, thus, reflects similarity in terms 
of the  relative distribution of their term components. There is nothing particularly special 
about this distance measure, however, it has been found historically to be quite effective 
in practical IR experiments. 
Figure 14.2 shows in pixel form the distance matrices for the toy document-term 
frequency matrix of table 14.2. Both the Euclidean distance and cosine distance matrices 
are shown. For both distance matrices it is clear that there are two clusters of 
documents, one for database documents and one for regression, which show up as two 
relatively light subblocks in the figure. Conversely, the pairwise distances between 
elements of the two groups are relatively large (i.e., dark pixels). However, the cosine 
distance produces a better separation of the two groups. For example, documents 3 and 
4 (in the database cluster) would be ranked as being closer to documents 6, 8, and 9 
(which are documents about regression) than to document 5 (which is another document 
about databases). This happens simply because the term vectors for documents 3 and 4 
(and 6, 8, and 9) are close to the origin compared to document 5. By using angle-based 
distances, the cosine distance emphasizes the relative contributions of individual terms, 
resulting in the better separation evident in figure 14.2 (bottom). 

 

Figure 14.2: Pairwise Document Distances for the Toy Document-Term Matrix in the Text, 
For Euclidean Distance (Top) and Cosine Distance (Bottom). Brighter Squares are More 
Similar and Darker Squares are Less Similar, According to the Relevant Distance. For the 
Euclidean Distance, White Corresponds to Zero (e.g., On the Diagonals) and Black is the 
Maximum Distance between any Two Documents. For the Cosine Distance Brighter Pixels 
Correspond to Larger Cosine Values (Closer Angles) and Darker Pixels Correspond to 
Smaller Cosine Values (Larger Angles).  

Each vector  Di can be thought of as a surrogate document for the original document. The 
entire set of vectors can be represented as an  N × T matrix. Typically this matrix is very 
sparse, for example with only about 0.03% of cells being nonzero for the TREC 
collections mentioned earlier. A natural interpretation of this matrix is that each row Di (a 
document) of this matrix is a vector in T-dimensional ""term space."" Thus, thinking in 
terms of the data matrix we have used to described data sets in earlier chapters, the 
documents play the role of individual objects, the terms play the role of variables, and the 
vector entries represent ""measurements"" on the documents. 
In a practical implementation of a text-retrieval system, due to the sparsity of the term-
document matrix, the original set of text documents are represented as an inverted file 
structure (rather than as a matrix directly), i.e., a file indexed by the T terms where each 

term tj points to a list of N numbers describing term occurrence (the dij, j fixed) for each 
document. 

The process of generating a term-document matrix can by itself be quite nontrivial, 
including issues such as how to define terms, e.g., are plural and singular nouns counted 
as the same term? should very common words be used as terms? and so forth. We will 
not dwell on this issue except to point out that clearly there can be a substantial amount 
of ""engineering"" required at this level. 

14.3.2 Matching Queries and Documents 
Queries can also be expressed using the same term-based representation as that used 
for documents. In effect, the query is itself represented as a document, albeit one that 
typically contains very few terms (although we can, of course use a real document as a 
query itself, e.g., ""Find documents that are similar to this one""). 
For the Boolean representation, a query is represented as a logical Boolean function on 
a subset of the available terms. Thus, for example, a typical query might be data AND 
mining AND NOT(coal). The basic mechanism for retrieval in this context consists of 
scanning the inverted file to determine which documents exactly match the query 
specifications. Extensions of the basic Boolean query language are possible, such as 
adding weights to indicate the relative importance of certain terms over others. However, 
a general drawback of the Boolean representation is that there is no natural semantics 
for the notion of distance between queries and documents, and thus, no natural way to 
perform ranking of documents based on relevance. In addition, and perhaps somewhat 
surprisingly, humans often have great difficulty constructing Boolean queries that reflect 
precisely what they intend. Nonetheless, despite these drawbacks, the Boolean query 
method is popular in practical IR systems because of its efficiency and relative simplicity. 
In the vector-space representation, a query can be expressed as a vector of weights. 
Terms not in the query are implicitly assigned zero weight. The simplest form of query 
assigns unit weight to each term in the query. More generally, individual weights can be 
assigned by the user to indicate the relative importance of each term (typically the 
weights are constrained to be between 0 and 1). In practice, it can be difficult for a user 
to know how to set the weights to effectively model his or her notion of relevance. Later 
we will see that a scheme called relevance feedback can be used to iteratively refine the 
weights over the course of multiple queries, but for now we will assume that the user 
provides the query as well as the weights for the terms in query. 
Let the Q = (q1, ..., qT) be a vector of query weights. In the simplest approach the query 
weights can be simply either 1 (the term is in the query) or 0 (the term is not in the 
query), or the same weighting scheme as used for document representation can be used 
(see below). As an example of the simple binary scheme consider three queries, each 
consisting of the single terms database, SQL, regression. For our toy example 
these three queries would be represented as three vectors: (1, 0, 0, 0, 0, 0), (0, 1, 0, 0, 0, 
0), and (0, 0, 0, 1, 0, 0). Using the cosine distance to match these three queries to our 
toy example data set of table 14.2 results in documents d2, d3, and d9 being ranked as 
closest, respectively. 
To discuss more general concepts in matching queries to documents, we must first 
revisit briefly the idea of weights in the vector-space model. Let dik be the weight (or 
component value) for the kth term, 1 = k = T in document Di. There have been many 
different (largely ad hoc) suggestions in the IR literature on how these weights should be 
set to improve retrieval performance. Ideally these terms should be chosen so that more 
relevant documents are ranked higher than less relevant ones. The Boolean approach of 
setting the weights to 1 if the term occurs anywhere in the document has been found to 
favor large documents (over relevant ones) simply because a larger documents is more 
likely to include a given query term somewhere in the document. 
One particular weighting scheme, known as the TF -IDF weighting, has proven very 
useful in practice. TF stands for term frequency and simply means that each term 
component in a term vector is multiplied by the frequency by which that term occurs in 
the document. This has the effect of increasing the weight on terms that occur frequently 
in a given document. The toy document-term matrix of table 14.2 is expressed in TF 
form. 

However, if a term occurs frequently in many documents in the document set, then using 
TF weights for retrieval may have little discriminative power, i.e., it will increase recall but 
may have poor precision. The inverse-document-frequency (IDF) weight helps to 
improve discrimination. It is defined as log(N/nj), i.e., the log of the inverse of the fraction 
of documents in the whole set that contain term j, where nj is the number of documents 
containing term j, and the total number of documents is N. The IDF weight favors terms 
that occur in relatively few documents; i.e., it has discriminative power. The use of the 
logarithm of IDF rather than IDF directly is motivated by the desire to make the weight 
relatively insensitive to the exact number of documents N. 

The TF-IDF weight is simply the product of TF and IDF for a particular term in a 
particular document. As with the cosine distance measure (with which it is often used), 
there is no particularly compelling motivation for defining weights in this manner. 
However, it has been found to be generally superior in terms of precision-recall 
performance when it is compared to alternative weighting schemes. There are various 
enhancements to the basic TF -IDF method, but TF-IDF weighting as described above is 
still usually considered the default baseline method in evaluation experiments. 
The same TF -IDF weights derived from the set of documents can be used to weight 
query terms as well. Another alternative for query-weighting is to use just IDF weights to 
emphasize query terms that are relatively rare. For example, if we were to issue the 
query Richard Nixon we would probably be happier to retrieve documents containing 
Nixon and not Richard than vice versa. 
The document-term matrix of table 14.2 produces the following set of IDF weights (using 
natural logs): (0.105, 0.693, 0.511, 0.693, 0.357, 0.693). Note, for example, that the first 
term database now receives a lower weight than the other terms, because it appears in 
more documents (i.e., is less discriminative). This results in the TF-IDF document-term 
matrix shown in table 14.3. 
Table 14.3: TF-IDF Document-Term Matrix Resulting From Table 14.2  

2.53 

3.37 

1.26 

0.63 

4.53 

0.63 

0.21 

0.31 

0.10 

14.56 

6.93 

11.09 

4.85 

21.48 

0 

0 

0 

0 

4.60 

2.55 

2.55 

1.02 

10.21 

0 

0 

0 

0 

0 

0 

0 

0 

0 

11.78 

22.18 

15.24 

23.56 

0 

1.07 

0 

0 

1.07 

1.42 

4.28 

1.42 

9.63 

2.07 

0 

0 

0 

0 

15.94 

0 

1.38 

17.33 

A classic approach to matching queries to documents is as follows:  

§ 

§ 

represent queries as term vectors with 1s for terms occurring in the query 
and 0s everywhere else, 
represent term-vectors for documents using TF-IDF weights for the 
vector components, and 

§  use the cosine distance measure to rank the documents in terms of 

distance to the query. 

Table 14.4 shows a simple example of a query comparing the TF and TF -IDF methods. 
Note that, unlike the exact match retrieval results of the Boolean case (where all 
matching documents are returned), here the distance measure produces a ranking of all 
documents that include at least one relevant term. 
Table 14.4: Distances Resulting From a Query Containing the Terms  Database 
and Index, i.e., Q = (1, 0, 1, 0, 0, 0), for the Document-Term Matrix of Table 14.2, 
Using the Cosine Distance Measure. Using the TF Matrix, Document d5 is 
Closest; For the TF-IDF Matrix, d2 is Closest.  

Document 

d1 

d2 

d3 

d4 

d5 

d6 

d7 

d8 

d9 

d10 

TF 
dista
nce 

0.70 

0.77 

0.58 

0.60 

0.79 

0.14 

0.06 

0.02 

0.09 

0.01 

TF-
IDF 
dista
nce 

0.32 

0.51 

0.24 

0.23 

0.43 

0.02 

0.01 

0.02 

0.01 

0.00 

14.3.3 Latent Semantic Indexing 
In the schemes we have discussed so far for text retrieval, we have relied exclusively on 
the notion of representing a document as a T-dimensional vector of term weights. A 
criticism of the term-based approach is that users may pose queries using different 
terminology than the terms used to index a document. For example, from a term 
similarity viewpoint the term data mining has nothing directly in common with the term 
knowledge discovery. However, semantically, these two terms have much in 
common and (presumably) if we posed a query with one of these terms, we would 
consider documents containing the other to be relevant. One solution to this problem is 
to use a knowledge base (a thesaurus or ontology) that is created a priori with the 
purpose of linking semantically related terms together. However, such knowledge bases 
are inherently subjective in that they depend on a particular viewpoint of how terms are 
related to semantic content. 
An interesting, and useful, alternative methodology goes by the name of latent semantic 
indexing (LSI). The name suggests that hidden semantic structure is extracted from text 
rather than just term occurrences. What LSI actually does is to approximate the original 
T-dimensional term space by the first k principal component directions in this space, 
using the N × T document -term matrix to estimate the directions. As discussed in chapter 
3, the first k principal component directions provide the best set of k orthogonal basis 
vectors in terms of explaining the most variance in the data matrix. The principal 
components approach will exploit redundancy in the terms, if it exists. Frequently in 
practice there is such redundancy. For example, terms such as database, SQL, 
indexing, query optimization can be expected to exhibit redundancy in the 
sense that many database-related documents may contain all four of these terms 
together. The intuition behind principal components is that a single vector consisting of a 
weighted combination of the original terms may be able to approximate quite closely the 
effect of a much larger set of terms. Thus the original document-term matrix of size N × T 
can be replaced by a matrix of size N × k, where k may be much smaller than T with little 
loss in information. From a text retrieval perspective, for fixed recall, LSI can increase 
precision compared to the vector-space methods discussed earlier. 
An interesting aspect of the the principal component representation for the document-
term matrix is that it captures relationships among terms by creating new terms that may 
more closely reflect the semantic content of the document. For example, if the terms 
database, SQL, indexing, query optimization are effectively combined into 
a single principal component term, we can think of this new term as defining whether the 
content of a document is about database concepts. Thus, for example, if the query is 
posed using the term SQL, but the database-related documents in the set of documents 

contain only the term indexing, that set of database documents will nonetheless be 
returned by the LSI method (but would not be returned by a strictly term-based 
approach). 
We can calculate a singular-value decomposition (SVD) for the document-term matrix M 
in table 14.2. That is, we find a decomposition M = USVT. Here U is a 10×6 matrix where 
each row is a vector of weights for a particular document, S is a 6 × 6 diagonal matrix of 
eigenvalues for each principal component direction, and the columns of the 6 × 6 matrix 
VT provide a new orthogonal basis for the data, often referred to as the principal 
component directions. 
The S matrix for M has diagonal elements 

; i.e., only 

?1, ..., ?6 = {77.4, 69.5, 22.9, 13.5, 12.1, 4.8}. 

In agreement with our intuition, most of the variance in the data is captured by the first 
two principal components. In fact, if we were to retain only these two principal 
components (as two surrogate terms instead of the six original terms), the fraction of 
variance that our two-dimensional representation retains is 
7.5% of the information has been lost (in a mean-square sense). If we represent the 
documents in the new two-dimensional principal component space, the coefficients for 
each document correspond to the first two columns of the U matrix: 
d1    30.8998  -11.4912 
d2    30.3131  -10.7801 
d3    18.0007   -7.7138 
d4     8.3765   -3.5611 
d5    52.7057  -20.6051 
d6    14.2118   21.8263 
d7    10.8052   21.9140 
d8    11.5080   28.0101 
d9     9.5259   17.7666 
d10   19.9219   45.0751 
and we can consider these two columns as representing new  pseudo-terms that are in 
effect linear combinations of the original six terms. 

It is informative to look at the first two principal component directions: 

(14.3)  
(14.4)  

 

These are the two directions (a plane) in the original six-dimensional term space where 
the data is most ""spread out"" (has the most variance). The first direction emphasizes the 
first two terms (query, SQL) more than the others: this is in effect the direction that 
characterizes documents relating to databases. The second direction emphasizes the 
last three terms regression, likelihood, linear, and can be considered the direction that 
characterizes documents relating to regression. Figure 14.3 demonstrates this 
graphically. We can see that the two different groups of documents are distributed in two 
different directions when they are projected onto the plane spanned by the first two 
principal component directions. Note that document 2 is almost on top of document 1, 
somewhat obscuring it. (Symbols D1 and D2 are discussed below.) The distances of the 
points from the origin reflect the magnitude of the term-vector (i.e., number of terms) for 
each document. For example, documents 5 and 10 have the most terms and are furthest 
from the origin. We can see here that the angle difference between documents is clearly 
a very useful indicator of similarity, since regression and database documents are each 
clustered around two different angles in the plane. 

 

Figure 14.3: Projected Locations of the 10 Documents (From Table 14.2) in the Two 
Dimensional Plane Spanned by the First Two Principal Components of the Document-Term 
Matrix M.  

An advantage of principal components in this context can be seen by considering a new 
document D1 which contains, for example, the term database 50 times and another 
document D2, which contains the term SQL 50 times, and both documents contain none 
of the other terms. In a direct keyword representation, these two documents would not 
be considered similar since they contain no common terms (among the specific terms 
used in our toy example). However, if we instead represent D1 and D2 using the two 
principal component terms, they are projected into this space, as shown in  figure 14.3, 
i.e., both are projected along the ""database"" direction even though each is missing two of 
the three terms associated with databases. The principal component approach has 
implicitly modeled the interrelationship between terms. This feature can be taken 
advantage of for querying. Imagine now that we pose a query using only the term SQL. If 
we use the principal component representation (e.g., the first two pseudo-terms) for our 
documents, we can also convert the query into the pseudo-term representation, where 
again the query will be closer (in angle) to the database documents than the regression 
documents, allowing retrieval of documents that do not contain the term SQL at all but 
are related in content. 

From a computational viewpoint, directly computing the principal component vectors (by 
seeking the eigenvectors of the correlation or covariance matrix for example) is usually 
neither computationally feasible or numerically stable. In practice, the PCA vectors can 
be estimated using special-purpose sparse SVD techniques that are well-suited for high-
dimensional sparse matrices. 

There are many other variations of this basic framework outline. The application of 
principal components for text retrieval is often referred to as latent semantic indexing 
(LSI). Use of this general technique has been shown in certain cases to improve retrieval 
performance in systematic tests, based on its ability to match documents and queries 
with no terms in common.  
We can also model the document-term matrix in a probabilistic manner as being 
generated by a mixture of simpler component models, where each component 
represents the distribution of words we would expect to see conditioned on a particular 
topic. Each of the component models can be (for example) conditional independence 
(naive Bayes) models or multinomials, and the EM algorithm for fitting mixtures, as 
described in chapter 9, can be applied directly and rather straightforwardly. 

14.3.4 Document and Text Classification 
It is clear from our discussion that the term-vector representation of documents provides 
a natural framework for classifying documents, where we can apply either the supervised 
classification framework of chapter 10 for documents that have labels preassigned, or 

the unsupervised learning (clustering) frameworks of chapter 9 for unlabeled documents. 
For example, a practical application of these ideas is that of automatically and accurately 
clustering Web documents into groups or taxonomies, for updating and maintaining the 
databases of large Web search engines. 

Because of the fact that typical term-vectors are very high-dimensional (e.g., on the 
order of 10,000 or more terms is quite common), classifiers that are both accurate and 
efficient in high-dimensional spaces are usually the methods of choice in this context. 
For example, although classification trees can be useful for high-dimensional problems in 
general, for document classification the individual features (individual terms) may not be 
so informative. For documents in the class ""sports"" it is more likely to be the case that 
such documents contain some subset of words such as ""score,"" ""field,"" ""stadium,"" ""win,"" 
etc., rather than always containing any particular single word from this set. Thus, 
classification models such as the first-order Bayes classifier (naive Bayes) or linear 
combinations of weights (such as linear support vector machines) tend to work well for 
document representations, since they can combine evidence from many different 
features in a relatively simple (e.g., linear) manner. A feedforward neural network would 
not be a practical choice for most document modeling problems by virtue of the fact that 
it would be extremely complex, from the point of view of both the number of parameters 
in the model and the amount of time that it would take to train it. 

Numerous interesting issues arise in the area of document classification that we will not 
dwell on here. For example, it makes sense to consider each document as belonging to 
multiple topics (classes) rather than just to a single class. Thus, instead of the usual 
framework where the class is considered to consist of mutually exclusive and exhaustive 
categories, for documents the classes need not be mutually exclusive. There are a 
number of different approaches to dealing with this ""multiple membership"" problem. One 
simple method is to train a binary classifier for each class separately, a method that is 
likely to be practical only if the total number of classes is relatively small. 

 
14.4 Modeling Individual Preferences 

14.4.1 Relevance Feedback 
As mentioned earlier, retrieval algorithms tend to have a much more interactive flavor 
than most of the other data mining algorithms we have discussed in this book. In 
particular, a user with a particular query Q may be willing to iterate through a few sets of 
different retrieval ""trials"" by the algorithm, and provide user-based feedback to the 
algorithm by labeling the returned documents as relevant or nonrelevant from the user's 
viewpoint. We can use this idea with any retrieval system, not just text retrieval, but we 
will limit our discussion to text to keep things specific. 
Rocchio's algorithm is particularly widely used in this context. The general idea is that 
relevance is essentially user-centric, i.e., if the user could (in theory) see all of the 
documents, he or she could in principle separate them into two sets, relevant R and 
irrelevant N R. Given these two sets, it can be shown that the optimal query (using a 
vector model) can be written as 

(14.5)  

where D denotes a term-vector representation for document whose labelings (by the 
user) are known. 
In practice, of course, a particular user has not personally labeled all of the documents in 
the database. Instead the user starts out with a specific query Qcurrent, which can be 
presumed to be suboptimal relative to Qoptimal. The algorithm uses this initial query to 
return a small set of documents that are then labeled by the user as being relevant R' or 
not N R'. Rocchio's algorithm then refines the query in the following manner: 

(14.6)  

Thus, the query is modified by (in effect) moving the current query toward the mean 
vector of the documents judged to be relevant and away from the mean vector of of the 
documents judged to be irrelevant. The parameters a, ß, and ? are positive constants 
(chosen heuristically) that control how sensitive the new query is to the most recent 
labeling of the documents, relative to the existing query vector Qcurrent. The process 
repeats; i.e., the new query Qnew is matched to the document set, and the user again 
labels the documents. Note that even if the initial query Q0 is stated incorrectly by the 
user, the algorithm can in principle adapt and learn the implicit preferences of the user in 
terms of relevance. In principle, if the labelings at each iteration are consistent, Qnew 
gradually approaches Qoptimal. 

Experimental evidence indicates that such user feedback does indeed improve 
precision-recall performance. In other words, incorporating feedback has been shown to 
be systematically effective for information retrieval. Of course, there are many details 
that must be specified to implement this approach in practice, such as the number of 
documents that should be presented to the user, the relative number of relevant and 
irrelevant documents to be used, the method used to choose the irrelevant documents, 
and so forth, leading to a large number of variations on this basic theme. 

14.4.2 Automated Recommender Systems 
Instead of modeling just the preferences of a single user, we can generalize to the case 
in which the database has information about multiple users and their preferences in 
terms of a large number of objects. The technique of collaborative filtering is a simple 
and well-known approach to leveraging this group information. For example, imagine you 
are interested in a certain musical group and buy a CD for this group at an online Web 
site. Several hundred other people also may have bought this particular CD, and it is 
likely that at least some of their general preferences in musical taste match yours. Thus, 
collaborative filtering in this context simply means that an algorithm running at the Web 
site can provide for you a list of other CDs purchased by people who bought the same 
CD as you. Clearly we can generalize the basic idea in various directions. For example, 
if we have a purchase history for each user and/or users are willing to provide more 
detailed information about their specific interests (in the form of user profiles), we can 
have a vector representation of each user, and the discussion earlier in the chapter 
about defining appropriate similarity metrics once again becomes quite relevant. 

Collaborative filtering is in a sense an attempt to capture the expert judgment and 
recommendations of a large group, where the group is automatically selected to match 
the interests of a specific user. The algorithms generally work by first finding the subset 
of profiles of those users that are judged to be most similar to the target profile and then 
calculating a recommendation as a function of the properties of the matched set of 
profiles. The quality of the recommendation will depend on the quality and quantity of 
information that is known about each user and the size of the user database. The 
technique tends to work best with large numbers of users. Acquiring a large number of 
user profiles can be difficult in practice, since users are naturally reluctant to take the 
time to provide detailed personal information. Capturing users preferences by their 
actions (e.g., by what they buy, or by what Web pages they visit) is a less intrusive 
approach that implicitly tries to estimate user preferences, and it is commonly employed 
in Internet-based recommendation systems (for example). A common practical 
requirement (for example, in e-commerce applications) is that the recommendation must 
be generated by the algorithm in real time, e.g., in less than a second. If we have a very 
large database of users (e.g., order of millions), this can pose significant computational 
and data engineering challenges. 

 
14.5 Image Retrieval 

Image and video data sets are increasingly common, from the hobbyist who stores digital 
images of family birthdays to organizations such as NASA and military agencies that 
gather and archive remotely sensed images of the earth on a continuous basis. Retrieval 
by content is particularly appealing in this context as the number of images becomes 

large. Manual annotation of images is time-consuming, subjective, and may miss certain 
characteristics of the image depending on the specific viewpoint of the annotator. A 
picture may be worth a thousand words, but which thousand words to use is a nontrivial 
problem! 

Thus, there is considerable motivation to develop efficient and accurate query-by-content 
algorithms for image databases, i.e., to develop interactive systems that allow a user to 
issue queries such as ""find the K most similar images to this query image"" or ""find the K 
images which best match this set of image properties."" Potential applications of such 
algorithms are numerous: searching for similar diagnostic images in radiology, finding 
relevant stock footage for advertising and journalism, and cataloging applications in 
geology, art, and fashion.  

14.5.1 Image Understanding 

Querying image data comes with an important caveat. Finding images that are similar to 
each other is in a certain sense equivalent to solving the general image understanding 
problem, namely the problem of extracting semantic content from image data. Humans 
excel at this. However, several decades of research in pattern recognition and computer 
vision have clearly shown that the performance of humans in visual understanding and 
recognition is extremely difficult to replicate with computer algorithms. (There is a direct 
analogy here to the NLP problem mentioned earlier in the context of text understanding.) 
Specific problems, such as face recognition or detection of airport runways, can be be 
successfully tackled, but general-purpose image understanding systems are still far off 
on the research horizon. For example, an infant can quickly learn to classify animals 
such as dogs of all sizes, colors, and shapes (including cartoon figures) in arbitrary 
scenes, whereas such completely unconstrained recognition is beyond the capability of 
any current vision algorithm. This ability to extract semantic content from raw image data 
is still relatively unique to the brain. Thus, not surprisingly, most current methods for 
image retrieval rely on relatively low-level visual cues. 

14.5.2 Image Representation 

For retrieval purposes, the original pixel data in an image can be abstracted to a feature 
representation. The features are typically expressed in terms of primitives such as color 
and texture features. As with text documents, the original images are converted into a 
more standard data matrix format where each row (object) represents a particular image 
and each column (variable) represents an image feature. Such feature representations 
are typically more robust to changes in scale and translation than the direct pixel 
measurements, but nonetheless may be invariant to only small changes in lighting, 
shading, and viewpoint. 
Typically the features for the images in the image database are precomputed and stored 
for use in retrieval. Distance calculations and retrieval are thus carried out in multi-
dimensional feature space. As with text, the original pixel data is reduced to a standard N 
× p data matrix, where each image is now represented as a p-dimensional vector in 
feature space. 

Spatial information can be introduced in a coarse manner by computing features in 
localized subregions of an image. For example, we could compute color information in 
each 32 × 32 subregion of a 1,024 × 1,024 pixel image. This allows coarse spatial 
constraints to be specified in image queries, such as ""Find images that are primarily red 
in the center and blue around the edges."" 
As well as regular m × n pixel images of scenes, an image database can also contain 
specific images of objects, namely objects on a constant background (such as an image 
of a black chair on a white background). Thus, we can also extract primitive properties 
that are object-specific such as features based on color, size, and shape (geometry) of 
the object. Video images represent a further generalization of image data, where images 
(frames) are linked sequentially over time. 
A well-known commercial example of a retrieval by content system for images is the 
Query by Image Content (QBIC) system developed by IBM researchers in the early 

1990s. The system is based on the general ideas described in section 14.5, allowing 
users to interactively query image and video databases based on example images, user-
entered sketches, color and texture patterns, object properties, and so forth. Queries are 
allowed on scenes, objects (parts of scenes), and sequences of video frames, or any 
combination of these. The QBIC system uses a variety of features and a variety of 
associated distance measures for retrieval: 

§  A three-dimensional color feature vector, spatially averaged over the 

whole image: the distance measure is simple Euclidean distance. 

§  K-dimensional color histograms, where the bins of the histogram can be 
selected by a partition-based clustering algorithm such as K-means, and 
K is application dependent. QBIC uses a Mahalanobis-type distance 
measure between color histogram vectors to account for color 
correlations. 

§  A three-dimensional texture vector consisting of features that measure 

coarseness/scale, directionality, and contrast. Distance is computed as a 
weighted Euclidean distance measure, where the default weights are 
inverse variances of the individual features. 

§  A 20-dimensional shape feature for objects, based on area, circularity, 

eccentricity, axis orientation, and various moments. Similarity is 
computed using Euclidean distance. 

14.5.3 Image Queries 

As with text data, the nature of the abstracted representation for images (i.e., the 
computed features) critically determines what types of query and retrieval operations can 
be performed. The feature representation provides a language for query formulation. 
Queries can be expressed in two basic forms. In query by example we provide a sample 
image of what we are looking for, or sketch the shape of the object of interest. Features 
are then computed for the example image, and the computed feature vector of the query 
is then matched to the precomputed database of feature vectors. Alternatively, the query 
can be expressed directly in terms of the feature representation itself, e.g., ""Find images 
that are 50% red in color and contain a texture with specific directional and coarseness 
properties."" If the query is expressed in terms of only a subset of the features (e.g., only 
color features are specified in the query), only that subset of features is used in the 
distance calculations. 

Clearly, depending on the application, we can generalize to relatively sophisticated 
queries (given a feature representation), allowing (for example) various Boolean 
combinations of query terms. For image data, the query language can also be 
specialized to allow queries that take advantage of spatial relations (such as ""Find 
images with object 1 above object 2"") or sequential relations (such as ""Find video 
sequences with soccer players taking shots at goal followed by images of players 
celebrating""). 

Representing images and queries in a common feature-vector form is quite similar to the 
vector-space representation for text retrieval discussed earlier. One important difference 
is that typically a feature for an image is a real number indicating (for example) a 
particular color intensity in a particular region of the image, while a term component in a 
term-vector is typically some form of weighted count of how often a term occurs in the 
document. Nonetheless, the general similarity of the retrieval-by-content problem for 
both problems has led to numerous applications of text retrieval techniques to image 
retrieval applications including (for example) the use of principal component analysis to 
reduce the dimensionality of the feature space and relevance feedback via Rocchio's 
algorithm to improve the image retrieval process. 

14.5.4 Image Invariants 

For retrieval by content with images, it is important to keep in mind that (with current 
techniques at least) we can realistically work only with a restricted notion of semantic 
content, based on relatively simple ""low-level"" measurements such as color, texture, and 

simple geometric properties of objects. There are many common distortions in visual 
data such as translations, rotations, nonlinear distortions, scale variability, and 
illumination changes (shadows, occlusion, lighting). The human visual system is able to 
handle many of these distortions with ease. For example, a human can view two scenes 
of the same object from completely different angles, with different lighting, and at 
different distances, and still easily attach the same semantic content to the scene (e.g., 
""It is a scene of my house taken after 1995""). 
However, the methods for content retrieval described here typically are  not invariant 
under such distortions. Changes in scale, illumination, or viewing angle will typically 
change the feature measurements such that the distorted version of a scene is in a very 
different part of the feature space, compared to the original version of the scene. In other 
words, the retrieval results will not be invariant to such distortions, unless such distortion 
invariance is designed into the feature representation. Again, typically, distortion-
invariant feature representations are currently known only for constrained visual 
environments, such as linear transformations of rigid objects, but not (for example) for 
general nonlinear transformations of nonrigid objects. 

14.5.5 Generalizations of Image Retrieval 
To conclude our discussion of image retrieval, we note that the term image can be 
interpreted much more broadly than the implicit interpretation as an image of a real-world 
scene (typically generated by a camera) as we have described it. More generally, image 
data can be embedded within text documents (such as books or Web pages). Other 
forms of images can include handwritten drawings (or text or equations), paintings, line 
drawings (such as in architecture or engineering), graphs, plots, maps, and so forth. 
Clearly, retrieval in each of these contexts needs to be handled in an application-specific 
manner, although many of the general principles discussed earlier will still apply. Video 
data also provides further challenges and opportunities in terms of automated indexing 
and interactive querying. For example, for a television news organization such as CNN, 
the ability to search through an archive of video footage to detect certain types of images 
would be quite useful. 
14.6 Time Series and Sequence Retrieval 

The problem of efficiently and accurately locating patterns of interest in time series and 
sequence data sets is an important and nontrivial problem in a wide variety of 
applications, including diagnosis and monitoring of complex systems, biomedical data 
analysis, and exploratory data analysis in scientific and business time series. Examples 
include 

§ finding customers whose spending patterns over time are similar to a given 

spending profile; 

§ searching for similar past examples of unusual current sensor signals for real-

time monitoring and fault diagnosis of complex systems such as aircraft; 

§ noisy matching of substrings in protein sequences. 

Sequential data can be considered to be the one-dimensional analog to two-dimensional 
image data. Time series data are perhaps the most well-known example, where a 
sequence of observations is measured over time, such that each observation is indexed 
by a time variable t. These measurements are often made at fixed time intervals, so that 
without loss of generality t can be treated as an integer taking values from 1 to T. The 
measurements at each time t can be multivariate (rather than just a single 
measurement), such as (for example) the daily closing stock prices of a set of individual 
stocks. Time series data are measured across a wide variety of applications, in areas as 
diverse as economics, biomedicine, ecology, atmospheric and ocean science, control 
engineering, and signal processing. 
The notion of sequential data is more general than time series data in the sense that the 
sequence need not necessarily be a function of time. For example, in computational 
biology, proteins are indexed by sequential position in a protein sequence. (Text could, 
of course, be considered as just another form of sequential data; however, it is usually 
treated as a separate data type in its own right.) 

As with image and text data, it is now commonplace to store large archives of sequential 
data sets. For example, several thousand sensors of each NASA Space Shuttle mission 

are archived once per second during the duration of each mission. With each mission 
lasting several days, this is an enormous repository of data (on the order of 10 Gbytes 
per mission, with order of 100 missions flown to date). 
Retrieval in this context can be stated as follows: find the subsequence that best 
matches a given query sequence Q. For example, for the Shuttle archive, an engineer 
might observe a potentially anomalous sensor behavior (expressed as a short query 
sequence Q) in real time and wish to determine whether similar behavior had been 
observed in past missions.  

14.6.1 Global Models for Time Series Data 
Traditional time series modeling techniques (such as statistical methods) are largely 
based on global linear models, as discussed in chapter 6. An example is the Box-Jenkins 
autoregressive family of models where the current value y(t) is modeled as a weighted 
linear combination of past values y(t - k) plus additive noise: 

(14.7)  

where the ai are the weighting coefficients and e(t) is the noise at time t (typically 
presumed to be Gaussian with zero mean). The term autoregression is derived from the 
notion of using a regression model on past values of the same variable. Techniques that 
are very similar in spirit to the linear regression techniques of chapter 11 can be used to 
estimate the parameters ai from data. Determination of the model structure (or order, k) 
can be performed by the usual techniques of penalized likelihood or cross-validation. 
This type of model is closely linked to the spectral representation of y, in the sense that 
specifying the as also specifies the frequency characteristics for a stationary time series 
process y. Thus it is clear that it only makes sense to consider the use of the 
autoregressive model for time series that can be well characterized by stationary spectral 
representations, i.e., a linear system whose frequency characteristics do not change with 
time. 
A general contribution of the Box-Jenkins methodology has been to show that if a time 
series has an identifiable systematic nonstationary component (such as a trend), the 
nonstationary component can often be removed to reduce the time series to a stationary 
form. For example, economic indices such as annual gross domestic product or the Dow 
Jones Index contain an inherent upward trend (on average) which is typically removed 
before modeling. Even if the nonstationarity is not particularly simple, another useful 
approach is to assume that the signal is locally stationary in time. For example, this is 
how speech recognition systems work, by modeling the sequence of phoneme sounds 
produced by the human vocal tract and mouth as coming from a set of different linear 
systems. The model is then defined as a mixture of these systems, and the data are 
assumed to come from some form of switching process (typically Markov) between these 
different component linear systems. 
Nonlinear global models generalize  equation 14.7 to allow (for example) a nonlinear 
dependence of  y(t) on the past: 

(14.8)  

where g(.) is a nonlinearity. 
There are a large number of extensions of both the linear and nonlinear models of this 
general form. One key aspect they have in common is that, given an initial condition y(0) 
and the parameters of the model, the statistics of the process (the distribution of y as a 
function of time) are completely determined, i.e., these models provide global aggregate 
descriptions of the expected behavior of the time series. 
In terms of data mining, if we assume that such a global model is an adequate 
description of the underlying time series, we can use the model parameters (e.g., the 
weights above) as the basis for representing the data, rather than the original time series 
itself. For example, given a set of different time series (such as daily returns over time of 
various stocks), we could fit a different global model to each time series (i.e., estimate p 
parameters of a model) and then perform similarity calculations among the time series in 
p-dimensional parameter space. A problem arises here if the different time series are not 
all adequately modeled by the same model structure. One solution to this is to assume a 

nested model structure (i.e., where the model structures form a nested sequence of 
increasing complexity) and to simply fit the maximum-order model to all the time series. 

By representing each time series as a vector of parameters, we have in essence 
performed the same trick as that used for representing documents and images in earlier 
sections of this chapter. Thus, in principle, we can define similarity measures in the 
vector space of parameters, define queries in this space for retrieval by content, and so 
forth. 
One interesting application of data mining in this context is a technique known as 
keyword spotting in speech recognition. Consider, for example, a national security 
organization that monitors and records international telephone conversations (ignore the 
moral and legal ramifications!). From a security viewpoint the goal is to detect suspicious 
behavior. Naturally it is impossible for a human to monitor and listen to all of the 
hundreds of thousands of telephone conversations recorded daily. One automated 
approach to the problem is to build statistical models of specific keywords of interest. For 
example, we could construct (from training data) a different Markov linear-switching 
model (as mentioned earlier) for each specific word of interest. The incoming voice 
streams are run through each model in parallel, and if the likelihood of the observed 
audio data exceeds a certain threshold for any of the models, then that word is 
considered to be detected, and the voice stream can be tagged with the identified word 
and the location in time of that word. Naturally there are numerous practical engineering 
considerations for such a system, but the basic concept is to use a set of trained models 
to adaptively monitor a real-time stream of data to detect patterns of interest. 

14.6.2 Structure and Shape in Time Series 
Consider a real valued subsequence of time series values, Q = [q(t), ..., q(t + m)], which 
we will call the query pattern, and a much larger archived time series X = [x(1), ..., x(T)]. 
The goal is find a subsequence within X that is most similar to Q. In reality X could be 
composed of many individual time series, but for simplicity imagine that they have all 
been concatenated into a single long series. In addition, for simplicity assume that both X 
and Q have been measured with the same time-sampling interval, i.e., the time units for 
one increment of t are the same for each. For example, Q could be a snapshot from an 
EEG monitor of a patient being monitored in real time, and X could be an archive of 
EEGs for patients who have already been diagnosed. 
Clearly there is considerable latitude in how similarity is defined in this context. Note that 
the general approach of the last section provides only a global statistical characterization 
of a time series. In particular, there is no notion of local shape such as a peak in time. 
Global statistical models typically average out such local structural properties; i.e., they 
are not explicitly retained in the representation. However, many time series are more 
naturally described by their structural features. A good example is the S-T waveform in 
cardiac monitoring, which has a distinctive visual signature. 
One approach is to perform a sequential scan of the query Q across the full length of the 
archival data X, moving the query Q along X one time point at a time, and calculating a 
distance metric (such as Euclidean distance) at each point. This is typically not only 
prohibitively expensive (O(mT) for a brute-force approach) but also focuses on low-level 
sampled data points of the the signal rather than high-level structural properties such as 
peaks, plateaus, trends, and valleys. Direct Euclidean-distance matching is also very 
susceptible to slight deformations of the query Q or data X, e.g., a slight ""stretching"" 
along the time axis of the ""ideal"" query Q can result in a large increase in distance even 
though from the viewpoint of the human visual observer the query Q and the data X may 
appear to match well.  
A popular approach in this context is to locally estimate shape-based features of both the 
query Q and the archived signal X, and to perform matching at the higher structural level. 
This can achieve a significant computational advantage in the matching process, since 
the abstraction is in essence a form of data compression, i.e., many irrelevant details of 
the signal can be ignored. More important, it can extract structural information in a form 
that is suitable for human interpretation. One example of this technique is to approximate 
a signal by piecewise linear (or polynomial) segments. The segmented series can now 
be represented as a list of locally parametrized curves, and structural features (such as 
peaks and valleys) can be directly calculated from the parametrized description. A 

probabilistic model can then be used to parametrize the expected shape and variability in 
terms of these features, allowing for a flexible family of deformable template models. 
Matching a query Q to a data archive X can be formulated as a search problem in terms 
of finding local regions in X that maximize the likelihood of the local data within that 
region given the probabilistic model for Q. This type of representation is particularly 
useful for the types of signals that are not easily handled by the global statistical models, 
i.e., nonstationary signals containing transients, step functions, trends, and various other 
shapelike patterns. 

For discrete-valued sequences we can also look for subpatterns within a longer 
sequence, e.g., the occurrence of motifs in biological sequence data. A wide variety of 
techniques are used for these sorts of problems, ranging from relatively nonparametric 
edit-distance methods for matching two strings, to probabilistic model-based approaches 
using generative Markov (or hidden Markov) models. 
14.7 Summary 

Retrieval by content is an important problem for the interactive exploration of large 
databases. In particular, for data types such as images, text, and sequences, algorithms 
for retrieval by content have great potential utility across a variety of applications. 
However, in their full generality, such algorithms require the solution of several 
fundamental and long-standing problems in artificial intelligence and pattern recognition, 
such as the general NLP problem (for text) or the general image understanding problem 
(for images). In short, we are a long way from developing general-purpose methods for 
automatically extracting semantic content from data such as text or images in a manner 
that is competitive with the human brain.  

Nonetheless, given that in many applications it is impossible to analyze the data 
manually because of its sheer volume, researchers have developed a variety of 
techniques for retrieval by content, largely relying on what we might term ""low-level 
semantic content."" Thus, for example, we retrieve images based on low-level features 
such as color or texture, and retrieve text based on word co-occurrences. 

Across different data types, we can see that a common strategy for retrieval is often 
used, roughly following these steps: 

1.  Determine a robust set of features by which to describe the objects of 

interest. 

2.  Convert the original objects (text, images, sequences) into a fixed-length 

vector representation using these features. 

3.  Perform matching, distance calculations, principal component analysis, 

and so forth, in this vector-space, taking advantage of the wealth of 
theory that exists for multivariate data analysis. 

We might term such systems as first-generation retrieval by content systems. Certainly 
they can be very useful, as is evidenced by Web search engines and the QBIC system, 
for example. However, it is clear that retrieval by content is far from being a solved 
problem, and there is considerable room for further advances. 
14.8 Further Reading 
The collected volume by Sparck Jones and Willett (1997) contains many of the classic 
papers in information (text) retrieval, with some very insightful and broad-ranging 
editorial discussion of central themes in retrieval problems and research. Van Rijsbergen 
(1979), Salton and McGill (1983), and Frakes and Baeza-Yates (1992) provide a more 
introductory coverage of the field. Salton (1971) contains many of the seminal early 
ideas on the vector-space representation and Raghavan and Wong (1986) provide a 
later perspective. Salton and Buckley (1988) is a brief but informative account of different 
term-weighting methods, with particular emphasis on the TF-IDF method. The TREC 
conferences are documented in Harman (1993–1999), and Harman (1995) provides a 
useful overview of the TREC experiments. A more recent treatment of general issues in 
evaluation in the context of text retrieval is in the special issue of the Journal of the 
American Society for Information Science (1996). Witten, Moffat, and Bell (1999) provide 
an excellent account of many of the practical data engineering issues involved in storing 
and accessing large text document archives. 

Deerwester et al. (1990) is one of the first papers to clearly demonstrate the utility of LSI 
in information retrieval. Landauer and Dumais (1997) provide a thought-provoking 
discussion of the general implications of the LSI approach for cognitive models of 
language and knowledge acquisition. Berry (1992) and Berry, Drmvac, and Jessup 
(1999) describe general numerical techniques for performing SVD computations on large 
sparse data sets such as term-document representations. Hofmann (1999) describes a 
probabilistic approach to dimension reduction in document -term matrices, based on 
mixture models, providing a general probabilistic framework for document modeling and 
demonstrating impressive empirical results. 
""Text mining"" is a phrase used to describe the application of data mining techniques to 
semi-automated discovery of new knowledge from text documents. An interesting line of 
work in this area is described in Swanson (1987) and Swanson and Smalheiser (1994, 
1997) where automated text search algorithms were used to discover interesting 
relationships between seemingly unrelated sub-fields in the medical literature. 
Rocchio (1971) introduced the original algorithm for relevance feedback. Salton and 
Buckley (1990) provide experimental evidence on the effectiveness of relevance 
feedback in improving recall-precision performance, and Buckley and Salton (1995) 
discuss optimality aspects of Rocchio's algorithm. Resnick et al. (1994) and Shardanand 
and Maes (1995) describe the initial work on collaborative filtering. Breese, Heckerman, 
and Cadie (1998) contains a more recent empirical evaluation of model-based 
collaborative filtering algorithms. Konstan and Riedl (in press) contains a useful recent 
overview of many of the practical factors involved in fielding automated recommender 
systems in e-commerce applications. Dumais et al. (1998) describe the use of support -
vector machines for document classification. 
The QBIC system is described in some detail in Faloutsos et al. (1994) and Flickner et 
al. (1995). The first paper contains a reasonably detailed discussion of the features, 
distance measures, and indexing schemes used, while the second paper focuses 
somewhat more on user interface issues. Other query by content systems for image and 
video retrieval are described in Kato, Kurita, and Shimogaki (1991), Smoliar and Zhang 
(1994), Pentland, Picard, and Sclaroff (1996), and Smith and Chang (1997). Rui et al. 
(1998) discuss the use of relevance feedback in an image-retrieval context. The edited 
collection by Maybury (1997) provides a useful overview of recent work in the general 
area of retrieval of multimedia objects such as images and video. 
Box and Jenkins (1976) is a comprehensive and classic text on the fundamentals of 
linear global models for time series data. Chatfield (1989) provides a somewhat broader 
perspective and is particularly noteworthy as a gentle introduction to time series 
concepts for readers unfamiliar with the field in general. MacDonald and Zucchini (1997) 
provide a comprehensive description of statistical approaches to modeling discrete-
valued time series, and  Durbin et al. (1998) illustrate the application of sequence 
modeling and pattern recognition techniques to protein sequences and related problems 
in computational biology. 
There are a large number of different techniques for efficient approximate subsequence 
matching for time series. The work of Faloutsos, Ranganathan, and Manolopolous 
(1994) is typical. Sequences are decomposed into windows, features are extracted from 
each window (locally estimated spectral coefficients in this case), and efficient matching 
is then performed using an R*-tree structure in feature space. Agrawal et al. (1995) 
proposed an alternative approach that can handle amplitude scaling, offset translation, 
and ""don't care"" regions in the data, where distance is determined from the envelopes of 
the original sequences. Berndt and Clifford (1994) use dynamic time-warping approach 
to allow for ""elasticity"" in the temporal axis when a query Q is matched to a reference 
sequence R. Another popular approach is to abstract the notion of shape. Relational 
trees can be used to capture the hierarchy of peaks (or valleys) in a sequence, and tree-
matching algorithms can then be used to compare two time series (Shaw and 
DeFigueiredo (1990); Wang et al., (1994)). Keogh and Smyth (1997) and Ge and Smyth 
(2000) illustrate the use of probabilistic deformable templates for flexible modeling and 
detection of time series shapes with applications to interactive analysis of Shuttle sensor 
data and online monitoring of semiconductor manufacturing data. 
Appendix: Random Variables 

A.1: Review of Univariate Random Variables 
A univariate random variable is a single random variable X. If the domain of X is finite (or 
denumerable), we can characterize the uncertainty about X by listing for each possible 
value x of X—e.g., x ? {x1, ..., xm}—the probability that X has value  x. We write this 
probability distribution of X as p(X = x), or, to refer to the probability distribution of single 
values in general, p(x). When the domain is finite, as in this case, the set of probabilities 
{p(x1), ..., p(xm)} is often called a probability mass function. Note that the expression p(X) 
refers to the set of m numbers {p(x1), ..., p(xm)}, and p(x) refers to some (arbitrary) 
member of this set. The cumulative distribution function P(x) of a random variable is the 
probability that it will take a value less than or equal to x (when the values x can be 
ordered). 
Cumulative distribution functions can also be defined for continuous random variables 
(those that can take any value on an interval or the real line). In this case we will usually 
denote the cumulative distribution by F(x) or P(x), and the derivative of F(x), the 
probability density function of x (often just ""density function"" for short), by ƒ(x) or  p(x). 
This function gives the probability that the observed value will lie in an infinitesimal 
interval surrounding  x. Here, for the sake of simplicity, we will often provide descriptions 
only in terms of density functions, but analogous arguments apply to probability mass 
functions. Introductory texts on mathematical statistics provide more formal descriptions 
of these concepts, but these informal definitions will suffice for our purposes. 
In terms of notation both p(x) and ƒ(x) are often used to denote a probability density 
function on a continuous variable x. It should be clear from the context, depending on 
whether x is discrete or continuous, whether p(x) is referring to a probability mass 
function or a probability density function for x. 
The randomness of a random variable may arise for various reasons—essentially the 
sources of uncertainty: perhaps we have observed a randomly selected member of the 
population, perhaps there is measurement error associated with the value, perhaps X is 
not directly observable, and so on. We often approximate this randomness by assuming 
that the actual observed values have arisen from some well known distribution of 
possible values. Certain distributions are especially useful in data mining, and some of 
these are described in appendix A.2. They include the Normal (or Gaussian) distribution 
and the Poisson distribution. 
We use the notion of the mean value (or expected value, or expectation) in chapter 2. 
For a sample (or finite population) the mean is the average value, obtained by dividing 
the sum of the values in the sample (or finite population) by the total number of values. 
More generally, suppose that value  x occurs in the population with probability p(x). Then 
the mean value of the variable  X for the population is given by ? xxp(x). However, if X can 
take a continuum of values, it is not meaningful to speak of the probability that any 
particular exact value x will occur, since exact values have zero probability of occurring. 
Instead we consider the probability that X lies in a small interval of width dx and find the 
limiting value of the sum ? xxƒ(x)dx as this width decreases toward zero. This leads us to 
replace summation with integration. If the probability density function of the continuous 
variable X is ƒ(x), the expected value is ?xƒ(x)dx. 
The notation E is often used to denote expectation, so that the expected value of a 
random variable X is E[X]. The Greek letter µ is also often used to denote a mean, or, if 
we need to make it clear that the random variable X is being discussed, µx may be used. 
More precisely, the expected value of X with respect to the density function ƒ(x) is 
denoted Eƒ(x)[X]. Note that we can define the expected value of a function of X, g(x), with 
respect to ƒ(x), as Eƒ(x)[g(x)] = ? g(x)ƒ(x)dx. If we let g(x) = (x - E[x])2 we get the usual 
definition of variance 
Expectation is a linear operator. This is quite a useful general property. For example, it 
means that the expected value of a weighted sum of random variables is equal to the 
weighted sum of their expected values, regardless of whether the variables are 
dependent in any way (chapter 4 discusses more precisely what we mean by 
dependence of random variables). 
The axioms of probability referred to above assign a probability of 0 to an event that 
cannot occur, and a probability of 1 to a certain event. If two events cannot occur 
together, the probability that one or the other will occur is the sum of their separate 
probabilities. Thus, in tossing a fair coin (with which the probability of obtaining a head is 

. 

1/2) the probability of obtaining either a head or a tail is 1/2+1/2 = 1. The situation starts 
to get more complicated—and more interesting—when events can occur together but are 
not certain to do so. This lead us to the notion of multivariate random variables, 
discussed in detail in chapter 4. 
A.2: Some Common Probability Distributions 

Above we discussed the general idea of a probability distribution. Here we describe 
some specific and important probability distributions that arise in data mining. 

The Bernoulli Distribution 
The Bernoulli distribution has just two possible outcomes. Situations which might be 
described by such a distribution include the outcome of a coin toss (heads or tails) or 
whether a particular customer buys a particular product or not. Denoting the outcomes 
by 0 and 1, let p be the probability of observing a 1 and (1-p) the probability of observing 
a 0. Then the probability mass function can be written as px(1 - p)1-x, with x taking the 
value 0 or 1. The mean of the distribution is p and its variance is p(1 - p). Note that this 
distribution has just a single parameter, namely p. 

The Binomial Distribution 
This is a generalization of the Bernoulli distribution, and describes the number x of ""type 
1 outcomes"" (e.g., successes) in n independent Bernoulli trials, each with parameter p. 
The probability mass function has the form 
, where x can take integer values 
between 0 and n. The mean is np and the variance is np(1 - p). 

The Multinomial Distribution 
The multinomial distribution is a generalization of the binomial distribution to the case 
where there are more than two potential outcomes; for example, there may be k possible 
outcomes, the ith having probability pi of occurring, 1 = i = k. The probabilities pi sum to 
1, and the model has k - 1 parameters p1, ..., pk-1 (since pk = 1 - ? ipi).  
Suppose that n observations have been independently drawn from a multinomial 
distribution. Then the mean number of observations yielding the ith outcome is npi and 
its variance is npi(1 - pi). Note that, since the occurrence of one outcome means the 
others cannot occur, the individual outcomes must be negatively correlated. In fact, the 
covariance between the ith and jth (i ? j) outcome is -npipj. 

The Poisson Distribution 
If random events are observed independently, with underlying rate ?, then we would 
expect to observe ?t events on average in a time interval of length t. Sometimes, of 
course, we would observe none in time t, at other times we would observe 1, and so on. 
If the rate is low, we would rarely expect to observe a large number of events (unless t 
was large). A distribution which describes this state of affairs is the Poisson distribution. 
It has probability mass function (?t)xe-?t/x!. The mean and variance of the Poisson 
distribution are the same, both being ?. 
Given a binomial distribution with large n and small p such that np is a constant, then this 
may be well approximated by a Poisson distribution. 

The Normal (or Gaussian) Distribution 

The probability density function takes the form 

 

where µ is the mean of the distribution and s2 is the variance. The standard normal 
distribution is the special case with zero mean and unit variance. The normal distribution 
is very important, partly as a consequence of the central limit theorem. Roughly 
speaking, this says that the distribution of the mean of a sample of n observations 
becomes more and more like the normal distribution as n increases, regardless of the 
form of the populations distribution from which the data are drawn. (Of course, 
mathematical niceties require this to be qualified for full rigor.) This is one reason why 

many statistical procedures are based on an assumption that various distributions are 
normal. 

The normal distribution is symmetric about its mean, and 95% of its probability lies within 
±1.96 standard deviations of the mean.  

The Student's t-distribution 

Consider a sample from a normal distribution with known standard deviation s. An 
appropriate test statistic to use to make inferences about the mean would be the ratio 

 

is the sample mean. Using this, for example, one can see how far the sample 

where 
mean deviates from a hypothesized value of the unknown mean. This ratio will be 
normally distributed by the central limit theorem (see  Normal distribution above). Note 
that here the denominator is a constant. Of course, in real life, one is more likely to be in 
a situation of making inferences about a mean when the standard deviation is unknown. 
This means that one would usually want to replace the above ratio by 

 

where s is the sample estimate of the standard deviation. As soon as one does this the 
ratio ceases to be normally distributed—extra random variation has been introduced by 
the fact that the denominator now varies from sample to sample. The distribution of this 
new ratio will have a larger spread than that of the corresponding normal distribution—it 
will have fatter tails. This distribution is called the t-distribution. Note that there are 
many—they differ according to how large is the sample size, since this affects the 
variability in s. They are indexed by (n - 1), known as the degrees of freedom of the 
distribution. 
We can also describe this situation by saying that the ratio of two random variables, the 
numerator following a normal distribution and the square of the denominator following a 
chi-squared distribution (see below), follows a t-distribution. 
The probability density function is quite complicated and it is unnecessary to reproduce it 
here (it is available in introductory texts on mathematical statistics). The mean is n - 1 
and the variance is (n - 1)/(n - 3). 

The Chi-Squared Distribution 
The distribution of the sum of the squares of n values, each following the standard 
normal distribution, is called the chi-squared distribution with n degrees of freedom. Such 
a distribution has mean n and variance 2n. Again it seems unnecessary to reproduce the 
probability density function here—it can be readily found in introductory mathematical 
statistics texts it needed. The chi-squared distribution is particularly widely used in tests 
of goodness-of-fit. 

The F Distribution 
If u and v are independently distributed Chi-squared random variables with n1 and n2 
degrees of freedom, respectively, then the ratio 

 

is said to follow an F distribution with n1 and n2 degrees of freedom. This is widely used 
in tests to compare variances, such as arise in analysis of variance applications. 

The Multivariate Normal Distribution 
This is an extension of the univariate normal distribution to multiple random variables. 
Let x = (x1, ..., xp) denote a p component random vector. Then the probability density 
function of the multivariate normal distribution has the form 

where µ is the p-dimensional mean vector of the distribution and S is the p × p 
covariance matrix. 

 

Just as the univariate normal distribution plays a unique role in probabilistic modeling, so 
does the multivariate normal distribution. It has the property that its marginal distributions 
are normal, as also are its conditional distributions (the joint distribution of a subset of 
variables, given fixed values of the others). Note, however, that the converse is not true: 
just because the p marginals of a distribution are normal, this does not mean the overall 
distribution is multivariate normal. 

References  

Abiteboul, S., Hull, R., and Vianu, V. (1995) Foundations of Databases. Reading, MA: 
Addison-Wesley.  

Adriaans, P., and Zantige, D. (1996) Data Mining. Harlow, UK: Addison-Wesley.  

Agrawal, R., Aggarwal, C., and Prasad, V. (in press) A tree projection algorithm for finding 
frequent itemsets. Journal of Parallel and Distributed Computing.  

Agrawal, R., Imielenski, T., and Swami, A. (1993) Mining association rules between sets of 
items in large databases. Proceedings of the ACM SIGMOD Conference on Management of 
Data (SIGMOD'98), New York: ACM Press, pp. 207–216.  

Agrawal, R., Lin, K.I., Sawhney, H.S., and Shim, K. (1995) Fast similarity search in the 
presence of noise, scaling, and translation in time-series databases. Proceedings of VLDB-
95, pp. 490–501.  

Agrawal, R., Mannila, H., Srikant, R., Toivonen, H., and Verkamo, A.I. (1996) Fast discovery 
of association rules. Advances in Knowledge Discovery and Data Mining, U.M., Fayyad, G., 
Piatetsky-Shapiro, P., Smyth, and Uthurasamy, R. (eds.). Menlo Park, CA: AAAI Press, pp. 
307–328.  

Agrawal, R., and Srikant, R. (1994) Fast algorithms for mining association rules in large 
databases. Proceedings of the Twentieth International Conference on Very Large Data Bases 
(VLDB'94), pp. 487–499.  

Akaike, H. (1973) Information theory and an extension of the maximum likelihood principle. In 
Second International Symposium on Information Theory, B.N. Petrov and F. Csaki (eds.), 
Academiai Kiado, Budapest, pp. 267–281.  

Alon, N., and Spencer, J.H. (1992) The Probabilistic Method. New York: Wiley.  

Anderberg, M.R. (1973) Cluster Analysis for Applications. New York: Academic Press.  

Applebaum, D. (1996) Probability and Information: An Integrated Approach, Cambridge, U.K.: 
Cambridge University Press.  

Aronis, J.M., and Provost, F.J. (1997) Increasing the efficiency of data mining algorithms with 
breadth-first marker propagation. Proceedings of the Third International Conference on 
Knowledge Discovery and Data Mining, Heckerman, D., Mannila, H., and Pregibon, D. (eds.). 
Menlo Park, CA: AAAI Press, pp. 119–122.  

Asimov, D. (1985) The grand tour: a tool for viewing multidimensional data. SIAM Journal of 
Scientific and Statistical Computing, 6, pp. 128–143.  

Atkeson, C.W., Schaal, S.A., and Moore, A.W. (1997) Locally weighted learning. Artificial 
Intelligence Review, 11, pp. 75–133.  

Azzalini, A., and Bowman, A.W. (1990) A look at some data on the Old Faithful geyser. 
Applied Statistics, 39, pp. 357–365.  

Babcock, C. (1994) Parallel processing mines retail data. Computer World, 6.  

Ballard, D.H. (1997) An Introduction to Natural Computation. Cambridge, MA: MIT Press.  

Banfield, J.D., and Raftery, A.E. (1993) Model-based Gaussian and non-Gaussian clustering. 
Biometrics, 49, pp. 803–821.  

Barnett, V. (1982) Comparative Statistical Inference. Chichester, U.K.: Wiley.  

Baum, L.E., and Petrie, T. (1966) Statistical inference for probabilistic functions of Markov 
chains. Annals of Mathematical Statistics, 37, pp. 1554–1563.  

Bayardo, R.J., and Agrawal, R. (1999) Mining the most interesting rules. Proc. 5th ACM 
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-99). New 
York: ACM Press, pp. 145–154.  

Becker, R.A., Cleveland, W.S., and Wilks, A.R. (1987) Dynamic graphics for data analysis. 
Statistical Science, 2, pp. 355–395.  

Becker, R.A., Eick, S.G., and Wilks, A.R. (1995) Visualizing network data. IEEE Transactions 
on Visualization and Computer Graphics, 1(1), pp. 16–28.  

Bennett, K., Fayyad, U., and Geiger, D. (1999) Density-based indexing for approximate 
nearest-neighbor queries. Proceedings of the Fifth ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining, New York, NY: ACM Press, pp. 233–243.  

Bernardo, J.M., and Smith, A.F.M. (1994) Bayesian Theory. New York, NY: Wiley.  

Berndt, D.J., and Clifford, J. (1996) Finding patterns in time-series, a dynamic programming 
approach. Advances in Knowledge Discovery and Data Mining, Fayyad, U.M., Piatetsky-
Shapiro, G., Smyth, P., and Uthurasamy, R. (eds). Menlo Park, CA: AAAI/MIT Press, pp. 
229–248.  

Berry, M.J.A., and Linoff, G. (1997) Data Mining Techniques for Marketing, Sales, and 
Customer Support. New York: Wiley.  

Berry, M.J.A., and Linoff, G. (2000) Mastering Data Mining. New York: Wiley.  

Berry, M.W. (1992) Large scale singular value computations. International Journal of 
Supercomputer Applications 6(1), pp. 13–49.  

Berry, M.W., Drmvac, Z., and Jessup, E.R. (1999) Matrices, vector-spaces, and information 
retrieval, SIAM Review, 41(2), pp. 335–362.  

Beyer, K., Goldstein, J., Ramakrishnan, R., and Shaft, U. (1999) When is ""nearest neighbor"" 
meaningful? Proceedings of the 7th International Conference on Data Theory, ICDT'99, 
Lecture Notes in Computer Science, LNCS, Number 1540. New York: Springer-Verlag, pp. 
217–235.  

Bhandari, I., Colet, E., Parker, J., Pines, Z., Pratap, R., and Ramanujam, K. (1997) Advanced 
Scout: data mining and knowledge discovery in NBA data. Data Mining and Knowledge 
Discovery, 1(1), pp. 121–125.  

Bishop, C.M. (1995) Neural Networks for Pattern Recognition. Oxford, U.K.: Clarendon Press, 
1995.  

Bishop, Y.M.M., Fienberg, S.E., and Holland, P.W. (1975) Discrete Multivariate Analysis, 
Cambridge, MA: MIT Press.  

Blasius, J., and Greenacre, M. (1998) Visualization of Categorical Data. San Diego, CA: 
Academic Press.  

Blum, T., Keislaer, D., Wheaton, J., and Wold, E. (1997) Audio databases with content-based 
retrieval. Intelligent Multimedia Information Retrieval, Maybury, M. T. (ed.). Menlo Park, CA: 
AAAI Press, pp. 113–135.  

Box, G.E.P., and Jenkins, G.M. (1976) Time Series Analysis: Forecasting and Control. 
Oakland, CA: Holden Day.  

Box, G.E.P., Jenkins, G.M., and Reinsel, G.C. (1994) Time Series Analysis: Forecasting and 
Control, 3rd ed. Englewood Cliffs, NJ: Prentice Hall.  

Bradley, P.S., Fayyad, U.M., and Mangasarian, O.L. (1999) Mathematical programming for 
data mining: formulation and challenges. INFORMS Journal on Computing, 11, pp. 217–238.  

Bradley, P.S., Fayyad, U.M., Reina, C. (1998) Scaling clustering algorithms to large 
databases. In Proceedings of the 4th International Conference on Knowledge Discovery and 
Data Mining, R. Agrawal, P. Stolorz, and G. Piatetsky-Shapiro (eds.), Menlo Park, CA: AAAI 
Press, pp. 9–15.  

Breese, J.S., Heckerman, D., and Kadie, C. (1998) Empirical analysis of predictive algorithms 
for collaborative filtering. Proceedings 14th Conference on Uncertainty in Artificial Intelligence, 
San Francisco, CA: Morgan Kaufmann, pp. 43–52..  

Breiman, L., Friedman, J.H., Olshen, R.A., and Stone, C.J. (1984) Classification and 
Regression Trees. Belmont, CA: Wadsworth Statistical Press.  

Brijs, T., Goethals, B., Swinnen, G., Vanhoof, K., and Wets, G. (2000) A data mining 
framework for optimal product selection in retail supermarket data: the generalized PROFSET 
model. Proceedings of the ACM Seventh International Conference on Knowledge Discovery 
and Data Mining, New York: ACM Press, pp. 300–304.  

Brin, S., and Page, L. (1998) The anatomy of a large-scale hypertextual Web search engine. 
Proceedings of the Seventh International World-Wide Web Conference, Brisbane, Australia, 
pp. 107–117.  

Brin, S., Motwani, R., and Silverstein, C. (1997) Beyond market baskets: generalizing 
association rules to correlations. Proceedings of the ACM SIGMOD Conference on 
Management of Data (SIGMOD'97), New York: ACM Press, pp. 265–276.  

Brooks, S.P., and Morgan, B.J.T. (1995) Optimisation using simulated annealing. The 
Statistician, 44, pp. 241–257.  

Buckley, C., and Salton, G. (1995) Optimization of relevance feedback weights. Proceedings 
of the 18th Annual ACM 1995 SIGIR Conference, pp. 351–356.  

Buja, A., Cook, D., and Swayne, D.F. (1996) Interactive high-dimensional data visualization. 
Journal of Computational and Graphical Statistics, 5(1), 78–99.  

Buntine, W. (1992) Learning classification trees. Statistics and Computing, 2, pp. 63–73.  

Buntine, W., Fischer, B., and Pressburger, T. (1999) Towards automated synthesis of data 
mining programs. In Proceedings of the Fifth ACM Conference on Knowledge Discovery and 
Data Mining, S. Chaudhuri and D. Madigan (eds.), New York, NY: ACM Press, pp. 372–376.  

Burges, C.J.C. (1998) A tutorial on support vector machines for pattern recognition. Data 
Mining and Knowledge Discovery, 2, pp. 121–167.  

Burnham, K.P., and Anderson, D.R. (1998) Model Selection and Inference: a Practical 
Information Theoretic Approach. New York: Springer-Verlag.  

Böhning, D. (1998) Computer Assisted Analysis of Mixtures, Boca Raton, FL: Chapman and 
Hall.  

Cadez, I.V., McLaren, C.E., Smyth, P., and McLachlan, G.J. (1999) Hierarchical models for 
screening of iron-deficient anemia. Proceedings of the 1999 International Conference on 
Machine Learning, I. Bratko and S. Dzeroski(eds.), San Francisco, CA: Morgan Kaufmann, 
pp. 77–86.  

Cadez, I.V., Heckerman, D., Meek, C., Smyth, P., and White, S. (2000) Visualization of 
navigation patterns on a Web site using model-based clustering. Proceedings of the ACM 
Seventh International Conference on Knowledge Discovery and Data Mining, New York, NY: 
ACM Press, pp. 280–284.  

Card, S.K., MacKinlay, J.D., and Shneiderman, B. (eds.) (1999) Readings in Information 
Visualization: Using Vision to Think. San Francisco, CA: Morgan Kaufmann.  

Carmines, E.G., and Zeller, R.A. (1979) Reliability and Validity Assessment. Beverly Hills, CA: 
Sage Publications.  

Carr, D.B., Littlefield, R.J., Nicholson, W.L., and Littlefield, J.S. (1987) Scat-terplot matrix 
techniques for large  N. Journal of the American Statistical Association, 82(398), pp. 424–436.  

Casti, J.L. (1990) Searching for Certainty: What Scientists Can Know about the Future. New 
York: Willam Morrow.  

Celeux, G., and Govaert, G. (1995) Gaussian parsimonious clustering models. Pattern 
Recognition, 28, pp. 781–793.  

Chambers, J.M., Cleveland, W.S., Kleiner, B., and Tukey, P.A. (1983) Graphical Methods for 
Data Analysis. Pacific Grove: Wadsworth and Brooks/Cole.  

Chatfield, C. (1996) The Analysis of Time Series: An Introduction. London: Chapman and 
Hall.  

Chatterjee, S., Handcock, M.S., and Simonoff, J.S. (1995) A Casebook for a First Course in 
Statistics and Data Analysis. New York: Wiley.  

Chaudhuri, S. (1998) An overview of query optimization in relational systems. Proceedings of 
the Seventeenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database 
Systems, New York: ACM Press, pp. 34–43.  

Chaudhuri, S., and Dayal, U. (1997) An overview of data warehousing and OLAP technology. 
Proceedings of the 1997 ACM/SIGMOD Conference, New York: ACM Press, pp. 65–75.  

Cheeseman, P., and Stutz. J. (1996) Bayesian classification (AutoClass): theory and results. 
In Advances in Knowledge Discovery and Data Mining, U.M. Fayyad, G. Piatetsky-Shapiro, P. 
Smyth, R. Uthurusamy (eds.), Cambridge, MA: AAAI/MIT Press, pp. 153–180.  

Cheng, X., and Wallace, J.M. (1993) Cluster analysis of the Northern Hemisphere wintertime 
500-hPa height field: spatial patterns. Journal of the Atmospheric Sciences, 50, pp. 2674–
2696.  

Cherkassky, V.S., and Muller, F. (1998). Learning from Data: Concepts, Theory, and 
Methods. New York: Wiley.  

Chernoff, H. (1973) The use of faces to represent points in k-dimensional space graphically. 
Journal of the American Statistical Association, 68, pp. 361–368.  

Chickering, D.M., and Heckerman, D. (1997) Efficient approximations for the marginal 
likelihood of Bayesian networks with hidden variables. Machine Learning, 29(2/3), pp. 181–
244.  

Chickering, D.M., Heckerman, D., and Meek, C. (1997) A Bayesian approach to learning 
Bayesian networks with local structure. Proceedings of Thirteenth Conference on Uncertainty 
in Artificial Intelligence. San Francisco, CA: Morgan Kaufmann, pp. 80–89.  

Chipman, H., George, E.I., and McCulloch, R.E. (1998) Bayesian CART model search (with 
discussion). Journal of the American Statistical Association, 93, pp. 935–960.  

Clark, P., and Niblett, T. (1989) The CN2 induction algorithm. Machine Learning, 3(4), pp. 
261–283.  

Clearwater, S., and Stern, E. (1991) A rule-learning program in high-energy physics event 
classification. Computational Physics Communications, 67, pp. 159–182.  

Cleveland, W.S., and McGill, M.E. (eds.) (1988) Dynamic Graphics for Statistics. Belmont, 
CA: Wadsworth and Brooks/Cole.  

Cleveland, W.S., and Devlin, S.J. (1988) Locally-weighted regression: An approach to 
regression analysis by local fitting. Journal of the American Statistical Association, 83, pp. 
597–610.  

Cochran, W.G. (1977) Sampling Techniques. New York: Wiley.  

Cohen, W. (1995) Fast effective rule induction. Proceedings of the Twelfth International 
Conference on Machine Learning, San Mateo, CA: Morgan Kaufmann, pp. 115–123.  

Cook, R.D., and Weisberg, S. (1994) An Introduction to Regression Graphics. New York: 
Wiley.  

Cook, R.D., and Weisberg, S. (1999) Applied Regression Including Computing and Graphics. 
New York: Wiley.  

Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., and Schrijver, A. (1998) Combinatorial 
Optimization. New York: Wiley.  

Corman, T.H., Leiserson, C. E., and Rivest, R.L. (1990) Introduction to Algorithms. 
Cambridge, MA: MIT Press.  

Cortes, C., and Pregibon, D. (1998) Giga-mining. In Proceedings of the Fourth International 
Conference on Knowledge Discovery and Data Mining, R. Agrawal and P. Stolorz (eds.), 
Menlo Park, CA: AAAI Press, pp. 174–178.  

Cox D.R., and Wermuth, N. (1996) Multivariate Dependencies: Models, Analysis, and 
Interpretation. London: Chapman and Hall.  

Cox, D.R., and Hinkley, D.V. (1974) Theoretical Statistics. London: Chapman and Hall.  

Cox, T.F., and Cox, M.A.A. (1994) Multidimensional Scaling. London: Chapman and Hall.  

Crawford, S.L. (1989) Extensions to the CART algorithm. International Journal of Man-
Machine Studies, 31, pp. 197–217.  

Cressie, N.A.C. (1981) Statistics for Spatial Data, New York: Wiley.  

Crowder, M. J., and Hand, D. J. (1990) Analysis of Repeated Measures. London: Chapman 
and Hall.  

Daly, F., Hand, D.J., Jones, M.C., Lunn, A.D., and McConway, K. (1995) Elements of 
Statistics, Wokingham, U.K.: Addison-Wesley.  

Dasarathy, B.V. (ed.) (1991) Nearest Neighbor (NN) Norms: NN Pattern Classification 
Techniques. Los Alamitos, CA: IEEE Computer Society Press.  

Davidson, M.L. (1983) Multidimensional Scaling. New York: Wiley.  

Dawes, R.M., and Smith, T.L. (1985) Attitude and opinion measurement. In The Handbook of 
Social Psychology, Volume I (3rd edition), G. Lindzey and E. Aronson (eds.), New York: 
Random House, pp. 509–566.  

Dawid, A.P. (1984) Statistical theory: The prequential approach (with discussion). Journal of 
the Royal Statistical Society A, 147, pp. 178–292.  

Deerwester, S., Dumais, S.T., Furnas, G.W., Landauer, T.K., and Harshman, R. (1990) 
Indexing by latent semantic analysis. Journal of the American Society for Information Science, 
41, pp. 391–407.  

DeFinetti, B. (1974, 1975) Theory of Probability, Vols. 1 and 2. Chichester, U.K.: Wiley.  

Della Pietra, S., Della Pietra, V., and Lafferty, J. (1997) Inducing features of random fields. 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4), pp. 380–393.  

Dempster, A.P., Laird, N.M., and Rubin, D.B. (1977) Maximum likelihood from incomplete 
data via the EM algorithm (with discussion). Journal of the Royal Statistical Society B, 39, pp. 
1–38.  

Devijver, P.A., and Kittler, J. (1982) Pattern Recognition: A Statistical Approach. Englewood 
Cliffs, NJ: Prentice-Hall.  

Devroye, L. (1984) Nonparametric Density Estimation: the L1 View. New York: Wiley.  

Devroye, L., Gyorfi, L., and Lugosi, G. (1996) A Probabilistic Theory of Pattern Recognition. 
New York: Springer-Verlag.  

Devroye, L.P., and Wagner, T.J. (1982) Nearest neighbor methods in discrimination. In 
Handbook of Statistics, vol. 2, P.R. Krishnaiah and L.N. Kanal, (eds.) Amsterdam: North-
Holland, pp. 193–197.  

Diaconis, P., and Shahshahani, M. (1984) On non-linear functions of linear combinations. 
SIAM Journal of Scientific Computing, 5, pp. 175–191.  

Diebolt, J., and Robert, C.P. (1994) Bayesian estimation of finite mixture distributions. Journal 
of the Royal Statistical Society B, 56, pp. 363–375.  

Dietterich, T.G. (1998) Approximate statistical tests for comparing supervised classification 
learning algorithms. Neural Computation, 10(7) pp. 1895–1924.  

Digby, P., and Kempton, R. (1987) Multivariate Analysis of Ecological Communities. London: 
Chapman and Hall.  

Diggle, P.J., Liang, K-Y., and Zeger, S.L. (1994) Analysis of Longitudinal Data. Oxford, U.K.: 
Clarendon Press.  

Domingos, P. (1996) Unifying instance-based and rule-based induction. Machine Learning, 
24, pp. 141–168.  

Domingos, P. (1999) A general method for making classifiers cost-sensitive. Proceedings of 
the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 
New York: ACM Press, pp. 155–164.  

Domingos, P., and Hulten, G. (2000) Mining high-speed data streams. Proceedings of the 
Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, New 
York:ACM Press, pp.71–80.  

Domingos, P., and Pazzani, M. (1997) On the optimality of the simple Bayesian classifier 
under zero-one loss. Machine Learning, 29, pp. 103–130.  

Draper, N.R., and Smith, H. (1981) Applied Regression Analysis, New York: Wiley.  

Dryden, I.L., and Mardia, K.V. (1998) Statistical Shape Analysis. Chichester, UK: Wiley.  

Du Mouchel, W., Volinsky, C., Johnson, T., Cortes, C., and Pregibon, D. (1999) Squashing 
flat files flatter. Proceedings of the Fifth ACM SIGKDD International Conference on 
Knowledge Discovery and Data Mining, New York: ACM Press, pp. 6–15.  

Duda, R.O., and Hart, P.E. (1973) Pattern Recognition and Scene Analysis, New York: Wiley.  

Duda, R.O., Hart, P.E., and Stork, D.J. (2001) Pattern Recognition New York: Wiley.  

Dumais, S.T., Platt, J., Heckerman, D., and Sahami, M. (1998) Inductive learning algorithms 
and representations for text categorization. Proceedings of the ACM Seventh International 
Conference on Information and Knowledge Management, New York: ACM Press, pp. 148–
155.  

Dunn, G. (1989) Design and Analysis of Reliability Studies. London: Arnold.  

Durbin, R., Eddy, S., Krogh, A., and Mitchison, G. (1998) Biological Sequence Analysis: 
Probabilistic Models of Proteins and Nucleic Acids. Cambridge, U.K.: Cambridge University 
Press.  

Edwards, D. (1995) Introduction to Graphical Modeling. New York: Springer Verlag.  

Edwards, A.W.F. (1972) Likelihood. Baltimore, MD: Johns Hopkins University Press, 
expanded edition. 

Efron, B., and Tibshirani, R.J. (1993) An Introduction to the Bootstrap, New York: Chapman 
and Hall.  

Ein-Dor, P., and Feldmesser, J. (1987) Attributes of the performance of central processing 
units: a relative performance prediction model. Communications of the ACM, 30, pp. 308–
317.  

Eisen, M.B., Spellman, P.T., Brown, P.O., and Botstein, D. (1998) Cluster analysis and 
display of genome-wide expression patterns. Science, 95(25), pp. 14863–68.  

Elliott, R.J., Aggoun, L., and Moore, J.B. (1995) Hidden Markov Models. New York: Springer-
Verlag.  

Everitt, B.S. (1981) A Monte Carlo investigation of the likelihood ratio test for the number of 
components in a mixture of normal distributions. Multivariate Behavioural Research, 16, pp. 
171–180.  

Everitt, B.S., and Hand, D.J. (1981) Finite Mixture Distributions. London: Chapman and Hall.  

Everitt, B.S., and Dunn, G. (1991) Applied Multivariate Data Analysis. New York: Halstead 
Press.  

Everitt, B.S., Gourlay, A.J., and Kendell, R.E. (1971) An attempt at validation of traditional 
psychiatric syndromes by cluster analysis. British Journal of Psychiatry, 138, pp. 336–339.  

Faloutsos, C., Barber, R., Flickner, M., Hafner, J., Niblack, W., Petkovic, D., and Equitz, W. 
(1994) Efficient and effective querying by image content. Journal of Intelligent Information 
Systems, 3, pp. 231–262.  

Faloutsos, C., Ranganathan, M., and Manolopoulos, Y. (1994) Fast subsequence matching in 
time-series databases. Proceedings of the 1994 Annual ACM SIGMOD Conference, New 
York, NY: ACM Press, pp. 419–429.  

Fan, J., and Gijbels, I. (1996) Local Polynomial Modeling and its Applications. London: 
Chapman and Hall.  

Fawcett, T., and Provost, F. (1997) Adaptive fraud detection. Data Mining and Knowledge 
Discovery, 1(3), pp. 291–316.  

Fayyad, U.M., Djorgovski S.G., and Weir N. (1996) Automating the analysis and cataloging of 
sky surveys. In Advances in Knowledge Discovery and Data Mining  U.M. Fayyad, G. 
Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy (eds.), Menlo Park, CA: AAAI Press, pp. 
471–493.  

Fayyad, U.M., Piatetsky-Shapiro, G., and Smyth, P. (1996) From data mining to knowledge 
discovery: an overview. In Advances in Knowledge Discovery and Data Mining, U.M. Fayyad, 
G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy (eds.). Menlo Park, CA: AAAI Press. pp. 
1–34.  

Feller, W. (1968) An Introduction to Probability Theory and its Applications, Vol. 1 (3rd ed.) 
New York: Wiley.  

Feng, Z.D., and McCulloch, C.E. (1996) Using bootstrap likelihood ratios in finite mixture 
models. Journal of the Royal Statistical Society B, 58(3), pp. 609–617.  

Fenton, N.E. (1991) Software Metrics. London: Chapman and Hall.  

Fine, T.L. (1999) Feedforward Neural Network Methodology. NewYork: Springer.  

Fisher, R.A. (1936) The use of multiple measurements on taxonomic problems. Annals of 
Eugenics, 7, pp. 179–188.  

Fletcher, R. (1987) Practical Methods of Optimization. New York: Wiley.  

Flickner, M., Sawhney, H., Niblack, W., Ashley, J., Huang, Q., Dom, B., Gorkani, M., Hafner, 
J., Lee, D., Petkovic, D., Steele, D., and Yanker, P. (1995) Query by image and video content. 
IEEE Computer, 28(9), pp. 23–31.  

Florek K., Lukasziwicz J., Perkal J., Steinhaus H., and Zubrzycki S. (1951) Sur la liaison et la 
division des points d'un ensemble fini. Colloquium Mathematicum, 2, pp. 282–285.  

Frakes, W.B., and Baeza-Yates, R. (eds.) (1992) Information Retrieval: Data Structures and 
Algorithms, Englewood Cliffs, N.J.: Prentice Hall.  

Fraley, C., and Raftery, A.E. (1998) How many clusters? Which clustering method? answers 
via model-based cluster analysis. Computer Journal, 41, pp. 578–588.  

Freund, Y., and Schapire, R.E. (1996) Experiments with a new boosting algorithm. In 
Proceedings of the Thirteenth International Conference on Machine Learning, San Francisco, 
CA: Morgan Kaufmann, pp. 148–156.  

Friedman, J. (1997) On bias, variance, 0/1 loss, and the curse of dimensionality. Data Mining 
and Knowledge Discovery, pp. 55–77.  

Friedman, J.H. (1991) Multivariate adaptive regression splines (with discussion). Annals of 
Statistics, 19, pp. 1–141.  

Friedman, J.H. and Stuetzle, W. (1981) Project pursuit regression. Journal of the American 
Statistical Association, 76, pp. 817–823.  

Friedman, J.H., and Fisher, N.I. (1999) Bump hunting in high dimensional data (with 
discussion). Statistics and Computing, 9, pp. 123–162.  

Friedman, J.H.F., Hastie, T., and Tibshirani, R. (2000) Additive logistic regression: a statistical 
view of boosting, Annals of Statistics, 28, 377–386.  

Friedman, N., and Goldszmidt, M. (1996) Learning Bayesian networks with local structure. 
Proceedings of Twelfth Conference on Uncertainty in Artificial Intelligence, San Francisco, 
CA: Morgan Kaufmann, pp. 252–262.  

Fukuda, T., Morimoto, Y., Morishita, S., and Tokuyama, T. (1996) Mining optimized 
association rules for numeric attributes. Proceedings of the 15th ACM SIGACT-SIGMOD-
SIGART Symposium on Principles of Database and Knowledgebase Systems (PODS'96), 
New York: ACM Press, pp. 182–191.  

Fukunaga, K. (1990) Introduction to Statistical Pattern Recognition, San Diego, CA: Academic 
Press.  

Fukunaga, K., and Flick, T.E. (1984) An optimal global nearest neighbor metric. IEEE 
Transactions on Pattern Recognition and Machine Intelligence, 6, pp. 314–318.  

Furnival, G.M., and Wilson, R.W. (1974) Regression by leaps and bounds. Technometrics, 
16, pp. 499–511.  

Gaffney, S., and Smyth, P. (1999) Trajectory clustering with mixtures of regression models. In 
Proceedings of the ACM 1999 Conference on Knowledge Discovery and Data Mining, New 
York, NY: ACM Press, pp. 63–72.  

Ganti, V., Gehrke, J., and Ramakrishnan, R. (1999) Mining very large databases. IEEE 
Computer, 32, pp. 38–45.  

Garcia-Molina, H., Ullman, J.D., and Widom, J. (1999) Database System Implementation. 
Englewood Cliffs, NJ: Prentice Hall.  

Ge, X., and Smyth, P. (2000) Deformable Markov model templates for time series pattern-
matching. Proceedings of the ACM Seventh International Conference on Knowledge 
Discovery and Data Mining, New York: ACM Press, pp. 81–90.  

Gehrke, J., Ganti, V., Ramakrishnan, R., and Loh, W–Y. (1999) BOAT—optimistic decision 
tree construction. Proceedings of the 1999 ACM SIGMOD conference. New York: ACM 
Press, pp. 169–180.  

Gehrke, J.E., Ramakrishnan, R., and Ganti, V. (1998) RainForest—a framework for fast 
decision tree construction of large datasets. Proceedings of the 24th International Conference 
on Very Large Databases (VLDB'98), pp. 416–427.  

Gelman, A., Carlin, J.B., Stern, H.S., and Rubin, D.B. (1995) Bayesian Data Analysis, 
London: Chapman and Hall.  

Geman, S., Bienenstock, E., and Doursat, R. (1992) Neural networks and the bias-variance 
dilemma. Neural Computation, 4(1), pp. 1–58.  

Gilks, W.R., Richardson, S., and Spiegelhalter, D.J. (1996) Markov Chain Monte Carlo in 
Practice. London: Chapman and Hall.  

Gill, P.E., Murray, W., and Wright, M.H. (1981) Practical Optimization. New York: Academic 
Press.  

Glymour, C., Madigan, D., Pregibon, D., and Smyth, P. (1997) Statistical themes and lessons 
for data mining. Data Mining and Knowledge Discovery. 1(1), pp. 11–28.  

Goer, J.C. (1967) A comparison of some methods of cluster analysis. Biometrics, 23, pp. 
623–628.  

Golden, R.M. (1996) Mathematical Methods for Neural Network Analysis and Design. 
Cambridge, MA: MIT Press.  

Goldstein, H. (1995) Multilevel Statistical Models (2nd ed.). London: Arnold.  

Gordon, A. (1981) Classification: Methods for the Exploratory Analysis of Multivariate Data. 
London: Chapman and Hall.  

Gower, J.C. (1974) Maximal predictive classification. Biometrics, 30, pp. 643–654.  

Gower, J.C., and Hand, D.J. (1996) Biplots. London: Chapman and Hall.  

Gray, J., Bosworth, A., Layman, A., and Pirahesh, H. (1996) Data cube: a relational 
aggregation operator generalizing group-by, cross-tab, and subtotals. 12th International 
Conference on Data Engineering (ICDE'96), New Orleans, Louisiana, pp. 152–159.  

Gray, J., Chaudhuri, S., Bosworth, A., Layman, A., Reichart, D., Venkatrao, M., Pellow, F., 
and Pirahesh, H. (1997) Data Cube: A relational aggregation operator generalizing group-by, 
cross-tab, and sub-totals. Data Mining and Knowledge Discovery, 1, pp. 29–53.  

Grenander, U. (1996) Elements of Pattern Theory. Baltimore, MD: Johns Hopkins University 
Press.  

Grimmett, G.R., and Stirzaker, D.R. (1992) Probability and Random Processes. (2nd ed.) 
Oxford: Clarenden Press.  

Gusfield, D. (1997) Algorithms on Strings, Trees and Sequences. New York, NY: Cambridge 
University Press.  

Hall, D.J., and Ball, G.B. (1965) ISODATA: A novel method of cluster analysis and pattern 
classification. Technical Report, Stanford Research Institute, Menlo Park, California.  

Halstead, M.H. (1977) Elements of Software Science. New York: Elsevier.  

Hamilton, J.D. (1994) Time Series Analysis. Princeton, NJ: Princeton University Press.  

Hamming, R.W. (1991) The Art of Probability for Scientists and Engineers, Redwood City, 
CA: Addison-Wesley.  

Han, J., and Fu, Y. (1995) Discovery of multiple-level association rules from large databases, 
Proceedings of the Twenty First International Conference on Very Large Data Bases 
(VLDB'95), San Mateo, CA: Morgan Kaufmann, pp. 420–431.  

Han, J., and Kamber, M. (2000) Data Mining: Concepts and Techniques, San Francisco, CA: 
Morgan Kaufmann.  

Hand, D.J. (1981) Discrimination and Classification. Chichester, U.K.: Wiley.  

Hand, D.J. (1982) Kernel Discriminant Analysis. Chichester, U.K.: Research Studies Press.  

Hand, D.J. (1986) Recent advances in error rate estimation. Pattern Recognition Letters, 4, 
pp. 335–346.  

Hand, D.J. (1996) Statistics and the theory of measurement (with discussion). Journal of the 
Royal Statistical Society, Series A, 159, pp. 445–492.  

Hand, D.J. (1997) Construction and Assessment of Classification Rules. London: Wiley.  

Hand, D.J., Blunt, G., Kelly, M.G., and Adams, N.M. (2000) Data mining for fun and profit. 
Statistical Science, 15, pp. 111–131.  

Hand, D.J., and Crowder, M.J. (1996) Practical Longitudinal Data Analysis. London: 
Chapman and Hall.  

Hand, D.J., Daly, F., Lunn, A.D., McConway, K.J., and Ostrowski, E. (eds.) (1994) A 
Handbook of Small Data Sets. London: Chapman and Hall.  

Hand, D.J., McConway, K.J., and Stanghellini, E. (1997) Graphical models of applicants for 
credit. IMA Journal of Mathematics Applied in Business and Industry, 8, pp. 143–155.  

Hand, D.J., and Yu, K. (1999) Idiot's Bayes—not so stupid after all? Working paper. 
Department of Mathematics, Imperial College, London.  

Harman, D.K. (1993) The First Text Retrieval Conference (TREC-1), NIST SP 500-207, 
National Institute of Standards and Technology, Gaithersburg, Md.: (annual series, 1993–
1999). 

Harman, D.K., (1995) Hypertext—Information Retrieval—Multimedia: Synergieeffekte 
Elektronischer Informationssysteme, Proceedings of HIM'95, R. Kuhlen and M. Rittberger 
(eds.), Konstanz, Germany: Universitaetsforlag Konstanz, pp. 9–28.  

Harrison, D. (1993) Backing up. Neural Computing, pp. 98–104.  

Harvey, A.C. (1989) Forecasting, Structural Time Series Models, and the Kalman Filter. 
Cambridge, UK: Cambridge University Press.  

Hastie, T., and Tibshirani, R.J. (1990) Generalized Additive Models. London: Chapman and 
Hall.  

Hastie, T., and Tibshirani, R.J. (1996) Discriminant adaptive nearest neighbor classification. 
IEEE Transactions on Pattern Analysis and Machine Intelligence, 18, pp. 607–616.  

Heckerman, D., Chickering, D.M., Meek, C., Rounthwaite, R., and Kadie, C. (2000) 
Dependency networks for inference, collaborative filtering, and data visualization. Journal of 
Machine Learning Research, 1, pp. 49–75.  

Hendry, D.F. (1995) Dynamic Econometrics. New York: Oxford University Press.  

Hilden, J. (1984) Statistical diagnosis based on conditional independence does not require it. 
Computers in Biology and Medicine, 14, pp. 429–435.  

Hjort, J.S.U. (1993) Computer Intensive Statistical Methods: Validation, Model Selection, and 
Bootstrap. Boca Raton, FL: CRC Press.  

Ho, T.K., Hull J.J., and Srihari, S.N. (1994) Decision combination in multiple classifier 
systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16, pp. 66–75.  

Hoffmann, T. (1999) Probabilistic latent sematic indexing. Proceedings of the ACM SIGIR 
Conference 1999, New York: ACM Press, pp. 50–57.  

Holsheimer, M., Kersten, M., Mannila, H., and Toivonen, H. (1995) A perspective on 
databases and data mining. Proceedings of the First International Conference on knowledge 
discovery and data mining, Fayyad, U.M., and Uthu-rusamy, R. (eds.), Menlo Park, CA: AAAI 
Press, pp. 150–155.  

Holte, R.C., (1993) Very simple classification rules perform well on most commonly used data 
sets. Machine Learning, 11, pp. 63–91.  

Huba, G.J., Wingard, J.A., and Bentler, P.M. (1981) A comparison of two latent variable 
causal models for adolescent drug use. Journal of Personality and Social Psychology, 40, pp. 
180–193.  

Huber, P. (1985) Projection pursuit. Annals of Statistics, 13(2), pp. 435–475.  

Huber, P.J. (1980) Robust Statistics. New York: Wiley.  

Hunter, J.S. (1980) The national system of scientific measurement. Science, 210, 21 
November 1980, pp. 869–874.  

Hyvarinen, A. (1999) Survey on independent component analysis. Neural Computing 
Surveys, 2, pp. 94–128.  

Imielinski, T., and Mannila, H. (1996) A database perspective on knowledge discovery. 
Communications of the ACM, 39(11), pp. 58–64.  

Imielinski, T., and Virmani, A. (1999) MSQL: A query language for database mining. Data 
Mining and Knowledge Discovery 3(4), pp. 373–408.  

Imielinski, T., Virmani, A., and Abdulghani, A. (1999) DMajor application programming 
interface for database mining. Data Mining and Knowledge Discovery, 3(4), pp. 347–372.  

Jacoby, W.G. (1997) Statistical Graphics for Univariate and Bivariate Data. London: Sage 
Publications.  

Jain, A., and Dubes, R. (1988) Algorithms for Clustering Data., Englewood Cliffs, Prentice-
Hall.  

Jensen, F.V. (1996) An Introduction to Bayesian Networks. New York: Springer-Verlag.  

Jolliffe, I.T. (1986) Principal Component Analysis. New York: Springer-Verlag.  

Jordan, M.I. (1999) Learning in Graphical Models, Cambridge, MA: MIT Press.  

Jordan, M.I., and Jacobs, R.A. (1994) Hierarchical mixtures of experts and the EM algorithm. 
Neural Computation, 6, pp. 181–214.  

Journal of the American Society for Information Science (1996) Special Issue on Evaluation, 
47:1–105.  

Karypis, G., and Kumar, V. (1998) A parallel algorithm for multilevel graph partitioning and 
sparse matrix ordering. Journal of Parallel and Distributed Computing, 48(1), pp. 71–95.  

Kass, R., and Raftery, A. (1995) Bayes factors. Journal of the American Statistical 
Association, 90, pp. 773–795.  

Kato, T., Kurita, T., and Shimogaki, H. (1991) Intelligent visual interaction with image 
database systems—towards the multimedia personal interface. Information Processing 
(Japan), 14, pp. 134–143.  

Kaufman, L., and Rousseeuw, P.J. (1990) Finding Groups in Data: An Introduction to Cluster 
Analysis. New York: Wiley.  

Keim, D.A., and Kriegel, H.-P. (1994) VisDB: database exploration using multidimensional 
visualization. IEEE Computer Graphics and Applications, September 1994, pp. 40–49.  

Kendall, M.G. (1980) Multivariate Analysis (2nd ed.). London: Griffin.  

Keogh, E., and Smyth, P. (1997) A probabilistic approach to fast pattern matching in time 
series databases. Proceedings of the 3rd International Conference on Knowledge Discovery 
and Data Mining, Menlo Park, CA: AAAI Press, pp. 24–30.  

Kim, C-J., and Nelson, C.R. (1999) State-Space Models with Regime Switching: Classical 
and Gibbs Sampling Approaches with Applications. Cambridge, MA: MIT Press.  

Kirkpatrick, S., Gelatt, C.D. Jr., and Vecchi, M.P. (1983) Optimization by simulated annealing. 
Science, 220, pp. 671–680.  

Kish, L. (1965) Survey Sampling. New York: Wiley.  

Klemettinen, M., Mannila, H., Ronkainen, P., Toivonen, H., and Verkamo, A.I. (1994) Finding 
interesting rules from large sets of discovered association rules. Proceedings of the Third 
International Conference on Information and Knowledge Management (CIKM'94), New York: 
ACM Press, pp. 401–407.  

Knight, K. (2000) Mathematical Statistics, Boca Raton, FL: Chapman and Hall.  

Knuth, D. (1997). The Art of Computer Programming: Fundamental Algorithms, 3rd ed. 
Reading, MA: Addison Wesley.  

Kohavi, R.(1996) Scaling up the accuracy of naive-Bayes classifiers: A decision-tree hybrid. 
Proceedings of the Second International Conference on Knowledge Discovery and Data 
Mining. Portland, OR: AAAI Press, pp. 202–207.  

Koontz, W.L.G., Narendra, P.M., and Fukunaga, K. (1975) A branch and bound clustering 
algorithm. IEEE Transactions on Computers, 24, pp. 908–915.  

Krantz, D.H., Luce, R.D., Suppes, P., and Tversky, A. (1971) Foundations of Measurement, 
Volume 1: Additive and Polynomial Representations. New York: Academic Press.  

Krzanowski, W.J., and Marriott, F.H.C. (1995) Multivariate Analysis vol. 2: Classification, 
Covariance Structures, and Repeated Measurements. London: Arnold.  

Lambert, J.M., and Williams, W.T. (1966) Multivariate methods in plant ecology IV: 
comparison of information analysis and association analysis. Journal of Ecology, 54, pp. 635–
664.  

Lance, G.N., and Williams, W.T. (1967) A general theory of classificatory sorting strategies: 1. 
Hierarchical systems. Computer Journal, 9, pp. 373–380.  

Landauer, T.K., and Dumais, S.T., (1997). A solution to Plato's problem: The latent semantic 
analysis theory of acquisition, induction, and representation of knowledge. Psychological 
Review, 104(2), pp. 211–240.  

Lange, K. (1995) A gradient algorithm locally equivalent to the EM algorithm. Journal of the 
Royal Statistical Society B, 57, pp. 425–437.  

Lange, K. (1999) Numerical Analysis for Statisticians. New York: Springer-Verlag.  

Lapointe, F.J., and Legendre, P. (1994) A classification of pure malt Scotch whiskies. Applied 
Statistics, 43, pp. 237–257.  

Lauritzen, S.L. (1996) Graphical Models. Oxford: Clarendon Press.  

Lavine, M. (1991) Problems in extrapolation illustrated with space shuttle O-ring data. Journal 
of the American Statistical Association, 86, pp. 919–922.  

Lavrac, N., and Dzeroski, S. (1994) Inductive Logic Programming: Techniques and 
Applications. Ellis Horwood.  

Lawrence, R.D., Almasi, G.S., Kotlyar, V., Viveros, M.S., and Duri, S.S. (2001) 
Personalization of supermarket product recommendations, Data Mining and Knowledge 
Discovery, to appear. 

Leamer, E.E. (1978) Specification Searches: Ad Hoc Inference with Experimental Data. New 
York: Wiley.  

Lee, P.M. (1989) Bayesian Statistics: An Introduction. London: Edward Arnold.  

Lehmann, E.L. (1986) Testing Statistical Hypotheses. New York: Wiley.  

Lehmann, E.L., and Casella, G. (1998) Theory of Point Estimation, New York: Springer-
Verlag.  

Leighton, G., and McKinlay, P.L. (1930) Milk Consumption and the Growth of School 
Children. London: HMSO.  

Leinweber, D. (personal communication)Stupid data miner tricks: Overfitting the S&P 500.  

Lewis, H.R., and Papadimitriou, C.H. (1998) Elements of the Theory of Computation, second 
edition. Upper Saddle River, NJ: Prentice-Hall.  

Li, M., and Vitanyi, P. (1993) An Introduction to Kolmogorov Complexity and Its Applications. 
New York: Springer.  

Lindsey, I. (1994) Credit Cards: The Authoritative Guide to Payment and Credit Cards. 
Leighton Buzzard: Rushmere Wynne.  

Lindsey, J.K. (1996) Parametric Statistical Inference. Oxford, U.K.: Clarendon Press.  

Lindsey, J.K. (1999) Models for Repeated Measurements, 2nd ed. Oxford, U.K.: Oxford 
University Press.  

Lindsey, J.K. (1999) Relationships among sample size, model selection and likelihood 
regions, and scientifically important differences. Journal of the Royal Statistical Society, 
Series D, 48, pp. 401–411.  

Linhart, H., and Zucchini, W. (1986) Model Selection. New York: Wiley.  

Little, R.J.A., and Rubin, D.B. (1987) Statistical Analysis with Missing Data. New York: Wiley.  

Looney, C.G. (1997) Pattern Recognition Using Neural Networks. Oxford, U.K.: Oxford 
University Press.  

Lovell, M.C. (1983) Data mining. Review of Economics and Statistics 65(1), pp. 1–12.  

Luce, R.D., Krantz, D.H., Suppes, P., and Tversky, A. (1990) Foundations of Measurement, 
Volume 3: Representation, Axiomatization, and Invariance. San Diego, CA: Academic Press.  

Luenberger, D.G. (1984) Introduction to Linear and Nonlinear Programming. Menlo Park, CA: 
Addison-Wesley.  

MacDonald, I.L., and Zucchini, W. (1997) Hidden Markov and Other Models for Discrete-
valued Time Series. London: Chapman and Hall.  

MacKay, D.J.C. (1992) A practical Bayesian framework for back-propagation networks. 
Neural Computation, 4, pp. 448–472.  

MacMillan, N.A., and Creelman, C.D. (1991) Signal Detection Theory: A User's Guide, New 
York, NY: Cambridge University Press.  

MacNaughton-Smith, P., Williams, W.T., Dale, M.B., and Mockett, L.G. (1964) Dissimilarity 
analysis. Nature, 202, pp. 1034–1035.  

MacQueen, J. (1967) Some methods for classification and analysis of multivariate 
observations. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and 
Probability, L.M. Le Cam, and J. Neyman (eds.) Berkeley: University of California Press, pp. 
281–297.  

Madigan, D., Raghavan, N., DuMouchel, W., Nason, M., Posse, C., and Ridgeway, G. (in 
press) Likelihood-based data squashing: A modeling approach to instance construction. Data 
Mining and Knowledge Discovery.  

Mangasarian, O. (1997) Mathematical programming in data mining. Data Mining and 
Knowledge Discovery, 1(2), pp.183–201.  

Mannila, H. (1997) Inductive databases and condensed representations: Concepts for data 
mining. International Logic Programming Symposium 1997, Cambridge, MA: MIT Press, pp. 
21–30.  

Mannila, H., Toivonen, H., and Verkamo, A.I. (1994) Efficient algorithms for discovering 
association rules. Knowledge Discovery in Databases: Papers from the AAAI-94 Workshop 
(KDD'94), Menlo Park, CA: AAAI Press, pp. 181–192.  

Mannila, H., Toivonen, H., and Verkamo, A.I. (1997) Discovery of frequent episodes in 
sequences. Data Mining and Knowledge Discovery. 1(3), pp. 259–290.  

Maritz, J.S. (1981) Distribution-Free Statistical Methods. London: Chapman and Hall.  

Marriott, F.H.C. (1971) Practical problems in a method of cluster analysis. Biometrics, 27, pp. 
501–514.  

Maybury, M.T. (ed.)  (1997) Intelligent Multimedia Information Retrieval. Menlo Park, CA: AAAI 
Press.  

McCullagh, P., and Nelder, J.A. (1989) Generalized Linear Models, 2nd ed. London: 
Chapman and Hall.  

McKendrick, A.G. (1926) Applications of mathematics to medical problems. Proceedings of 
the Edinburgh Mathematical Society, 44, pp. 98–130.  

McLachlan, G.J. (1987) Error rate estimation in discriminant analysis: recent advances. In 
Advances in Multivariate Statistical Analysis, A.K. Gupta, ed. The Netherlands: Reidel, pp. 
233–252.  

McLachlan, G.J. (1987) On bootstrapping the likelihood ratio test for the number of 
components in a normal mixture. Applied Statistics, 36, pp. 318–324.  

McLachlan, G.J. (1992) Discriminant Analysis and Statistical Pattern Recognition. New York: 
Wiley.  

McLachlan, G.J., and Basford, K.E. (1988) Mixture Models: Inference and Applications to 
Clustering. New York: Marcel Dekker.  

McLachlan, G.J., and Krishnan, T. (1998) The EM Algorithm and Extensions. New York: 
Wiley.  

McLachlan, G.J., and Peel, D. (1997) On a resampling approach to choosing the number of 
components in normal mixture models. In Computing Science and Statistics (Vol 28), L. 
Billard, and N.I. Fisher (eds.). Fairfax Station, VA: Interface Foundation of North America, pp. 
260–266.  

McLachlan, G.J., and Peel, D. (1998) MIXFIT: An algorithm for the automatic fitting and 
testing of normal mixture models. Proceedings of the 14th International Conference on 
Pattern Recognition, vol. 1, Los Alamitos, CA: IEEE Computer Society, pp. 553–557.  

McLachlan, G.J., and Peel, D. (2000) Finite Mixture Models. New York: Wiley.  

McLaren, C.E. (1996) Mixture models in haematology: A series of case studies. Statistical 
Methods in Medical Research, 5, pp. 129–153.  

Meilijson, I. (1989) A fast improvement to the EM algorithm on its own terms. Journal of the 
Royal Statistical Society B, 51, pp. 127–138.  

Mendell, N.R., Finch, S.J., and Thode, H.C. (1993) Where is the likelihood ratio test powerful 
for detecting two component normal mixtures? Biometrics, 49, pp. 907–915.  

Meo, R., Psaila, G., and Ceri, S. (1996) A new SQL-like operator for mining association rules. 
Proceedings of the 22nd International Conference on Very Large Data Bases (VLDB'96), San 
Mateo, CA: Morgan Kaufmann.  

Michell, J. (1986) Measurement scales and statistics: A clash of paradigms. Psychological 
Bulletin, 100, pp. 398–407.  

Michell, J. (1990) An Introduction to the Logic of Psychological Measurement. Hillsdale: 
Lawrence Erlbaum.  

Mitchell, M. (1997) An Introduction to Genetic Algorithms. Cambridge, MA: MIT Press.  

Mitchell, T. (1997) Machine Learning, New York: McGraw-Hill.  

Moore, A. (1999) Very fast EM-based mixture model clustering using multiresolution kd-trees. 
In Advances in Neural Information Processing Systems 12, San Francisco, CA: Morgan 
Kaufmann.  

Moore, A.W. (1999) Cached sufficient statistics for automated discovery and data mining from 
massive data sources. Online white paper, Department of Computer Science, Carnegie 
Mellon University, Pittsburgh, PA.  

Moore, A.W., and Lee, M. (1998) Cached sufficient statistics for efficient machine learning 
with large data sets. Journal of Artificial Intelligence Research, 8, pp. 67–91.  

Morgan, B.J.T. (1981) Three applications of methods of cluster analysis. The Statistician, 30, 
pp. 205–223.  

Morgan, J.N., and Sonquist, J.A. (1963) Problems in the analysis of survey data, and a 
proposal. Journal of the American Statistical Association, 58, pp. 415–434.  

Mosteller, F. (1968) Nonsampling errors. In International Encyclopedia of the Social Sciences, 
5, D.L. Sills (ed.), New York: MacMillan and Free Press, pp. 113–132.  

Muggleton, S. (1995) Foundations of Inductive Logic Programming, Englewood Cliffs, NJ: 
Prentice Hall.  

Murthy, S.K. (1998) Automatic construction of decision trees from data: A multi-disciplinary 
survey. Data Mining and Knowledge Discovery, 2, pp. 345–389.  

Myles, J.P., and Hand, D.J. (1990) The multi-class metric problem in nearest neighbour 
discrimination rules. Pattern Recognition, 23, pp. 1291–1297.  

Nakhaeizadeh, G., and Taylor, C.C. (eds.) (1997) Machine Learning and Statistics. New York: 
Wiley.  

Neal, R. (1996) Bayesian Learning for Neural Networks. Lecture Notes in Statistics 118, New 
York: Springer.  

Neal, R., and Hinton, G. (1998) A view of the EM algorithm that justifies incremental, sparse, 
and other variants. In Learning in Graphical Models, Jordan, M.I. (ed.), Cambridge, MA: MIT 
Press, pp. 355–371.  

Nering, E.D., and Tucker, A.W. (1993) Linear Programs and Related Problems. Academic 
Press Inc.  

Newcomb, S. (1886) A generalized theory of the combination of observations so as to obtain 
the best result. American Journal of Mathematics, 8, pp. 343–366.  

Nightingale, F. (1858) Notes on Matters Affecting the Health, Efficiency, and Hospital 
Administration of the British Army, founded chiefly on the Experience of the Late War. 
London: Harrison.  

Oliver, J.J., and Hand, D.J. (1996) Averaging over decision trees. Journal of Classification, 
13, pp. 281–297.  

Papadimitriou, C.H., and Steiglitz, K (1982) Combinatorial Optimization—Algorithms and 
Complexity. Englewood Cliffs, NJ: Prentice-Hall.  

Park, J.S., Chen, M.S., and Yu, P.S. (1995) An effective hash-based algorithm for mining 
association rules. Proceedings of the ACM SIGMOD Conference on Management of Data 
(SIGMOD'95), New York: ACM Press, pp. 175–186.  

Pearl, J. (1984) Heuristics: Intelligent Search Strategies for Computer Problem Solving. 
Reading, MA: Addison-Wesley.  

Pearl, J. (1988) Probabilistic Reasoning in Intelligent Systems, San Mateo, CA: Morgan 
Kaufmann.  

Peixoto, J.L. (1990) A property of well-formulated polynomial regression models. American 
Statistician, 44, pp. 26–30.  

Pentland, A., Picard, R.W., and Sclaroff, S. (1994) Photobook: Tools for content -based 
manipulation of image databases. International Journal of Computer Vision, 18, pp. 233–254.  

Piatetsky-Shapiro, G. (1991) Discovery, analysis, and presentation of strong rules. In 
Knowledge Discovery in Databases. G. Piatetsky-Shapiro and W. Frawley (eds.), Menlo Park, 
CA: AAAI Press.  

Piatetsky-Shapiro, G. (1999) The data-mining industry coming of age. IEEE Expert, 14(6), pp. 
32–34.  

Platt, J. (1999) Fast training of support vector machines using sequential minimal 
optimization. In Advances in Kernel Methods—Support Vector Learning, B. Scholkopf, C.J.C. 
Burges, and A.J. Smola (eds.), Cambridge, MA: MIT Press, pp. 185–208.  

Poulsen, C.S. (1990) Mixed Markov and latent Markov modelling applied to brand choice 
behavior. International Journal of Research in Marketing, 7, pp. 5–19.  

Press, W.H., Flannery, B.P., Teukolsky, S.A., and Vetterling, W.T. (1988) Numerical Recipes 
in C: The Art of Scientific Computing. Cambridge, UK: Cambridge University Press.  

Provost, F., and Kolluri, V. (1999) A survey of methods for scaling up inductive algorithms. 
Data Mining and Knowledge Discovery, 3, pp. 131–169.  

Provost, F., Jensen, D., and Oates, T. (1999) Efficient progressive sampling. Proceedings of 
the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 
New York: ACM Press, pp. 23–32.  

Quandt, R.E., and Ramsey, J.B. (1978) Estimating mixtures of normal distributions and 
switching regressions. Journal of the American Statistical Association, 73(364), 730–738.  

Quinlan, J.R. (1986) Induction of decision trees. Machine Learning, 1, pp. 81–106.  

Quinlan, J.R. (1987) Generating production rules from decision trees. Proceedings of the 
Tenth International Joint Conference on Artificial Intelligence, San Mateo, CA: Morgan 
Kaufmann, pp. 304–307.  

Quinlan, J.R. (1990) Learning logical definitions from relations, Machine Learning, 5, pp. 239–
266.  

Quinlan, J.R. (1993) C4.5: Programs for Machine Learning. San Mateo, CA: Morgan 
Kaufmann.  

Raghavan, V.V., and Wong, S.K.M. (1986) A critical analysis of the vector space model for 
information retrieval. Journal of the American Society for Information Science, 37(5), pp. 100–
124.  

Ramakrishnan, R., and Gehrke, J. (1999) Database Management Systems, Second Edition. 
New York: McGraw Hill.  

Ramsey, J.O., and Silverman, B.W. (1996) Functional Data Analysis. New York: Springer-
Verlag.  

Randles, R.H., and Wolfe, D.A. (1979) Introduction to the Theory of Nonparametric Statistics. 
New York: Wiley.  

Rao, M.R. (1971) Cluster analysis and mathematical programming. Journal of the American 
Statistical Association, 66, pp. 622–626.  

Rastogi, R., and Shim, K. (1998) PUBLIC: A decision tree classifier that integrates building 
and pruning. Proceedings of the 24th International Conference on Very Large Databases 
(VLDB'98), pp. 405–415.  

Redner, R.A., and Walker, H.F. (1984) Mixture densities, maximum likelihood, and the EM 
algorithm. SIAM Review, 26, pp. 195–239.  

Resnick, P., Iacovou, N., Suchak, M., Bergstrom, P., and Riedl, J. (1994) GroupLens: An 
open architecture for collaborative filtering of netnews. Proceedings of the ACM 1994 
Conference on Computer Supported Cooperative Work, Chapel Hill, N.C.: ACM Press, pp. 
175–186.  

Reyment, R., and Jöreskog K.G. (1993) Applied Factor Analysis in the Natural Sciences, 
Cambridge: Cambridge University Press.  

Ridgeway, G. (1997) Finite discrete Markov process clustering. Technical Report TR 97-24, 
Microsoft Research, Redmond, WA.  

Ripley, B.D. (1996) Pattern Recognition and Neural Networks. Cambridge, U.K.: Cambridge 
University Press.  

Rissanen, J. (1987) Stochastic complexity (with discussion). Journal of the Royal Statistical 
Society, Series B, 49, pp. 223–239 and pp. 253–265. 

Robbins, H., and Monro, S. (1951) A stochastic approximation method. Annals of 
Mathematical Statistics, 22, pp. 400–407.  

Roberts, F.S. (1979) Measurement Theory with Applications to Decision-making, Utility, and 
the Social Sciences. Reading: Addison-Wesley.  

Rocchio, J.J. (1971) Relevance feedback in information retrieval. The SMART Retrieval 
System: Experiments in Automatic Document Processing, Salton, G. (ed.). Englewood Cliffs, 
N.J.: Prentice Hall, pp. 313–323.  

Ross, S.M. (1997) Introduction to Probability Models. San Diego, CA: Academic Press, 6th 
ed.  

Rui, Y., Huang, T.S., Ortega, M., and Mehrotra, S. (1997) Relevance feedback: a power tool 
in interactive content-based image retrieval. Proceedings of the IEEE Transactions on Circuits 
and Systems for Video Technology, 8(5), pp. 644–655.  

RuleQuest Research (2000) http://www.rulequest.com/cubist-info.html.  

Russek, E., Kronmal, R.A., and Fisher, L.D. (1983) The effect of assuming independence in 
applying Bayes' theorem to risk estimation and classification in diagnosis. Computers and 
Biomedical Research, 16, pp. 537–552.  

Salton, G. (ed.)  (1971) The SMART Retrieval System: Experiments in Automatic Document 
Processing. Englewood Cliffs, N.J.: Prentice Hall.  

Salton, G., and Buckley, C. (1988) Term-weighting approaches in automatic text retrieval. 
Information Processing and Management, 24:513–523.  

Salton, G., and Buckley, C. (1990) Improving retrieval performance by relevance feedback. 
Journal of the American Society of Information Science, 41(4), pp. 288–297.  

Salton, G., and McGill, M. (1983) Introduction to Modern Information Retrieval, New York: 
McGraw Hill.  

Salzberg, S. (1997) On comparing classifiers: Pitfalls to avoid and a recommended approach. 
Data Mining and Knowledge Discovery, 1(3), pp. 317–327.  

Salzberg, S. (1999) Gene discovery in DNA sequences. IEEE Expert, 14(6), pp. 44–48.  

Sarawagi, S., Thomas, S., and Agrawal, R. (1998) Integrating mining with relational database 
systems: Alternatives and implications. Proceedings of the ACM SIGMOD Conference on 
Mangement of Data (SIGMOD 1998), New York: ACM Press, pp. 343–354.  

Sarawagi, S., Thomas, S., and Agrawal, R. (2000) Integrating association rule mining with 
relational database systems: alternatives and implications. Data Mining and Knowledge 
Discovery, 4, pp. 89–125.  

Savasere, A., Omiecinski, E., and Navathe, S. (1995) An efficient algorithm for mining 
association rules. Proceedings of the 21st International Conference on Very Large Data 
Bases (VLDB'95), San Mateo, CA: Morgan Kaufmann, pp. 432–444.  

Schafer, J.B., Konstan, J., and Riedl, J. (in press) Electronic commerce recommender 
applications. Data Mining and Knowledge Discovery.  

Schaffer, C. (1994) Cross-validation, stacking and bi-level stacking: Meta-methods for 
classification and learning. In Selecting Mo dels from Data: AI and Statistics IV, P. Cheeseman 
and R.W. Oldford (eds.), New York: Springer-Verlag.  

Schapire, R.E., Freund, Y., Bartlett, P., and Lee, W.S. (1998) Boosting the margin: A new 
explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5), pp. 1651–
1686.  

Schervish, M.J. (1995) Theory of Statistics. New York: Springer-Verlag.  

Schiavo, R., and Hand, D.J. (2000) Ten more years of error rate research. International 
Statistical Review, 68, pp. 295–310.  

Scholkopf, B., Burges, C.J.C., and Smola, A.J. (eds.) (1999) Advances in Kernel Methods—
Support Vector Learning. Cambridge, MA: MIT Press.  

Schwarz, G. (1978) Estimating the dimension of a model. Annals of Statistics, 6, pp. 461–464.  

Scott, D.F. (1992) Multivariate Density Estimation: Theory and Visualization. New York: Wiley.  

Segal, R., and Etzioni, O. (1994) Learning decision lists using homogenous rules. 
Proceedings of the Twelfth National Conference on Artificial Intelligence, Menlo Park, CA: 
AAAI Press, pp. 619–625.  

Shafer, G., and Pearl, J. (1990) Readings in Uncertain Reasoning. San Mateo: CA, Morgan 
Kaufman.  

Shafer, J.C., Agrawal, R., and Mehta, M. (1996), SPRINT: A scalable parallel classifier for 
data mining. Proceedings of the 22nd International Conference on Very Large Databases 
(VLDB'96), San Francisco, CA: Morgan Kaufmann, pp. 544–555.  

Shardanand, U., and Maes, P., (1995) Social information filtering: Algorithms for automating 
""word of mouth."" Proceedings of CHI'95-Human Factors in Computing Systems, pp. 210–217.  

Shaw, S.W., Defigueiredo, R.J.P. (1990) Structural processing of waveforms as trees. IEEE 
Transactions on Acoustic, Speech, and Signal Processing, 38(2), pp. 328–338.  

Shepard, R.N., and Arabie, P. (1979) Additive clustering: Representation of similarities as 
combinations of discrete overlapping properties. Psychological Review, 86, pp. 87–123.  

Short, R.D., and Fukunaga, K. (1981) The optimal distance measure for nearest neighbor 
classification. IEEE Transactions on Information Theory, 27, pp. 622–627.  

Shoshani, A. (1997) OLAP and statistical databases: Similarities and differences. 
Proceedings of the Sixteenth ACM SIGACT-SIGMOD-SIGART Symposium on Principles of 
Database Systems, New York: ACM Press, pp. 185–196.  

Sibson, R. (1973) SLINK: An optimally efficient algorithm for the single link method. Computer 
Journal, 16, pp. 30–34.  

Silberschatz, A., and Tuzhilin, A. (1996) What makes patterns interesting in knowledge-
discovery systems, IEEE Transactions on Knowledge and Data Engineering, 8(6), pp. 970–
974.  

Silverman, B.W. (1986) Density Estimation for Statistics and Data Analysis. London: 
Chapman and Hall.  

Simpson, C.H. (1951) The interpretation of interaction in contingency tables. Journal of the 
Royal Statistical Society, Series B, 13, pp. 238–241.  

Smith, J.R., and Chang, S. (1997) Querying by color regions using VisualSEEk content-based 
visual query system. Intelligent Multimedia Information Retrieval, Maybury, M.T. (ed.). Menlo 
Park, CA: AAAI Press, pp. 23–41.  

Smoliar, S., and Zhang, H. (1994) Content-based video indexing and retrieval. IEEE 
Multimedia, 1, pp. 62–72.  

Smyth, P. (1994) Hidden Markov models for fault detection in dynamic systems. Pattern 
Recognition, 27(1), pp.149–164.  

Smyth, P. (1997) Clustering sequences using hidden Markov models. In Advances in Neural 
Information Processing 9, M.C. Mozer, M.I. Jordan, and T. Petsche (eds.), Cambridge, MA: 
MIT Press, pp. 648–654.  

Smyth, P. (1999) Probabilistic model-based clustering of multivariate and sequential data. In 
Proceedings of the Seventh International Workshop on AI and Statistics, D. Heckerman, and 
J. Whittaker eds., San Francisco, CA: Morgan Kaufman, pp. 299–304.  

Smyth, P. (2000) Data mining: Data analysis on a grand scale? Statistical Methods in Medical 
Research. 9, pp. 309–327.  

Smyth, P. (2000) Model selection for probabilistic clustering using cross-validated likelihood. 
Statistics and Computing, 9, pp. 63–72.  

Smyth, P., and Goodman, R. (1992) An information-theoretic approach to rule induction from 
databases. IEEE Transactions on Knowledge and Data Engineering, 4(4), pp. 301–306.  

Smyth, P., Ide, K., and Ghil, M. (1999) Multiple regimes in northern hemisphere height fields 
via mixture model clustering. Journal of the Atmospheric Sciences, 56(21), pp. 3704–3723.  

Sparck Jones, K., and Willett, P. (1997) Readings in Information Retrieval. San Francisco: 
Morgan Kaufmann.  

Späth, H. (1979) Clusterwise linear regression. Computing, 22(4), pp. 367–73.  

Späth, H. (1985) Cluster Analysis and Dissection. Chichester, U.K.: Ellis Horwood.  

Srikant, R., and Agrawal, R. (1995) Mining generalized association rules. Proceedings of the 
21st International Conference on Very Large Data Bases (VLDB'95), San Mateo, CA: Morgan 
Kaufmann, pp. 407–419.  

Srikant, R., and Agrawal, R. (1996) Mining quantitative association rules in large relational 
tables. Proceedings of the ACM SIGMOD Conference on Management of Data (SIGMOD'96), 
New York: ACM Press, pp. 1–12.  

Srikant, R., Vu, Q., and Agrawal, R. (1997) Mining association rules with item constraints. 
Proceedings of the Third International Conference on Knowledge Discovery and Data Mining 
(KDD'97), Heckerman, D., Mannila, H., and Pregibon, D. (eds.). Menlo Park, CA: AAAI Press, 
pp. 67–73.  

Stone, M. (1974) Cross-validatory choice and assessment of statistical predictions (with 
Discussion). Journal of the Royal Statistical Society, Series B, 36, pp. 111–147.  

Streiner, D.L., and Norman, G.R. (1995) Health Measurement Scales, second edition. Oxford: 
Oxford University Press.  

Swanson, D.R. (1987) Two medical literatures that are logically but not bibliographically 
connected. Journal of the American Society for Information Retrieval, 38(4), pp. 228–233.  

Swanson, D.R., and Smalheiser, N.R. (1994) Assessing a gap in the biomedical literature: 
Magnesium deficiency and neurologic disease. Neuroscience Research Communications, 15, 
pp. 1–9.  

Swanson, D.R., and Smalheiser N.R. (1997) An interactive system for finding complementary 
literatures: A stimulus to scientific discovery. Artificial Intelligence, 91, pp. 183–203.  

Suppes, P., Krantz, D.H., Luce, R.D., and Tversky, A. (1989) Foundations of Measurement, 
Volume 2: Geometrical, Threshold, and Probabilistic Representations. San Diego, CA: 
Academic Press.  

Szalay, A.S., Kunszt, P., Thakar, A., and Gray, J. (1999) Designing and mining multi-terabyte 
astronomy archives: The Sloan Digital Sky Survey. Technical Report MS-TR-99-30, San 
Francisco, CA: Microsoft Research.  

Thall, P.F., and Vail, S.C. (1990) Some covariance models for longitudinal count data with 
overdispersion. Biometrics, 46, pp. 657–671.  

Thisted, R.A., (1988) Elements of Statistical Computing. London, Chapman and Hall.  

Titterington, D.M., Smith, A.F.M., and Makov, U.E. (1985) Statistical Analysis of Finite Mixture 
Distributions. Chichester,U.K.: Wiley.  

Toivonen, H. (1996) Sampling large databases for association rules, Proceedings of the 
Twenty Second International Conference on Very Large Data Bases (VLDB'96), San Mateo, 
CA: Morgan Kaufmann, pp. 134–145.  

Toussaint, G.T. (1974) Bibliography on estimation of misclassification. IEEE Transactions on 
Information Theory, 20, pp. 472–479.  

Tsur, D., Ullman, J.D., Abiteboul, S., Clifton, C., Motwani, R., Nestorov, S., and Rosenthal, A. 
(1998) QueryFlocks: A generalization of association rule mining. Proceedings of the ACM 
SIGMOD Conference on Management of Data (SIGMOD'98), New York, NY: ACM Press, pp. 
1–12.  

Tufte, E.R. (1983) The Visual Display of Quantitative Information. Cheshire, CT: Graphics 
Press.  

Tufte, E.R. (1990) Envisioning Information. Cheshire, CT: Graphics Press.  

Tukey, J.W. (1977) Exploratory Data Analysis. Reading, MA: Addison-Wesley.  

Ullman, J.D. (1988) Principles of Database and Knowledge-Base Systems, vol. 1. Rockville, 
MD: Computer Science Press.  

Ullman, J.D., and Widom, J. (1997) A First Course in Database Systems. Upper Saddle River, 
NJ: Prentice-Hall.  

van Laarhoven, P.J.M., and Aarts, E.H.L. (1987) Simulated Annealing: Theory and 
Applications. Dordrecht, Netherlands: D. Reidel.  

Van Rijsbergen, C.J. (1979) Information Retrieval. London: Butterworth Press.  

Vapnik, V. (1995) The Nature of Statistical Learning Theory. Berlin: Springer-Verlag.  

Vapnik, V. (1998) Statistical Learning Theory. Chichester, U.K.: Wiley.  

Wand, M.P., and Jones, M.C. (1995) Kernel Smoothing. London: Chapman and Hall.  

Wang, J.T., Zhang, K., Jeong, K., and Shasha, D. (1994) A system for approximate tree 
matching. IEEE Transactions on Knowledge and Data Engineering, 6(4), 559–571.  

Webb, A. (1999) Statistical Pattern Recognition. London: Arnold.  

Webb, G. (2000) Efficient search for association rules. Proceedings of the ACM Seventh 
International Conference on Knowledge Discovery and Data Mining,  New York, NY: ACM 
Press, pp. 300–304.  

Wedel, M., and Kamakura, W.A. (1998) Market Segmentation: Conceptual and 
Methodological Foundations. Boston, MA: Kluwer.  

Wegman, E.J. (1990) Hyperdimensional data analysis using parallel coordinates. Journal of 
the American Statistical Association, 85(411), pp. 664–675.  

Weiss, S., and Indurkhya, N. (1993) Rule-based regression. Proceedings of the International 
Joint Conference on Artificial Intelligence, IJCAI-93, San Mateo, CA: Morgan Kaufmann, pp. 
1072–1078.  

Weiss, S., and Indurkhya, N. (1995) Rule-based machine learning methods for functional 
prediction. Journal of Artificial Intelligence Research, 3, pp. 383–403.  

Weiss, S.M., and Indurkhya, N. (1998) Predictive Data Mining: A Practical Guide. San 
Francisco, CA: Morgan Kaufmann.  

Whittaker, J. (1990) Graphical Models in Applied Multivariate Statistics, Chichester, U.K.: 
Wiley.  

Wilkinson, L. (1999) The Grammar of Graphics. New York: Springer-Verlag.  

Witten, I.H., and Franke, E. (2000) Data Mining: Practical Machine Learning Tools and 
Techniques with Java Implementations. San Francisco, CA: Morgan Kaufmann.  

Witten, I.H., Moffat, A., and Bell, T.C. (1999) Managing Gigabytes: Compressing and Indexing 
Documents and Images, 2nd ed. San Francisco, CA: Morgan Kaufmann.  

Xu, L., Krzyzak, A., and Suen, C.Y. (1992) Methods of combining multiple classifiers and their 
applications to handwriting recognition. IEEE Transactions on Pattern Analysis and Machine 
Intelligence, 22, pp. 418–435.  

Zamir, O., and Etzioni, O. (1998) Web document clustering: A feasibility demonstration. 
Proceedings of the 21st International ACM SIGIR Conference, New York: ACM Press, pp. 
46–54.  

Zhang, T., Ramakrishnan, R., and Livny, M. (1997) BIRCH: an efficient data clustering 
method for very large databases. Data Mining and Knowledge Discovery, 1(2), pp. 141–182.  

List of Figures 
Chapter 1: Introduction 
Figure 1.1: A Portion of a Retail Transaction Data Set Displayed as a Binary Image, 
With 100 Individual Customers (Rows) and 40 Categories of Items (Columns).  
Chapter 2: Measurement and Data 
Figure 2.1: A Sample Correlation Matrix Plotted as a Pixel Image. White Corresponds 
to +1 and Black to -1. The Three Rightmost Columns Contain Values of -1, 0, and +1 
(Respectively) to Provide a Reference for Pixel Intensities. The Remaining 11 × 11 
Pixels Represent the 11 × 11 Correlation Matrix. The Data Come From a well-known 
Data Set in the Regression Research Literature, in Which Each Data Vector is a 
Suburb of Boston and Each Variable Represents a Certain General Characteristic of a 
Suburb. The Variable Names are (1) Per-Capita Crime Rate, (2) Proportion of Area 
Zoned for Large Residential Lots, (3) Proportion of Non-Retail Business Acres, (4) 
Nitric Oxide Concentration, (5) Average Number of Rooms Perdwelling, (6) Proportion 
of Pre-1940 Homes, (7) Distance to Retail Centers Index, (8) Accessibility to Highways 
Index, (9) Property Tax Rate, (10) Pupil-to-Teacher Ratio, and (11) Median Value of 
Owner-Occupied Homes.  
Figure 2.2: A Simple Nonlinear Relationship between Variable V1 and V2. (In These 
and Subsequent Figures V1 and V2 are on the X and Y Axes Respectively).  
Figure 2.3: The Data of Figure 2.2 after the Simple Transformation of  V2 to 1/V2.  
Figure 2.4: Another Simple Nonlinear Relationship. Here the Variance of V2 Increases 
as V1 Increases.  
Figure 2.5: The Data of Figure 2.4 after a Simple Square Root Transformation of V2. 
Now the Variance of V2 is Relatively Constant as V1 Increases.  
Figure 2.6: A Plot of 200 Points From Highly Positively Correlated Bivariate Data (From 
a Bivariate Normal Distribution), With a Single Easily Identifiable Outlier.  
Chapter 3: Visualizing and Exploring Data 
Figure 3.1: Histogram of the Number of Weeks of the Year a Particular Brand of Credit 
Card was Used.  
Figure 3.2: Histogram of Diastolic Blood Pressure for 768 Females of Pima Indian 
Descent.  
Figure 3.3: Kernel Estimate of the Weights (in Kg) of 856 Elderly Women.  
Figure 3.4: As Figure 3.3, but with More Smoothing.  
Figure 3.5: Boxplots on Four Different Variables From the Pima Indians Diabetes Data 
Set. For Each Variable, a Separate Boxplot is Produced for the Healthy Subjects 

(Labeled 1) and the Diabetic Subjects (Labeled 2). The Upper and Lower Boundaries 
of Each Box Represent the Upper and Lower Quartiles of the Data Respectively. The 
Horizontal Line within Each Box Represents the Median of the Data. The Whiskers 
Extend 1.5 Times the Interquartile Range From the End of Each Box. All Data Points 
Outside the Whiskers are Plotted Individually (Although Some Overplotting is Present, 
e.g., for Values of 0).  
Figure 3.6: A Standard Scatterplot for Two Banking Variables.  
Figure 3.7: A Scatterplot of 96,000 Cases, with Much Overprinting. Each Data Point 
Represents an Individual Applicant for a Loan. The Vertical Axis Shows the Age of the 
Applicant, and the Horizontal Axis Indicates the Day on Which the Application was 
Made.  
Figure 3.8: Overprinting Conceals the Actual Strength of the Correlation.  
Figure 3.9: A Contour Plot of the Data from Figure 3.7.  
Figure 3.10: A Plot of the Number of Credit Cards in Circulation in the United Kingdom, 
By Year.  
Figure 3.11: Patterns of Change over Time in the Number of Miles Flown by UK 
Airlines in the 1960s.  
Figure 3.12: Weight Changes Over Time in a Group of 10,000 School Children in the 
1930s. The Steplike Pattern in the Data Highlights a Problem with the Measurement 
Process.  
Figure 3.13: A scatterplot Matrix for the Computer CPU Data.  
Figure 3.14: A Trellis Plot for the Epileptic Seizures Data.  
Figure 3.15: An Example of a Star Plot.  
Figure 3.16: A Parallel Coordinates Plot for the Epileptic Seizure Data.  
Figure 3.17: Scree Plots for the Computer CPU Data Set. The Upper Plot Displays the 
Eigenvalues From the Correlation Matrix, and the Lower Plot is for the Covariance 
Matrix.  
Figure 3.18: Projection Onto the First Two Principal Components.  
Figure 3.19: A Multidimensional Scaling Plot of the Village Dialect Similarities Data.  
Chapter 4: Data Analysis and Uncertainty 
Figure 4.1: An Illustration of the Dual Roles of Probability and Statistics in Data 
Analysis. Probability Specifies How Observed Data Can be Generated From Models. 
Statistical Inference Allows Us to Infer Models From Observed Data.  
Figure 4.2: The Likelihood Function for Three Hypothetical Data Sets Under a Binomial 
Model: r = 7, n = 10 (Top), r = 70, n = 100 (Center), and r = 700, n = 1000 (Bottom).  
Figure 4.3: The Likelihood as a Function of ? for a Sample of 20 Data Points From a 
Normal Density with a True Mean of 0 and a Known Standard Deviation of 1: (a) a 
Histogram of 20 Data Points Generated From the True Model (top), (b) the Likelihood 
Function for ? (Center), and (c) the Log-Likelihood Function for ? (Bottom).  
Figure 4.4: The Likelihood Function for the Same Model as in Figure 4.3 but with 200 
Data Points: (a) a Histogram of the 200 Data Points Generated From the True Model 
(Top), (b) the Likelihood Function for ? (Center), and (c) the Log-Likelihood Function for 
? (Bottom).  
Figure 4.5: Means of Samples of Size 10(a), 100(b), and 1000(c) Drawn From a 
Population with a Mean of 0.5.  
Chapter 5: A Systematic Overview of Data Mining Algorithms 
Figure 5.1: A Scatterplot of Data Showing Color Intensity versus Alcohol Content for a 
Set of Wines. The Data Mining Task is to Classify the Wines into One of Three Classes 
(Three Different Cultivars), Each Shown with a Different Symbol in the Plot. The Data 
Originate From a 13-Dimensional Data Set in Which Each Variable Measures of a 
Particular Characteristic of a Specific Wine.  
Figure 5.2: A Classification Tree for the Data in Figure 5.1 in Which the Tests Consist 
of Thresholds (Shown Beside the Branches) on Variables at Each Internal Node and 
Leaves Contain Class Decisions. Note that One Leaf is Denoted ? to Illustrate that 
there is Considerable Uncertainty About the Class Labels of Data Points in this Region 
of the Space.  
Figure 5.3: The Decision Boundaries From the Classification Tree in Figure 5.2 are 
Superposed on the Original Data. Note the Axis-Parallel Nature of the Boundaries.  

Figure 5.4: A Hypothetical Plot of Misclassification Error Rates for Both Training and 
Test Data as a Function of Tree Complexity (e.g., Number of Leaves in the Tree).  
Figure 5.5: A Diagram of a Simple Multilayer Perceptron (or Neural Network) Model 
with Two Hidden Nodes (d1 = 2) and a Single Output Node (d2 = 1).  
Figure 5.6: An Example of the Type of Decision Boundaries that a Neural Network 
Model Would Produce for the Two-Dimensional Wine Data of Figure 5.2(a).  
Chapter 6: Models and Patterns 
Figure 6.1: (a) Fifty Data Points that are Simulated From a Third-Order Polynomial 
Equation with Additive Gaussian (Normal) Noise, (b) The Fit of the Model aX + b (Solid 
Line), (c) The Fit of the Model aX2 + bX + c (Solid Line). The Dotted Lines in (b) and (c) 
Indicate the True Model From Which the Data Points were Generated (See Text). The 
Model Parameters in Each Case were Estimated by Minimizing the Sum of Squared 
Errors between Model Predictions and Observed Data.  
Figure 6.2: An Example of a Piecewise Linear Fit to the Data of Figure 6.1 with k = 5 
Linear Segments.  
Figure 6.3: Nitrous Oxide (NOx) as a Function of Ethanol (E) using Kernel Regression 
with Triangular Kernels, With Bandwidt hs h = 0.5, 0.1, and 0.02, in Clockwise Order.  
Figure 6.4: An Example of Linear Decision Boundaries for the Two-Dimensional Wine 
Classification Data Set of Chapter 5 (See Figure 5.1).  
Figure 6.5: An Example of Piecewise Linear Decision Boundaries for the Two-
Dimensional Wine Classification Data Set of Chapter 5 (See Figure 5.1).  
Figure 6.6: From the Top: (a) Data Points Generated From a Mixture of Three Bivariate 
Normal Distributions (Appendix 1) with Equal Weights, (b) the Underlying Component 
Densities Plotted as Contours that are Located 3s From the Means, and (c) the 
Resulting Contours of the Overall Mixture Density Function.  
Figure 6.7: A Graphical Model Structure Corresponding to a First-Order Markov 
Assumption.  
Figure 6.8: A Plausible Graphical Model Structure for Two Variables Education and 
Baldness that are Conditionally Independent Given Age.  
Figure 6.9: The Graphical Model Structure for a Problem with Two Diseases that are 
Marginally (Unconditionally) Independent, A Single Intermediate Variable Z that Directly 
Depends on Both Diseases, And Six Symptom Variables that are Conditionally 
Independent Given Z.  
Figure 6.10: The First-Order Bayes Graphical Model Structure, With a Single Class Y 
and 6 Conditionally Independent Feature Variables X1, . . . , X6.  
Figure 6.11: A Graphical Model Structure Corresponding to a First-Order Hidden 
Markov Assumption.  
Chapter 7: Score Functions for Data Mining Algorithms 
Figure 7.1: Classification Accuracy of the Best Model Selected on a Validation Data Set 
From a Set of K Models, 1 = K = 100, Where Each Model is Making Random 
Predictions.  
Chapter 8: Search and Optimization Methods 
Figure 8.1: An Example of a Simple State-Space Involving Four Variables X1, X2, X3, 
X4. The Node on the Left is the Null Set—i.e., No Variables in the Model or Pattern.  
Figure 8.2: An Example of a Simple Search Tree for the State-Space of Figure 8.1.  
Figure 8.3: An Example of a Score Function S(?) of a Single Univariate Parameter ? 
with Both a Global Minimum and a Local Minimum.  
Figure 8.4: An Example of a Situation in Which We Minimize a Score Function of Two 
Variables and the Shape of the Score Function is a Parabolic ""Bowl"" with the Minimum 
in the Center. Gradient Descent Does Not Point Directly to the Minimum but Instead 
Tends to Point ""Across"" the Bowl (Solid Lines on the Left), Leading to a Series of 
Indirect Steps before the Minimum is Reached.  
Chapter 9: Descriptive Modeling 
Figure 9.1: Illustration of the Density Contours for a Two-Dimensional Normal Density 
Function, With Mean [3, 3] and Covariance Matrix 
. Also Shown are 100 Data 
Points Simulated From this Density.  
Figure 9.2: The Log-Likelihood of the Red-Blood Cell Data Under a Two-Component 
Normal Mixture Model (See Figure 9.11) as a Function of Iteration Number.  

Figure 9.3: Density Estimates for the Variable Ethanol (E) using a Histogram (Top Left) 
and Gaussian Kernel Estimates with Three Different Bandwidths: h = 0.5 (Top Right), h 
= 0.25 (Lower Left), and h = 0.1 (Lower Right).  
Figure 9.4: Antenna Data. On Top the Data Points are Shown without Class Labels, 
and on the Bottom Different Symbols are Used for the Three Known Classes (Dots are 
Normal, Circles are Tachometer Noise, and x's are Short Circuit.)  
Figure 9.5: Example of Running the K-Means Algorithm on the Two-Dimensional 
Antenna Data. The Plots Show the Locations of the Means of the Clusters (Large 
Circles) at Various Iterations of the K-Means Algorithm, as well as the Classification of 
the Data Points at Each Iteration According to the Closest Mean (Dots, Circles, and xs 
for Each of the Three Clusters).  
Figure 9.6: A Summary of the Trajectories of the Three Cluster Means During the K-
Means Iterations of Figure 9.5.  
Figure 9.7: Duration of Eruptions Versus Waiting Time between Eruptions (in Minutes) 
for the Old Faithful Geyser in Yellowstone Park.  
Figure 9.8: Dendrogram Resulting From Clustering of Data in Figure 9.7 using the 
Criterion of Merging Clusters that Leads to the Smallest Increase in the Total Sum of 
Squared Errors.  
Figure 9.9: Dendrogram of the Single Link Method Applied to the Data in Figure 9.7.  
Figure 9.10: Red Blood Cell Measurements (Mean Volume and Mean Hemoglobin 
Concentration) From 182 Individuals Showing the Separation of the Individuals into two 
Groups: Healthy (Circles) and Iron Deficient Anemia (Crosses).  
Figure 9.11: Example of Running the EM Algorithm on the Red Blood Cell 
Measurements of Figure 9.10. The Plots (Running Top to Bottom, Left First, then Right) 
Show the 3s Covariance Ellipses and Means of the Fitted Components at Various 
Stages of the EM Algorithm.  
Figure 9.12: Log-Likelihood and BIC Score as a Function of the Number of Normal 
Components Fitted to the Red Blood Cell Data of Figure 9.11.  
Chapter 10: Predictive Modeling for Classification 
Figure 10.1: A Simple Example Illustrating Posterior Class Probabilities for a Two-
Class One-Dimensional Classification Problem.  
Figure 10.2: Posterior Probability Contours for p(c1|x) Where c1 is the Label for the 
Healthy Class for the Red Blood Cell Data Discussed in Chapter 9. The Heavy Line is 
the Decision Boundary (p(c1|x) = p(c2|x) = 0.5) and the Other Two Contour Lines 
Correspond to p(c1|x) = 0.01 and p(c1|x) = 0.99. Also Plotted for Reference are the 
Original Data Points and the Fitted Covariance Ellipses for Each Class (Plotted as 
Dotted Lines).  
Figure 10.3: Posterior Probability Contours for p(c1|x) Where c1 is the Label for the 
Diabetic Class for the Pima Indians Data of Chapter 3. The Heavy Line is the Decision 
Boundary (p(c1|x) = p(c2|x) = 0.5) and the Other Two Contour Lines Correspond to 
p(c1|x) = 0.1 and p(c1|x) = 0.9. The Fitted Covariance Ellipses for Each Class are 
Plotted as Dotted Lines.  
Figure 10.4: Decision Boundary Produced by the Fisher Linear Discriminant Applied to 
the Red Blood Cell Data From Chapter 9, Where the Crosses are the Healthy Class 
and the Circles Correspond to Iron Deficient Anemia.  
Figure 10.5: Decision Boundary for a Decision Tree for the Red Blood Cell Data from 
Chapter 9, Composed of ""Axis-Parallel"" Linear Segments (Contrast with the Simpler 
Boundaries in Figure 10.4).  
Figure 10.6: An Illustration of the Potential Pitfalls of using Principal Component 
Analysis as a Preprocessor for Classification. This is an Artificial Two-Dimensional 
Classification Problem, With Data from Each Class Plotted with Different Symbols. The 
First Principal Component Direction (Which Would be the First Candidate Direction on 
Which to Project the Data If this were Actually a High-Dimensional Problem) is in Fact 
Almost Completely Orthogonal to the Best Linear Projection for Discrimination as 
Determined by Fisher's Linear Discriminant Technique.  
Chapter 11: Predictive Modeling for Regression 
Figure 11.1: Expired Ventilation Plotted Against Oxygen Uptake in a Series of Trials, 
with Fitted Straight Line.  
Figure 11.2: The Data From Figure 11.1 with a Model that Includes a Term in x2.  

Figure 11.3: A Plot of Record Time (in Minutes) Against Distance (in Miles) for 35 
Scottish Hill Races From 1984.  
Figure 11.4: Temperature (Degrees F) Against Latitude (Degrees N) for 56 Cities in the 
United States.  
Figure 11.5: A Plot of Tensile Strength of Paper against the Percentage of Hardwood in 
the Pulp.  
Figure 11.6: Number of O-Rings Damaged (Vertical Axis) against Temperature on Day 
of Flight, (a) Data Examined before the Flight, and (b) The Complete Data.  
Figure 11.7: The Transformation Function of Log(dose) in the Model for Predicting 
Time for Blood Pressure to Revert to Normal.  
Figure 11.8: The Transformation Function of Blood Pressure During Administration in 
the Model for Predicting Time for Blood Pressure to Revert to Normal.  
Chapter 12: Data Organization and Databases 
Figure 12.1: Representing Market Basket Data as a Table with an Attribute for Each 
Product.  
Figure 12.2: A More Realistic Representation of Market Basket Data.  
Figure 12.3: Representing Prices of Products.  
Figure 12.4: Representing the Hierarchy of Products as a Table.  
Figure 12.5: The Concept of Data Mining Algorithms Which Operate on an 
Approximate Version of the Full Data Set.  
Chapter 13: Finding Patterns and Rules 
Figure 13.1: An Artificial Example of Basket Data.  
Figure 13.2: Episodes a, ß, and ?.  
Chapter 14: Retrieval by Content 
Figure 14.1: A Simple (Synthetic) Example of Precision-Recall Curves for Three 
Hypothetical Query Algorithms. Algorithm A Has the Highest Precision for Low Recall 
Values, While Algorithm B Has the Highest Precision for High Recall Values. Algorithm 
C is Universally Worse Than A or B, But We Cannot Make a Clear Distinction between 
A or B Unless (For Example) We were to Operate at a Specific Recall Value. The 
Actual Recall-Precision Numbers Shown are Fairly Typical of Current Text-Retrieval 
Algorithms; e.g., the Ballpark Figures of 50% Precision at 50% Recall are Not 
Uncommon Across Different Text-Retrieval Applications.  
Figure 14.2: Pairwise Document Distances for the Toy Document-Term Matrix in the 
Text, For Euclidean Distance (Top) and Cosine Distance (Bottom). Brighter Squares 
are More Similar and Darker Squares are Less Similar, According to the Relevant 
Distance. For the Euclidean Distance, White Corresponds to Zero (e.g., On the 
Diagonals) and Black is the Maximum Distance between any Two Documents. For the 
Cosine Distance Brighter Pixels Correspond to Larger Cosine Values (Closer Angles) 
and Darker Pixels Correspond to Smaller Cosine Values (Larger Angles).  
Figure 14.3: Projected Locations of the 10 Documents (From Table 14.2) in the Two 
Dimensional Plane Spanned by the First Two Principal Components of the Document-
Term Matrix M.  

 

List of Tables 
Chapter 1: Introduction 
Table 1.1: Examples of Data in Public Use Microdata Sample Data Sets.  
Chapter 2: Measurement and Data 
Table 2.1: A Cross-Classification of Two Binary Variables.  
Chapter 3: Visualizing and Exploring Data 
Table 3.1: Numerical Codes, Names, And Counties for the 25 Villages with Dialect 
Similarities Displayed in Figure 3.19.  
Chapter 5: A Systematic Overview of Data Mining Algorithms 
Table 5.1: Three Well-Known Data Mining Algorithms Broken Down in Terms of their 
Algorithm Components.  

Chapter 6: Models and Patterns 
Table 6.1: A Simple Contingency Table for Two-Dimensional Categorical Data for a 
Hypothetical Data Set of Medical Patients Who Have been Diagnosed for Dementia.  
Chapter 11: Predictive Modeling for Regression 
Table 11.1: The Analysis of Variance Decomposition Table for a Regression.  
Table 11.2: The Analysis of Variance Decomposition Table for Model Building.  
Table 11.3: Analysis of Deviance Table.  
Chapter 14: Retrieval by Content 
Table 14.1: A Schematic of the Four Possible Outcomes in a Retrieval Experiment 
Where Documents are Labeled as Being ""Relevant"" or ""Not Relevant"" (Relative to a 
Particular Query Q). The Columns Correspond to Truth and Rows Correspond to the 
Algorithm's Decisions on the Documents. TP, FP, FN, TN Refer to True Positive, False 
Positive, False Negative, And True Negative Respectively, Where Positive/Negative 
Refers to the Relevant/Nonrelevant Classification Provided by the Algorithm. A Perfect 
Retrieval Algorithm Would Produce a Diagonal Matrix with FP = FN = 0. This Form of 
Reporting Classification Results is Sometimes Referred to as a Confusion Matrix.  
Table 14.2: A Toy Document-Term Matrix for 10 Documents and 6 Terms. Each ijth 
Entry Contains the Number of Times that Term j Appears in Document i.  
Table 14.3: TF-IDF Document -Term Matrix Resulting From Table 14.2  
Table 14.4: Distances Resulting From a Query Containing the Terms Database and 
Index, i.e., Q = (1, 0, 1, 0, 0, 0), for the Document-Term Matrix of Table 14.2, Using the 
Cosine Distance Measure. Using the TF Matrix, Document d5 is Closest; For the TF -
IDF Matrix, d2 is Closest.  
List of Examples 
Chapter 1: Introduction 
Example 1.1  
Example 1.2  
Example 1.3  
Example 1.4  
Chapter 2: Measurement and Data 
Example 2.1  
Example 2.2  
Example 2.3  
Example 2.4  
Chapter 3: Visualizing and Exploring Data 
Example 3.1  
Chapter 4: Data Analysis and Uncertainty 
Example 4.1  
Example 4.2  
Example 4.3  
Example 4.4  
Example 4.5  
Example 4.6  
Example 4.7  
Example 4.8  
Example 4.9  
Example 4.10  
Example 4.11  
Example 4.12  
Example 4.13  
Example 4.14  
Chapter 6: Models and Patterns 
Example 6.1  
Example 6.2  
Example 6.3  
Example 6.4  

Chapter 7: Score Functions for Data Mining Algorithms 
Example 7.1  
Chapter 8: Search and Optimization Methods 
Example 8.1  
Example 8.2  
Chapter 9: Descriptive Modeling 
Example 9.1  
Example 9.2  
Example 9.3  
Example 9.4  
Example 9.5  
Chapter 10: Predictive Modeling for Classification 
Example 10.1  
Example 10.2  
Example 10.3  
Chapter 11: Predictive Modeling for Regression 
Example 11.1  
Example 11.2  
Example 11.3  
Example 11.4  
Chapter 12: Data Organization and Databases 
Example 12.1  
Example 12.2  
Example 12.3  
Example 12.4  
Example 12.5  
Example 12.6  
Chapter 13: Finding Patterns and Rules 
Example 13.1  

 
 

",False,2001.0,{},False,False,book,False,G9DSVLIY,[],self.user,False,False,False,False,,,Principles of data mining,G9DSVLIY,False,False
N33NGTQ4,,Parsing Error,False,2008.0,{},False,False,book,False,N33NGTQ4,,self.user,False,False,False,False,,,Information retrieval: a health and biomedical perspective,,False,False
XAV8JAHQ,,Parsing Error,False,2013.0,{},False,False,journalArticle,False,XAV8JAHQ,,self.user,False,False,False,False,,,A survey of visualization pipelines,,False,False
CVT48PHE,,Parsing Error,False,2006.0,{},False,False,conferencePaper,False,CVT48PHE,,self.user,False,False,False,False,,,Evaluating the effectiveness of tree visualization systems for knowledge discovery.,,False,False
DK8EQKH6,,Parsing Error,False,2011.0,{},False,False,conferencePaper,False,DK8EQKH6,,self.user,False,False,False,False,,,Baobabview: Interactive construction and analysis of decision trees,,False,False
T52BVIFW,,Parsing Error,False,2003.0,{},False,False,journalArticle,False,T52BVIFW,,self.user,False,False,False,False,,,From visual data exploration to visual data mining: a survey,,False,False
RCG6Z64X,,Parsing Error,False,2016.0,{},False,False,journalArticle,False,RCG6Z64X,,self.user,False,False,False,False,,,A survey of visualization-driven interactive data mining approaches,,False,False
SS6MK6FS,,Parsing Error,False,0.0,{},False,False,journalArticle,False,SS6MK6FS,,self.user,False,False,False,False,,,Bayesian Visual Analytics: Interactive Visualization for High-Dimensional Data,,False,False
WEPB2HQI,,Parsing Error,False,2005.0,{},False,False,bookSection,False,WEPB2HQI,,self.user,False,False,False,False,,,Beyond tools: Visual support for the entire process of GIScience,,False,False
BMGLXJXK,,Parsing Error,False,2000.0,{},False,False,journalArticle,False,BMGLXJXK,,self.user,False,False,False,False,,,Data preprocessing in data mining,,False,False
VGKWNWFK,,Parsing Error,False,2000.0,{},False,False,conferencePaper,False,VGKWNWFK,,self.user,False,False,False,False,,,A taxonomy of visualization techniques using the data state reference model,,False,False
DGS45FM2,,Parsing Error,False,0.0,{},False,False,journalArticle,False,DGS45FM2,,self.user,False,False,False,False,,,The Sensemaking Process and Leverage Points for Analyst Technology as Identified Through Cognitive Task Analysis,,False,False
SM2DZEBK,,Parsing Error,False,2010.0,{},False,False,conferencePaper,False,SM2DZEBK,,self.user,False,False,False,False,,,Effective Visualization Techniques for Data Discovery and Analysis,,False,False
6DQQKXGE,,Parsing Error,False,2002.0,{},False,False,book,False,6DQQKXGE,,self.user,False,False,False,False,,,Information visualization in data mining and knowledge discovery,,False,False
URV9JVVW,,Parsing Error,False,2005.0,{},False,False,report,False,URV9JVVW,,self.user,False,False,False,False,,,Illuminating the path: The research and development agenda for visual analytics,,False,False
3UPBC7LI,,Parsing Error,False,2012.0,{},False,False,book,False,3UPBC7LI,,self.user,False,False,False,False,,,Information visualization: perception for design,,False,False
CZV32UCQ,,Parsing Error,False,1973.0,{},False,False,journalArticle,False,CZV32UCQ,,self.user,False,False,False,False,,,Hume and Julius Caesar,,False,False
A5ZWZVDU,,Parsing Error,False,1985.0,{},False,False,journalArticle,False,A5ZWZVDU,,self.user,False,False,False,False,,,Object permanence in five-month-old infants,,False,False
5KXP886W,,Parsing Error,False,1993.0,{},False,False,book,False,5KXP886W,,self.user,False,False,False,False,,,Visualizing data,,False,False
CXTS6LPH,,Parsing Error,False,2016.0,{},False,False,book,False,CXTS6LPH,,self.user,False,False,False,False,,,Introduction to text visualization,,False,False
JXACL8Y6,,Parsing Error,False,1983.0,{},False,False,book,False,JXACL8Y6,,self.user,False,False,False,False,,,"Semiology of graphics: diagrams, networks, maps",,False,False
LKZZ6LG3,,Parsing Error,False,2016.0,{},False,False,journalArticle,False,LKZZ6LG3,,self.user,False,False,False,False,,,A survey of visual analytic pipelines,,False,False
